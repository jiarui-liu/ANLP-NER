<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>North American Chapter of the Association for Computational Linguistics (2022) - ACL Anthology</title><meta name=generator content="Hugo 0.118.2"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-light bg-light bg-gradient-light shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL Anthology</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/contrib/>Submissions<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-primary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>North American Chapter of the Association for Computational Linguistics (2022)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Volumes</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2022naacl-main>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a>
<span class="badge badge-info align-middle ml-1">443&nbsp;papers</span></li><li><a class=align-middle href=#2022naacl-srw>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop</a>
<span class="badge badge-info align-middle ml-1">37&nbsp;papers</span></li><li><a class=align-middle href=#2022naacl-demo>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#2022naacl-tutorials>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2022naacl-industry>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track</a>
<span class="badge badge-info align-middle ml-1">41&nbsp;papers</span></li><li><a class=align-middle href=#2022findings-naacl>Findings of the Association for Computational Linguistics: NAACL 2022</a>
<span class="badge badge-info align-middle ml-1">210&nbsp;papers</span></li><li><a class=align-middle href=#2022autosimtrans-1>Proceedings of the Third Workshop on Automatic Simultaneous Translation</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#2022bea-1>Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022)</a>
<span class="badge badge-info align-middle ml-1">32&nbsp;papers</span></li><li><a class=align-middle href=#2022clinicalnlp-1>Proceedings of the 4th Clinical Natural Language Processing Workshop</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#2022clpsych-1>Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology</a>
<span class="badge badge-info align-middle ml-1">26&nbsp;papers</span></li><li><a class=align-middle href=#2022dadc-1>Proceedings of the First Workshop on Dynamic Adversarial Data Collection</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2022deeplo-1>Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">24&nbsp;papers</span></li><li><a class=align-middle href=#2022distcurate-1>Proceedings of the Workshop on Dimensions of Meaning: Distributional and Curated Semantics (DistCurate 2022)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2022dlg4nlp-1>Proceedings of the 2nd Workshop on Deep Learning on Graphs for Natural Language Processing (DLG4NLP 2022)</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2022emoji-1>Proceedings of the Fifth International Workshop on Emoji Understanding and Applications in Social Media</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li><li><a class=align-middle href=#2022gebnlp-1>Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)</a>
<span class="badge badge-info align-middle ml-1">29&nbsp;papers</span></li><li><a class=align-middle href=#2022hcinlp-1>Proceedings of the Second Workshop on Bridging Human--Computer Interaction and Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2022mia-1>Proceedings of the Workshop on Multilingual Information Access (MIA)</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#2022privatenlp-1>Proceedings of the Fourth Workshop on Privacy in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2022semeval-1>Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</a>
<span class="badge badge-info align-middle ml-1">234&nbsp;papers</span></li><li><a class=align-middle href=#2022sigmorphon-1>Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a>
<span class="badge badge-info align-middle ml-1">26&nbsp;papers</span></li><li><a class=align-middle href=#2022sigtyp-1>Proceedings of the 4th Workshop on Research in Computational Linguistic Typology and Multilingual NLP</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#2022socialnlp-1>Proceedings of the Tenth International Workshop on Natural Language Processing for Social Media</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2022starsem-1>Proceedings of the 11th Joint Conference on Lexical and Computational Semantics</a>
<span class="badge badge-info align-middle ml-1">31&nbsp;papers</span></li><li><a class=align-middle href=#2022suki-1>Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI)</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2022trustnlp-1>Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022)</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2022unimplicit-1>Proceedings of the Second Workshop on Understanding Implicit and Underspecified Language</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2022wnu-1>Proceedings of the 4th Workshop of Narrative Understanding (WNU2022)</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2022woah-1>Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)</a>
<span class="badge badge-info align-middle ml-1">25&nbsp;papers</span></li><li><a class=align-middle href=#2022wordplay-1>Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022)</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li></ul></div></div><button class="btn btn-sm btn-info d-block mb-3" id=toggle-all-abstracts data-toggle-state=hide disabled>
<span class=on-toggle-state-hide>Show all abstracts<i class="ml-2 fas fa-angle-double-down"></i></span><span class=on-toggle-state-show>Hide all abstracts<i class="ml-2 fas fa-angle-double-up"></i></span></button><div id=2022naacl-main><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.naacl-main.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.naacl-main/>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-main.0/>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></strong><br><a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Vladimir Meza Ruiz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.1/>Social Norms Guide Reference Resolution</a></strong><br><a href=/people/m/mitchell-abrams/>Mitchell Abrams</a>
|
<a href=/people/m/matthias-scheutz/>Matthias Scheutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--1><div class="card-body p-3 small">Humans use natural language, vision, and context to resolve referents in their environment. While some situated reference resolution is trivial, ambiguous cases arise when the language is underspecified or there are multiple candidate referents. This study investigates howpragmatic modulators external to the linguistic content are critical for the correct interpretation of referents in these scenarios. Inparticular, we demonstrate in a human subjects experiment how the social norms applicable in the given context influence theinterpretation of referring expressions. Additionally, we highlight how current coreference tools in natural language processing fail tohandle these ambiguous cases. We also briefly discuss the implications of this work for assistive robots which will routinely need to resolve referents in their environment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.2.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.2/>Learning Natural Language Generation with Truncated Reinforcement Learning</a></strong><br><a href=/people/a/alice-martin/>Alice Martin</a>
|
<a href=/people/g/guillaume-quispe/>Guillaume Quispe</a>
|
<a href=/people/c/charles-ollion/>Charles Ollion</a>
|
<a href=/people/s/sylvain-le-corff/>Sylvain Le Corff</a>
|
<a href=/people/f/florian-strub/>Florian Strub</a>
|
<a href=/people/o/olivier-pietquin/>Olivier Pietquin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--2><div class="card-body p-3 small">This paper introduces TRUncated ReinForcement Learning for Language (TrufLL), an original approach to train conditional languagemodels without a supervised learning phase, by only using reinforcement learning (RL). As RL methods unsuccessfully scale to large action spaces, we dynamically truncate the vocabulary space using a generic language model. TrufLL thus enables to train a language agent by solely interacting with its environment without any task-specific prior knowledge; it is only guided with a task-agnostic language model. Interestingly, this approach avoids the dependency to labelled datasets and inherently reduces pretrained policy flaws such as language or exposure biases. We evaluate TrufLL on two visual question generation tasks, for which we report positive results over performance and language metrics, which we then corroborate with a human evaluation. To our knowledge, it is the first approach that successfully learns a language generation policy without pre-training, using only reinforcement learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.3/>Language Model Augmented Monotonic Attention for Simultaneous Translation</a></strong><br><a href=/people/s/sathish-reddy-indurthi/>Sathish Reddy Indurthi</a>
|
<a href=/people/m/mohd-abbas-zaidi/>Mohd Abbas Zaidi</a>
|
<a href=/people/b/beomseok-lee/>Beomseok Lee</a>
|
<a href=/people/n/nikhil-kumar-lakumarapu/>Nikhil Kumar Lakumarapu</a>
|
<a href=/people/s/sangha-kim/>Sangha Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--3><div class="card-body p-3 small">The state-of-the-art adaptive policies for Simultaneous Neural Machine Translation (SNMT) use monotonic attention to perform read/write decisions based on the partial source and target sequences. The lack of sufficient information might cause the monotonic attention to take poor read/write decisions, which in turn negatively affects the performance of the SNMT model. On the other hand, human translators make better read/write decisions since they can anticipate the immediate future words using linguistic information and domain knowledge. In this work, we propose a framework to aid monotonic attention with an external language model to improve its decisions. Experiments on MuST-C English-German and English-French speech-to-text translation tasks show the future information from the language model improves the state-of-the-art monotonic multi-head attention model further.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.4/>What Makes a Good and Useful Summary? <span class=acl-fixed-case>I</span>ncorporating Users in Automatic Summarization Research</a></strong><br><a href=/people/m/maartje-ter-hoeve/>Maartje Ter Hoeve</a>
|
<a href=/people/j/julia-kiseleva/>Julia Kiseleva</a>
|
<a href=/people/m/maarten-rijke/>Maarten Rijke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--4><div class="card-body p-3 small">Automatic text summarization has enjoyed great progress over the years and is used in numerous applications, impacting the lives of many. Despite this development, there is little research that meaningfully investigates how the current research focus in automatic summarization aligns with users’ needs. To bridge this gap, we propose a survey methodology that can be used to investigate the needs of users of automatically generated summaries. Importantly, these needs are dependent on the target group. Hence, we design our survey in such a way that it can be easily adjusted to investigate different user groups. In this work we focus on university students, who make extensive use of summaries during their studies. We find that the current research directions of the automatic summarization community do not fully align with students’ needs. Motivated by our findings, we present ways to mitigate this mismatch in future research on automatic summarization: we propose research directions that impact the design, the development and the evaluation of automatically generated summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.5/><span class=acl-fixed-case>E</span>r<span class=acl-fixed-case>AC</span>on<span class=acl-fixed-case>D</span>: Error Annotated Conversational Dialog Dataset for Grammatical Error Correction</a></strong><br><a href=/people/x/xun-yuan/>Xun Yuan</a>
|
<a href=/people/d/derek-pham/>Derek Pham</a>
|
<a href=/people/s/sam-davidson/>Sam Davidson</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--5><div class="card-body p-3 small">Currently available grammatical error correction (GEC) datasets are compiled using essays or other long-form text written by language learners, limiting the applicability of these datasets to other domains such as informal writing and conversational dialog. In this paper, we present a novel GEC dataset consisting of parallel original and corrected utterances drawn from open-domain chatbot conversations; this dataset is, to our knowledge, the first GEC dataset targeted to a human-machine conversational setting. We also present a detailed annotation scheme which ranks errors by perceived impact on comprehension, making our dataset more representative of real-world language learning applications. To demonstrate the utility of the dataset, we use our annotated data to fine-tune a state-of-the-art GEC model. Experimental results show the effectiveness of our data in improving GEC model performance in a conversational scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.6/>Semantic Diversity in Dialogue with Natural Language Inference</a></strong><br><a href=/people/k/katherine-stasaski/>Katherine Stasaski</a>
|
<a href=/people/m/marti-a-hearst/>Marti Hearst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--6><div class="card-body p-3 small">Generating diverse, interesting responses to chitchat conversations is a problem for neural conversational agents. This paper makes two substantial contributions to improving diversity in dialogue generation. First, we propose a novel metric which uses Natural Language Inference (NLI) to measure the semantic diversity of a set of model responses for a conversation. We evaluate this metric using an established framework (Tevet and Berant, 2021) and find strong evidence indicating NLI Diversity is correlated with semantic diversity. Specifically, we show that the contradiction relation is more useful than the neutral relation for measuring this diversity and that incorporating the NLI model’s confidence achieves state-of-the-art results. Second, we demonstrate how to iteratively improve the semantic diversity of a sampled set of responses via a new generation procedure called Diversity Threshold Generation, which results in an average 137% increase in NLI Diversity compared to standard generation procedures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.7/><span class=acl-fixed-case>LEA</span>: Meta Knowledge-Driven Self-Attentive Document Embedding for Few-Shot Text Classification</a></strong><br><a href=/people/s/s-k-hong/>S. K. Hong</a>
|
<a href=/people/t/tae-young-jang/>Tae Young Jang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--7><div class="card-body p-3 small">Text classification has achieved great success with the prosperity of deep learning and pre-trained language models. However, we often encounter labeled data deficiency problems in real-world text-classification tasks. To overcome such challenging scenarios, interest in few-shot learning has increased, whereas most few-shot text classification studies suffer from a difficulty of utilizing pre-trained language models. In the study, we propose a novel learning method for learning how to attend, called LEA, through which meta-level attention aspects are derived based on our meta-learning strategy. This enables the generation of task-specific document embedding with leveraging pre-trained language models even though a few labeled data instances are given. We evaluate our proposed learning method on five benchmark datasets. The results show that the novel method robustly provides the competitive performance compared to recent few-shot learning methods for all the datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.8/>Enhancing Self-Attention with Knowledge-Assisted Attention Maps</a></strong><br><a href=/people/j/jiangang-bai/>Jiangang Bai</a>
|
<a href=/people/y/yujing-wang/>Yujing Wang</a>
|
<a href=/people/h/hong-sun/>Hong Sun</a>
|
<a href=/people/r/ruonan-wu/>Ruonan Wu</a>
|
<a href=/people/t/tianmeng-yang/>Tianmeng Yang</a>
|
<a href=/people/p/pengfei-tang/>Pengfei Tang</a>
|
<a href=/people/d/defu-cao/>Defu Cao</a>
|
<a href=/people/m/mingliang-zhang1/>Mingliang Zhang1</a>
|
<a href=/people/y/yunhai-tong/>Yunhai Tong</a>
|
<a href=/people/y/yaming-yang/>Yaming Yang</a>
|
<a href=/people/j/jing-bai/>Jing Bai</a>
|
<a href=/people/r/ruofei-zhang/>Ruofei Zhang</a>
|
<a href=/people/h/hao-sun/>Hao Sun</a>
|
<a href=/people/w/wei-shen/>Wei Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--8><div class="card-body p-3 small">Large-scale pre-trained language models have attracted extensive attentions in the research community and shown promising results on various tasks of natural language processing. However, the attention maps, which record the attention scores between tokens in self-attention mechanism, are sometimes ineffective as they are learned implicitly without the guidance of explicit semantic knowledge. Thus, we aim to infuse explicit external knowledge into pre-trained language models to further boost their performance. Existing works of knowledge infusion largely depend on multi-task learning frameworks, which are inefficient and require large-scale re-training when new knowledge is considered. In this paper, we propose a novel and generic solution, KAM-BERT, which directly incorporates knowledge-generated attention maps into the self-attention mechanism. It requires only a few extra parameters and supports efficient fine-tuning once new knowledge is added. KAM-BERT achieves consistent improvements on various academic datasets for natural language understanding. It also outperforms other state-of-the-art methods which conduct knowledge infusion into transformer-based architectures. Moreover, we apply our model to an industry-scale ad relevance application and show its advantages in the real-world scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.9/>Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks</a></strong><br><a href=/people/a/anton-chernyavskiy/>Anton Chernyavskiy</a>
|
<a href=/people/d/dmitry-ilvovsky/>Dmitry Ilvovsky</a>
|
<a href=/people/p/pavel-kalinin/>Pavel Kalinin</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--9><div class="card-body p-3 small">The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP).Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks. We introduce and study a number of variations in the calculation of the loss as well as in the overall training procedure; in particular, we find that a special data shuffling can be quite important. Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression. Finally, we offer detailed analysis and discussion, which should be useful for researchers aiming to explore the utility of contrastive loss in NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable mention for contributions to resources"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.10/><span class=acl-fixed-case>N</span>ews<span class=acl-fixed-case>E</span>dits: A News Article Revision Dataset and a Novel Document-Level Reasoning Challenge</a></strong><br><a href=/people/a/alexander-spangher/>Alexander Spangher</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--10><div class="card-body p-3 small">News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution, we present the first publicly available dataset of news revision histories, NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 English- and French-language newspaper sources based in three countries, spanning 15 years of coverage (2006-2021).We define article-level edit actions: Addition, Deletion, Edit and Refactor, and develop a high-accuracy extraction algorithm to identify these actions. To underscore the factual nature of many edit actions, we conduct analyses showing that added and deleted sentences are more likely to contain updating events, main content and quotes than unchanged sentences. Finally, to explore whether edit actions are predictable, we introduce three novel tasks aimed at predicting actions performed during version updates. We show that these tasks are possible for expert humans but are challenging for large NLP models. We hope this can spur research in narrative framing and help provide predictive tools for journalists chasing breaking news.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.11.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.11/>Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia</a></strong><br><a href=/people/s/samee-ibraheem/>Samee Ibraheem</a>
|
<a href=/people/g/gaoyue-zhou/>Gaoyue Zhou</a>
|
<a href=/people/j/john-denero/>John DeNero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--11><div class="card-body p-3 small">While neural networks demonstrate a remarkable ability to model linguistic content, capturing contextual information related to a speaker’s conversational role is an open area of research. In this work, we analyze the effect of speaker role on language use through the game of Mafia, in which participants are assigned either an honest or a deceptive role. In addition to building a framework to collect a dataset of Mafia game records, we demonstrate that there are differences in the language produced by players with different roles. We confirm that classification models are able to rank deceptive players as more suspicious than honest ones based only on their use of language. Furthermore, we show that training models on two auxiliary tasks outperforms a standard BERT-based text classification approach. We also present methods for using our trained models to identify features that distinguish between player roles, which could be used to assist players during the Mafia game.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.12/><span class=acl-fixed-case>SUBS</span>: Subtree Substitution for Compositional Semantic Parsing</a></strong><br><a href=/people/j/jingfeng-yang/>Jingfeng Yang</a>
|
<a href=/people/l/le-zhang/>Le Zhang</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--12><div class="card-body p-3 small">Although sequence-to-sequence models often achieve good performance in semantic parsing for i.i.d. data, their performance is still inferior in compositional generalization. Several data augmentation methods have been proposed to alleviate this problem. However, prior work only leveraged superficial grammar or rules for data augmentation, which resulted in limited improvement. We propose to use subtree substitution for compositional data augmentation, where we consider subtrees with similar semantic functions as exchangeable. Our experiments showed that such augmented data led to significantly better performance on Scan and GeoQuery, and reached new SOTA on compositional split of GeoQuery.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.13.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.13/>Two Contrasting Data Annotation Paradigms for Subjective <span class=acl-fixed-case>NLP</span> Tasks</a></strong><br><a href=/people/p/paul-rottger/>Paul Rottger</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/j/janet-pierrehumbert/>Janet Pierrehumbert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--13><div class="card-body p-3 small">Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss benefits and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.14.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.14.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.14/>Do Deep Neural Nets Display Human-like Attention in Short Answer Scoring?</a></strong><br><a href=/people/z/zijie-zeng/>Zijie Zeng</a>
|
<a href=/people/x/xinyu-li/>Xinyu Li</a>
|
<a href=/people/d/dragan-gasevic/>Dragan Gasevic</a>
|
<a href=/people/g/guanliang-chen/>Guanliang Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--14><div class="card-body p-3 small">Deep Learning (DL) techniques have been increasingly adopted for Automatic Text Scoring in education. However, these techniques often suffer from their inabilities to explain and justify how a prediction is made, which, unavoidably, decreases their trustworthiness and hinders educators from embracing them in practice. This study aimed to investigate whether (and to what extent) DL-based graders align with human graders regarding the important words they identify when marking short answer questions. To this end, we first conducted a user study to ask human graders to manually annotate important words in assessing answer quality and then measured the overlap between these human-annotated words and those identified by DL-based graders (i.e., those receiving large attention weights). Furthermore, we ran a randomized controlled experiment to explore the impact of highlighting important words detected by DL-based graders on human grading. The results showed that: (i) DL-based graders, to a certain degree, displayed alignment with human graders no matter whether DL-based graders and human graders agreed on the quality of an answer; and (ii) it is possible to facilitate human grading by highlighting those DL-detected important words, though further investigations are necessary to understand how human graders exploit such highlighted words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.15.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.15/>Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation</a></strong><br><a href=/people/y/yu-li/>Yu Li</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/y/yi-mao/>Yi Mao</a>
|
<a href=/people/l/lars-liden/>Lars Liden</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--15><div class="card-body p-3 small">Knowledge-grounded dialogue systems are challenging to build due to the lack of training data and heterogeneous knowledge sources. Existing systems perform poorly on unseen topics due to limited topics covered in the training data. In addition, it is challenging to generalize to the domains that require different types of knowledge sources. To address the above challenges, we present PLUG, a language model that homogenizes different knowledge sources to a unified knowledge representation for knowledge-grounded dialogue generation tasks. We first retrieve relevant information from heterogeneous knowledge sources (e.g., wiki, dictionary, or knowledge graph); Then the retrieved knowledge is transformed into text and concatenated with dialogue history to feed into the language model for generating responses. PLUG is pre-trained on a large-scale knowledge-grounded dialogue corpus. The empirical evaluation on two benchmarks shows that PLUG generalizes well across different knowledge-grounded dialogue tasks. It achieves comparable performance with state-of-the-art methods in the fully-supervised setting and significantly outperforms other approaches in zero-shot and few-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-main.16/><span class=acl-fixed-case>CERES</span>: Pretraining of Graph-Conditioned Transformer for Semi-Structured Session Data</a></strong><br><a href=/people/r/rui-feng/>Rui Feng</a>
|
<a href=/people/c/chen-luo/>Chen Luo</a>
|
<a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a>
|
<a href=/people/t/tuo-zhao/>Tuo Zhao</a>
|
<a href=/people/c/chao-zhang/>Chao Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--16><div class="card-body p-3 small">User sessions empower many search and recommendation tasks on a daily basis. Such session data are semi-structured, which encode heterogeneous relations between queries and products, and each item is described by the unstructured text. Despite recent advances in self-supervised learning for text or graphs, there lack of self-supervised learning models that can effectively capture both intra-item semantics and inter-item interactions for semi-structured sessions. To fill this gap, we propose CERES, a graph-based transformer model for semi-structured session data. CERES learns representations that capture both inter- and intra-item semantics with (1) a graph-conditioned masked language pretraining task that jointly learns from item text and item-item relations; and (2) a graph-conditioned transformer architecture that propagates inter-item contexts to item-level representations. We pretrained CERES using ~468 million Amazon sessions and find that CERES outperforms strong pretraining baselines by up to 9% in three session search and entity linking tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.17.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.17/>Political Ideology and Polarization: A Multi-dimensional Approach</a></strong><br><a href=/people/b/barea-sinno/>Barea Sinno</a>
|
<a href=/people/b/bernardo-oviedo/>Bernardo Oviedo</a>
|
<a href=/people/k/katherine-atwell/>Katherine Atwell</a>
|
<a href=/people/m/malihe-alikhani/>Malihe Alikhani</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--17><div class="card-body p-3 small">Analyzing ideology and polarization is of critical importance in advancing our grasp of modern politics. Recent research has made great strides towards understanding the ideological bias (i.e., stance) of news media along the left-right spectrum. In this work, we instead take a novel and more nuanced approach for the study of ideology based on its left or right positions on the issue being discussed. Aligned with the theoretical accounts in political science, we treat ideology as a multi-dimensional construct, and introduce the first diachronic dataset of news articles whose ideological positions are annotated by trained political scientists and linguists at the paragraph level. We showcase that, by controlling for the author’s stance, our method allows for the quantitative and temporal measurement and analysis of polarization as a multidimensional ideological distance. We further present baseline models for ideology prediction, outlining a challenging task distinct from stance detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.18.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.18/>Cooperative Self-training of Machine Reading Comprehension</a></strong><br><a href=/people/h/hongyin-luo/>Hongyin Luo</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/m/mingye-gao/>Mingye Gao</a>
|
<a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/j/james-glass/>James Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--18><div class="card-body p-3 small">Pretrained language models have significantly improved the performance of downstream language understanding tasks, including extractive question answering, by providing high-quality contextualized word embeddings. However, training question answering models still requires large amounts of annotated data for specific domains. In this work, we propose a cooperative self-training framework, RGX, for automatically generating more non-trivial question-answer pairs to improve model performance. RGX is built upon a masked answer extraction task with an interactive learning environment containing an answer entity Recognizer, a question Generator, and an answer eXtractor. Given a passage with a masked entity, the generator generates a question around the entity, and the extractor is trained to extract the masked entity with the generated question and raw texts. The framework allows the training of question generation and answering models on any text corpora without annotation. We further leverage a self-training technique to improve the performance of both question generation and answer extraction models. Experiment results show that RGX outperforms the state-of-the-art (SOTA) pretrained language models and transfer learning approaches on standard question-answering benchmarks, and yields the new SOTA performance under given model size and transfer learning settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.19.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.19/><span class=acl-fixed-case>G</span>lob<span class=acl-fixed-case>E</span>nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers</a></strong><br><a href=/people/a/ali-modarressi/>Ali Modarressi</a>
|
<a href=/people/m/mohsen-fayyaz/>Mohsen Fayyaz</a>
|
<a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--19><div class="card-body p-3 small">There has been a growing interest in interpreting the underlying dynamics of Transformers. While self-attention patterns were initially deemed as the primary option, recent studies have shown that integrating other components can yield more accurate explanations. This paper introduces a novel token attribution analysis method that incorporates all the components in the encoder block and aggregates this throughout layers. Through extensive quantitative and qualitative experiments, we demonstrate that our method can produce faithful and meaningful global token attributions. Our experiments reveal that incorporating almost every encoder component results in increasingly more accurate analysis in both local (single layer) and global (the whole model) settings. Our global attribution analysis significantly outperforms previous methods on various tasks regarding correlation with gradient-based saliency scores. Our code is freely available at <a href=https://github.com/mohsenfayyaz/GlobEnc class=acl-markup-url>https://github.com/mohsenfayyaz/GlobEnc</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.20.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.20/>A Robustly Optimized <span class=acl-fixed-case>BMRC</span> for Aspect Sentiment Triplet Extraction</a></strong><br><a href=/people/s/shu-liu/>Shu Liu</a>
|
<a href=/people/k/kaiwen-li/>Kaiwen Li</a>
|
<a href=/people/z/zuhe-li/>Zuhe Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--20><div class="card-body p-3 small">Aspect sentiment triplet extraction (ASTE) is a challenging subtask in aspect-based sentiment analysis. It aims to explore the triplets of aspects, opinions and sentiments with complex correspondence from the context. The bidirectional machine reading comprehension (BMRC), can effectively deal with ASTE task, but several problems remains, such as query conflict and probability unilateral decrease. Therefore, this paper presents a robustly optimized BMRC method by incorporating four improvements. The word segmentation is applied to facilitate the semantic learning. Exclusive classifiers are designed to avoid the interference between different queries. A span matching rule is proposed to select the aspects and opinions that better represent the expectations of the model. The probability generation strategy is also introduced to obtain the predicted probability for aspects, opinions and aspect-opinion pairs. We have conducted extensive experiments on multiple benchmark datasets, where our model achieves the state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.21.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.21/>Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds</a></strong><br><a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/y/yu-meng/>Yu Meng</a>
|
<a href=/people/x/xuan-wang/>Xuan Wang</a>
|
<a href=/people/s/sheng-wang/>Sheng Wang</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--21><div class="card-body p-3 small">Discovering latent topics from text corpora has been studied for decades. Many existing topic models adopt a fully unsupervised setting, and their discovered topics may not cater to users’ particular interests due to their inability of leveraging user guidance. Although there exist seed-guided topic discovery approaches that leverage user-provided seeds to discover topic-representative terms, they are less concerned with two factors: (1) the existence of out-of-vocabulary seeds and (2) the power of pre-trained language models (PLMs). In this paper, we generalize the task of seed-guided topic discovery to allow out-of-vocabulary seeds. We propose a novel framework, named SeeTopic, wherein the general knowledge of PLMs and the local semantics learned from the input corpus can mutually benefit each other. Experiments on three real datasets from different domains demonstrate the effectiveness of SeeTopic in terms of topic coherence, accuracy, and diversity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.22.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.22/>Towards Process-Oriented, Modular, and Versatile Question Generation that Meets Educational Needs</a></strong><br><a href=/people/x/xu-wang/>Xu Wang</a>
|
<a href=/people/s/simin-fan/>Simin Fan</a>
|
<a href=/people/j/jessica-houghton/>Jessica Houghton</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--22><div class="card-body p-3 small">NLP-powered automatic question generation (QG) techniques carry great pedagogical potential of saving educators’ time and benefiting student learning. Yet, QG systems have not been widely adopted in classrooms to date. In this work, we aim to pinpoint key impediments and investigate how to improve the usability of automatic QG techniques for educational purposes by understanding how instructors construct questions and identifying touch points to enhance the underlying NLP models. We perform an in-depth need finding study with 11 instructors across 7 different universities, and summarize their thought processes and needs when creating questions. While instructors show great interests in using NLP systems to support question design, none of them has used such tools in practice. They resort to multiple sources of information, ranging from domain knowledge to students’ misconceptions, all of which missing from today’s QG systems. We argue that building effective human-NLP collaborative QG systems that emphasize instructor control and explainability is imperative for real-world adoption. We call for QG systems to provide process-oriented support, use modular design, and handle diverse sources of input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.23.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.23/><span class=acl-fixed-case>S</span>wah<span class=acl-fixed-case>BERT</span>: Language Model of <span class=acl-fixed-case>S</span>wahili</a></strong><br><a href=/people/g/gati-martin/>Gati Martin</a>
|
<a href=/people/m/medard-edmund-mswahili/>Medard Edmund Mswahili</a>
|
<a href=/people/y/young-seob-jeong/>Young-Seob Jeong</a>
|
<a href=/people/j/jiyoung-woo/>Jiyoung Woo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--23><div class="card-body p-3 small">The rapid development of social networks, electronic commerce, mobile Internet, and other technologies, has influenced the growth of Web data. Social media and Internet forums are valuable sources of citizens’ opinions, which can be analyzed for community development and user behavior analysis. Unfortunately, the scarcity of resources (i.e., datasets or language models) become a barrier to the development of natural language processing applications in low-resource languages. Thanks to the recent growth of online forums and news platforms of Swahili, we introduce two datasets of Swahili in this paper: a pre-training dataset of approximately 105MB with 16M words and annotated dataset of 13K instances for the emotion classification task. The emotion classification dataset is manually annotated by two native Swahili speakers. We pre-trained a new monolingual language model for Swahili, namely SwahBERT, using our collected pre-training data, and tested it with four downstream tasks including emotion classification. We found that SwahBERT outperforms multilingual BERT, a well-known existing language model, in almost all downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.24.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.24/>Deconstructing <span class=acl-fixed-case>NLG</span> Evaluation: Evaluation Practices, Assumptions, and Their Implications</a></strong><br><a href=/people/k/kaitlyn-zhou/>Kaitlyn Zhou</a>
|
<a href=/people/s/su-lin-blodgett/>Su Lin Blodgett</a>
|
<a href=/people/a/adam-trischler/>Adam Trischler</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a>
|
<a href=/people/k/kaheer-suleman/>Kaheer Suleman</a>
|
<a href=/people/a/alexandra-olteanu/>Alexandra Olteanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--24><div class="card-body p-3 small">There are many ways to express similar things in text, which makes evaluating natural language generation (NLG) systems difficult. Compounding this difficulty is the need to assess varying quality criteria depending on the deployment setting. While the landscape of NLG evaluation has been well-mapped, practitioners’ goals, assumptions, and constraints—which inform decisions about what, when, and how to evaluate—are often partially or implicitly stated, or not stated at all. Combining a formative semi-structured interview study of NLG practitioners (N=18) with a survey study of a broader sample of practitioners (N=61), we surface goals, community practices, assumptions, and constraints that shape NLG evaluations, examining their implications and how they embody ethical considerations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.25.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.25/><span class=acl-fixed-case>TSTR</span>: Too Short to Represent, Summarize with Details! Intro-Guided Extended Summary Generation</a></strong><br><a href=/people/s/sajad-sotudeh/>Sajad Sotudeh</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--25><div class="card-body p-3 small">Many scientific papers such as those in arXiv and PubMed data collections have abstracts with varying lengths of 50-1000 words and average length of approximately 200 words, where longer abstracts typically convey more information about the source paper. Up to recently, scientific summarization research has typically focused on generating short, abstract-like summaries following the existing datasets used for scientific summarization. In domains where the source text is relatively long-form, such as in scientific documents, such summary is not able to go beyond the general and coarse overview and provide salient information from the source document. The recent interest to tackle this problem motivated curation of scientific datasets, arXiv-Long and PubMed-Long, containing human-written summaries of 400-600 words, hence, providing a venue for research in generating long/extended summaries. Extended summaries facilitate a faster read while providing details beyond coarse information. In this paper, we propose TSTR, an extractive summarizer that utilizes the introductory information of documents as pointers to their salient information. The evaluations on two existing large-scale extended summarization datasets indicate statistically significant improvement in terms of Rouge and average Rouge (F1) scores (except in one case) as compared to strong baselines and state-of-the-art. Comprehensive human evaluations favor our generated extended summaries in terms of cohesion and completeness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.26.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.26.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.26/>Empathic Machines: Using Intermediate Features as Levers to Emulate Emotions in Text-To-Speech Systems</a></strong><br><a href=/people/s/saiteja-kosgi/>Saiteja Kosgi</a>
|
<a href=/people/s/sarath-sivaprasad/>Sarath Sivaprasad</a>
|
<a href=/people/n/niranjan-pedanekar/>Niranjan Pedanekar</a>
|
<a href=/people/a/anil-nelakanti/>Anil Nelakanti</a>
|
<a href=/people/v/vineet-gandhi/>Vineet Gandhi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--26><div class="card-body p-3 small">We present a method to control the emotional prosody of Text to Speech (TTS) systems by using phoneme-level intermediate features (pitch, energy, and duration) as levers. As a key idea, we propose Differential Scaling (DS) to disentangle features relating to affective prosody from those arising due to acoustics conditions and speaker identity. With thorough experimental studies, we show that the proposed method improves over the prior art in accurately emulating the desired emotions while retaining the naturalness of speech. We extend the traditional evaluation of using individual sentences for a more complete evaluation of HCI systems. We present a novel experimental setup by replacing an actor with a TTS system in offline and live conversations. The emotion to be rendered is either predicted or manually assigned. The results show that the proposed method is strongly preferred over the state-of-the-art TTS system and adds the much-coveted “human touch” in machine dialogue. Audio samples from our experiments and the code are available at: <a href=https://emtts.github.io/tts-demo/ class=acl-markup-url>https://emtts.github.io/tts-demo/</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.27.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.27.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.27/>The Why and The How: A Survey on Natural Language Interaction in Visualization</a></strong><br><a href=/people/h/henrik-voigt/>Henrik Voigt</a>
|
<a href=/people/o/ozge-alacam/>Ozge Alacam</a>
|
<a href=/people/m/monique-meuschke/>Monique Meuschke</a>
|
<a href=/people/k/kai-lawonn/>Kai Lawonn</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--27><div class="card-body p-3 small">Natural language as a modality of interaction is becoming increasingly popular in the field of visualization. In addition to the popular query interfaces, other language-based interactions such as annotations, recommendations, explanations, or documentation experience growing interest. In this survey, we provide an overview of natural language-based interaction in the research area of visualization. We discuss a renowned taxonomy of visualization tasks and classify 119 related works to illustrate the state-of-the-art of how current natural language interfaces support their performance. We examine applied NLP methods and discuss human-machine dialogue structures with a focus on initiative, duration, and communicative functions in recent visualization-oriented dialogue interfaces. Based on this overview, we point out interesting areas for the future application of NLP methods in the field of visualization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.28.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.28.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.28/>Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding</a></strong><br><a href=/people/h/hao-huang/>Hao Huang</a>
|
<a href=/people/x/xiubo-geng/>Xiubo Geng</a>
|
<a href=/people/g/guodong-long/>Guodong Long</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--28><div class="card-body p-3 small">This work studies temporal reading comprehension (TRC), which reads a free-text passage and answers temporal ordering questions. Precise question understanding is critical for temporal reading comprehension. For example, the question “What happened before the victory” and “What happened after the victory” share almost all words except one, while their answers are totally different. Moreover, even if two questions query about similar temporal relations, different varieties might also lead to various answers. For example, although both the question “What usually happened during the press release?” and “What might happen during the press release” query events which happen after “the press release”, they convey divergent semantics. To this end, we propose a novel reading comprehension approach with precise question understanding. Specifically, a temporal ordering question is embedded into two vectors to capture the referred event and the temporal relation. Then we evaluate the temporal relation between candidate events and the referred event based on that. Such fine-grained representations offer two benefits. First, it enables a better understanding of the question by focusing on different elements of a question. Second, it provides good interpretability when evaluating temporal relations. Furthermore, we also harness an auxiliary contrastive loss for representation learning of temporal relations, which aims to distinguish relations with subtle but critical changes. The proposed approach outperforms strong baselines and achieves state-of-the-art performance on the TORQUE dataset. It also increases the accuracy of four pre-trained language models (BERT base, BERT large, RoBERTa base, and RoBETRa large), demonstrating its generic effectiveness on divergent models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.29.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best paper on human-centered NLP special theme"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.29.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.29/>User-Driven Research of Medical Note Generation Software</a></strong><br><a href=/people/t/tom-knoll/>Tom Knoll</a>
|
<a href=/people/f/francesco-moramarco/>Francesco Moramarco</a>
|
<a href=/people/a/alex-papadopoulos-korfiatis/>Alex Papadopoulos Korfiatis</a>
|
<a href=/people/r/rachel-young/>Rachel Young</a>
|
<a href=/people/c/claudia-ruffini/>Claudia Ruffini</a>
|
<a href=/people/m/mark-perera/>Mark Perera</a>
|
<a href=/people/c/christian-perstl/>Christian Perstl</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a>
|
<a href=/people/a/anja-belz/>Anya Belz</a>
|
<a href=/people/a/aleksandar-savkov/>Aleksandar Savkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--29><div class="card-body p-3 small">A growing body of work uses Natural Language Processing (NLP) methods to automatically generate medical notes from audio recordings of doctor-patient consultations. However, there are very few studies on how such systems could be used in clinical practice, how clinicians would adjust to using them, or how system design should be influenced by such considerations. In this paper, we present three rounds of user studies, carried out in the context of developing a medical note generation system. We present, analyse and discuss the participating clinicians’ impressions and views of how the system ought to be adapted to be of value to them. Next, we describe a three-week test run of the system in a live telehealth clinical practice. Major findings include (i) the emergence of five different note-taking behaviours; (ii) the importance of the system generating notes in real time during the consultation; and (iii) the identification of a number of clinical use cases that could prove challenging for automatic note generation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.30.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.30.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.30/>Ask Me Anything in Your Native Language</a></strong><br><a href=/people/n/nikita-sorokin/>Nikita Sorokin</a>
|
<a href=/people/d/dmitry-abulkhanov/>Dmitry Abulkhanov</a>
|
<a href=/people/i/irina-piontkovskaya/>Irina Piontkovskaya</a>
|
<a href=/people/v/valentin-malykh/>Valentin Malykh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--30><div class="card-body p-3 small">Cross-lingual question answering is a thriving field in the modern world, helping people to search information on the web more efficiently. One of the important scenarios is to give an answer even there is no answer in the language a person asks a question with. We present a novel approach based on single encoder for query and passage for retrieval from multi-lingual collection, together with cross-lingual generative reader. It achieves a new state of the art in both retrieval and end-to-end tasks on the XOR TyDi dataset outperforming the previous results up to 10% on several languages. We find that our approach can be generalized to more than 20 languages in zero-shot approach and outperform all previous models by 12%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.31.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.31.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.31/>Diversifying Neural Dialogue Generation via Negative Distillation</a></strong><br><a href=/people/y/yiwei-li/>Yiwei Li</a>
|
<a href=/people/s/shaoxiong-feng/>Shaoxiong Feng</a>
|
<a href=/people/b/bin-sun/>Bin Sun</a>
|
<a href=/people/k/kan-li/>Kan Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--31><div class="card-body p-3 small">Generative dialogue models suffer badly from the generic response problem, limiting their applications to a few toy scenarios. Recently, an interesting approach, namely negative training, has been proposed to alleviate this problem by reminding the model not to generate high-frequency responses during training. However, its performance is hindered by two issues, ignoring low-frequency but generic responses and bringing low-frequency but meaningless responses. In this paper, we propose a novel negative training paradigm, called negative distillation, to keep the model away from the undesirable generic responses while avoiding the above problems. First, we introduce a negative teacher model that can produce query-wise generic responses, and then the student model is required to maximize the distance with multi-level negative knowledge. Empirical results show that our method outperforms previous negative training methods significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.32.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.32.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.32" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.32/>On Synthetic Data for Back Translation</a></strong><br><a href=/people/j/jiahao-xu/>Jiahao Xu</a>
|
<a href=/people/y/yubin-ruan/>Yubin Ruan</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/g/guoping-huang/>Guoping Huang</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/l/lihui-chen/>Lihui Chen</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--32><div class="card-body p-3 small">Back translation (BT) is one of the most significant technologies in NMT research fields. Existing attempts on BT share a common characteristic: they employ either beam search or random sampling to generate synthetic data with a backward model but seldom work studies the role of synthetic data in the performance of BT. This motivates us to ask a fundamental question: what kind of synthetic data contributes to BT performance?Through both theoretical and empirical studies, we identify two key factors on synthetic data controlling the back-translation NMT performance, which are quality and importance. Furthermore, based on our findings, we propose a simple yet effective method to generate synthetic data to better trade off both factors so as to yield the better performance for BT. We run extensive experiments on WMT14 DE-EN, EN-DE, and RU-EN benchmark tasks. By employing our proposed method to generate synthetic data, our BT model significantly outperforms the standard BT baselines (i.e., beam and sampling based methods for data generation), which proves the effectiveness of our proposed methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.33.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.33.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.33/>Mapping the Design Space of Human-<span class=acl-fixed-case>AI</span> Interaction in Text Summarization</a></strong><br><a href=/people/r/ruijia-cheng/>Ruijia Cheng</a>
|
<a href=/people/a/alison-smith-renner/>Alison Smith-Renner</a>
|
<a href=/people/k/ke-zhang/>Ke Zhang</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/a/alejandro-jaimes-larrarte/>Alejandro Jaimes-Larrarte</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--33><div class="card-body p-3 small">Automatic text summarization systems commonly involve humans for preparing data or evaluating model performance, yet, there lacks a systematic understanding of humans’ roles, experience, and needs when interacting with or being assisted by AI. From a human-centered perspective, we map the design opportunities and considerations for human-AI interaction in text summarization and broader text generation tasks. We first conducted a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions. We designed text summarization prototypes for each interaction. We then interviewed 16 users, aided by the prototypes, to understand their expectations, experience, and needs regarding efficiency, control, and trust with AI in text summarization and propose design considerations accordingly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.34.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.34.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.34" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.34/>Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer</a></strong><br><a href=/people/s/sharan-narasimhan/>Sharan Narasimhan</a>
|
<a href=/people/s/suvodip-dey/>Suvodip Dey</a>
|
<a href=/people/m/maunendra-desarkar/>Maunendra Desarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--34><div class="card-body p-3 small">Recent studies show that auto-encoder based approaches successfully perform language generation, smooth sentence interpolation, and style transfer over unseen attributes using unlabelled datasets in a zero-shot manner. The latent space geometry of such models is organised well enough to perform on datasets where the style is “coarse-grained” i.e. a small fraction of words alone in a sentence are enough to determine the overall style label. A recent study uses a discrete token-based perturbation approach to map “similar” sentences (“similar” defined by low Levenshtein distance/ high word overlap) close by in latent space. This definition of “similarity” does not look into the underlying nuances of the constituent words while mapping latent space neighbourhoods and therefore fails to recognise sentences with different style-based semantics while mapping latent neighbourhoods. We introduce EPAAEs (Embedding Perturbed Adversarial AutoEncoders) which completes this perturbation model, by adding a finely adjustable noise component on the continuous embeddings space. We empirically show that this (a) produces a better organised latent space that clusters stylistically similar sentences together, (b) performs best on a diverse set of text style transfer tasks than its counterparts, and (c) is capable of fine-grained control of Style Transfer strength. We also extend the text style transfer tasks to NLI datasets and show that these more complex definitions of style are learned best by EPAAE. To the best of our knowledge, extending style transfer to NLI tasks has not been explored before.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.35.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.35.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.35/>An Exploration of Post-Editing Effectiveness in Text Summarization</a></strong><br><a href=/people/v/vivian-lai/>Vivian Lai</a>
|
<a href=/people/a/alison-smith-renner/>Alison Smith-Renner</a>
|
<a href=/people/k/ke-zhang/>Ke Zhang</a>
|
<a href=/people/r/ruijia-cheng/>Ruijia Cheng</a>
|
<a href=/people/w/wenjuan-zhang/>Wenjuan Zhang</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/a/alejandro-jaimes-larrarte/>Alejandro Jaimes-Larrarte</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--35><div class="card-body p-3 small">Automatic summarization methods are efficient but can suffer from low quality. In comparison, manual summarization is expensive but produces higher quality. Can humans and AI collaborate to improve summarization performance? In similar text generation tasks (e.g., machine translation), human-AI collaboration in the form of “post-editing” AI-generated text reduces human workload and improves the quality of AI output. Therefore, we explored whether post-editing offers advantages in text summarization. Specifically, we conducted an experiment with 72 participants, comparing post-editing provided summaries with manual summarization for summary quality, human efficiency, and user experience on formal (XSum news) and informal (Reddit posts) text. This study sheds valuable insights on when post-editing is useful for text summarization: it helped in some cases (e.g., when participants lacked domain knowledge) but not in others (e.g., when provided summaries include inaccurate information). Participants’ different editing strategies and needs for assistance offer implications for future human-AI summarization systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.36.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best new task (tied) and new resource paper"><i class="fas fa-award"></i></span><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable mention for contribution to special theme on human-centered NLP"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.36/>Automatic Correction of Human Translations</a></strong><br><a href=/people/j/jessy-lin/>Jessy Lin</a>
|
<a href=/people/g/geza-kovacs/>Geza Kovacs</a>
|
<a href=/people/a/aditya-shastry/>Aditya Shastry</a>
|
<a href=/people/j/joern-wuebker/>Joern Wuebker</a>
|
<a href=/people/j/john-denero/>John DeNero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--36><div class="card-body p-3 small">We introduce translation error correction (TEC), the task of automatically correcting human-generated translations. Imperfections in machine translations (MT) have long motivated systems for improving translations post-hoc with automatic post-editing. In contrast, little attention has been devoted to the problem of automatically correcting human translations, despite the intuition that humans make distinct errors that machines would be well-suited to assist with, from typos to inconsistencies in translation conventions. To investigate this, we build and release the Aced corpus with three TEC datasets (available at: github.com/lilt/tec). We show that human errors in TEC exhibit a more diverse range of errors and far fewer translation fluency errors than the MT errors in automatic post-editing datasets, suggesting the need for dedicated TEC models that are specialized to correct human errors. We show that pre-training instead on synthetic errors based on human errors improves TEC F-score by as much as 5.1 points. We conducted a human-in-the-loop user study with nine professional translation editors and found that the assistance of our TEC system led them to produce significantly higher quality revised translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.37.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.37.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.37/>On the Robustness of Reading Comprehension Models to Entity Renaming</a></strong><br><a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/y/yang-xiao/>Yang Xiao</a>
|
<a href=/people/s/sagnik-mukherjee/>Sagnik Mukherjee</a>
|
<a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--37><div class="card-body p-3 small">We study the robustness of machine reading comprehension (MRC) models to entity renaming—do models make more wrong predictions when the same questions are asked about an entity whose name has been changed? Such failures imply that models overly rely on entity information to answer questions, and thus may generalize poorly when facts about the world change or questions are asked about novel entities. To systematically audit this issue, we present a pipeline to automatically generate test examples at scale, by replacing entity names in the original test sample with names from a variety of sources, ranging from names in the same test set, to common names in life, to arbitrary strings. Across five datasets and three pretrained model architectures, MRC models consistently perform worse when entities are renamed, with particularly large accuracy drops on datasets constructed via distant supervision. We also find large differences between models: SpanBERT, which is pretrained with span-level masking, is more robust than RoBERTa, despite having similar accuracy on unperturbed test data. We further experiment with different masking strategies as the continual pretraining objective and find that entity-based masking can improve the robustness of MRC models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.38.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.38.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.38.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.38/>Explaining Why: How Instructions and User Interfaces Impact Annotator Rationales When Labeling Text Data</a></strong><br><a href=/people/j/jamar-sullivan-jr/>Jamar Sullivan Jr.</a>
|
<a href=/people/w/will-brackenbury/>Will Brackenbury</a>
|
<a href=/people/a/andrew-mcnutt/>Andrew McNutt</a>
|
<a href=/people/k/kevin-bryson/>Kevin Bryson</a>
|
<a href=/people/k/kwam-byll/>Kwam Byll</a>
|
<a href=/people/y/yuxin-chen/>Yuxin Chen</a>
|
<a href=/people/m/michael-littman/>Michael Littman</a>
|
<a href=/people/c/chenhao-tan/>Chenhao Tan</a>
|
<a href=/people/b/blase-ur/>Blase Ur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--38><div class="card-body p-3 small">In the context of data labeling, NLP researchers are increasingly interested in having humans select rationales, a subset of input tokens relevant to the chosen label. We conducted a 332-participant online user study to understand how humans select rationales, especially how different instructions and user interface affordances impact the rationales chosen. Participants labeled ten movie reviews as positive or negative, selecting words and phrases supporting their label as rationales. We varied the instructions given, the rationale-selection task, and the user interface. Participants often selected about 12% of input tokens as rationales, but selected fewer if unable to drag over multiple tokens at once. Whereas participants were near unanimous in their data labels, they were far less consistent in their rationales. The user interface affordances and task greatly impacted the types of rationales chosen. We also observed large variance across participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.39.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.39.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.39.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.39" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.39/>Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization</a></strong><br><a href=/people/h/haode-zhang/>Haode Zhang</a>
|
<a href=/people/h/haowen-liang/>Haowen Liang</a>
|
<a href=/people/y/yuwei-zhang/>Yuwei Zhang</a>
|
<a href=/people/l/li-ming-zhan/>Li-Ming Zhan</a>
|
<a href=/people/x/xiao-ming-wu/>Xiao-Ming Wu</a>
|
<a href=/people/x/xiaolei-lu/>Xiaolei Lu</a>
|
<a href=/people/a/albert-lam/>Albert Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--39><div class="card-body p-3 small">It is challenging to train a good intent classifier for a task-oriented dialogue system with only a few annotations. Recent studies have shown that fine-tuning pre-trained language models with a small set of labeled utterances from public benchmarks in a supervised manner is extremely helpful. However, we find that supervised pre-training yields an anisotropic feature space, which may suppress the expressive power of the semantic representations. Inspired by recent research in isotropization, we propose to improve supervised pre-training by regularizing the feature space towards isotropy. We propose two regularizers based on contrastive learning and correlation matrix respectively, and demonstrate their effectiveness through extensive experiments. Our main finding is that it is promising to regularize supervised pre-training with isotropization to further improve the performance of few-shot intent detection. The source code can be found at <a href=https://github.com/fanolabs/isoIntentBert-main class=acl-markup-url>https://github.com/fanolabs/isoIntentBert-main</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.40.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.40.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.40" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.40/>Cross-document Misinformation Detection based on Event Graph Reasoning</a></strong><br><a href=/people/x/xueqing-wu/>Xueqing Wu</a>
|
<a href=/people/k/kung-hsiang-huang/>Kung-Hsiang Huang</a>
|
<a href=/people/y/yi-fung/>Yi Fung</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--40><div class="card-body p-3 small">For emerging events, human readers are often exposed to both real news and fake news. Multiple news articles may contain complementary or contradictory information that readers can leverage to help detect fake news. Inspired by this process, we propose a novel task of cross-document misinformation detection. Given a cluster of topically related news documents, we aim to detect misinformation at both document level and a more fine-grained level, event level. Due to the lack of data, we generate fake news by manipulating real news, and construct 3 new datasets with 422, 276, and 1,413 clusters of topically related documents, respectively. We further propose a graph-based detector that constructs a cross-document knowledge graph using cross-document event coreference resolution and employs a heterogeneous graph neural network to conduct detection at two levels. We then feed the event-level detection results into the document-level detector. Experimental results show that our proposed method significantly outperforms existing methods by up to 7 F1 points on this new task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.41.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.41.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.41/>Disentangled Action Recognition with Knowledge Bases</a></strong><br><a href=/people/z/zhekun-luo/>Zhekun Luo</a>
|
<a href=/people/s/shalini-ghosh/>Shalini Ghosh</a>
|
<a href=/people/d/devin-guillory/>Devin Guillory</a>
|
<a href=/people/k/keizo-kato/>Keizo Kato</a>
|
<a href=/people/t/trevor-darrell/>Trevor Darrell</a>
|
<a href=/people/h/huijuan-xu/>Huijuan Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--41><div class="card-body p-3 small">Action in video usually involves the interaction of human with objects. Action labels are typically composed of various combinations of verbs and nouns, but we may not have training data for all possible combinations. In this paper, we aim to improve the generalization ability of the compositional action recognition model to novel verbs or novel nouns that are unseen during training time, by leveraging the power of knowledge graphs. Previous work utilizes verb-noun compositional action nodes in the knowledge graph, making it inefficient to scale since the number of compositional action nodes grows quadratically with respect to the number of verbs and nouns. To address this issue, we propose our approach: Disentangled Action Recognition with Knowledge-bases (DARK), which leverages the inherent compositionality of actions. DARK trains a factorized model by first extracting disentangled feature representations for verbs and nouns, and then predicting classification weights using relations in external knowledge graphs. The type constraint between verb and noun is extracted from external knowledge bases and finally applied when composing actions. DARK has better scalability in the number of objects and verbs, and achieves state-of-the-art performance on the Charades dataset. We further propose a new benchmark split based on the Epic-kitchen dataset which is an order of magnitude bigger in the numbers of classes and samples, and benchmark various models on this benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.42.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.42.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.42" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.42/>Machine-in-the-Loop Rewriting for Creative Image Captioning</a></strong><br><a href=/people/v/vishakh-padmakumar/>Vishakh Padmakumar</a>
|
<a href=/people/h/he-he/>He He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--42><div class="card-body p-3 small">Machine-in-the-loop writing aims to build models that assist humans to accomplish their writing tasks more effectively. Prior work has found that providing users a machine-written draft or sentence-level continuations has limited success since the generated text tends to deviate from users’ intention. To allow the user to retain control over the content, we train a rewriting model that, when prompted, modifies specified spans of text within the user’s original draft to introduce descriptive and figurative elements in the text. We evaluate the model on its ability to collaborate with humans on the task of creative image captioning. On a user study through Amazon Mechanical Turk, our model is rated to be more helpful by users than a baseline infilling language model. In addition, third-party evaluation shows that users write more descriptive and figurative captions when collaborating with our model compared to completing the task alone. However, the improvement is not uniform across user groups: the model is more helpful to skilled users, which risks widening the gap between skilled and novice users, highlighting a need for careful, user-centric evaluation of interactive systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.43.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.43.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.43/>A Word is Worth A Thousand Dollars: Adversarial Attack on Tweets Fools Stock Prediction</a></strong><br><a href=/people/y/yong-xie/>Yong Xie</a>
|
<a href=/people/d/dakuo-wang/>Dakuo Wang</a>
|
<a href=/people/p/pin-yu-chen/>Pin-Yu Chen</a>
|
<a href=/people/j/jinjun-xiong/>Jinjun Xiong</a>
|
<a href=/people/s/sijia-liu/>Sijia Liu</a>
|
<a href=/people/o/oluwasanmi-koyejo/>Oluwasanmi Koyejo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--43><div class="card-body p-3 small">More and more investors and machine learning models rely on social media (e.g., Twitter and Reddit) to gather information and predict movements stock prices. Although text-based models are known to be vulnerable to adversarial attacks, whether stock prediction models have similar vulnerability given necessary constraints is underexplored. In this paper, we experiment with a variety of adversarial attack configurations to fool three stock prediction victim models. We address the task of adversarial generation by solving combinatorial optimization problems with semantics and budget constraints. Our results show that the proposed attack method can achieve consistent success rates and cause significant monetary loss in trading simulation by simply concatenating a perturbed but semantically similar tweet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.44.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.44.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.44/>Building Multilingual Machine Translation Systems That Serve Arbitrary <span class=acl-fixed-case>XY</span> Translations</a></strong><br><a href=/people/a/akiko-eriguchi/>Akiko Eriguchi</a>
|
<a href=/people/s/shufang-xie/>Shufang Xie</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/h/hany-hassan-awadalla/>Hany Hassan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--44><div class="card-body p-3 small">Multilingual Neural Machine Translation (MNMT) enables one system to translate sentences from multiple source languages to multiple target languages, greatly reducing deployment costs compared with conventional bilingual systems. The MNMT training benefit, however, is often limited to many-to-one directions. The model suffers from poor performance in one-to-many and many-to-many with zero-shot setup. To address this issue, this paper discusses how to practically build MNMT systems that serve arbitrary X-Y translation directions while leveraging multilinguality with a two-stage training strategy of pretraining and finetuning. Experimenting with the WMT’21 multilingual translation task, we demonstrate that our systems outperform the conventional baselines of direct bilingual models and pivot translation models for most directions, averagely giving +6.0 and +4.1 BLEU, without the need for architecture change or extra data collection. Moreover, we also examine our proposed approach in an extremely large-scale data setting to accommodate practical deployment scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.45.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.45.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.45/>Non-Autoregressive Neural Machine Translation with Consistency Regularization Optimized Variational Framework</a></strong><br><a href=/people/m/minghao-zhu/>Minghao Zhu</a>
|
<a href=/people/j/junli-wang/>Junli Wang</a>
|
<a href=/people/c/chungang-yan/>Chungang Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--45><div class="card-body p-3 small">Variational Autoencoder (VAE) is an effective framework to model the interdependency for non-autoregressive neural machine translation (NAT). One of the prominent VAE-based NAT frameworks, LaNMT, achieves great improvements to vanilla models, but still suffers from two main issues which lower down the translation quality: (1) mismatch between training and inference circumstances and (2) inadequacy of latent representations. In this work, we target on addressing these issues by proposing posterior consistency regularization. Specifically, we first perform stochastic data augmentation on the input samples to better adapt the model for inference circumstance, and then conduct consistency training on posterior latent variables to construct a more robust latent representations without any expansion on latent size. Experiments on En&lt;->De and En&lt;->Ro benchmarks confirm the effectiveness of our methods with about 1.5/0.7 and 0.8/0.3 BLEU points improvement to the baseline model with about <span class=tex-math>12.6×</span> faster than autoregressive Transformer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.46.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.46.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.46" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.46/>User-Centric Gender Rewriting</a></strong><br><a href=/people/b/bashar-alhafni/>Bashar Alhafni</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--46><div class="card-body p-3 small">In this paper, we define the task of gender rewriting in contexts involving two users (I and/or You) – first and second grammatical persons with independent grammatical gender preferences. We focus on Arabic, a gender-marking morphologically rich language. We develop a multi-step system that combines the positive aspects of both rule-based and neural rewriting models. Our results successfully demonstrate the viability of this approach on a recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5 on a blind test set. Our proposed system improves over previous work on the first-person-only version of this task, by 3.05 absolute increase in M2 F0.5. We demonstrate a use case of our gender rewriting system by using it to post-edit the output of a commercial MT system to provide personalized outputs based on the users’ grammatical gender preferences. We make our code, data, and pretrained models publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.47.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.47.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.47" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.47/>Reframing Human-<span class=acl-fixed-case>AI</span> Collaboration for Generating Free-Text Explanations</a></strong><br><a href=/people/s/sarah-wiegreffe/>Sarah Wiegreffe</a>
|
<a href=/people/j/jack-hessel/>Jack Hessel</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/m/mark-riedl/>Mark Riedl</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--47><div class="card-body p-3 small">Large language models are increasingly capable of generating fluent-appearing text with relatively little task-specific supervision. But can these models accurately explain classification decisions? We consider the task of generating free-text explanations using human-written examples in a few-shot manner. We find that (1) authoring higher quality prompts results in higher quality generations; and (2) surprisingly, in a head-to-head comparison, crowdworkers often prefer explanations generated by GPT-3 to crowdsourced explanations in existing datasets. Our human studies also show, however, that while models often produce factual, grammatical, and sufficient explanations, they have room to improve along axes such as providing novel information and supporting the label. We create a pipeline that combines GPT-3 with a supervised filter that incorporates binary acceptability judgments from humans in the loop. Despite the intrinsic subjectivity of acceptability judgments, we demonstrate that acceptability is partially correlated with various fine-grained attributes of explanations. Our approach is able to consistently filter GPT-3-generated explanations deemed acceptable by humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.48.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.48.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--48 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.48 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.48.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.48" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.48/><span class=acl-fixed-case>E</span>m<span class=acl-fixed-case>R</span>el: Joint Representation of Entities and Embedded Relations for Multi-triple Extraction</a></strong><br><a href=/people/b/benfeng-xu/>Benfeng Xu</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/y/yabing-shi/>Yabing Shi</a>
|
<a href=/people/y/yong-zhu/>Yong Zhu</a>
|
<a href=/people/j/jie-gao/>Jie Gao</a>
|
<a href=/people/z/zhendong-mao/>Zhendong Mao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--48><div class="card-body p-3 small">Multi-triple extraction is a challenging task due to the existence of informative inter-triple correlations, and consequently rich interactions across the constituent entities and relations. While existing works only explore entity representations, we propose to explicitly introduce <i>relation</i> representation, jointly represent it with entities, and novelly align them to identify valid triples.We perform comprehensive experiments on document-level relation extraction and joint entity and relation extraction along with ablations to demonstrate the advantage of the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.49.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.49.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.49/>Meta Learning for Natural Language Processing: A Survey</a></strong><br><a href=/people/h/hung-yi-lee/>Hung-yi Lee</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/t/thang-vu/>Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--49><div class="card-body p-3 small">Deep learning has been the mainstream technique in the natural language processing (NLP) area. However, deep learning requires many labeled data and is less generalizable across domains. Meta-learning is an arising field in machine learning. It studies approaches to learning better learning algorithms and aims to improve algorithms in various aspects, including data efficiency and generalizability. The efficacy of meta-learning has been shown in many NLP tasks, but there is no systematic survey of these approaches in NLP, which hinders more researchers from joining the field. Our goal with this survey paper is to offer researchers pointers to relevant meta-learning works in NLP and attract more attention from the NLP community to drive future innovation. This paper first introduces the general concepts of meta-learning and the common approaches. Then we summarize task construction settings, applications of meta-learning for various NLP problems and review the development of meta-learning in the NLP community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.50.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.50.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.50" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.50/>Analyzing Modality Robustness in Multimodal Sentiment Analysis</a></strong><br><a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/y/yingting-li/>Yingting Li</a>
|
<a href=/people/b/bo-cheng/>Bo Cheng</a>
|
<a href=/people/s/shuai-zhao/>Shuai Zhao</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--50><div class="card-body p-3 small">Building robust multimodal models are crucial for achieving reliable deployment in the wild. Despite its importance, less attention has been paid to identifying and improving the robustness of Multimodal Sentiment Analysis (MSA) models. In this work, we hope to address that by (i) Proposing simple diagnostic checks for modality robustness in a trained multimodal model. Using these checks, we find MSA models to be highly sensitive to a single modality, which creates issues in their robustness; (ii) We analyze well-known robust training strategies to alleviate the issues. Critically, we observe that robustness can be achieved without compromising on the original performance. We hope our extensive study–performed across five models and two benchmark datasets–and proposed procedures would make robustness an integral component in MSA research. Our diagnostic checks and robust training solutions are simple to implement and available at <a href=https://github.com/declare-lab/MSA-Robustness class=acl-markup-url>https://github.com/declare-lab/MSA-Robustness</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.51.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.51.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.51.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.51" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.51/>Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation</a></strong><br><a href=/people/j/jinyi-hu/>Jinyi Hu</a>
|
<a href=/people/x/xiaoyuan-yi/>Xiaoyuan Yi</a>
|
<a href=/people/w/wenhao-li/>Wenhao Li</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--51><div class="card-body p-3 small">The past several years have witnessed Variational Auto-Encoder’s superiority in various text generation tasks. However, due to the sequential nature of the text, auto-regressive decoders tend to ignore latent variables and then reduce to simple language models, known as the <span class=tex-math><em>KL vanishing</em></span> problem, which would further deteriorate when VAE is combined with Transformer-based structures. To ameliorate this problem, we propose Della, a novel variational Transformer framework. Della learns a series of layer-wise latent variables with each inferred from those of lower layers and tightly coupled with the hidden states by low-rank tensor product. In this way, Della forces these posterior latent variables to be fused deeply with the whole computation path and hence incorporate more information. We theoretically demonstrate that our method can be regarded as entangling latent variables to avoid posterior information decrease through layers, enabling Della to get higher non-zero KL values even without any annealing or thresholding tricks. Experiments on four unconditional and three conditional generation tasks show that Della could better alleviate KL vanishing and improve both quality and diversity compared to several strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.52.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.52.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.52.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.52" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.52/>Easy Adaptation to Mitigate Gender Bias in Multilingual Text Classification</a></strong><br><a href=/people/x/xiaolei-huang/>Xiaolei Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--52><div class="card-body p-3 small">Existing approaches to mitigate demographic biases evaluate on monolingual data, however, multilingual data has not been examined. In this work, we treat the gender as domains (e.g., male vs. female) and present a standard domain adaptation model to reduce the gender bias and improve performance of text classifiers under multilingual settings. We evaluate our approach on two text classification tasks, hate speech detection and rating prediction, and demonstrate the effectiveness of our approach with three fair-aware baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.53.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.53.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.53" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.53/>On the Use of External Data for Spoken Named Entity Recognition</a></strong><br><a href=/people/a/ankita-pasad/>Ankita Pasad</a>
|
<a href=/people/f/felix-wu/>Felix Wu</a>
|
<a href=/people/s/suwon-shon/>Suwon Shon</a>
|
<a href=/people/k/karen-livescu/>Karen Livescu</a>
|
<a href=/people/k/kyu-han/>Kyu Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--53><div class="card-body p-3 small">Spoken language understanding (SLU) tasks involve mapping from speech signals to semantic labels. Given the complexity of such tasks, good performance is expected to require large labeled datasets, which are difficult to collect for each new task and domain. However, recent advances in self-supervised speech representations have made it feasible to consider learning SLU models with limited labeled data. In this work, we focus on low-resource spoken named entity recognition (NER) and address the question: Beyond self-supervised pre-training, how can we use external speech and/or text data that are not annotated for the task? We consider self-training, knowledge distillation, and transfer learning for end-to-end (E2E) and pipeline (speech recognition followed by text NER) approaches. We find that several of these approaches improve performance in resource-constrained settings beyond the benefits from pre-trained representations. Compared to prior work, we find relative improvements in F1 of up to 16%. While the best baseline model is a pipeline approach, the best performance using external data is ultimately achieved by an E2E model. We provide detailed comparisons and analyses, developing insights on, for example, the effects of leveraging external data on (i) different categories of NER errors and (ii) the switch in performance trends between pipeline and E2E models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.54.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.54.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.54.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.54" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.54/>Long-term Control for Dialogue Generation: Methods and Evaluation</a></strong><br><a href=/people/r/ramya-ramakrishnan/>Ramya Ramakrishnan</a>
|
<a href=/people/h/hashan-narangodage/>Hashan Narangodage</a>
|
<a href=/people/m/mauro-schilman/>Mauro Schilman</a>
|
<a href=/people/k/kilian-weinberger/>Kilian Weinberger</a>
|
<a href=/people/r/ryan-mcdonald/>Ryan McDonald</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--54><div class="card-body p-3 small">Current approaches for controlling dialogue response generation are primarily focused on high-level attributes like style, sentiment, or topic. In this work, we focus on constrained long-term dialogue generation, which involves more fine-grained control and requires a given set of control words to appear in generated responses. This setting requires a model to not only consider the generation of these control words in the immediate context, but also produce utterances that will encourage the generation of the words at some time in the (possibly distant) future. We define the problem of constrained long-term control for dialogue generation, identify gaps in current methods for evaluation, and propose new metrics that better measure long-term control. We also propose a retrieval-augmented method that improves performance of long-term controlled generation via logit modification techniques. We show through experiments on three task-oriented dialogue datasets that our metrics better assess dialogue control relative to current alternatives and that our method outperforms state-of-the-art constrained generation baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.55.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.55.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.55" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.55/>Learning Dialogue Representations from Consecutive Utterances</a></strong><br><a href=/people/z/zhihan-zhou/>Zhihan Zhou</a>
|
<a href=/people/d/dejiao-zhang/>Dejiao Zhang</a>
|
<a href=/people/w/wei-xiao/>Wei Xiao</a>
|
<a href=/people/n/nicholas-dingwall/>Nicholas Dingwall</a>
|
<a href=/people/x/xiaofei-ma/>Xiaofei Ma</a>
|
<a href=/people/a/andrew-arnold/>Andrew Arnold</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--55><div class="card-body p-3 small">Learning high-quality dialogue representations is essential for solving a variety of dialogue-oriented tasks, especially considering that dialogue systems often suffer from data scarcity. In this paper, we introduce Dialogue Sentence Embedding (DSE), a self-supervised contrastive learning method that learns effective dialogue representations suitable for a wide range of dialogue tasks. DSE learns from dialogues by taking consecutive utterances of the same dialogue as positive pairs for contrastive learning. Despite its simplicity, DSE achieves significantly better representation capability than other dialogue representation and universal sentence representation models. We evaluate DSE on five downstream dialogue tasks that examine dialogue representation at different semantic granularities. Experiments in few-shot and zero-shot settings show that DSE outperforms baselines by a large margin, for example, it achieves 13% average performance improvement over the strongest unsupervised baseline in 1-shot intent classification on 6 datasets. We also provide analyses on the benefits and limitations of our model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.56.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.56.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.56/>On the Machine Learning of Ethical Judgments from Natural Language</a></strong><br><a href=/people/z/zeerak-talat/>Zeerak Talat</a>
|
<a href=/people/h/hagen-blix/>Hagen Blix</a>
|
<a href=/people/j/josef-valvoda/>Josef Valvoda</a>
|
<a href=/people/m/maya-indira-ganesh/>Maya Indira Ganesh</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--56><div class="card-body p-3 small">Ethics is one of the longest standing intellectual endeavors of humanity. In recent years, the fields of AI and NLP have attempted to address issues of harmful outcomes in machine learning systems that are made to interface with humans. One recent approach in this vein is the construction of NLP morality models that can take in arbitrary text and output a moral judgment about the situation described. In this work, we offer a critique of such NLP methods for automating ethical decision-making. Through an audit of recent work on computational approaches for predicting morality, we examine the broader issues that arise from such efforts. We conclude with a discussion of how machine ethics could usefully proceed in NLP, by focusing on current and near-future uses of technology, in a way that centers around transparency, democratic values, and allows for straightforward accountability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.57.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best new method paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.57" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.57/><span class=acl-fixed-case>N</span>euro<span class=acl-fixed-case>L</span>ogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics</a></strong><br><a href=/people/x/ximing-lu/>Ximing Lu</a>
|
<a href=/people/s/sean-welleck/>Sean Welleck</a>
|
<a href=/people/p/peter-west/>Peter West</a>
|
<a href=/people/l/liwei-jiang/>Liwei Jiang</a>
|
<a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/l/lianhui-qin/>Lianhui Qin</a>
|
<a href=/people/y/youngjae-yu/>Youngjae Yu</a>
|
<a href=/people/r/rowan-zellers/>Rowan Zellers</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--57><div class="card-body p-3 small">The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the <span class=tex-math>A<sup>*</sup></span> search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-<span class=tex-math>k</span> sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.58.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.58.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.58" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.58/><span class=acl-fixed-case>PARADISE</span>: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining</a></strong><br><a href=/people/m/machel-reid/>Machel Reid</a>
|
<a href=/people/m/mikel-artetxe/>Mikel Artetxe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--58><div class="card-body p-3 small">Despite the success of multilingual sequence-to-sequence pretraining, most existing approaches rely on monolingual corpora and do not make use of the strong cross-lingual signal contained in parallel data. In this paper, we present PARADISE (PARAllel &amp;Denoising Integration in SEquence-to-sequence models), which extends the conventional denoising objective used to train these models by (i) replacing words in the noised sequence according to a multilingual dictionary, and (ii) predicting the reference translation according to a parallel corpus instead of recovering the original sequence. Our experiments on machine translation and cross-lingual natural language inference show an average improvement of 2.0 BLEU points and 6.7 accuracy points from integrating parallel data into pretraining, respectively, obtaining results that are competitive with several popular models at a fraction of their computational cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.59.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.59.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.59/>Explaining Toxic Text via Knowledge Enhanced Text Generation</a></strong><br><a href=/people/r/rohit-sridhar/>Rohit Sridhar</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--59><div class="card-body p-3 small">Warning: This paper contains content that is offensive and may be upsetting. Biased or toxic speech can be harmful to various demographic groups. Therefore, it is not only important for models to detect these speech, but to also output explanations of why a given text is toxic. Previous literature has mostly focused on classifying and detecting toxic speech, and existing efforts on explaining stereotypes in toxic speech mainly use standard text generation approaches, resulting in generic and repetitive explanations. Building on these prior works, we introduce a novel knowledge-informed encoder-decoder framework to utilize multiple knowledge sources to generate implications of biased text. Experiments show that our knowledge informed models outperform prior state-of-the-art models significantly, and can generate detailed explanations of stereotypes in toxic speech compared to baselines, both quantitatively and qualitatively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.60.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.60.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.60/>Teaching <span class=acl-fixed-case>BERT</span> to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection</a></strong><br><a href=/people/a/angelica-chen/>Angelica Chen</a>
|
<a href=/people/v/vicky-zayats/>Vicky Zayats</a>
|
<a href=/people/d/daniel-walker/>Daniel Walker</a>
|
<a href=/people/d/dirk-padfield/>Dirk Padfield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--60><div class="card-body p-3 small">In modern interactive speech-based systems, speech is consumed and transcribed incrementally prior to having disfluencies removed. While this post-processing step is crucial for producing clean transcripts and high performance on downstream tasks (e.g. machine translation), most current state-of-the-art NLP models such as the Transformer operate non-incrementally, potentially causing unacceptable delays for the user. In this work we propose a streaming BERT-based sequence tagging model that, combined with a novel training objective, is capable of detecting disfluencies in real-time while balancing accuracy and latency. This is accomplished by training the model to decide whether to immediately output a prediction for the current input or to wait for further context, in essence learning to dynamically size the lookahead window. Our results demonstrate that our model produces comparably accurate predictions and does so sooner than our baselines, with lower flicker. Furthermore, the model attains state-of-the-art latency and stability scores when compared with recent work on incremental disfluency detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.61.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.61.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.61" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.61/><span class=acl-fixed-case>GRAM</span>: <span class=acl-fixed-case>F</span>ast <span class=acl-fixed-case>F</span>ine-tuning of <span class=acl-fixed-case>P</span>re-trained <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>M</span>odels for <span class=acl-fixed-case>C</span>ontent-based <span class=acl-fixed-case>C</span>ollaborative <span class=acl-fixed-case>F</span>iltering</a></strong><br><a href=/people/y/yoonseok-yang/>Yoonseok Yang</a>
|
<a href=/people/k/kyu-seok-kim/>Kyu Seok Kim</a>
|
<a href=/people/m/minsam-kim/>Minsam Kim</a>
|
<a href=/people/j/juneyoung-park/>Juneyoung Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--61><div class="card-body p-3 small">Content-based collaborative filtering (CCF) predicts user-item interactions based on both users’ interaction history and items’ content information. Recently, pre-trained language models (PLM) have been used to extract high-quality item encodings for CCF. However, it is resource-intensive to train a PLM-based CCF model in an end-to-end (E2E) manner, since optimization involves back-propagating through every content encoding within a given user interaction sequence. To tackle this issue, we propose GRAM (GRadient Accumulation for Multi-modality in CCF), which exploits the fact that a given item often appears multiple times within a batch of interaction histories. Specifically, Single-step GRAM aggregates each item encoding’s gradients for back-propagation, with theoretic equivalence to the standard E2E training. As an extension of Single-step GRAM, we propose Multi-step GRAM, which increases the gradient update latency, achieving a further speedup with drastically less GPU memory. GRAM significantly improves training efficiency (up to 146x) on five datasets from two task domains of Knowledge Tracing and News Recommendation. Our code is available at <a href=https://github.com/yoonseok312/GRAM class=acl-markup-url>https://github.com/yoonseok312/GRAM</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.62.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.62.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.62" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.62/>Generating Repetitions with Appropriate Repeated Words</a></strong><br><a href=/people/t/toshiki-kawamoto/>Toshiki Kawamoto</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/k/kotaro-funakoshi/>Kotaro Funakoshi</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--62><div class="card-body p-3 small">A repetition is a response that repeats words in the previous speaker’s utterance in a dialogue. Repetitions are essential in communication to build trust with others, as investigated in linguistic studies. In this work, we focus on repetition generation. To the best of our knowledge, this is the first neural approach to address repetition generation. We propose Weighted Label Smoothing, a smoothing method for explicitly learning which words to repeat during fine-tuning, and a repetition scoring method that can output more appropriate repetitions during decoding. We conducted automatic and human evaluations involving applying these methods to the pre-trained language model T5 for generating repetitions. The experimental results indicate that our methods outperformed baselines in both evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.63.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.63.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.63/>Textless Speech-to-Speech Translation on Real Data</a></strong><br><a href=/people/a/ann-lee/>Ann Lee</a>
|
<a href=/people/h/hongyu-gong/>Hongyu Gong</a>
|
<a href=/people/p/paul-ambroise-duquenne/>Paul-Ambroise Duquenne</a>
|
<a href=/people/h/holger-schwenk/>Holger Schwenk</a>
|
<a href=/people/p/peng-jen-chen/>Peng-Jen Chen</a>
|
<a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/s/sravya-popuri/>Sravya Popuri</a>
|
<a href=/people/y/yossi-adi/>Yossi Adi</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/j/jiatao-gu/>Jiatao Gu</a>
|
<a href=/people/w/wei-ning-hsu/>Wei-Ning Hsu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--63><div class="card-body p-3 small">We present a textless speech-to-speech translation (S2ST) system that can translate speech from one language into another language and can be built without the need of any text data. Different from existing work in the literature, we tackle the challenge in modeling multi-speaker target speech and train the systems with real-world S2ST data. The key to our approach is a self-supervised unit-based speech normalization technique, which finetunes a pre-trained speech encoder with paired audios from multiple speakers and a single reference speaker to reduce the variations due to accents, while preserving the lexical content. With only 10 minutes of paired data for speech normalization, we obtain on average 3.2 BLEU gain when training the S2ST model on the VoxPopuli S2ST dataset, compared to a baseline trained on un-normalized speech target. We also incorporate automatically mined S2ST data and show an additional 2.0 BLEU gain. To our knowledge, we are the first to establish a textless S2ST technique that can be trained with real-world data and works for multiple language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.64.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.64.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.64/><span class=acl-fixed-case>WALNUT</span>: A Benchmark on Semi-weakly Supervised Learning for Natural Language Understanding</a></strong><br><a href=/people/g/guoqing-zheng/>Guoqing Zheng</a>
|
<a href=/people/g/giannis-karamanolakis/>Giannis Karamanolakis</a>
|
<a href=/people/k/kai-shu/>Kai Shu</a>
|
<a href=/people/a/ahmed-awadallah/>Ahmed Awadallah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--64><div class="card-body p-3 small">Building machine learning models for natural language understanding (NLU) tasks relies heavily on labeled data. Weak supervision has been proven valuable when large amount of labeled data is unavailable or expensive to obtain. Existing works studying weak supervision for NLU either mostly focus on a specific task or simulate weak supervision signals from ground-truth labels. It is thus hard to compare different approaches and evaluate the benefit of weak supervision without access to a unified and systematic benchmark with diverse tasks and real-world weak labeling rules. In this paper, we propose such a benchmark, named WALNUT, to advocate and facilitate research on weak supervision for NLU. WALNUT consists of NLU tasks with different types, including document-level and token-level prediction tasks. WALNUT is the first semi-weakly supervised learning benchmark for NLU, where each task contains weak labels generated by multiple real-world weak sources, together with a small set of clean labels. We conduct baseline evaluations on WALNUT to systematically evaluate the effectiveness of various weak supervision methods and model architectures. Our results demonstrate the benefit of weak supervision for low-resource NLU tasks and highlight interesting patterns across tasks. We expect WALNUT to stimulate further research on methodologies to leverage weak supervision more effectively. The benchmark and code for baselines are available at aka.ms/walnut_benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.65.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.65.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.65" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.65/><span class=acl-fixed-case>C</span>ompact<span class=acl-fixed-case>IE</span>: Compact Facts in Open Information Extraction</a></strong><br><a href=/people/f/farima-fatahi-bayat/>Farima Fatahi Bayat</a>
|
<a href=/people/n/nikita-bhutani/>Nikita Bhutani</a>
|
<a href=/people/h/h-jagadish/>H. Jagadish</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--65><div class="card-body p-3 small">A major drawback of modern neural OpenIE systems and benchmarks is that they prioritize high coverage of information in extractions over compactness of their constituents. This severely limits the usefulness of OpenIE extractions in many downstream tasks. The utility of extractions can be improved if extractions are compact and share constituents. To this end, we study the problem of identifying compact extractions with neural-based methods. We propose CompactIE, an OpenIE system that uses a novel pipelined approach to produce compact extractions with overlapping constituents. It first detects constituents of the extractions and then links them to build extractions. We train our system on compact extractions obtained by processing existing benchmarks. Our experiments on CaRB and Wire57 datasets indicate that CompactIE finds 1.5x-2x more compact extractions than previous systems, with high precision, establishing a new state-of-the-art performance in OpenIE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.66.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.66.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.66.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.66" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.66/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>SI</span>m: Commonsense Reasoning for Counterfactual Scene Imagination</a></strong><br><a href=/people/h/hyounghun-kim/>Hyounghun Kim</a>
|
<a href=/people/a/abhay-zala/>Abhay Zala</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--66><div class="card-body p-3 small">As humans, we can modify our assumptions about a scene by imagining alternative objects or concepts in our minds. For example, we can easily anticipate the implications of the sun being overcast by rain clouds (e.g., the street will get wet) and accordingly prepare for that. In this paper, we introduce a new dataset called Commonsense Reasoning for Counterfactual Scene Imagination (CoSIm) which is designed to evaluate the ability of AI systems to reason about scene change imagination. To be specific, in this multimodal task/dataset, models are given an image and an initial question-response pair about the image. Next, a counterfactual imagined scene change (in textual form) is applied, and the model has to predict the new response to the initial question based on this scene change. We collect 3.5K high-quality and challenging data instances, with each instance consisting of an image, a commonsense question with a response, a description of a counterfactual change, a new response to the question, and three distractor responses. Our dataset contains various complex scene change types (such as object addition/removal/state change, event description, environment change, etc.) that require models to imagine many different scenarios and reason about the changed scenes. We present a baseline model based on a vision-language Transformer (i.e., LXMERT) and ablation studies. Through human evaluation, we demonstrate a large human-model performance gap, suggesting room for promising future work on this challenging, counterfactual multimodal task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.67.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.67.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.67" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.67/>Abstraction not Memory: <span class=acl-fixed-case>BERT</span> and the <span class=acl-fixed-case>E</span>nglish Article System</a></strong><br><a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a>
|
<a href=/people/d/dagmar-divjak/>Dagmar Divjak</a>
|
<a href=/people/p/petar-milin/>Petar Milin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--67><div class="card-body p-3 small">Article prediction is a task that has long defied accurate linguistic description. As such, this task is ideally suited to evaluate models on their ability to emulate native-speaker intuition. To this end, we compare the performance of native English speakers and pre-trained models on the task of article prediction set up as a three way choice (a/an, the, zero). Our experiments with BERT show that BERT outperforms humans on this task across all articles. In particular, BERT is far superior to humans at detecting the zero article, possibly because we insert them using rules that the deep neural model can easily pick up. More interestingly, we find that BERT tends to agree more with annotators than with the corpus when inter-annotator agreement is high but switches to agreeing more with the corpus as inter-annotator agreement drops. We contend that this alignment with annotators, despite being trained on the corpus, suggests that BERT is not memorising article use, but captures a high level generalisation of article use akin to human intuition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.68.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.68.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.68" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.68/><span class=acl-fixed-case>O</span>mni<span class=acl-fixed-case>T</span>ab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering</a></strong><br><a href=/people/z/zhengbao-jiang/>Zhengbao Jiang</a>
|
<a href=/people/y/yi-mao/>Yi Mao</a>
|
<a href=/people/p/pengcheng-he/>Pengcheng He</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--68><div class="card-body p-3 small">The information in tables can be an important complement to text, making table-based question answering (QA) systems of great value. The intrinsic complexity of handling tables often adds an extra burden to both model design and data annotation. In this paper, we aim to develop a simple table-based QA model with minimal annotation effort. Motivated by the fact that table-based QA requires both alignment between questions and tables and the ability to perform complicated reasoning over multiple table elements, we propose an omnivorous pretraining approach that consumes both natural and synthetic data to endow models with these respective abilities. Specifically, given freely available tables, we leverage retrieval to pair them with relevant natural sentences for mask-based pretraining, and synthesize NL questions by converting SQL sampled from tables for pretraining with a QA loss. We perform extensive experiments in both few-shot and full settings, and the results clearly demonstrate the superiority of our model OmniTab, with the best multitasking approach achieving an absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively, also establishing a new state-of-the-art on WikiTableQuestions. Detailed ablations and analyses reveal different characteristics of natural and synthetic data, shedding light on future directions in omnivorous pretraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.69.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.69.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.69" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.69/>Provably Confidential Language Modelling</a></strong><br><a href=/people/x/xuandong-zhao/>Xuandong Zhao</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/y/yu-xiang-wang/>Yu-Xiang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--69><div class="card-body p-3 small">Large language models are shown to memorize privacy information such as social security numbers in training data. Given the sheer scale of the training corpus, it is challenging to screen and filter these privacy data, either manually or automatically. In this paper, we propose Confidentially Redacted Training (CRT), a method to train language generation models while protecting the confidential segments. We borrow ideas from differential privacy (which solves a related but distinct problem) and show that our method is able to provably prevent unintended memorization by randomizing parts of the training process. Moreover, we show that redaction with an approximately correct screening policy amplifies the confidentiality guarantee. We implement the method for both LSTM and GPT language models. Our experimental results show that the models trained by CRT obtain almost the same perplexity while preserving strong confidentiality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.70.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.70.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.70" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.70/><span class=acl-fixed-case>KAT</span>: A Knowledge Augmented Transformer for Vision-and-Language</a></strong><br><a href=/people/l/liangke-gui/>Liangke Gui</a>
|
<a href=/people/b/borui-wang/>Borui Wang</a>
|
<a href=/people/q/qiuyuan-huang/>Qiuyuan Huang</a>
|
<a href=/people/a/alexander-g-hauptmann/>Alexander Hauptmann</a>
|
<a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--70><div class="card-body p-3 small">The primary focus of recent work with large-scale transformers has been on optimizing the amount of information packed into the model’s parameters. In this work, we ask a complementary question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used, and how the reasoning processes over implicit and explicit knowledge should be integrated. To address these challenges, we propose a - Knowledge Augmented Transformer (KAT) - which achieves a strong state-of-the-art result (+6% absolute) on the open-domain multimodal task of OK-VQA. Our approach integrates implicit and explicit knowledge in an encoder-decoder architecture, while still jointly reasoning over both knowledge sources during answer generation. Additionally, explicit knowledge integration improves interpretability of model predictions in our analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.71.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.71.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.71" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.71/>When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it</a></strong><br><a href=/people/s/sebastian-schuster/>Sebastian Schuster</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--71><div class="card-body p-3 small">Understanding longer narratives or participating in conversations requires tracking of discourse entities that have been mentioned. Indefinite noun phrases (NPs), such as ‘a dog’, frequently introduce discourse entities but this behavior is modulated by sentential operators such as negation. For example, ‘a dog’ in ‘Arthur doesn’t own a dog’ does not introduce a discourse entity due to the presence of negation. In this work, we adapt the psycholinguistic assessment of language models paradigm to higher-level linguistic phenomena and introduce an English evaluation suite that targets the knowledge of the interactions between sentential operators and indefinite NPs. We use this evaluation suite for a fine-grained investigation of the entity tracking abilities of the Transformer-based models GPT-2 and GPT-3. We find that while the models are to a certain extent sensitive to the interactions we investigate, they are all challenged by the presence of multiple NPs and their behavior is not systematic, which suggests that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.72.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.72.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--72 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.72 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.72.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.72.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.72" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.72/>On Curriculum Learning for Commonsense Reasoning</a></strong><br><a href=/people/a/adyasha-maharana/>Adyasha Maharana</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--72><div class="card-body p-3 small">Commonsense reasoning tasks follow a standard paradigm of finetuning pretrained language models on the target task data, where samples are introduced to the model in a random order during training. However, recent research suggests that data order can have a significant impact on the performance of finetuned models for natural language understanding. Hence, we examine the effect of a human-like easy-to-difficult curriculum during finetuning of language models for commonsense reasoning tasks. We use paced curriculum learning to rank data and sample training mini-batches with increasing levels of difficulty from the ranked dataset during finetuning. Further, we investigate the effect of an adaptive curriculum, i.e., the data ranking is dynamically updated during training based on the current state of the learner model. We use a teacher model to measure difficulty of each sample and experiment with three measures based on question answering probability, variability and out-of-distribution. To understand the effectiveness of curriculum learning in various scenarios, we apply it on full model fine-tuning as well as parameter-efficient prompt-tuning settings. Our results show that fixed as well as adaptive curriculum learning significantly improve performance for five commonsense reasoning tasks, i.e., SocialIQA, CosmosQA, CODAH, HellaSwag, WinoGrande in both tuning settings. Further, we find that prioritizing the difficult samples in the tail end of training improves generalization to unseen in-domain data as well as out-of-domain data. Our work provides evidence and encourages research into curriculum learning for commonsense reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.73.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.73.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.73/><span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>T</span>ime: A Document-level Temporal Dependency Graph Parser</a></strong><br><a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/v/vlad-morariu/>Vlad Morariu</a>
|
<a href=/people/v/verena-kaynig-fittkau/>Verena Kaynig-Fittkau</a>
|
<a href=/people/j/jiuxiang-gu/>Jiuxiang Gu</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/q/quan-hung-tran/>Quan Tran</a>
|
<a href=/people/a/ani-nenkova/>Ani Nenkova</a>
|
<a href=/people/d/dinesh-manocha/>Dinesh Manocha</a>
|
<a href=/people/r/rajiv-jain/>Rajiv Jain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--73><div class="card-body p-3 small">We introduce DocTime - a novel temporal dependency graph (TDG) parser that takes as input a text document and produces a temporal dependency graph. It outperforms previous BERT-based solutions by a relative 4-8% on three datasets from modeling the problem as a graph network with path-prediction loss to incorporate longer range dependencies. This work also demonstrates how the TDG graph can be used to improve the downstream tasks of temporal questions answering and NLI by a relative 4-10% with a new framework that incorporates the temporal dependency graph into the self-attention layer of Transformer models (Time-transformer). Finally, we develop and evaluate on a new temporal dependency graph dataset for the domain of contractual documents, which has not been previously explored in this setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.74.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.74.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.74" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.74/><span class=acl-fixed-case>F</span>act<span class=acl-fixed-case>PEGASUS</span>: Factuality-Aware Pre-training and Fine-tuning for Abstractive Summarization</a></strong><br><a href=/people/d/david-wan/>David Wan</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--74><div class="card-body p-3 small">We present FactPEGASUS, an abstractive summarization model that addresses the problem of factuality during pre-training and fine-tuning: (1) We augment the sentence selection strategy of PEGASUS’s (Zhang et al., 2019) pre-training objective to create pseudo-summaries that are both important and factual; (2) We introduce three complementary components for fine-tuning. The corrector removes hallucinations present in the reference summary, the contrastor uses contrastive learning to better differentiate nonfactual summaries from factual ones, and the connector bridges the gap between the pre-training and fine-tuning for better transfer of knowledge. Experiments on three downstream tasks demonstrate that FactPEGASUS substantially improves factuality evaluated by multiple automatic metrics and humans. Our thorough analysis suggests that FactPEGASUS is more factual than using the original pre-training objective in zero-shot and few-shot settings, retains factual behavior more robustly than strong baselines, and does not rely entirely on becoming more extractive to improve factuality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.75.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.75.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.75" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.75/><span class=acl-fixed-case>S</span>c<span class=acl-fixed-case>AN</span>: Suicide Attempt and Ideation Events Dataset</a></strong><br><a href=/people/b/bhanu-pratap-singh-rawat/>Bhanu Pratap Singh Rawat</a>
|
<a href=/people/s/samuel-kovaly/>Samuel Kovaly</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a>
|
<a href=/people/w/wilfred-pigeon/>Wilfred Pigeon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--75><div class="card-body p-3 small">Suicide is an important public health concern and one of the leading causes of death worldwide. Suicidal behaviors, including suicide attempts (SA) and suicide ideations (SI), are leading risk factors for death by suicide. Information related to patients’ previous and current SA and SI are frequently documented in the electronic health record (EHR) notes. Accurate detection of such documentation may help improve surveillance and predictions of patients’ suicidal behaviors and alert medical professionals for suicide prevention efforts. In this study, we first built Suicide Attempt and Ideation Events (ScAN) dataset, a subset of the publicly available MIMIC III dataset spanning over 12k+ EHR notes with 19k+ annotated SA and SI events information. The annotations also contain attributes such as method of suicide attempt. We also provide a strong baseline model ScANER (Suicide Attempt and Ideation Events Retriever), a multi-task RoBERTa-based model with a retrieval module to extract all the relevant suicidal behavioral evidences from EHR notes of an hospital-stay and, and a prediction module to identify the type of suicidal behavior (SA and SI) concluded during the patient’s stay at the hospital. ScANER achieved a macro-weighted F1-score of 0.83 for identifying suicidal behavioral evidences and a macro F1-score of 0.78 and 0.60 for classification of SA and SI for the patient’s hospital-stay, respectively. ScAN and ScANER are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.76.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.76.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.76.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.76" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.76/>Socially Aware Bias Measurements for <span class=acl-fixed-case>H</span>indi Language Representations</a></strong><br><a href=/people/v/vijit-malik/>Vijit Malik</a>
|
<a href=/people/s/sunipa-dev/>Sunipa Dev</a>
|
<a href=/people/a/akihiro-nishi/>Akihiro Nishi</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--76><div class="card-body p-3 small">Language representations are an efficient tool used across NLP, but they are strife with encoded societal biases. These biases are studied extensively, but with a primary focus on English language representations and biases common in the context of Western society. In this work, we investigate the biases present in Hindi language representations such as caste and religion associated biases. We demonstrate how biases are unique to specific language representations based on the history and culture of the region they are widely spoken in, and also how the same societal bias (such as binary gender associated biases) when investigated across languages is encoded by different words and text spans. With this work, we emphasize on the necessity of social-awareness along with linguistic and grammatical artefacts when modeling language representations, in order to understand the biases encoded.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.77.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.77.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.77" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.77/><span class=acl-fixed-case>A</span>mbi<span class=acl-fixed-case>P</span>un: Generating Humorous Puns with Ambiguous Context</a></strong><br><a href=/people/a/anirudh-mittal/>Anirudh Mittal</a>
|
<a href=/people/y/yufei-tian/>Yufei Tian</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--77><div class="card-body p-3 small">In this paper, we propose a simple yet effective way to generate pun sentences that does not require any training on existing puns. Our approach is inspired by humor theories that ambiguity comes from the context rather than the pun word itself. Given a pair of definitions of a pun word, our model first produces a list of related concepts through a reverse dictionary. We then utilize one-shot GPT3 to generate context words and then generate puns incorporating context words from both concepts. Human evaluation shows that our method successfully generates pun 52% of the time, outperforming well-crafted baselines and the state-of-the-art models by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.78.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.78.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--78 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.78 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.78.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.78" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.78/><span class=acl-fixed-case>E</span>mp<span class=acl-fixed-case>H</span>i: Generating Empathetic Responses with Human-like Intents</a></strong><br><a href=/people/m/mao-yan-chen/>Mao Yan Chen</a>
|
<a href=/people/s/siheng-li/>Siheng Li</a>
|
<a href=/people/y/yujiu-yang/>Yujiu Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--78><div class="card-body p-3 small">In empathetic conversations, humans express their empathy to others with empathetic intents. However, most existing empathetic conversational methods suffer from a lack of empathetic intents, which leads to monotonous empathy. To address the bias of the empathetic intents distribution between empathetic dialogue models and humans, we propose a novel model to generate empathetic responses with human-consistent empathetic intents, EmpHi for short. Precisely, EmpHi learns the distribution of potential empathetic intents with a discrete latent variable, then combines both implicit and explicit intent representation to generate responses with various empathetic intents. Experiments show that EmpHi outperforms state-of-the-art models in terms of empathy, relevance, and diversity on both automatic and human evaluation. Moreover, the case studies demonstrate the high interpretability and outstanding performance of our model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.79.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.79.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.79/>Yes, No or <span class=acl-fixed-case>IDK</span>: The Challenge of Unanswerable Yes/No Questions</a></strong><br><a href=/people/e/elior-sulem/>Elior Sulem</a>
|
<a href=/people/j/jamaal-hay/>Jamaal Hay</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--79><div class="card-body p-3 small">The Yes/No QA task (Clark et al., 2019) consists of “Yes” or “No” questions about a given context. However, in realistic scenarios, the information provided in the context is not always sufficient in order to answer the question. For example, given the context “She married a lawyer from New-York.”, we don’t know whether the answer to the question “Did she marry in New York?” is “Yes” or “No”. In this paper, we extend the Yes/No QA task, adding questions with an IDK answer, and show its considerable difficulty compared to the original 2-label task. For this purpose, we (i) enrich the BoolQ dataset (Clark et al., 2019) to include unanswerable questions and (ii) create out-of-domain test sets for the Yes/No/IDK QA task. We study the contribution of training on other Natural Language Understanding tasks. We focus in particular on Extractive QA (Rajpurkar et al., 2018) and Recognizing Textual Entailments (RTE; Dagan et al., 2013), analyzing the differences between 2 and 3 labels using the new data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.80.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.80.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.80" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.80/>Inducing and Using Alignments for Transition-based <span class=acl-fixed-case>AMR</span> Parsing</a></strong><br><a href=/people/a/andrew-drozdov/>Andrew Drozdov</a>
|
<a href=/people/j/jiawei-zhou/>Jiawei Zhou</a>
|
<a href=/people/r/radu-florian/>Radu Florian</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a>
|
<a href=/people/t/tahira-naseem/>Tahira Naseem</a>
|
<a href=/people/y/yoon-kim/>Yoon Kim</a>
|
<a href=/people/r/ramon-fernandez-astudillo/>Ramón Astudillo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--80><div class="card-body p-3 small">Transition-based parsers for Abstract Meaning Representation (AMR) rely on node-to-word alignments. These alignments are learned separately from parser training and require a complex pipeline of rule-based components, pre-processing, and post-processing to satisfy domain-specific constraints. Parsers also train on a point-estimate of the alignment pipeline, neglecting the uncertainty due to the inherent ambiguity of alignment. In this work we explore two avenues for overcoming these limitations. First, we propose a neural aligner for AMR that learns node-to-word alignments without relying on complex pipelines. We subsequently explore a tighter integration of aligner and parser training by considering a distribution over oracle action sequences arising from aligner uncertainty. Empirical results show this approach leads to more accurate alignments and generalization better from the AMR2.0 to AMR3.0 corpora. We attain a new state-of-the art for gold-only trained models, matching silver-trained performance without the need for beam search on AMR3.0.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.81.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.81.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.81" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.81/>Masked Part-Of-Speech Model: Does Modeling Long Context Help Unsupervised <span class=acl-fixed-case>POS</span>-tagging?</a></strong><br><a href=/people/x/xiang-zhou/>Xiang Zhou</a>
|
<a href=/people/s/shiyue-zhang/>Shiyue Zhang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--81><div class="card-body p-3 small">Previous Part-Of-Speech (POS) induction models usually assume certain independence assumptions (e.g., Markov, unidirectional, local dependency) that do not hold in real languages. For example, the subject-verb agreement can be both long-term and bidirectional. To facilitate flexible dependency modeling, we propose a Masked Part-of-Speech Model (MPoSM), inspired by the recent success of Masked Language Models (MLM). MPoSM can model arbitrary tag dependency and perform POS induction through the objective of masked POS reconstruction. We achieve competitive results on both the English Penn WSJ dataset as well as the universal treebank containing 10 diverse languages. Though modeling the long-term dependency should ideally help this task, our ablation study shows mixed trends in different languages. To better understand this phenomenon, we design a novel synthetic experiment that can specifically diagnose the model’s ability to learn tag agreement. Surprisingly, we find that even strong baselines fail to solve this problem consistently in a very simplified setting: the agreement between adjacent words. Nonetheless, MPoSM achieves overall better performance. Lastly, we conduct a detailed error analysis to shed light on other remaining challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.82.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.82.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.82" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.82/><span class=acl-fixed-case>DREAM</span>: Improving Situational <span class=acl-fixed-case>QA</span> by First Elaborating the Situation</a></strong><br><a href=/people/y/yuling-gu/>Yuling Gu</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--82><div class="card-body p-3 small">When people answer questions about a specific situation, e.g., “I cheated on my mid-term exam last week. Was that wrong?”, cognitive science suggests that they form a mental picture of that situation before answering. While we do not know how language models (LMs) answer such questions, we conjecture that they may answer more accurately if they are also provided with additional details about the question situation, elaborating the “scene”. To test this conjecture, we train a new model, DREAM, to answer questions that elaborate the scenes that situated questions are about, and then provide those elaborations as additional context to a question-answering (QA) model. We find that DREAM is able to create better scene elaborations (more accurate, useful, and consistent) than a representative state-of-the-art, zero-shot model (Macaw). We also find that using the scene elaborations as additional context improves the answer accuracy of a downstream QA system, including beyond that obtainable by simply further fine-tuning the QA system on DREAM’s training data. These results suggest that adding focused elaborations about a situation can improve a system’s reasoning about it, and may serve as an effective way of injecting new scenario-based knowledge into QA models. Finally, our approach is dataset-neutral; we observe improved QA performance across different models, with even bigger gains on models with fewer parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.83.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.83.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.83/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>S</span>e-Co: Text Conditioned Generative <span class=acl-fixed-case>C</span>ommon<span class=acl-fixed-case>S</span>ense Contextualizer</a></strong><br><a href=/people/r/rachit-bansal/>Rachit Bansal</a>
|
<a href=/people/m/milan-aggarwal/>Milan Aggarwal</a>
|
<a href=/people/s/sumit-bhatia/>Sumit Bhatia</a>
|
<a href=/people/j/jivat-kaur/>Jivat Kaur</a>
|
<a href=/people/b/balaji-krishnamurthy/>Balaji Krishnamurthy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--83><div class="card-body p-3 small">Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have leveraged structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to improve the scale at which knowledge can be obtained. However, training on symbolic KG entities limits their applicability in tasks involving natural language text where they ignore overall context. To mitigate this, we propose a CommonSense Contextualizer (CoSe-Co) conditioned on sentences as input to make it generically usable in tasks for generating knowledge relevant to the overall context of input text. To train CoSe-Co, we propose a novel dataset comprising of sentence and commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and contain novel entities not present in the underlying KG. We augment generated knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets. We also demonstrate its applicability in improving performance of a baseline model for paraphrase generation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.84.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.84.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.84" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.84/>Probing via Prompting</a></strong><br><a href=/people/j/jiaoda-li/>Jiaoda Li</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--84><div class="card-body p-3 small">Probing is a popular approach to understand what linguistic information is contained in the representations of pre-trained language models. However, the mechanism of selecting the probe model has recently been subject to intense debate, as it is not clear if the probes are merely extracting information or modelling the linguistic property themselves. To address this challenge, this paper introduces a novel model-free approach to probing via prompting, which formulates probing as a prompting task. We conduct experiments on five probing tasks and show that PP is comparable or better at extracting information than diagnostic probes while learning much less on its own. We further combine the probing via prompting approach with pruning to analyze where the model stores the linguistic information in its architecture. Finally, we apply the probing via prompting approach to examine the usefulness of a linguistic property for pre-training by removing the heads that are essential to it and evaluating the resulting model’s performance on language modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.85.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.85.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.85/>Database Search Results Disambiguation for Task-Oriented Dialog Systems</a></strong><br><a href=/people/k/kun-qian/>Kun Qian</a>
|
<a href=/people/s/satwik-kottur/>Satwik Kottur</a>
|
<a href=/people/a/ahmad-beirami/>Ahmad Beirami</a>
|
<a href=/people/s/shahin-shayandeh/>Shahin Shayandeh</a>
|
<a href=/people/p/paul-a-crook/>Paul Crook</a>
|
<a href=/people/a/alborz-geramifard/>Alborz Geramifard</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/c/chinnadhurai-sankar/>Chinnadhurai Sankar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--85><div class="card-body p-3 small">As task-oriented dialog systems are becoming increasingly popular in our lives, more realistic tasks have been proposed and explored. However, new practical challenges arise. For instance, current dialog systems cannot effectively handle multiplesearch results when querying a database, due to the lack of such scenarios in existing public datasets. In this paper, we propose Database Search Result (DSR) Disambiguation, a novel task that focuses on disambiguating database search results, which enhances user experience by allowing them to choose from multiple options instead of just one. To study this task, we augment the popular task-oriented dialog datasets (MultiWOZ and SGD) with turns that resolve ambiguities by (a) synthetically generating turns through a pre-defined grammar, and (b) collecting human paraphrases for a subset. We find that training on our augmented dialog data improves the model’s ability to deal with ambiguous scenarios, without sacrificing performance on unmodified turns. Furthermore, pre-fine tuning and multi-task learning help our model to improve performance on DSR-disambiguation even in the absence of in-domain data, suggesting that it can be learned as a universal dialog skill. Our data and code will be made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.86.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-main.86/>Unsupervised Slot Schema Induction for Task-oriented Dialog</a></strong><br><a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/m/mingqiu-wang/>Mingqiu Wang</a>
|
<a href=/people/y/yuan-cao/>Yuan Cao</a>
|
<a href=/people/i/izhak-shafran/>Izhak Shafran</a>
|
<a href=/people/l/laurent-shafey/>Laurent Shafey</a>
|
<a href=/people/h/hagen-soltau/>Hagen Soltau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--86><div class="card-body p-3 small">Carefully-designed schemas describing how to collect and annotate dialog corpora are a prerequisite towards building task-oriented dialog systems. In practical applications, manually designing schemas can be error-prone, laborious, iterative, and slow, especially when the schema is complicated. To alleviate this expensive and time consuming process, we propose an unsupervised approach for slot schema induction from unlabeled dialog corpora. Leveraging in-domain language models and unsupervised parsing structures, our data-driven approach extracts candidate slots without constraints, followed by coarse-to-fine clustering to induce slot types. We compare our method against several strong supervised baselines, and show significant performance improvement in slot schema induction on MultiWoz and SGD datasets. We also demonstrate the effectiveness of induced schemas on downstream applications including dialog state tracking and response generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.87.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.87.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.87/>Towards a Progression-Aware Autonomous Dialogue Agent</a></strong><br><a href=/people/a/abraham-sanders/>Abraham Sanders</a>
|
<a href=/people/t/tomek-strzalkowski/>Tomek Strzalkowski</a>
|
<a href=/people/m/mei-si/>Mei Si</a>
|
<a href=/people/a/albert-chang/>Albert Chang</a>
|
<a href=/people/d/deepanshu-dey/>Deepanshu Dey</a>
|
<a href=/people/j/jonas-braasch/>Jonas Braasch</a>
|
<a href=/people/d/dakuo-wang/>Dakuo Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--87><div class="card-body p-3 small">Recent advances in large-scale language modeling and generation have enabled the creation of dialogue agents that exhibit human-like responses in a wide range of conversational scenarios spanning a diverse set of tasks, from general chit-chat to focused goal-oriented discourse. While these agents excel at generating high-quality responses that are relevant to prior context, they suffer from a lack of awareness of the overall direction in which the conversation is headed, and the likelihood of task success inherent therein. Thus, we propose a framework in which dialogue agents can evaluate the progression of a conversation toward or away from desired outcomes, and use this signal to inform planning for subsequent responses. Our framework is composed of three key elements: (1) the notion of a “global” dialogue state (GDS) space, (2) a task-specific progression function (PF) computed in terms of a conversation’s trajectory through this space, and (3) a planning mechanism based on dialogue rollouts by which an agent may use progression signals to select its next response.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.88.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.88.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.88" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.88/>Cross-Domain Detection of <span class=acl-fixed-case>GPT</span>-2-Generated Technical Text</a></strong><br><a href=/people/j/juan-diego-rodriguez/>Juan Diego Rodriguez</a>
|
<a href=/people/t/todd-hay/>Todd Hay</a>
|
<a href=/people/d/david-gros/>David Gros</a>
|
<a href=/people/z/zain-shamsi/>Zain Shamsi</a>
|
<a href=/people/r/ravi-srinivasan/>Ravi Srinivasan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--88><div class="card-body p-3 small">Machine-generated text presents a potential threat not only to the public sphere, but also to the scientific enterprise, whereby genuine research is undermined by convincing, synthetic text. In this paper we examine the problem of detecting GPT-2-generated technical research text. We first consider the realistic scenario where the defender does not have full information about the adversary’s text generation pipeline, but is able to label small amounts of in-domain genuine and synthetic text in order to adapt to the target distribution. Even in the extreme scenario of adapting a physics-domain detector to a biomedical detector, we find that only a few hundred labels are sufficient for good performance. Finally, we show that paragraph-level detectors can be used to detect the tampering of full-length documents under a variety of threat models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.89.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.89.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.89/><span class=acl-fixed-case>DISAPERE</span>: A Dataset for Discourse Structure in Peer Review Discussions</a></strong><br><a href=/people/n/neha-kennard/>Neha Kennard</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/akshay-sharma/>Akshay Sharma</a>
|
<a href=/people/c/chhandak-bagchi/>Chhandak Bagchi</a>
|
<a href=/people/m/matthew-clinton/>Matthew Clinton</a>
|
<a href=/people/p/pranay-kumar-yelugam/>Pranay Kumar Yelugam</a>
|
<a href=/people/h/hamed-zamani/>Hamed Zamani</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--89><div class="card-body p-3 small">At the foundation of scientific evaluation is the labor-intensive process of peer review. This critical task requires participants to consume vast amounts of highly technical text. Prior work has annotated different aspects of review argumentation, but discourse relations between reviews and rebuttals have yet to be examined. We present DISAPERE, a labeled dataset of 20k sentences contained in 506 review-rebuttal pairs in English, annotated by experts. DISAPERE synthesizes label sets from prior work and extends them to include fine-grained annotation of the rebuttal sentences, characterizing their context in the review and the authors’ stance towards review arguments. Further, we annotate <i>every</i> review and rebuttal sentence. We show that discourse cues from rebuttals can shed light on the quality and interpretation of reviews. Further, an understanding of the argumentative strategies employed by the reviewers and authors provides useful signal for area chairs and other decision makers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.90.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.90.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.90" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.90/><span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>S</span>pan<span class=acl-fixed-case>QA</span>: A Dataset for Multi-Span Question Answering</a></strong><br><a href=/people/h/haonan-li/>Haonan Li</a>
|
<a href=/people/m/martin-tomko/>Martin Tomko</a>
|
<a href=/people/m/maria-vasardani/>Maria Vasardani</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--90><div class="card-body p-3 small">Most existing reading comprehension datasets focus on single-span answers, which can be extracted as a single contiguous span from a given text passage. Multi-span questions, i.e., questions whose answer is a series of multiple discontiguous spans in the text, are common real life but are less studied. In this paper, we present MultiSpanQA, a new dataset that focuses on multi-span questions. Raw questions and contexts are extracted from the Natural Questions dataset. After multi-span re-annotation, MultiSpanQA consists of over a total of 6,000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version. We introduce new metrics for the purposes of multi-span question answering evaluation, and establish several baselines using advanced models. Finally, we propose a new model which beats all baselines and achieves state-of-the-art on our dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.91.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.91.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.91/>Context-Aware Abbreviation Expansion Using Large Language Models</a></strong><br><a href=/people/s/shanqing-cai/>Shanqing Cai</a>
|
<a href=/people/s/subhashini-venugopalan/>Subhashini Venugopalan</a>
|
<a href=/people/k/katrin-tomanek/>Katrin Tomanek</a>
|
<a href=/people/a/ajit-narayanan/>Ajit Narayanan</a>
|
<a href=/people/m/meredith-morris/>Meredith Morris</a>
|
<a href=/people/m/michael-brenner/>Michael Brenner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--91><div class="card-body p-3 small">Motivated by the need for accelerating text entry in augmentative and alternative communication (AAC) for people with severe motor impairments, we propose a paradigm in which phrases are abbreviated aggressively as primarily word-initial letters. Our approach is to expand the abbreviations into full-phrase options by leveraging conversation context with the power of pretrained large language models (LLMs). Through zero-shot, few-shot, and fine-tuning experiments on four public conversation datasets, we show that for replies to the initial turn of a dialog, an LLM with 64B parameters is able to exactly expand over 70% of phrases with abbreviation length up to 10, leading to an effective keystroke saving rate of up to about 77% on these exact expansions. Including a small amount of context in the form of a single conversation turn more than doubles abbreviation expansion accuracies compared to having no context, an effect that is more pronounced for longer phrases. Additionally, the robustness of models against typo noise can be enhanced through fine-tuning on noisy data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.92.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.92.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.92" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.92/>Theory-Grounded Measurement of <span class=acl-fixed-case>U</span>.<span class=acl-fixed-case>S</span>. Social Stereotypes in <span class=acl-fixed-case>E</span>nglish Language Models</a></strong><br><a href=/people/y/yang-trista-cao/>Yang Trista Cao</a>
|
<a href=/people/a/anna-sotnikova/>Anna Sotnikova</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/l/linda-zou/>Linda Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--92><div class="card-body p-3 small">NLP models trained on text have been shown to reproduce human stereotypes, which can magnify harms to marginalized groups when systems are deployed at scale. We adapt the Agency-Belief-Communion (ABC) stereotype model of Koch et al. (2016) from social psychology as a framework for the systematic study and discovery of stereotypic group-trait associations in language models (LMs). We introduce the sensitivity test (SeT) for measuring stereotypical associations from language models. To evaluate SeT and other measures using the ABC model, we collect group-trait judgments from U.S.-based subjects to compare with English LM stereotypes. Finally, we extend this framework to measure LM stereotyping of intersectional identities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.93.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.93.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.93/>Sort by Structure: Language Model Ranking as Dependency Probing</a></strong><br><a href=/people/m/max-muller-eberstein/>Max Müller-Eberstein</a>
|
<a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--93><div class="card-body p-3 small">Making an informed choice of pre-trained language model (LM) is critical for performance, yet environmentally costly, and as such widely underexplored. The field of Computer Vision has begun to tackle encoder ranking, with promising forays into Natural Language Processing, however they lack coverage of linguistic tasks such as structured prediction. We propose probing to rank LMs, specifically for parsing dependencies in a given language, by measuring the degree to which labeled trees are recoverable from an LM’s contextualized embeddings. Across 46 typologically and architecturally diverse LM-language pairs, our probing approach predicts the best LM choice 79% of the time using orders of magnitude less compute than training a full parser. Within this study, we identify and analyze one recently proposed decoupled LM—RemBERT—and find it strikingly contains less inherent dependency information, but often yields the best parser after full fine-tuning. Without this outlier our approach identifies the best LM in 89% of cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.94.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.94.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.94/>Quantifying Synthesis and Fusion and their Impact on Machine Translation</a></strong><br><a href=/people/a/arturo-oncevay/>Arturo Oncevay</a>
|
<a href=/people/d/duygu-ataman/>Duygu Ataman</a>
|
<a href=/people/n/niels-van-berkel/>Niels Van Berkel</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--94><div class="card-body p-3 small">Theoretical work in morphological typology offers the possibility of measuring morphological diversity on a continuous scale. However, literature in Natural Language Processing (NLP) typically labels a whole language with a strict type of morphology, e.g. fusional or agglutinative. In this work, we propose to reduce the rigidity of such claims, by quantifying morphological typology at the word and segment level. We consider Payne (2017)’s approach to classify morphology using two indices: synthesis (e.g. analytic to polysynthetic) and fusion (agglutinative to fusional). For computing synthesis, we test unsupervised and supervised morphological segmentation methods for English, German and Turkish, whereas for fusion, we propose a semi-automatic method using Spanish as a case study. Then, we analyse the relationship between machine translation quality and the degree of synthesis and fusion at word (nouns and verbs for English-Turkish, and verbs in English-Spanish) and segment level (previous language pairs plus English-German in both directions). We complement the word-level analysis with human evaluation, and overall, we observe a consistent impact of both indexes on machine translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.95.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.95.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.95.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.95" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.95/>Commonsense and Named Entity Aware Knowledge Grounded Dialogue Generation</a></strong><br><a href=/people/d/deeksha-varshney/>Deeksha Varshney</a>
|
<a href=/people/a/akshara-prabhakar/>Akshara Prabhakar</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--95><div class="card-body p-3 small">Grounding dialogue on external knowledge and interpreting linguistic patterns in dialogue history context, such as ellipsis, anaphora, and co-reference is critical for dialogue comprehension and generation. In this paper, we present a novel open-domain dialogue generation model which effectively utilizes the large-scale commonsense and named entity based knowledge in addition to the unstructured topic-specific knowledge associated with each utterance. We enhance the commonsense knowledge with named entity-aware structures using co-references. Our proposed model utilizes a multi-hop attention layer to preserve the most accurate and critical parts of the dialogue history and the associated knowledge. In addition, we employ a Commonsense and Named Entity Enhanced Attention Module, which starts with the extracted triples from various sources and gradually finds the relevant supporting set of triples using multi-hop attention with the query vector obtained from the interactive dialogue-knowledge module. Empirical results on two benchmark datasets demonstrate that our model significantly outperforms the state-of-the-art methods in terms of both automatic evaluation metrics and human judgment. Our code is publicly available at <a href=https://github.com/deekshaVarshney/CNTF class=acl-markup-url>https://github.com/deekshaVarshney/CNTF</a>; <a href=https://www.iitp.ac.in/-ai-nlp-ml/resources/codes/CNTF.zip class=acl-markup-url>https://www.iitp.ac.in/-ai-nlp-ml/resources/codes/CNTF.zip</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.96.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.96.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.96" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.96/>Efficient Hierarchical Domain Adaptation for Pretrained Language Models</a></strong><br><a href=/people/a/alexandra-chronopoulou/>Alexandra Chronopoulou</a>
|
<a href=/people/m/matthew-e-peters/>Matthew Peters</a>
|
<a href=/people/j/jesse-dodge/>Jesse Dodge</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--96><div class="card-body p-3 small">The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.97.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.97.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.97" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.97/><span class=acl-fixed-case>H</span>atemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-Based Hate</a></strong><br><a href=/people/h/hannah-kirk/>Hannah Kirk</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/p/paul-rottger/>Paul Rottger</a>
|
<a href=/people/t/tristan-thrush/>Tristan Thrush</a>
|
<a href=/people/s/scott-hale/>Scott Hale</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--97><div class="card-body p-3 small">Detecting online hate is a complex task, and low-performing models have harmful consequences when used for sensitive applications such as content moderation. Emoji-based hate is an emerging challenge for automated detection. We present HatemojiCheck, a test suite of 3,930 short-form statements that allows us to evaluate performance on hateful language expressed with emoji. Using the test suite, we expose weaknesses in existing hate detection models. To address these weaknesses, we create the HatemojiBuild dataset using a human-and-model-in-the-loop approach. Models built with these 5,912 adversarial examples perform substantially better at detecting emoji-based hate, while retaining strong performance on text-only hate. Both HatemojiCheck and HatemojiBuild are made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.98.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.98.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.98/>On the Economics of Multilingual Few-shot Learning: Modeling the Cost-Performance Trade-offs of Machine Translated and Manual Data</a></strong><br><a href=/people/k/kabir-ahuja/>Kabir Ahuja</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/s/sandipan-dandapat/>Sandipan Dandapat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--98><div class="card-body p-3 small">Borrowing ideas from Production functions in micro-economics, in this paper we introduce a framework to systematically evaluate the performance and cost trade-offs between machine-translated and manually-created labelled data for task-specific fine-tuning of massively multilingual language models. We illustrate the effectiveness of our framework through a case-study on the TyDIQA-GoldP dataset. One of the interesting conclusion of the study is that if the cost of machine translation is greater than zero, the optimal performance at least cost is always achieved with at least some or only manually-created data. To our knowledge, this is the first attempt towards extending the concept of production functions to study data collection strategies for training multilingual models, and can serve as a valuable tool for other similar cost vs data trade-offs in NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.99.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.99.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.99/>Learning to Selectively Learn for Weakly Supervised Paraphrase Generation with Model-based Reinforcement Learning</a></strong><br><a href=/people/h/haiyan-yin/>Haiyan Yin</a>
|
<a href=/people/d/dingcheng-li/>Dingcheng Li</a>
|
<a href=/people/p/ping-li/>Ping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--99><div class="card-body p-3 small">Paraphrase generation is an important language generation task attempting to interpret user intents and systematically generate new phrases of identical meanings to the given ones. However, the effectiveness of paraphrase generation is constrained by the access to the golden labeled data pairs where both the amount and the quality of the training data pairs are restricted. In this paper, we propose a new weakly supervised paraphrase generation approach that extends the success of a recent work that leverages reinforcement learning for effective model training with data selection. While data selection is privileged for the target task which has noisy data, developing a reinforced selective learning regime faces several unresolved challenges. In this paper, we carry on important discussions about the above problem and present a new model that could partially overcome the discussed issues with a model-based planning feature and a reward normalization feature. We perform extensive evaluation on four weakly supervised paraphrase generation tasks where the results show that our method could significantly improve the state-of-the-art performance on the evaluation datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.100.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--100 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.100.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.100" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.100/>Quality-Aware Decoding for Neural Machine Translation</a></strong><br><a href=/people/p/patrick-fernandes/>Patrick Fernandes</a>
|
<a href=/people/a/antonio-farinhas/>António Farinhas</a>
|
<a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/j/jose-g-c-de-souza/>José G. C. de Souza</a>
|
<a href=/people/p/perez-ogayo/>Perez Ogayo</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/a/andre-f-t-martins/>Andre Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--100><div class="card-body p-3 small">Despite the progress in machine translation quality estimation and evaluation in the last years, decoding in neural machine translation (NMT) is mostly oblivious to this and centers around finding the most probable translation according to the model (MAP decoding), approximated with beam search. In this paper, we bring together these two lines of research and propose <i>quality-aware decoding</i> for NMT, by leveraging recent breakthroughs in reference-free and reference-based MT evaluation through various inference methods like <span class=tex-math>N</span>-best reranking and minimum Bayes risk decoding. We perform an extensive comparison of various possible candidate generation and ranking methods across four datasets and two model classes and find that quality-aware decoding consistently outperforms MAP-based decoding according both to state-of-the-art automatic metrics (COMET and BLEURT) and to human assessments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.101.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.101.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.101/>Pretrained Models for Multilingual Federated Learning</a></strong><br><a href=/people/o/orion-weller/>Orion Weller</a>
|
<a href=/people/m/marc-marone/>Marc Marone</a>
|
<a href=/people/v/vladimir-braverman/>Vladimir Braverman</a>
|
<a href=/people/d/dawn-lawrie/>Dawn Lawrie</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--101><div class="card-body p-3 small">Since the advent of Federated Learning (FL), research has applied these methods to natural language processing (NLP) tasks. Despite a plethora of papers in FL for NLP, no previous works have studied how multilingual text impacts FL algorithms. Furthermore, multilingual text provides an interesting avenue to examine the impact of non-IID text (e.g. different languages) on FL in naturally occurring data. We explore three multilingual language tasks, language modeling, machine translation, and text classification using differing federated and non-federated learning algorithms. Our results show that using pretrained models reduces the negative effects of FL, helping them to perform near or better than centralized (no privacy) learning, even when using non-IID partitioning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.102.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.102.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.102/><span class=acl-fixed-case>A</span>c<span class=acl-fixed-case>T</span>une: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models</a></strong><br><a href=/people/y/yue-yu/>Yue Yu</a>
|
<a href=/people/l/lingkai-kong/>Lingkai Kong</a>
|
<a href=/people/j/jieyu-zhang/>Jieyu Zhang</a>
|
<a href=/people/r/rongzhi-zhang/>Rongzhi Zhang</a>
|
<a href=/people/c/chao-zhang/>Chao Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--102><div class="card-body p-3 small">Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop AcTune, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training. AcTune switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from low-uncertainty regions are used for model self-training. Additionally, we design (1) a region-aware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model’s pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2% on average. Our implementation is available at <a href=https://github.com/yueyu1030/actune class=acl-markup-url>https://github.com/yueyu1030/actune</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.103.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.103.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.103/>Label Anchored Contrastive Learning for Language Understanding</a></strong><br><a href=/people/z/zhenyu-zhang/>Zhenyu Zhang</a>
|
<a href=/people/y/yuming-zhao/>Yuming Zhao</a>
|
<a href=/people/m/meng-chen/>Meng Chen</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--103><div class="card-body p-3 small">Contrastive learning (CL) has achieved astonishing progress in computer vision, speech, and natural language processing fields recently with self-supervised learning. However, CL approach to the supervised setting is not fully explored, especially for the natural language understanding classification task. Intuitively, the class label itself has the intrinsic ability to perform hard positive/negative mining, which is crucial for CL. Motivated by this, we propose a novel label anchored contrastive learning approach (denoted as LaCon) for language understanding. Specifically, three contrastive objectives are devised, including a multi-head instance-centered contrastive loss (ICL), a label-centered contrastive loss (LCL), and a label embedding regularizer (LER). Our approach does not require any specialized network architecture or any extra data augmentation, thus it can be easily plugged into existing powerful pre-trained language models. Compared to the state-of-the-art baselines, LaCon obtains up to 4.1% improvement on the popular datasets of GLUE and CLUE benchmarks. Besides, LaCon also demonstrates significant advantages under the few-shot and data imbalance settings, which obtains up to 9.4% improvement on the FewGLUE and FewCLUE benchmarking tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.104.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.104.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.104.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.104/>Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts</a></strong><br><a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/h/hong-chen/>Hong Chen</a>
|
<a href=/people/y/yufei-tian/>Yufei Tian</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--104><div class="card-body p-3 small">Stories or narratives are comprised of a sequence of events. To compose interesting stories, professional writers often leverage a creative writing technique called *flashback* that inserts past events into current storylines as we commonly observe in novels and plays. However, it is challenging for machines to generate *flashback* as it requires a solid understanding of event **temporal order** (e.g. *feeling hungry* before *eat*, not vice versa), and the creativity to arrange storylines so that earlier events do not always appear first in **narrative order**. Two major issues in existing systems that exacerbate the challenges: 1) temporal bias in pertaining and story datasets that leads to monotonic event temporal orders; 2) lack of explicit guidance that helps machines decide where to insert *flashbacks*. We propose to address these issues using structured storylines to encode events and their pair-wise temporal relations (before, after and vague) as **temporal prompts** that guide how stories should unfold temporally. We leverage a Plan-and-Write framework enhanced by reinforcement learning to generate storylines and stories end-to-end. Evaluation results show that the proposed method can generate more interesting stories with *flashbacks* while maintaining textual diversity, fluency, and temporal coherence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.105.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.105.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.105/>Forecasting <span class=acl-fixed-case>COVID</span>-19 Caseloads Using Unsupervised Embedding Clusters of Social Media Posts</a></strong><br><a href=/people/f/felix-drinkall/>Felix Drinkall</a>
|
<a href=/people/s/stefan-zohren/>Stefan Zohren</a>
|
<a href=/people/j/janet-pierrehumbert/>Janet Pierrehumbert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--105><div class="card-body p-3 small">We present a novel approach incorporating transformer-based language models into infectious disease modelling. Text-derived features are quantified by tracking high-density clusters of sentence-level representations of Reddit posts within specific US states’ COVID-19 subreddits. We benchmark these clustered embedding features against features extracted from other high-quality datasets. In a threshold-classification task, we show that they outperform all other feature types at predicting upward trend signals, a significant result for infectious disease modelling in areas where epidemiological data is unreliable. Subsequently, in a time-series forecasting task, we fully utilise the predictive power of the caseload and compare the relative strengths of using different supplementary datasets as covariate feature sets in a transformer-based time-series model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.106.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.106.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.106.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.106" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.106/>Many Hands Make Light Work: Using Essay Traits to Automatically Score Essays</a></strong><br><a href=/people/r/rahul-kumar/>Rahul Kumar</a>
|
<a href=/people/s/sandeep-mathias/>Sandeep Mathias</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--106><div class="card-body p-3 small">Most research in the area of automatic essay grading (AEG) is geared towards scoring the essay <i>holistically</i> while there has also been little work done on scoring individual essay traits. In this paper, we describe a way to score essays using a multi-task learning (MTL) approach, where scoring the essay holistically is the primary task, and scoring the essay traits is the auxiliary task. We compare our results with a single-task learning (STL) approach, using both LSTMs and BiLSTMs. To find out which traits work best for different types of essays, we conduct ablation tests for each of the essay traits. We also report the runtime and number of training parameters for each system. We find that MTL-based BiLSTM system gives the best results for scoring the essay holistically, as well as performing well on scoring the essay traits. The MTL systems also give a speed-up of between <b>2.30</b> to <b>3.70</b> times the speed of the STL system, when it comes to scoring the essay and all the traits.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.107.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--107 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.107.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.107/>Natural Language Inference with Self-Attention for Veracity Assessment of Pandemic Claims</a></strong><br><a href=/people/m/miguel-arana-catania/>Miguel Arana-Catania</a>
|
<a href=/people/e/elena-kochkina/>Elena Kochkina</a>
|
<a href=/people/a/arkaitz-zubiaga/>Arkaitz Zubiaga</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a>
|
<a href=/people/r/robert-procter/>Robert Procter</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--107><div class="card-body p-3 small">We present a comprehensive work on automated veracity assessment from dataset creation to developing novel methods based on Natural Language Inference (NLI), focusing on misinformation related to the COVID-19 pandemic. We first describe the construction of the novel PANACEA dataset consisting of heterogeneous claims on COVID-19 and their respective information sources. The dataset construction includes work on retrieval techniques and similarity measurements to ensure a unique set of claims. We then propose novel techniques for automated veracity assessment based on Natural Language Inference including graph convolutional networks and attention based approaches. We have carried out experiments on evidence retrieval and veracity assessment on the dataset using the proposed techniques and found them competitive with SOTA methods, and provided a detailed discussion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.108.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.108.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.108/>Beyond Emotion: A Multi-Modal Dataset for Human Desire Understanding</a></strong><br><a href=/people/a/ao-jia/>Ao Jia</a>
|
<a href=/people/y/yu-he/>Yu He</a>
|
<a href=/people/y/yazhou-zhang/>Yazhou Zhang</a>
|
<a href=/people/s/sagar-uprety/>Sagar Uprety</a>
|
<a href=/people/d/dawei-song/>Dawei Song</a>
|
<a href=/people/c/christina-lioma/>Christina Lioma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--108><div class="card-body p-3 small">Desire is a strong wish to do or have something, which involves not only a linguistic expression, but also underlying cognitive phenomena driving human feelings. As the most primitive and basic human instinct, conscious desire is often accompanied by a range of emotional responses. As a strikingly understudied task, it is difficult for machines to model and understand desire due to the unavailability of benchmarking datasets with desire and emotion labels. To bridge this gap, we present MSED, the first multi-modal and multi-task sentiment, emotion and desire dataset, which contains 9,190 text-image pairs, with English text. Each multi-modal sample is annotated with six desires, three sentiments and six emotions. We also propose the state-of-the-art baselines to evaluate the potential of MSED and show the importance of multi-task and multi-modal clues for desire understanding. We hope this study provides a benchmark for human desire analysis. MSED will be publicly available for research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.109.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.109.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.109/>Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction</a></strong><br><a href=/people/j/jiaxin-yu/>Jiaxin Yu</a>
|
<a href=/people/d/deqing-yang/>Deqing Yang</a>
|
<a href=/people/s/shuyu-tian/>Shuyu Tian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--109><div class="card-body p-3 small">Compared with traditional sentence-level relation extraction, document-level relation extraction is a more challenging task where an entity in a document may be mentioned multiple times and associated with multiple relations. However, most methods of document-level relation extraction do not distinguish between mention-level features and entity-level features, and just apply simple pooling operation for aggregating mention-level features into entity-level features. As a result, the distinct semantics between the different mentions of an entity are overlooked. To address this problem, we propose RSMAN in this paper which performs selective attentions over different entity mentions with respect to candidate relations. In this manner, the flexible and relation-specific representations of entities are obtained which indeed benefit relation classification. Our extensive experiments upon two benchmark datasets show that our RSMAN can bring significant improvements for some backbone models to achieve state-of-the-art performance, especially when an entity have multiple mentions in the document.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.110.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.110.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.110" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.110/><span class=acl-fixed-case>T</span>witter-<span class=acl-fixed-case>COMM</span>s: Detecting Climate, <span class=acl-fixed-case>COVID</span>, and Military Multimodal Misinformation</a></strong><br><a href=/people/g/giscard-biamby/>Giscard Biamby</a>
|
<a href=/people/g/grace-luo/>Grace Luo</a>
|
<a href=/people/t/trevor-darrell/>Trevor Darrell</a>
|
<a href=/people/a/anna-rohrbach/>Anna Rohrbach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--110><div class="card-body p-3 small">Detecting out-of-context media, such as “miscaptioned” images on Twitter, is a relevant problem, especially in domains of high public significance. In this work we aim to develop defenses against such misinformation for the topics of Climate Change, COVID-19, and Military Vehicles. We first present a large-scale multimodal dataset with over 884k tweets relevant to these topics. Next, we propose a detection method, based on the state-of-the-art CLIP model, that leverages automatically generated hard image-text mismatches. While this approach works well on our automatically constructed out-of-context tweets, we aim to validate its usefulness on data representative of the real world. Thus, we test it on a set of human-generated fakes, created by mimicking in-the-wild misinformation. We achieve an 11% detection improvement in a high precision regime over a strong baseline. Finally, we share insights about our best model design and analyze the challenges of this emerging threat.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.111.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.111.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.111/><span class=acl-fixed-case>BlonDe</span>: An Automatic Evaluation Metric for Document-level Machine Translation</a></strong><br><a href=/people/y/yuchen-jiang/>Yuchen Jiang</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/j/jian-yang/>Jian Yang</a>
|
<a href=/people/h/haoyang-huang/>Haoyang Huang</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--111><div class="card-body p-3 small">Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT evaluation. They can neither distinguish document-level improvements in translation quality from sentence-level ones, nor identify the discourse phenomena that cause context-agnostic translations. This paper introduces a novel automatic metric BlonDe to widen the scope of automatic MT evaluation from sentence to document level. BlonDe takes discourse coherence into consideration by categorizing discourse-related spans and calculating the similarity-based F1 measure of categorized spans. We conduct extensive comparisons on a newly constructed dataset BWB. The experimental results show that BlonDe possesses better selectivity and interpretability at the document-level, and is more sensitive to document-level nuances. In a large-scale human study, BlonDe also achieves significantly higher Pearson’s r correlation with human judgments compared to previous metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.112.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.112.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.112/>Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media</a></strong><br><a href=/people/l/lixing-zhu/>Lixing Zhu</a>
|
<a href=/people/z/zheng-fang/>Zheng Fang</a>
|
<a href=/people/g/gabriele-pergola/>Gabriele Pergola</a>
|
<a href=/people/r/robert-procter/>Robert Procter</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--112><div class="card-body p-3 small">Building models to detect vaccine attitudes on social media is challenging because of the composite, often intricate aspects involved, and the limited availability of annotated data. Existing approaches have relied heavily on supervised training that requires abundant annotations and pre-defined aspect categories. Instead, with the aim of leveraging the large amount of unannotated data now available on vaccination, we propose a novel semi-supervised approach for vaccine attitude detection, called VADet. A variational autoencoding architecture based on language models is employed to learn from unlabelled data the topical information of the domain. Then, the model is fine-tuned with a few manually annotated examples of user attitudes. We validate the effectiveness of VADet on our annotated data and also on an existing vaccination corpus annotated with opinions on vaccines. Our results show that VADet is able to learn disentangled stance and aspect topics, and outperforms existing aspect-based sentiment analysis models on both stance detection and tweet clustering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.113.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.113.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.113/><span class=acl-fixed-case>SKILL</span>: Structured Knowledge Infusion for Large Language Models</a></strong><br><a href=/people/f/fedor-moiseev/>Fedor Moiseev</a>
|
<a href=/people/z/zhe-dong/>Zhe Dong</a>
|
<a href=/people/e/enrique-alfonseca/>Enrique Alfonseca</a>
|
<a href=/people/m/martin-jaggi/>Martin Jaggi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--113><div class="card-body p-3 small">Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size KG, WikiMovies, we saw 3x improvement of exact match score on MetaQA task. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.114.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.114.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.114/>Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models</a></strong><br><a href=/people/k/karolina-stanczak/>Karolina Stanczak</a>
|
<a href=/people/e/edoardo-ponti/>Edoardo Ponti</a>
|
<a href=/people/l/lucas-torroba-hennigen/>Lucas Torroba Hennigen</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--114><div class="card-body p-3 small">The success of multilingual pre-trained models is underpinned by their ability to learn representations shared by multiple languages even in absence of any explicit supervision. However, it remains unclear how these models learn to generalise across languages. In this work, we conjecture that multilingual pre-trained models can derive language-universal abstractions about grammar. In particular, we investigate whether morphosyntactic information is encoded in the same subset of neurons in different languages. We conduct the first large-scale empirical study over 43 languages and 14 morphosyntactic categories with a state-of-the-art neuron-level probe. Our findings show that the cross-lingual overlap between neurons is significant, but its extent may vary across categories and depends on language proximity and pre-training data size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.115.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.115.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.115/>Aspect Is Not You Need: No-aspect Differential Sentiment Framework for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/j/jiahao-cao/>Jiahao Cao</a>
|
<a href=/people/r/rui-liu/>Rui Liu</a>
|
<a href=/people/h/huailiang-peng/>Huailiang Peng</a>
|
<a href=/people/l/lei-jiang/>Lei Jiang</a>
|
<a href=/people/x/xu-bai/>Xu Bai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--115><div class="card-body p-3 small">Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Most recent efforts adopt pre-trained model to classify the sentences with aspects. However, the aspect sentiment bias from pre-trained model brings some noise to the ABSA task. Besides, traditional methods using cross-entropy loss are hard to find the potential associations between sentiment polarities. In this work, we analyze the ABSA task from a novel cognition perspective: humans can often judge the sentiment of an aspect even if they do not know what the aspect is. Moreover, it is easier to distinguish positive and negative sentiments than others for human beings because positive and negative are two opposite sentiments. To this end, we propose a no-aspect differential sentiment (NADS) framework for the ABSA task. We first design a no-aspect template by replacing the aspect with a special unbiased character to eliminate the sentiment bias and obtain a stronger representation. To better get the benefits from the template, we adopt contrastive learning between the no-aspect template and the original sentence. Then we propose a differential sentiment loss instead of the cross-entropy loss to better classify the sentiments by distinguishing the different distances between sentiments. Our proposed model is a general framework and can be combined with almost all traditional ABSA methods. Experiments on SemEval 2014 show that our framework is still able to predict the sentiment of the aspect even we don’t konw what the aspect is. Moreover, our NADS framework boosts three typical ABSA methods and achieves state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.116.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.116.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.116" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.116/><span class=acl-fixed-case>M</span>o<span class=acl-fixed-case>EBERT</span>: from <span class=acl-fixed-case>BERT</span> to Mixture-of-Experts via Importance-Guided Adaptation</a></strong><br><a href=/people/s/simiao-zuo/>Simiao Zuo</a>
|
<a href=/people/q/qingru-zhang/>Qingru Zhang</a>
|
<a href=/people/c/chen-liang/>Chen Liang</a>
|
<a href=/people/p/pengcheng-he/>Pengcheng He</a>
|
<a href=/people/t/tuo-zhao/>Tuo Zhao</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--116><div class="card-body p-3 small">Pre-trained language models have demonstrated superior performance in various natural language processing tasks. However, these models usually contain hundreds of millions of parameters, which limits their practicality because of latency requirements in real-world applications. Existing methods train small compressed models via knowledge distillation. However, performance of these small models drops significantly compared with the pre-trained models due to their reduced model capacity. We propose MoEBERT, which uses a Mixture-of-Experts structure to increase model capacity and inference speed. We initialize MoEBERT by adapting the feed-forward neural networks in a pre-trained model into multiple experts. As such, representation power of the pre-trained model is largely retained. During inference, only one of the experts is activated, such that speed can be improved. We also propose a layer-wise distillation method to train MoEBERT. We validate the efficiency and efficacy of MoEBERT on natural language understanding and question answering tasks. Results show that the proposed method outperforms existing task-specific distillation algorithms. For example, our method outperforms previous approaches by over 2% on the MNLI (mismatched) dataset. Our code is publicly available at <a href=https://github.com/SimiaoZuo/MoEBERT class=acl-markup-url>https://github.com/SimiaoZuo/MoEBERT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.117.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--117 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.117.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.117" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.117/>Implicit n-grams Induced by Recurrence</a></strong><br><a href=/people/x/xiaobing-sun/>Xiaobing Sun</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--117><div class="card-body p-3 small">Although self-attention based models such as Transformers have achieved remarkable successes on natural language processing (NLP)tasks, recent studies reveal that they have limitations on modeling sequential transformations (Hahn, 2020), which may promptre-examinations of recurrent neural networks (RNNs) that demonstrated impressive results on handling sequential data. Despite manyprior attempts to interpret RNNs, their internal mechanisms have not been fully understood, and the question on how exactly they capturesequential features remains largely unclear. In this work, we present a study that shows there actually exist some explainable componentsthat reside within the hidden states, which are reminiscent of the classical n-grams features. We evaluated such extracted explainable features from trained RNNs on downstream sentiment analysis tasks and found they could be used to model interesting linguistic phenomena such as negation and intensification. Furthermore, we examined the efficacy of using such n-gram components alone as encoders on tasks such as sentiment analysis and language modeling, revealing they could be playing important roles in contributing to the overall performance of RNNs. We hope our findings could add interpretability to RNN architectures, and also provide inspirations for proposing new architectures for sequential data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.118.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.118.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.118/>Guiding Visual Question Generation</a></strong><br><a href=/people/n/nihir-vedd/>Nihir Vedd</a>
|
<a href=/people/z/zixu-wang/>Zixu Wang</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/y/yishu-miao/>Yishu Miao</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--118><div class="card-body p-3 small">In traditional Visual Question Generation (VQG), most images have multiple concepts (e.g. objects and categories) for which a question could be generated, but models are trained to mimic an arbitrary choice of concept as given in their training data. This makes training difficult and also poses issues for evaluation – multiple valid questions exist for most images but only one or a few are captured by the human references. We present Guiding Visual Question Generation - a variant of VQG which conditions the question generator on categorical information based on expectations on the type of question and the objects it should explore. We propose two variant families: (i) an explicitly guided model that enables an actor (human or automated) to select which objects and categories to generate a question for; and (ii) 2 types of implicitly guided models that learn which objects and categories to condition on, based on discrete variables. The proposed models are evaluated on an answer-category augmented VQA dataset and our quantitative results show a substantial improvement over the current state of the art (over 9 BLEU-4 increase). Human evaluation validates that guidance helps the generation of questions that are grammatically coherent and relevant to the given image and objects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.119.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.119.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.119" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.119/><span class=acl-fixed-case>OPERA</span>: Operation-Pivoted Discrete Reasoning over Text</a></strong><br><a href=/people/y/yongwei-zhou/>Yongwei Zhou</a>
|
<a href=/people/j/junwei-bao/>Junwei Bao</a>
|
<a href=/people/c/chaoqun-duan/>Chaoqun Duan</a>
|
<a href=/people/h/haipeng-sun/>Haipeng Sun</a>
|
<a href=/people/j/jiahui-liang/>Jiahui Liang</a>
|
<a href=/people/y/yifan-wang/>Yifan Wang</a>
|
<a href=/people/j/jing-zhao/>Jing Zhao</a>
|
<a href=/people/y/youzheng-wu/>Youzheng Wu</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--119><div class="card-body p-3 small">Machine reading comprehension (MRC) that requires discrete reasoning involving symbolic operations, e.g., addition, sorting, and counting, is a challenging task. According to this nature, semantic parsing-based methods predict interpretable but complex logical forms. However, logical form generation is nontrivial and even a little perturbation in a logical form will lead to wrong answers. To alleviate this issue, multi-predictor -based methods are proposed to directly predict different types of answers and achieve improvements. However, they ignore the utilization of symbolic operations and encounter a lack of reasoning ability and interpretability. To inherit the advantages of these two types of methods, we propose OPERA, an operation-pivoted discrete reasoning framework, where lightweight symbolic operations (compared with logical forms) as neural modules are utilized to facilitate the reasoning ability and interpretability. Specifically, operations are first selected and then softly executed to simulate the answer reasoning procedure. Extensive experiments on both DROP and RACENum datasets show the reasoning ability of OPERA. Moreover, further analysis verifies its interpretability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.120.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.120.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.120" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.120/>Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness</a></strong><br><a href=/people/y/yun-zhu-song/>Yun-Zhu Song</a>
|
<a href=/people/y/yi-syuan-chen/>Yi-Syuan Chen</a>
|
<a href=/people/h/hong-han-shuai/>Hong-Han Shuai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--120><div class="card-body p-3 small">A notable challenge in Multi-Document Summarization (MDS) is the extremely-long length of the input. In this paper, we present an extract-then-abstract Transformer framework to overcome the problem. Specifically, we leverage pre-trained language models to construct a hierarchical extractor for salient sentence selection across documents and an abstractor for rewriting the selected contents as summaries. However, learning such a framework is challenging since the optimal contents for the abstractor are generally unknown. Previous works typically create <i>pseudo extraction oracle</i> to enable the supervised learning for both the extractor and the abstractor. Nevertheless, we argue that the performance of such methods could be restricted due to the insufficient information for prediction and inconsistent objectives between training and testing. To this end, we propose a loss weighting mechanism that makes the model aware of the unequal importance for the sentences not in the pseudo extraction oracle, and leverage the fine-tuned abstractor to generate summary references as auxiliary signals for learning the extractor. Moreover, we propose a reinforcement learning method that can efficiently apply to the extractor for harmonizing the optimization between training and testing. Experiment results show that our framework substantially outperforms strong baselines with comparable model sizes and achieves the best results on the Multi-News, Multi-XScience, and WikiCatSum corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.121.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--121 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.121.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.121/>Improving Constituent Representation with Hypertree Neural Networks</a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/g/gongshen-liu/>Gongshen Liu</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--121><div class="card-body p-3 small">Many natural language processing tasks involve text spans and thus high-quality span representations are needed to enhance neural approaches to these tasks. Most existing methods of span representation are based on simple derivations (such as max-pooling) from word representations and do not utilize compositional structures of natural language. In this paper, we aim to improve representations of constituent spans using a novel hypertree neural networks (HTNN) that is structured with constituency parse trees. Each node in the HTNN represents a constituent of the input sentence and each hyperedge represents a composition of smaller child constituents into a larger parent constituent. In each update iteration of the HTNN, the representation of each constituent is computed based on all the hyperedges connected to it, thus incorporating both bottom-up and top-down compositional information. We conduct comprehensive experiments to evaluate HTNNs against other span representation models and the results show the effectiveness of HTNN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.122.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--122 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.122.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.122.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.122/>Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models</a></strong><br><a href=/people/p/pieter-delobelle/>Pieter Delobelle</a>
|
<a href=/people/e/ewoenam-tokpo/>Ewoenam Tokpo</a>
|
<a href=/people/t/toon-calders/>Toon Calders</a>
|
<a href=/people/b/bettina-berendt/>Bettina Berendt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--122><div class="card-body p-3 small">An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify ‘bias’ and ‘fairness’ in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.123.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--123 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.123.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.123" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.123/><span class=acl-fixed-case>M</span>u<span class=acl-fixed-case>CPAD</span>: A Multi-Domain <span class=acl-fixed-case>C</span>hinese Predicate-Argument Dataset</a></strong><br><a href=/people/y/yahui-liu/>Yahui Liu</a>
|
<a href=/people/h/haoping-yang/>Haoping Yang</a>
|
<a href=/people/c/chen-gong/>Chen Gong</a>
|
<a href=/people/q/qingrong-xia/>Qingrong Xia</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--123><div class="card-body p-3 small">During the past decade, neural network models have made tremendous progress on in-domain semantic role labeling (SRL). However, performance drops dramatically under the out-of-domain setting. In order to facilitate research on cross-domain SRL, this paper presents MuCPAD, a multi-domain Chinese predicate-argument dataset, which consists of 30,897 sentences and 92,051 predicates from six different domains. MuCPAD exhibits three important features. 1) Based on a frame-free annotation methodology, we avoid writing complex frames for new predicates. 2) We explicitly annotate omitted core arguments to recover more complete semantic structure, considering that omission of content words is ubiquitous in multi-domain Chinese texts. 3) We compile 53 pages of annotation guidelines and adopt strict double annotation for improving data quality. This paper describes in detail the annotation methodology and annotation process of MuCPAD, and presents in-depth data analysis. We also give benchmark results on cross-domain SRL based on MuCPAD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.124.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.124.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.124/>Representation Learning for Conversational Data using Discourse Mutual Information Maximization</a></strong><br><a href=/people/b/bishal-santra/>Bishal Santra</a>
|
<a href=/people/s/sumegh-roychowdhury/>Sumegh Roychowdhury</a>
|
<a href=/people/a/aishik-mandal/>Aishik Mandal</a>
|
<a href=/people/v/vasu-gurram/>Vasu Gurram</a>
|
<a href=/people/a/atharva-naik/>Atharva Naik</a>
|
<a href=/people/m/manish-gupta/>Manish Gupta</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--124><div class="card-body p-3 small">Although many pretrained models exist for text or images, there have been relatively fewer attempts to train representations specifically for dialog understanding. Prior works usually relied on finetuned representations based on generic text representation models like BERT or GPT-2. But such language modeling pretraining objectives do not take the structural information of conversational text into consideration. Although generative dialog models can learn structural features too, we argue that the structure-unaware word-by-word generation is not suitable for effective conversation modeling. We empirically demonstrate that such representations do not perform consistently across various dialog understanding tasks. Hence, we propose a structure-aware Mutual Information based loss-function DMI (Discourse Mutual Information) for training dialog-representation models, that additionally captures the inherent uncertainty in response prediction. Extensive evaluation on nine diverse dialog modeling tasks shows that our proposed DMI-based models outperform strong baselines by significant margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.125.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.125.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.125" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.125/><span class=acl-fixed-case>V</span>al<span class=acl-fixed-case>CAT</span>: Variable-Length Contextualized Adversarial Transformations Using Encoder-Decoder Language Model</a></strong><br><a href=/people/c/chuyun-deng/>Chuyun Deng</a>
|
<a href=/people/m/mingxuan-liu/>Mingxuan Liu</a>
|
<a href=/people/y/yue-qin/>Yue Qin</a>
|
<a href=/people/j/jia-zhang/>Jia Zhang</a>
|
<a href=/people/h/hai-xin-duan/>Hai-Xin Duan</a>
|
<a href=/people/d/donghong-sun/>Donghong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--125><div class="card-body p-3 small">Adversarial texts help explore vulnerabilities in language models, improve model robustness, and explain their working mechanisms. However, existing word-level attack methods trap in a one-to-one attack pattern, i.e., only a single word can be modified in one transformation round, and they ignore the interactions between several consecutive words. In this paper, we propose ValCAT, a black-box attack framework that misleads the language model by applying variable-length contextualized transformations to the original text. Compared to word-level methods, ValCAT expands the basic units of perturbation from single words to spans composed of multiple consecutive words, enhancing the perturbation capability. Experiments show that our method outperforms state-of-the-art methods in terms of attack success rate, perplexity, and semantic similarity on several classification tasks and inference tasks. The comprehensive human evaluation demonstrates that ValCAT has a significant advantage in ensuring the fluency of the adversarial examples and achieves better semantic consistency. We release the code at <a href=https://github.com/linerxliner/ValCAT class=acl-markup-url>https://github.com/linerxliner/ValCAT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.126.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--126 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.126.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.126/>A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation</a></strong><br><a href=/people/k/kexun-zhang/>Kexun Zhang</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/x/xu-tan/>Xu Tan</a>
|
<a href=/people/j/junliang-guo/>Junliang Guo</a>
|
<a href=/people/y/yi-ren/>Yi Ren</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/t/tie-yan-liu/>Tie-Yan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--126><div class="card-body p-3 small">It is difficult for non-autoregressive translation (NAT) models to capture the multi-modal distribution of target translations due to their conditional independence assumption, which is known as the “multi-modality problem”, including the lexical multi-modality and the syntactic multi-modality. While the first one has been well studied, the syntactic multi-modality brings severe challenges to the standard cross entropy (XE) loss in NAT and is understudied. In this paper, we conduct a systematic study on the syntactic multi-modality problem. Specifically, we decompose it into short- and long-range syntactic multi-modalities and evaluate several recent NAT algorithms with advanced loss functions on both carefully designed synthesized datasets and real datasets. We find that the Connectionist Temporal Classification (CTC) loss and the Order-Agnostic Cross Entropy (OAXE) loss can better handle short- and long-range syntactic multi-modalities respectively. Furthermore, we take the best of both and design a new loss function to better handle the complicated syntactic multi-modality in real-world datasets. To facilitate practical usage, we provide a guide to using different loss functions for different kinds of syntactic multi-modality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.127.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.127.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.127.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.127" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.127/><span class=acl-fixed-case>CIA</span>ug: Equipping Interpolative Augmentation with Curriculum Learning</a></strong><br><a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/r/ritesh-soun/>Ritesh Soun</a>
|
<a href=/people/s/shrey-pandit/>Shrey Pandit</a>
|
<a href=/people/m/megh-thakkar/>Megh Thakkar</a>
|
<a href=/people/s/sarvagya-malaviya/>Sarvagya Malaviya</a>
|
<a href=/people/y/yuval-pinter/>Yuval Pinter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--127><div class="card-body p-3 small">Interpolative data augmentation has proven to be effective for NLP tasks. Despite its merits, the sample selection process in mixup is random, which might make it difficult for the model to generalize better and converge faster. We propose CIAug, a novel curriculum-based learning method that builds upon mixup. It leverages the relative position of samples in hyperbolic embedding space as a complexity measure to gradually mix up increasingly difficult and diverse samples along training. CIAug achieves state-of-the-art results over existing interpolative augmentation methods on 10 benchmark datasets across 4 languages in text classification and named-entity recognition tasks. It also converges and achieves benchmark F1 scores 3 times faster. We empirically analyze the various components of CIAug, and evaluate its robustness against adversarial attacks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.128.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.128.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.128" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.128/>Proposition-Level Clustering for Multi-Document Summarization</a></strong><br><a href=/people/o/ori-ernst/>Ori Ernst</a>
|
<a href=/people/a/avi-caciularu/>Avi Caciularu</a>
|
<a href=/people/o/ori-shapira/>Ori Shapira</a>
|
<a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/j/jacob-goldberger/>Jacob Goldberger</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--128><div class="card-body p-3 small">Text clustering methods were traditionally incorporated into multi-document summarization (MDS) as a means for coping with considerable information repetition. Particularly, clusters were leveraged to indicate information saliency as well as to avoid redundancy. Such prior methods focused on clustering sentences, even though closely related sentences usually contain also non-aligned parts. In this work, we revisit the clustering approach, grouping together sub-sentential propositions, aiming at more precise information alignment. Specifically, our method detects salient propositions, clusters them into paraphrastic clusters, and generates a representative sentence for each cluster via text fusion. Our summarization method improves over the previous state-of-the-art MDS method in the DUC 2004 and TAC 2011 datasets, both in automatic ROUGE scores and human preference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.129.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.129.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.129/>Non-Autoregressive Machine Translation: It’s Not as Fast as it Seems</a></strong><br><a href=/people/j/jindrich-helcl/>Jindřich Helcl</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--129><div class="card-body p-3 small">Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide a fair comparison between a state-of-the-art NAR model and the autoregressive submissions to the shared task. We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used methods for improving efficiency. We run experiments with a connectionist-temporal-classification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times. Our results show that, although NAR models are faster on GPUs, with small batch sizes, they are almost always slower under more realistic usage conditions. We call for more realistic and extensive evaluation of NAR models in future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.130.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.130.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.130" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.130/><span class=acl-fixed-case>BAD</span>-<span class=acl-fixed-case>X</span>: Bilingual Adapters Improve Zero-Shot Cross-Lingual Transfer</a></strong><br><a href=/people/m/marinela-parovic/>Marinela Parović</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--130><div class="card-body p-3 small">Adapter modules enable modular and efficient zero-shot cross-lingual transfer, where current state-of-the-art adapter-based approaches learn specialized language adapters (LAs) for individual languages. In this work, we show that it is more effective to learn bilingual language pair adapters (BAs) when the goal is to optimize performance for a particular source-target transfer direction. Our novel BAD-X adapter framework trades off some modularity of dedicated LAs for improved transfer performance: we demonstrate consistent gains in three standard downstream tasks, and for the majority of evaluated low-resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.131.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--131 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.131.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.131" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.131/>Combining Humor and Sarcasm for Improving Political Parody Detection</a></strong><br><a href=/people/x/xiao-ao/>Xiao Ao</a>
|
<a href=/people/d/danae-sanchez-villegas/>Danae Sanchez Villegas</a>
|
<a href=/people/d/daniel-preotiuc-pietro/>Daniel Preotiuc-Pietro</a>
|
<a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--131><div class="card-body p-3 small">Parody is a figurative device used for mimicking entities for comedic or critical purposes. Parody is intentionally humorous and often involves sarcasm. This paper explores jointly modelling these figurative tropes with the goal of improving performance of political parody detection in tweets. To this end, we present a multi-encoder model that combines three parallel encoders to enrich parody-specific representations with humor and sarcasm information. Experiments on a publicly available data set of political parody tweets demonstrate that our approach outperforms previous state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.132.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.132.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.132" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.132/><span class=acl-fixed-case>TIE</span>: Topological Information Enhanced Structural Reading Comprehension on Web Pages</a></strong><br><a href=/people/z/zihan-zhao/>Zihan Zhao</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/r/ruisheng-cao/>Ruisheng Cao</a>
|
<a href=/people/h/hongshen-xu/>Hongshen Xu</a>
|
<a href=/people/x/xingyu-chen/>Xingyu Chen</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--132><div class="card-body p-3 small">Recently, the structural reading comprehension (SRC) task on web pages has attracted increasing research interests. Although previous SRC work has leveraged extra information such as HTML tags or XPaths, the informative topology of web pages is not effectively exploited. In this work, we propose a Topological Information Enhanced model (TIE), which transforms the token-level task into a tag-level task by introducing a two-stage process (i.e. node locating and answer refining). Based on that, TIE integrates Graph Attention Network (GAT) and Pre-trained Language Model (PLM) to leverage the topological information of both logical structures and spatial structures. Experimental results demonstrate that our model outperforms strong baselines and achieves state-of-the-art performances on the web-based SRC benchmark WebSRC at the time of writing. The code of TIE will be publicly available at <a href=https://github.com/X-LANCE/TIE class=acl-markup-url>https://github.com/X-LANCE/TIE</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.133.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.133.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.133/><span class=acl-fixed-case>RSTG</span>en: Imbuing Fine-Grained Interpretable Control into Long-<span class=acl-fixed-case>F</span>orm<span class=acl-fixed-case>T</span>ext Generators</a></strong><br><a href=/people/r/rilwan-adewoyin/>Rilwan Adewoyin</a>
|
<a href=/people/r/ritabrata-dutta/>Ritabrata Dutta</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--133><div class="card-body p-3 small">In this paper, we study the task of improving the cohesion and coherence of long-form text generated by language models. To this end, we propose RSTGen, a framework that utilises Rhetorical Structure Theory (RST), a classical language theory, to control the discourse structure, semantics and topics of generated text. Firstly, we demonstrate our model’s ability to control structural discourse and semantic features of generated text in open generation evaluation. Then we experiment on the two challenging long-form text tasks of argument generation and story generation. Evaluation using automated metrics and a metric with high correlation to human evaluation, shows that our model performs competitively against existing models, while offering significantly more controls over generated text than alternative methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.134.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.134.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.134/>Intent Detection and Discovery from User Logs via Deep Semi-Supervised Contrastive Clustering</a></strong><br><a href=/people/r/rajat-kumar/>Rajat Kumar</a>
|
<a href=/people/m/mayur-patidar/>Mayur Patidar</a>
|
<a href=/people/v/vaibhav-varshney/>Vaibhav Varshney</a>
|
<a href=/people/l/lovekesh-vig/>Lovekesh Vig</a>
|
<a href=/people/g/gautam-shroff/>Gautam Shroff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--134><div class="card-body p-3 small">Intent Detection is a crucial component of Dialogue Systems wherein the objective is to classify a user utterance into one of multiple pre-defined intents. A pre-requisite for developing an effective intent identifier is a training dataset labeled with all possible user intents. However, even skilled domain experts are often unable to foresee all possible user intents at design time and for practical applications, novel intents may have to be inferred incrementally on-the-fly from user utterances. Therefore, for any real-world dialogue system, the number of intents increases over time and new intents have to be discovered by analyzing the utterances outside the existing set of intents. In this paper, our objective is to i) detect known intent utterances from a large number of unlabeled utterance samples given a few labeled samples and ii) discover new unknown intents from the remaining unlabeled samples. Existing SOTA approaches address this problem via alternate representation learning and clustering wherein pseudo labels are used for updating the representations and clustering is used for generating the pseudo labels. Unlike existing approaches that rely on epoch wise cluster alignment, we propose an end-to-end deep contrastive clustering algorithm that jointly updates model parameters and cluster centers via supervised and self-supervised learning and optimally utilizes both labeled and unlabeled data. Our proposed approach outperforms competitive baselines on five public datasets for both settings: (i) where the number of undiscovered intents are known in advance, and (ii) where the number of intents are estimated by an algorithm. We also propose a human-in-the-loop variant of our approach for practical deployment which does not require an estimate of new intents and outperforms the end-to-end approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.135.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.135.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.135" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.135/>Extending Multi-Text Sentence Fusion Resources via Pyramid Annotations</a></strong><br><a href=/people/d/daniela-brook-weiss/>Daniela Brook Weiss</a>
|
<a href=/people/p/paul-roit/>Paul Roit</a>
|
<a href=/people/o/ori-ernst/>Ori Ernst</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--135><div class="card-body p-3 small">NLP models that process multiple texts often struggle in recognizing corresponding and salient information that is often differently phrased, and consolidating the redundancies across texts. To facilitate research of such challenges, the sentence fusion task was proposed, yet previous datasets for this task were very limited in their size and scope. In this paper, we revisit and substantially extend previous dataset creation efforts. With careful modifications, relabeling, and employing complementing data sources, we were able to more than triple the size of a notable earlier dataset. Moreover, we show that our extended version uses more representative texts for multi-document tasks and provides a more diverse training set, which substantially improves model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.136.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.136.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.136/>The Devil is in the Details: On the Pitfalls of Vocabulary Selection in Neural Machine Translation</a></strong><br><a href=/people/t/tobias-domhan/>Tobias Domhan</a>
|
<a href=/people/e/eva-hasler/>Eva Hasler</a>
|
<a href=/people/k/ke-m-tran/>Ke Tran</a>
|
<a href=/people/s/sony-trenous/>Sony Trenous</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a>
|
<a href=/people/f/felix-hieber/>Felix Hieber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--136><div class="card-body p-3 small">Vocabulary selection, or lexical shortlisting, is a well-known technique to improve latency of Neural Machine Translation models by constraining the set of allowed output words during inference. The chosen set is typically determined by separately trained alignment model parameters, independent of the source-sentence context at inference time. While vocabulary selection appears competitive with respect to automatic quality metrics in prior work, we show that it can fail to select the right set of output words, particularly for semantically non-compositional linguistic phenomena such as idiomatic expressions, leading to reduced translation quality as perceived by humans. Trading off latency for quality by increasing the size of the allowed set is often not an option in real-world scenarios. We propose a model of vocabulary selection, integrated into the neural translation model, that predicts the set of allowed output words from contextualized encoder representations. This restores translation quality of an unconstrained system, as measured by human evaluations on WMT newstest2020 and idiomatic expressions, at an inference latency competitive with alignment-based selection using aggressive thresholds, thereby removing the dependency on separately trained alignment models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.137.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--137 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.137.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.137/><span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>C</span>ite: Modeling realistic citations requires moving beyond the single-sentence single-label setting</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/b/brandon-ko/>Brandon Ko</a>
|
<a href=/people/b/bailey-kuehl/>Bailey Kuehl</a>
|
<a href=/people/s/sophie-johnson/>Sophie Johnson</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/k/kyle-lo/>Kyle Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--137><div class="card-body p-3 small">Citation context analysis (CCA) is an important task in natural language processing that studies how and why scholars discuss each others’ work. Despite decades of study, computational methods for CCA have largely relied on overly-simplistic assumptions of how authors cite, which ignore several important phenomena. For instance, scholarly papers often contain rich discussions of cited work that span multiple sentences and express multiple intents concurrently. Yet, recent work in CCA is often approached as a single-sentence, single-label classification task, and thus many datasets used to develop modern computational approaches fail to capture this interesting discourse. To address this research gap, we highlight three understudied phenomena for CCA and release MULTICITE, a new dataset of 12.6K citation contexts from 1.2K computational linguistics papers that fully models these phenomena. Not only is it the largest collection of expert-annotated citation contexts to-date, MULTICITE contains multi-sentence, multi-label citation contexts annotated through-out entire full paper texts. We demonstrate how MULTICITE can enable the development of new computational methods on three important CCA tasks. We release our code and dataset at <a href=https://github.com/allenai/multicite class=acl-markup-url>https://github.com/allenai/multicite</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.138.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.138.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.138.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.138" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.138/><span class=acl-fixed-case>DEGREE</span>: A Data-Efficient Generation-Based Event Extraction Model</a></strong><br><a href=/people/i/i-hung-hsu/>I-Hung Hsu</a>
|
<a href=/people/k/kuan-hao-huang/>Kuan-Hao Huang</a>
|
<a href=/people/e/elizabeth-boschee/>Elizabeth Boschee</a>
|
<a href=/people/s/scott-miller/>Scott Miller</a>
|
<a href=/people/p/prem-natarajan/>Prem Natarajan</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--138><div class="card-body p-3 small">Event extraction requires high-quality expert human annotations, which are usually expensive. Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge. In this paper, we focus on low-resource end-to-end event extraction and propose DEGREE, a data-efficient model that formulates event extraction as a conditional generation problem. Given a passage and a manually designed prompt, DEGREE learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern. The final event predictions are then extracted from the generated sentence with a deterministic algorithm. DEGREE has three advantages to learn well with less training data. First, our designed prompts provide semantic guidance for DEGREE to leverage DEGREE and thus better capture the event arguments. Moreover, DEGREE is capable of using additional weakly-supervised information, such as the description of events encoded in the prompts. Finally, DEGREE learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them. Our experimental results demonstrate the strong performance of DEGREE for low-resource event extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.139.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--139 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.139.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.139/>Bridging the Gap between Language Models and Cross-Lingual Sequence Labeling</a></strong><br><a href=/people/n/nuo-chen/>Nuo Chen</a>
|
<a href=/people/l/linjun-shou/>Linjun Shou</a>
|
<a href=/people/m/ming-gong/>Ming Gong</a>
|
<a href=/people/j/jian-pei/>Jian Pei</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--139><div class="card-body p-3 small">Large-scale cross-lingual pre-trained language models (xPLMs) have shown effective in cross-lingual sequence labeling tasks (xSL), such as machine reading comprehension (xMRC) by transferring knowledge from a high-resource language to low-resource languages. Despite the great success, we draw an empirical observation that there is an training objective gap between pre-training and fine-tuning stages: e.g., mask language modeling objective requires <i>local</i> understanding of the masked token and the span-extraction objective requires understanding and reasoning of the <i>global</i> input passage/paragraph and question, leading to the discrepancy between pre-training and xMRC. In this paper, we first design a pre-training task tailored for xSL named Cross-lingual Language Informative Span Masking (CLISM) to eliminate the objective gap in a self-supervised manner. Second, we present ContrAstive-Consistency Regularization (CACR), which utilizes contrastive learning to encourage the consistency between representations of input parallel sequences via unsupervised cross-lingual instance-wise training signals during pre-training. By these means, our methods not only bridge the gap between pretrain-finetune, but also enhance PLMs to better capture the alignment between different languages. Extensive experiments prove that our method achieves clearly superior results on multiple xSL benchmarks with limited pre-training data. Our methods also surpass the previous state-of-the-art methods by a large margin in few-shot data setting, where only a few hundred training examples are available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.140.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.140.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.140.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.140/>Hero-Gang Neural Model For Named Entity Recognition</a></strong><br><a href=/people/j/jinpeng-hu/>Jinpeng Hu</a>
|
<a href=/people/y/yaling-shen/>Yaling Shen</a>
|
<a href=/people/y/yang-liu-hk/>Yang Liu</a>
|
<a href=/people/x/xiang-wan/>Xiang Wan</a>
|
<a href=/people/t/tsung-hui-chang/>Tsung-Hui Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--140><div class="card-body p-3 small">Named entity recognition (NER) is a fundamental and important task in NLP, aiming at identifying named entities (NEs) from free text. Recently, since the multi-head attention mechanism applied in the Transformer model can effectively capture longer contextual information, Transformer-based models have become the mainstream methods and have achieved significant performance in this task. Unfortunately, although these models can capture effective global context information, they are still limited in the local feature and position information extraction, which is critical in NER. In this paper, to address this limitation, we propose a novel Hero-Gang Neural structure (HGN), including the Hero and Gang module, to leverage both global and local information to promote NER. Specifically, the Hero module is composed of a Transformer-based encoder to maintain the advantage of the self-attention mechanism, and the Gang module utilizes a multi-window recurrent module to extract local features and position information under the guidance of the Hero module. Afterward, the proposed multi-window attention effectively combines global information and multiple local features for predicting entity labels. Experimental results on several benchmark datasets demonstrate the effectiveness of our proposed model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.141.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.141.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.141/><span class=acl-fixed-case>MGIMN</span>: Multi-Grained Interactive Matching Network for Few-shot Text Classification</a></strong><br><a href=/people/j/jianhai-zhang/>Jianhai Zhang</a>
|
<a href=/people/m/mieradilijiang-maimaiti/>Mieradilijiang Maimaiti</a>
|
<a href=/people/g/gao-xing/>Gao Xing</a>
|
<a href=/people/y/yuanhang-zheng/>Yuanhang Zheng</a>
|
<a href=/people/j/ji-zhang/>Ji Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--141><div class="card-body p-3 small">Text classification struggles to generalize to unseen classes with very few labeled text instances per class. In such a few-shot learning (FSL) setting, metric-based meta-learning approaches have shown promising results. Previous studies mainly aim to derive a prototype representation for each class. However, they neglect that it is challenging-yet-unnecessary to construct a compact representation which expresses the entire meaning for each class. They also ignore the importance to capture the inter-dependency between query and the support set for few-shot text classification. To deal with these issues, we propose a meta-learning based method MGIMN which performs instance-wise comparison followed by aggregation to generate class-wise matching vectors instead of prototype learning. The key of instance-wise comparison is the interactive matching within the class-specific context and episode-specific context. Extensive experiments demonstrate that the proposed method significantly outperforms the existing SOTA approaches, under both the standard FSL and generalized FSL settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.142.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.142.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.142" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.142/>All You May Need for <span class=acl-fixed-case>VQA</span> are Image Captions</a></strong><br><a href=/people/s/soravit-changpinyo/>Soravit Changpinyo</a>
|
<a href=/people/d/doron-kukliansy/>Doron Kukliansy</a>
|
<a href=/people/i/idan-szpektor/>Idan Szpektor</a>
|
<a href=/people/x/xi-chen/>Xi Chen</a>
|
<a href=/people/n/nan-ding/>Nan Ding</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--142><div class="card-body p-3 small">Visual Question Answering (VQA) has benefited from increasingly sophisticated models, but has not enjoyed the same level of engagement in terms of data creation. In this paper, we propose a method that automatically derives VQA examples at volume, by leveraging the abundance of existing image-caption annotations combined with neural models for textual question generation. We show that the resulting data is of high-quality. VQA models trained on our data improve state-of-the-art zero-shot accuracy by double digits and achieve a level of robustness that lacks in the same model trained on human-annotated VQA data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.143.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.143.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.143.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.143" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.143/>Frustratingly Easy System Combination for Grammatical Error Correction</a></strong><br><a href=/people/m/muhammad-reza-qorib/>Muhammad Reza Qorib</a>
|
<a href=/people/s/seung-hoon-na/>Seung-Hoon Na</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--143><div class="card-body p-3 small">In this paper, we formulate system combination for grammatical error correction (GEC) as a simple machine learning task: binary classification. We demonstrate that with the right problem formulation, a simple logistic regression algorithm can be highly effective for combining GEC models. Our method successfully increases the F0.5 score from the highest base GEC system by 4.2 points on the CoNLL-2014 test set and 7.2 points on the BEA-2019 test set. Furthermore, our method outperforms the state of the art by 4.0 points on the BEA-2019 test set, 1.2 points on the CoNLL-2014 test set with original annotation, and 3.4 points on the CoNLL-2014 test set with alternative annotation. We also show that our system combination generates better corrections with higher F0.5 scores than the conventional ensemble.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.144.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.144.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.144" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.144/>Simple Local Attentions Remain Competitive for Long-Context Tasks</a></strong><br><a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/b/barlas-oguz/>Barlas Oguz</a>
|
<a href=/people/a/anchit-gupta/>Anchit Gupta</a>
|
<a href=/people/x/xilun-chen/>Xilun Chen</a>
|
<a href=/people/d/diana-liskovich/>Diana Liskovich</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/s/scott-yih/>Scott Yih</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--144><div class="card-body p-3 small">Many NLP tasks require processing long contexts beyond the length limit of pretrained models. In order to scale these models to longer text sequences, many efficient long-range attention variants have been proposed. Despite the abundance of research along this direction, it is still difficult to gauge the relative effectiveness of these models in practical use cases, e.g., if we apply these models following the pretrain-and-finetune paradigm. In this work, we aim to conduct a thorough analysis of these emerging models with large-scale and controlled experiments. For each attention variant, we pretrain large-size models using the same long-doc corpus and then finetune these models for real-world long-context tasks. Our findings reveal pitfalls of an existing widely-used long-range benchmark and show none of the tested efficient attentions can beat a simple local window attention under standard pretraining paradigms. Further analysis on local attention variants suggests that even the commonly used attention-window overlap is not necessary to achieve good downstream results — using disjoint local attentions, we are able to build a simpler and more efficient long-doc QA model that matches the performance of Longformer with half of its pretraining compute.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.145.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.145.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.145.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.145/><span class=acl-fixed-case>E</span>ven the Simplest Baseline Needs Careful Re-investigation: A Case Study on <span class=acl-fixed-case>XML</span>-<span class=acl-fixed-case>CNN</span></a></strong><br><a href=/people/s/si-an-chen/>Si-An Chen</a>
|
<a href=/people/j/jie-jyun-liu/>Jie-jyun Liu</a>
|
<a href=/people/t/tsung-han-yang/>Tsung-Han Yang</a>
|
<a href=/people/h/hsuan-tien-lin/>Hsuan-Tien Lin</a>
|
<a href=/people/c/chih-jen-lin/>Chih-Jen Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--145><div class="card-body p-3 small">The power and the potential of deep learning models attract many researchers to design advanced and sophisticated architectures. Nevertheless, the progress is sometimes unreal due to various possible reasons. In this work, through an astonishing example we argue that more efforts should be paid to ensure the progress in developing a new deep learning method. For a highly influential multi-label text classification method XML-CNN, we show that the superior performance claimed in the original paper was mainly due to some unbelievable coincidences. We re-examine XML-CNN and make a re-implementation which reveals some contradictory findings to the claims in the original paper. Our study suggests suitable baselines for multi-label text classification tasks and confirms that the progress on a new architecture cannot be confidently justified without a cautious investigation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.146.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.146.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.146" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.146/>Multi-Relational Graph Transformer for Automatic Short Answer Grading</a></strong><br><a href=/people/r/rajat-agarwal/>Rajat Agarwal</a>
|
<a href=/people/v/varun-khurana/>Varun Khurana</a>
|
<a href=/people/k/karish-grover/>Karish Grover</a>
|
<a href=/people/m/mukesh-mohania/>Mukesh Mohania</a>
|
<a href=/people/v/vikram-goyal/>Vikram Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--146><div class="card-body p-3 small">The recent transition to the online educational domain has increased the need for Automatic Short Answer Grading (ASAG). ASAG automatically evaluates a student’s response against a (given) correct response and thus has been a prevalent semantic matching task. Most existing methods utilize sequential context to compare two sentences and ignore the structural context of the sentence; therefore, these methods may not result in the desired performance. In this paper, we overcome this problem by proposing a Multi-Relational Graph Transformer, MitiGaTe, to prepare token representations considering the structural context. Abstract Meaning Representation (AMR) graph is created by parsing the text response and then segregated into multiple subgraphs, each corresponding to a particular relationship in AMR. A Graph Transformer is used to prepare relation-specific token embeddings within each subgraph, then aggregated to obtain a subgraph representation. Finally, we compare the correct answer and the student response subgraph representations to yield a final score. Experimental results on Mohler’s dataset show that our system outperforms the existing state-of-the-art methods. We have released our implementation <a href=https://github.com/kvarun07/asag-gt class=acl-markup-url>https://github.com/kvarun07/asag-gt</a>, as we believe that our model can be useful for many future applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.147.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.147.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.147.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.147/>Event Schema Induction with Double Graph Autoencoders</a></strong><br><a href=/people/x/xiaomeng-jin/>Xiaomeng Jin</a>
|
<a href=/people/m/manling-li/>Manling Li</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--147><div class="card-body p-3 small">Event schema depicts the typical structure of complex events, serving as a scaffolding to effectively analyze, predict, and possibly intervene in the ongoing events. To induce event schemas from historical events, previous work uses an event-by-event scheme, ignoring the global structure of the entire schema graph. We propose a new event schema induction framework using double graph autoencoders, which captures the global dependencies among nodes in event graphs. Specifically, we first extract the event skeleton from an event graph and design a variational directed acyclic graph (DAG) autoencoder to learn its global structure. Then we further fill in the event arguments for the skeleton, and use another Graph Convolutional Network (GCN) based autoencoder to reconstruct entity-entity relations as well as to detect coreferential entities. By performing this two-stage induction decomposition, the model can avoid reconstructing the entire graph in one step, allowing it to focus on learning global structures between events. Experimental results on three event graph datasets demonstrate that our method achieves state-of-the-art performance and induces high-quality event schemas with global consistency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.148.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.148.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.148.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.148" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.148/><span class=acl-fixed-case>CS</span>1<span class=acl-fixed-case>QA</span>: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course</a></strong><br><a href=/people/c/changyoon-lee/>Changyoon Lee</a>
|
<a href=/people/y/yeon-seonwoo/>Yeon Seonwoo</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--148><div class="card-body p-3 small">We introduce CS1QA, a dataset for code-based question answering in the programming education domain. CS1QA consists of 9,237 question-answer pairs gathered from chat logs in an introductory programming class using Python, and 17,698 unannotated chat data with code. Each question is accompanied with the student’s code, and the portion of the code relevant to answering the question. We carefully design the annotation process to construct CS1QA, and analyze the collected dataset in detail. The tasks for CS1QA are to predict the question type, the relevant code snippet given the question and the code and retrieving an answer from the annotated corpus. Results for the experiments on several baseline models are reported and thoroughly analyzed. The tasks for CS1QA challenge models to understand both the code and natural language. This unique dataset can be used as a benchmark for source code comprehension and question answering in the educational setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.149.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.149.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.149" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.149/>Unsupervised Cross-Lingual Transfer of Structured Predictors without Source Data</a></strong><br><a href=/people/k/kemal-kurniawan/>Kemal Kurniawan</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a>
|
<a href=/people/p/philip-schulz/>Philip Schulz</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--149><div class="card-body p-3 small">Providing technologies to communities or domains where training data is scarce or protected e.g., for privacy reasons, is becoming increasingly important. To that end, we generalise methods for unsupervised transfer from multiple input models for structured prediction. We show that the means of aggregating over the input models is critical, and that multiplying marginal probabilities of substructures to obtain high-probability structures for distant supervision is substantially better than taking the union of such structures over the input models, as done in prior work. Testing on 18 languages, we demonstrate that the method works in a cross-lingual setting, considering both dependency parsing and part-of-speech structured prediction problems. Our analyses show that the proposed method produces less noisy labels for the distant supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.150.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--150 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.150.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.150" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.150/>Don’t Take It Literally: An Edit-Invariant Sequence Loss for Text Generation</a></strong><br><a href=/people/g/guangyi-liu/>Guangyi Liu</a>
|
<a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/t/tianhua-tao/>Tianhua Tao</a>
|
<a href=/people/x/xiaodan-liang/>Xiaodan Liang</a>
|
<a href=/people/j/junwei-bao/>Junwei Bao</a>
|
<a href=/people/z/zhen-li/>Zhen Li</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/s/shuguang-cui/>Shuguang Cui</a>
|
<a href=/people/z/zhiting-hu/>Zhiting Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--150><div class="card-body p-3 small">Neural text generation models are typically trained by maximizing log-likelihood with the sequence cross entropy (CE) loss, which encourages an exact token-by-token match between a target sequence with a generated sequence. Such training objective is sub-optimal when the target sequence is not perfect, e.g., when the target sequence is corrupted with noises, or when only weak sequence supervision is available. To address the challenge, we propose a novel Edit-Invariant Sequence Loss (EISL), which computes the matching loss of a target <span class=tex-math>n</span>-gram with all <span class=tex-math>n</span>-grams in the generated sequence. EISL is designed to be robust to various noises and edits in the target sequences. Moreover, the EISL computation is essentially an approximate convolution operation with target <span class=tex-math>n</span>-grams as kernels, which is easy to implement and efficient to compute with existing libraries. To demonstrate the effectiveness of EISL, we conduct experiments on a wide range of tasks, including machine translation with noisy target sequences, unsupervised text style transfer with only weak training signals, and non-autoregressive generation with non-predefined generation order. Experimental results show our method significantly outperforms the common CE loss and other strong baselines on all the tasks. EISL has a simple API that can be used as a drop-in replacement of the CE loss: <a href=https://github.com/guangyliu/EISL class=acl-markup-url>https://github.com/guangyliu/EISL</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.151.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.151.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.151/>Modeling Exemplification in Long-form Question Answering via Retrieval</a></strong><br><a href=/people/s/shufan-wang/>Shufan Wang</a>
|
<a href=/people/f/fangyuan-xu/>Fangyuan Xu</a>
|
<a href=/people/l/laure-thompson/>Laure Thompson</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--151><div class="card-body p-3 small">Exemplification is a process by which writers explain or clarify a concept by providing an example. While common in all forms of writing, exemplification is particularly useful in the task of long-form question answering (LFQA), where a complicated answer can be made more understandable through simple examples. In this paper, we provide the first computational study of exemplification in QA, performing a fine-grained annotation of different types of examples (e.g., hypotheticals, anecdotes) in three corpora. We show that not only do state-of-the-art LFQA models struggle to generate relevant examples, but also that standard evaluation metrics such as ROUGE are insufficient to judge exemplification quality. We propose to treat exemplification as a <i>retrieval</i> problem in which a partially-written answer is used to query a large set of human-written examples extracted from a corpus. Our approach allows a reliable ranking-type automatic metrics that correlates well with human evaluation. A human evaluation shows that our model’s retrieved examples are more relevant than examples generated from a state-of-the-art LFQA model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.152.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.152.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.152/><span class=acl-fixed-case>D</span>2<span class=acl-fixed-case>U</span>: Distance-to-Uniform Learning for Out-of-Scope Detection</a></strong><br><a href=/people/e/eyup-yilmaz/>Eyup Yilmaz</a>
|
<a href=/people/c/cagri-toraman/>Cagri Toraman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--152><div class="card-body p-3 small">Supervised training with cross-entropy loss implicitly forces models to produce probability distributions that follow a discrete delta distribution. Model predictions in test time are expected to be similar to delta distributions if the classifier determines the class of an input correctly. However, the shape of the predicted probability distribution can become similar to the uniform distribution when the model cannot infer properly. We exploit this observation for detecting out-of-scope (OOS) utterances in conversational systems. Specifically, we propose a zero-shot post-processing step, called Distance-to-Uniform (D2U), exploiting not only the classification confidence score, but the shape of the entire output distribution. We later combine it with a learning procedure that uses D2U for loss calculation in the supervised setup. We conduct experiments using six publicly available datasets. Experimental results show that the performance of OOS detection is improved with our post-processing when there is no OOS training data, as well as with D2U learning procedure when OOS training data is available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.153.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--153 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.153.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.153.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.153" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.153/>Reference-free Summarization Evaluation via Semantic Correlation and Compression Ratio</a></strong><br><a href=/people/y/yizhu-liu/>Yizhu Liu</a>
|
<a href=/people/q/qi-jia/>Qi Jia</a>
|
<a href=/people/k/kenny-zhu/>Kenny Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--153><div class="card-body p-3 small">A document can be summarized in a number of ways. Reference-based evaluation of summarization has been criticized for its inflexibility. The more sufficient the number of abstracts, the more accurate the evaluation results. However, it is difficult to collect sufficient reference summaries. In this paper, we propose a new automatic reference-free evaluation metric that compares semantic distribution between source document and summary by pretrained language models and considers summary compression ratio. The experiments show that this metric is more consistent with human evaluation in terms of coherence, consistency, relevance and fluency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.154.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--154 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.154.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.154/><span class=acl-fixed-case>K</span>ronecker<span class=acl-fixed-case>BERT</span>: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation</a></strong><br><a href=/people/m/marzieh-tahaei/>Marzieh Tahaei</a>
|
<a href=/people/e/ella-charlaix/>Ella Charlaix</a>
|
<a href=/people/v/vahid-nia/>Vahid Nia</a>
|
<a href=/people/a/ali-ghodsi/>Ali Ghodsi</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--154><div class="card-body p-3 small">The development of over-parameterized pre-trained language models has made a significant contribution toward the success of natural language processing. While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices. We push the limits of state-of-the-art Transformer-based pre-trained language model compression using Kronecker decomposition. We present our KroneckerBERT, a compressed version of the BERT_BASE model obtained by compressing the embedding layer and the linear mappings in the multi-head attention, and the feed-forward network modules in the Transformer layers. Our KroneckerBERT is trained via a very efficient two-stage knowledge distillation scheme using far fewer data samples than state-of-the-art models like MobileBERT and TinyBERT. We evaluate the performance of KroneckerBERT on well-known NLP benchmarks. We show that our KroneckerBERT with compression factors of 7.7x and 21x outperforms state-of-the-art compression methods on the GLUE and SQuAD benchmarks. In particular, using only 13% of the teacher model parameters, it retain more than 99% of the accuracy on the majority of GLUE tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.155.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.155.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.155" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.155/>Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models</a></strong><br><a href=/people/s/sanghwan-bae/>Sanghwan Bae</a>
|
<a href=/people/d/donghyun-kwak/>Donghyun Kwak</a>
|
<a href=/people/s/sungdong-kim/>Sungdong Kim</a>
|
<a href=/people/d/donghoon-ham/>Donghoon Ham</a>
|
<a href=/people/s/soyoung-kang/>Soyoung Kang</a>
|
<a href=/people/s/sang-woo-lee/>Sang-Woo Lee</a>
|
<a href=/people/w/woomyoung-park/>Woomyoung Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--155><div class="card-body p-3 small">Recent open-domain dialogue models have brought numerous breakthroughs. However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety. In this work, we study the challenge of imposing roles on open-domain dialogue systems, with the goal of making the systems maintain consistent roles while conversing naturally with humans. To accomplish this, the system must satisfy a role specification that includes certain conditions on the stated features as well as a system policy on whether or not certain types of utterances are allowed. For this, we propose an efficient data collection framework leveraging in-context few-shot learning of large-scale language models for building role-satisfying dialogue dataset from scratch. We then compare various architectures for open-domain dialogue systems in terms of meeting role specifications while maintaining conversational abilities. Automatic and human evaluations show that our models return few out-of-bounds utterances, keeping competitive performance on general metrics. We release a Korean dialogue dataset we built for further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.156.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--156 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.156.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.156" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.156/>Sentence-Level Resampling for Named Entity Recognition</a></strong><br><a href=/people/x/xiaochen-wang/>Xiaochen Wang</a>
|
<a href=/people/y/yue-wang/>Yue Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--156><div class="card-body p-3 small">As a fundamental task in natural language processing, named entity recognition (NER) aims to locate and classify named entities in unstructured text. However, named entities are always the minority among all tokens in the text. This data imbalance problem presents a challenge to machine learning models as their learning objective is usually dominated by the majority of non-entity tokens. To alleviate data imbalance, we propose a set of sentence-level resampling methods where the importance of each training sentence is computed based on its tokens and entities. We study the generalizability of these resampling methods on a wide variety of NER models (CRF, Bi-LSTM, and BERT) across corpora from diverse domains (general, social, and medical texts). Extensive experiments show that the proposed methods improve span-level macro F1-scores of the evaluated NER models on multiple corpora, frequently outperforming sub-sentence-level resampling, data augmentation, and special loss functions such as focal and Dice loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.157.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.157.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.157.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.157" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.157/>Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem</a></strong><br><a href=/people/r/ryoma-sato/>Ryoma Sato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--157><div class="card-body p-3 small">Word embeddings are one of the most fundamental technologies used in natural language processing. Existing word embeddings are high-dimensional and consume considerable computational resources. In this study, we propose WordTour, unsupervised one-dimensional word embeddings. To achieve the challenging goal, we propose a decomposition of the desiderata of word embeddings into two parts, completeness and soundness, and focus on soundness in this paper. Owing to the single dimensionality, WordTour is extremely efficient and provides a minimal means to handle word embeddings. We experimentally confirmed the effectiveness of the proposed method via user study and document classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.158.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.158.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.158/>On the Diversity and Limits of Human Explanations</a></strong><br><a href=/people/c/chenhao-tan/>Chenhao Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--158><div class="card-body p-3 small">A growing effort in NLP aims to build datasets of human explanations. However, it remains unclear whether these datasets serve their intended goals. This problem is exacerbated by the fact that the term explanation is overloaded and refers to a broad range of notions with different properties and ramifications. Our goal is to provide an overview of the diversity of explanations, discuss human limitations in providing explanations, and ultimately provide implications for collecting and using human explanations in NLP.Inspired by prior work in psychology and cognitive sciences, we group existing human explanations in NLP into three categories: proximal mechanism, evidence, and procedure. These three types differ in nature and have implications for the resultant explanations. For instance, procedure is not considered explanation in psychology and connects with a rich body of work on learning from instructions. The diversity of explanations is further evidenced by proxy questions that are needed for annotators to interpret and answer “why is [input] assigned [label]”. Finally, giving explanations may require different, often deeper, understandings than predictions, which casts doubt on whether humans can provide valid explanations in some tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.159.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.159.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.159/>Locally Aggregated Feature Attribution on Natural Language Model Understanding</a></strong><br><a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/h/haitao-jiang/>Haitao Jiang</a>
|
<a href=/people/r/rui-song/>Rui Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--159><div class="card-body p-3 small">With the growing popularity of deep-learning models, model understanding becomes more important. Much effort has been devoted to demystify deep neural networks for better explainability. Some feature attribution methods have shown promising results in computer vision, especially the gradient-based methods where effectively smoothing the gradients with reference data is the key to a robust and faithful result. However, direct application of these gradient-based methods to NLP tasks is not trivial due to the fact that the input consists of discrete tokens and the “reference” tokens are not explicitly defined. In this work, we propose Locally Aggregated Feature Attribution (LAFA), a novel gradient-based feature attribution method for NLP models. Instead of relying on obscure reference tokens, it smooths gradients by aggregating similar reference texts derived from language model embeddings. For evaluation purpose, we also design experiments on different NLP tasks including Entity Recognition and Sentiment Analysis on public datasets and key words detection on constructed Amazon catalogue dataset. The superior performance of the proposed method is demonstrated through experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.160.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.160.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.160/>Generic and Trend-aware Curriculum Learning for Relation Extraction</a></strong><br><a href=/people/n/nidhi-vakil/>Nidhi Vakil</a>
|
<a href=/people/h/hadi-amiri/>Hadi Amiri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--160><div class="card-body p-3 small">We present a generic and trend-aware curriculum learning approach that effectively integrates textual and structural information in text graphs for relation extraction between entities, which we consider as node pairs in graphs. The proposed model extends existing curriculum learning approaches by incorporating sample-level loss trends to better discriminate easier from harder samples and schedule them for training. The model results in a robust estimation of sample difficulty and shows sizable improvement over the state-of-the-art approaches across several datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.161.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--161 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.161.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.161/>On Systematic Style Differences between Unsupervised and Supervised <span class=acl-fixed-case>MT</span> and an Application for High-Resource Machine Translation</a></strong><br><a href=/people/k/kelly-marchisio/>Kelly Marchisio</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/d/david-grangier/>David Grangier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--161><div class="card-body p-3 small">Modern unsupervised machine translation (MT) systems reach reasonable translation quality under clean and controlled data conditions. As the performance gap between supervised and unsupervised MT narrows, it is interesting to ask whether the different training methods result in systematically different output beyond what is visible via quality metrics like adequacy or BLEU. We compare translations from supervised and unsupervised MT systems of similar quality, finding that unsupervised output is more fluent and more structurally different in comparison to human translation than is supervised MT. We then demonstrate a way to combine the benefits of both methods into a single system which results in improved adequacy and fluency as rated by human evaluators. Our results open the door to interesting discussions about how supervised and unsupervised MT might be different yet mutually-beneficial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.162.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.162.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.162/>Evidentiality-guided Generation for Knowledge-Intensive <span class=acl-fixed-case>NLP</span> Tasks</a></strong><br><a href=/people/a/akari-asai/>Akari Asai</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--162><div class="card-body p-3 small">Retrieval-augmented generation models have shown state-of-the-art performance across many knowledge-intensive NLP tasks such as open-domain question answering and fact verification. These models are trained to generate a final output given retrieved passages that can be irrelevant to an input query, leading to learning spurious cues or memorization. This work introduces a method to incorporate <i>evidentiality</i> of passages—whether a passage contains correct evidence to support the output—into training the generator. We introduce a multi-task learning framework to jointly generate the final output and predict the <i>evidentiality</i> of each passage. Furthermore, we introduce a new task-agnostic method for obtaining high-quality <i>silver</i> evidentiality labels, addressing the issues of gold evidentiality labels being unavailable in most domains. Our experiments on five datasets across three knowledge-intensive tasks show that our new evidentiality-guided generator significantly outperforms its direct counterpart on all of them, and advances the state of the art on three of them. Our analysis shows that multi-task learning and silver evidentiality mining play key roles. Our code is available at <a href=https://github.com/AkariAsai/evidentiality_qa class=acl-markup-url>https://github.com/AkariAsai/evidentiality_qa</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.163.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--163 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.163.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.163/>Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning</a></strong><br><a href=/people/y/yu-jin-kim/>Yu Jin Kim</a>
|
<a href=/people/b/beong-woo-kwak/>Beong-woo Kwak</a>
|
<a href=/people/y/youngwook-kim/>Youngwook Kim</a>
|
<a href=/people/r/reinald-kim-amplayo/>Reinald Kim Amplayo</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a>
|
<a href=/people/j/jinyoung-yeo/>Jinyoung Yeo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--163><div class="card-body p-3 small">Commonsense reasoning systems should be able to generalize to diverse reasoning cases. However, most state-of-the-art approaches depend on expensive data annotations and overfit to a specific benchmark without learning how to perform general semantic reasoning. To overcome these drawbacks, zero-shot QA systems have shown promise as a robust learning scheme by transforming a commonsense knowledge graph (KG) into synthetic QA-form samples for model training. Considering the increasing type of different commonsense KGs, this paper aims to extend the zero-shot transfer learning scenario into multiple-source settings, where different KGs can be utilized synergetically. Towards this goal, we propose to mitigate the loss of knowledge from the interference among the different knowledge sources, by developing a modular variant of the knowledge aggregation as a new zero-shot commonsense reasoning framework. Results on five commonsense reasoning benchmarks demonstrate the efficacy of our framework, improving the performance with multiple KGs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.164.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.164.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.164/>Learning to Express in Knowledge-Grounded Conversation</a></strong><br><a href=/people/x/xueliang-zhao/>Xueliang Zhao</a>
|
<a href=/people/t/tingchen-fu/>Tingchen Fu</a>
|
<a href=/people/c/chongyang-tao/>Chongyang Tao</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--164><div class="card-body p-3 small">Grounding dialogue generation by extra knowledge has shown great potentials towards building a system capable of replying with knowledgeable and engaging responses. Existing studies focus on how to synthesize a response with proper knowledge, yet neglect that the same knowledge could be expressed differently by speakers even under the same context. In this work, we mainly consider two aspects of knowledge expression, namely the structure of the response and style of the content in each part. We therefore introduce two sequential latent variables to represent the structure and the content style respectively. We propose a segmentation-based generation model and optimize the model by a variational approach to discover the underlying pattern of knowledge expression in a response. Evaluation results on two benchmarks indicate that our model can learn the structure style defined by a few examples and generate responses in desired content style.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.165.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--165 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.165.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.165" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.165/>End-to-End <span class=acl-fixed-case>C</span>hinese Speaker Identification</a></strong><br><a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/b/ben-zhou/>Ben Zhou</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--165><div class="card-body p-3 small">Speaker identification (SI) in texts aims to identify the speaker(s) for each utterance in texts. Previous studies divide SI into several sub-tasks (e.g., quote extraction, named entity recognition, gender identification, and coreference resolution). However, we are still far from solving these sub-tasks, making SI systems that rely on them seriously suffer from error propagation. End-to-end SI systems, on the other hand, are not limited by individual modules, but suffer from insufficient training data from the existing small-scale datasets. To make large end-to-end models possible, we design a new annotation guideline that regards SI as span extraction from the local context, and we annotate by far the largest SI dataset for Chinese named CSI based on eighteen novels. Viewing SI as a span selection task also introduces the possibility of applying existing storng extractive machine reading comprehension (MRC) baselines. Surprisingly, simply using such a baseline without human-annotated character names and carefully designed rules, we can already achieve performance comparable or better than those of previous state-of-the-art SI methods on all public SI datasets for Chinese. Furthermore, we show that our dataset can serve as additional training data for existing benchmarks, which leads to further gains (up to 6.5% in accuracy). Finally, using CSI as a clean source, we design an effective self-training paradigm to continuously leverage hundreds of unlabeled novels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.166.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--166 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.166.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.166/><span class=acl-fixed-case>MINION</span>: a Large-Scale and Diverse Dataset for Multilingual Event Detection</a></strong><br><a href=/people/a/amir-pouran-ben-veyseh/>Amir Pouran Ben Veyseh</a>
|
<a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/thien-nguyen/>Thien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--166><div class="card-body p-3 small">Event Detection (ED) is the task of identifying and classifying trigger words of event mentions in text. Despite considerable research efforts in recent years for English text, the task of ED in other languages has been significantly less explored. Switching to non-English languages, important research questions for ED include how well existing ED models perform on different languages, how challenging ED is in other languages, and how well ED knowledge and annotation can be transferred across languages. To answer those questions, it is crucial to obtain multilingual ED datasets that provide consistent event annotation for multiple languages. There exist some multilingual ED datasets; however, they tend to cover a handful of languages and mainly focus on popular ones. Many languages are not covered in existing multilingual ED datasets. In addition, the current datasets are often small and not accessible to the public. To overcome those shortcomings, we introduce a new large-scale multilingual dataset for ED (called MINION) that consistently annotates events for 8 different languages; 5 of them have not been supported by existing multilingual datasets. We also perform extensive experiments and analysis to demonstrate the challenges and transferability of ED across languages in MINION that in all call for more research effort in this area. We will release the dataset to promote future research on multilingual ED.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.167.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--167 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.167.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.167" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.167/>Do Prompt-Based Models Really Understand the Meaning of Their Prompts?</a></strong><br><a href=/people/a/albert-webson/>Albert Webson</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--167><div class="card-body p-3 small">Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively “good” prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2021). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models’ impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans’ use of task instructions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.168.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--168 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.168.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.168.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.168" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.168/><span class=acl-fixed-case>GPL</span>: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval</a></strong><br><a href=/people/k/kexin-wang/>Kexin Wang</a>
|
<a href=/people/n/nandan-thakur/>Nandan Thakur</a>
|
<a href=/people/n/nils-reimers/>Nils Reimers</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--168><div class="card-body p-3 small">Dense retrieval approaches can overcome the lexical gap and lead to significantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets. In this paper, we propose the novel unsupervised domain adaptation method <i>Generative Pseudo Labeling</i> (GPL), which combines a query generator with pseudo labeling from a cross-encoder. On six representative domain-specialized datasets, we find the proposed GPL can outperform an out-of-the-box state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL requires less (unlabeled) data from the target domain and is more robust in its training than previous methods. We further investigate the role of six recent pre-training methods in the scenario of domain adaptation for retrieval tasks, where only three could yield improved results. The best approach, TSDAE (Wang et al., 2021) can be combined with GPL, yielding another average improvement of 1.4 points nDCG@10 across the six tasks. The code and the models are available at <a href=https://github.com/UKPLab/gpl class=acl-markup-url>https://github.com/UKPLab/gpl</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.169.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.169.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.169/>Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models</a></strong><br><a href=/people/q/qinyuan-ye/>Qinyuan Ye</a>
|
<a href=/people/m/madian-khabsa/>Madian Khabsa</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/s/sinong-wang/>Sinong Wang</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/a/aaron-jaech/>Aaron Jaech</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--169><div class="card-body p-3 small">Distilling state-of-the-art transformer models into lightweight student models is an effective way to reduce computation cost at inference time. The student models are typically compact transformers with fewer parameters, while expensive operations such as self-attention persist. Therefore, the improved inference speed may still be unsatisfactory for real-time or high-volume use cases. In this paper, we aim to further push the limit of inference speed by distilling teacher models into bigger, sparser student models – bigger in that they scale up to billions of parameters; sparser in that most of the model parameters are n-gram embeddings. Our experiments on six single-sentence text classification tasks show that these student models retain 97% of the RoBERTa-Large teacher performance on average, and meanwhile achieve up to 600x speed-up on both GPUs and CPUs at inference time. Further investigation reveals that our pipeline is also helpful for sentence-pair classification tasks, and in domain generalization settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.170.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.170.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.170/>Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models</a></strong><br><a href=/people/p/patrick-huber/>Patrick Huber</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--170><div class="card-body p-3 small">In this paper, we extend the line of BERTology work by focusing on the important, yet less explored, alignment of pre-trained and fine-tuned PLMs with large-scale discourse structures. We propose a novel approach to infer discourse information for arbitrarily long documents. In our experiments, we find that the captured discourse information is local and general, even across a collection of fine-tuning tasks. We compare the inferred discourse trees with supervised, distantly supervised and simple baselines to explore the structural overlap, finding that constituency discourse trees align well with supervised models, however, contain complementary discourse information. Lastly, we individually explore self-attention matrices to analyze the information redundancy. We find that similar discourse information is consistently captured in the same heads.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.171.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--171 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.171.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.171.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.171/><span class=acl-fixed-case>SAIS</span>: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction</a></strong><br><a href=/people/y/yuxin-xiao/>Yuxin Xiao</a>
|
<a href=/people/z/zecheng-zhang/>Zecheng Zhang</a>
|
<a href=/people/y/yuning-mao/>Yuning Mao</a>
|
<a href=/people/c/carl-yang/>Carl Yang</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--171><div class="card-body p-3 small">Stepping from sentence-level to document-level, the research on relation extraction (RE) confronts increasing text length and more complicated entity interactions. Consequently, it is more challenging to encode the key information sources—relevant contexts and entity types. However, existing methods only implicitly learn to model these critical information sources while being trained for RE. As a result, they suffer the problems of ineffective supervision and uninterpretable model predictions. In contrast, we propose to explicitly teach the model to capture relevant contexts and entity types by supervising and augmenting intermediate steps (SAIS) for RE. Based on a broad spectrum of carefully designed tasks, our proposed SAIS method not only extracts relations of better quality due to more effective supervision, but also retrieves the corresponding supporting evidence more accurately so as to enhance interpretability. By assessing model uncertainty, SAIS further boosts the performance via evidence-based data augmentation and ensemble inference while reducing the computational cost. Eventually, SAIS delivers state-of-the-art RE results on three benchmarks (DocRED, CDR, and GDA) and outperforms the runner-up by 5.04% relatively in F1 score in evidence retrieval on DocRED.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.172.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--172 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.172.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.172" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.172/><span class=acl-fixed-case>LITE</span>: <span class=acl-fixed-case>I</span>ntent-based Task Representation Learning Using Weak Supervision</a></strong><br><a href=/people/n/naoki-otani/>Naoki Otani</a>
|
<a href=/people/m/michael-gamon/>Michael Gamon</a>
|
<a href=/people/s/sujay-kumar-jauhar/>Sujay Kumar Jauhar</a>
|
<a href=/people/m/mei-yang/>Mei Yang</a>
|
<a href=/people/s/sri-raghu-malireddi/>Sri Raghu Malireddi</a>
|
<a href=/people/o/oriana-riva/>Oriana Riva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--172><div class="card-body p-3 small">Users write to-dos as personal notes to themselves, about things they need to complete, remember or organize. To-do texts are usually short and under-specified, which poses a challenge for current text representation models. Yet, understanding and representing their meaning is the first step towards providing intelligent assistance for to-do management. We address this problem by proposing a neural multi-task learning framework, LITE, which extracts representations of English to-do tasks with a multi-head attention mechanism on top of a pre-trained text encoder. To adapt representation models to to-do texts, we collect weak-supervision labels from semantically rich external resources (e.g., dynamic commonsense knowledge bases), following the principle that to-do tasks with similar intents have similar labels. We then train the model on multiple generative/predictive training objectives jointly. We evaluate our representation model on four downstream tasks and show that our approach consistently improves performance over baseline models, achieving error reduction of up to 38.7%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.173.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--173 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.173.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.173/>Does Summary Evaluation Survive Translation to Other Languages?</a></strong><br><a href=/people/s/spencer-braun/>Spencer Braun</a>
|
<a href=/people/o/oleg-vasilyev/>Oleg Vasilyev</a>
|
<a href=/people/n/neslihan-iskender/>Neslihan Iskender</a>
|
<a href=/people/j/john-bohannon/>John Bohannon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--173><div class="card-body p-3 small">The creation of a quality summarization dataset is an expensive, time-consuming effort, requiring the production and evaluation of summaries by both trained humans and machines. The returns to such an effort would increase significantly if the dataset could be used in additional languages without repeating human annotations. To investigate how much we can trust machine translation of summarization datasets, we translate the English SummEval dataset to seven languages and compare performances across automatic evaluation measures. We explore equivalence testing as the appropriate statistical paradigm for evaluating correlations between human and automated scoring of summaries. We also consider the effect of translation on the relative performance between measures. We find some potential for dataset reuse in languages similar to the source and along particular dimensions of summary quality. Our code and data can be found at <a href=https://github.com/PrimerAI/primer-research/ class=acl-markup-url>https://github.com/PrimerAI/primer-research/</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.174.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.174.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.174/>A Shoulder to Cry on: Towards A Motivational Virtual Assistant for Assuaging Mental Agony</a></strong><br><a href=/people/t/tulika-saha/>Tulika Saha</a>
|
<a href=/people/s/saichethan-reddy/>Saichethan Reddy</a>
|
<a href=/people/a/anindya-das/>Anindya Das</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--174><div class="card-body p-3 small">Mental Health Disorders continue plaguing humans worldwide. Aggravating this situation is the severe shortage of qualified and competent mental health professionals (MHPs), which underlines the need for developing Virtual Assistants (VAs) that can <i>assist</i> MHPs. The data+ML for automation can come from platforms that allow visiting and posting messages in peer-to-peer anonymous manner for sharing their experiences (frequently stigmatized) and seeking support. In this paper, we propose a VA that can act as the first point of contact and comfort for mental health patients. We curate a dataset, Motivational VA: MotiVAte comprising of 7k dyadic conversations collected from a peer-to-peer support platform. The system employs two mechanisms: (i) Mental Illness Classification: an attention based BERT classifier that outputs the mental disorder category out of the 4 categories, viz., Major Depressive Disorder (MDD), Anxiety, Obsessive Compulsive Disorder (OCD) and Post-traumatic Stress Disorder (PTSD), based on the input ongoing dialog between the support seeker and the VA; and (ii) Mental Illness Conditioned Motivational Dialogue Generation (MI-MDG): a sentiment driven Reinforcement Learning (RL) based motivational response generator. The empirical evaluation demonstrates the system capability by way of outperforming several baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.175.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--175 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.175.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.175" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.175/><span class=acl-fixed-case>S</span>ue<span class=acl-fixed-case>N</span>es: A Weakly Supervised Approach to Evaluating Single-Document Summarization via Negative Sampling</a></strong><br><a href=/people/f/forrest-bao/>Forrest Bao</a>
|
<a href=/people/g/ge-luo/>Ge Luo</a>
|
<a href=/people/h/hebi-li/>Hebi Li</a>
|
<a href=/people/m/minghui-qiu/>Minghui Qiu</a>
|
<a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/y/youbiao-he/>Youbiao He</a>
|
<a href=/people/c/cen-chen/>Cen Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--175><div class="card-body p-3 small">Canonical automatic summary evaluation metrics, such as ROUGE, focus on lexical similarity which cannot well capture semantics nor linguistic quality and require a reference summary which is costly to obtain. Recently, there have been a growing number of efforts to alleviate either or both of the two drawbacks. In this paper, we present a proof-of-concept study to a weakly supervised summary evaluation approach without the presence of reference summaries. Massive data in existing summarization datasets are transformed for training by pairing documents with corrupted reference summaries. In cross-domain tests, our strategy outperforms baselines with promising improvements, and show a great advantage in gauging linguistic qualities over all metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.176.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.176.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.176" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.176/>Combating the Curse of Multilinguality in Cross-Lingual <span class=acl-fixed-case>WSD</span> by Aligning Sparse Contextualized Word Representations</a></strong><br><a href=/people/g/gabor-berend/>Gábor Berend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--176><div class="card-body p-3 small">In this paper, we advocate for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism. We also report rigorous experiments that illustrate the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure. Our experimental results demonstrate that the above modifications yield a significant improvement of nearly 6.5 points of increase in the average F-score (from 62.0 to 68.5) over a collection of 17 typologically diverse set of target languages. We release our source code for replicating our experiments at <a href=https://github.com/begab/sparsity_makes_sense class=acl-markup-url>https://github.com/begab/sparsity_makes_sense</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.177.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--177 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.177.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.177/>Cheat Codes to Quantify Missing Source Information in Neural Machine Translation</a></strong><br><a href=/people/p/proyag-pal/>Proyag Pal</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--177><div class="card-body p-3 small">This paper describes a method to quantify the amount of information <span class=tex-math>H(t|s)</span> added by the target sentence <span class=tex-math>t</span> that is not present in the source <span class=tex-math>s</span> in a neural machine translation system. We do this by providing the model the target sentence in a highly compressed form (a “cheat code”), and exploring the effect of the size of the cheat code. We find that the model is able to capture extra information from just a single float representation of the target and nearly reproduces the target with two 32-bit floats per target token.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.178.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.178.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.178/><span class=acl-fixed-case>W</span>i<span class=acl-fixed-case>C</span> = <span class=acl-fixed-case>TSV</span> = <span class=acl-fixed-case>WSD</span>: On the Equivalence of Three Semantic Tasks</a></strong><br><a href=/people/b/bradley-hauer/>Bradley Hauer</a>
|
<a href=/people/g/grzegorz-kondrak/>Grzegorz Kondrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--178><div class="card-body p-3 small">The Word-in-Context (WiC) task has attracted considerable attention in the NLP community, as demonstrated by the popularity of the recent MCL-WiC SemEval shared task. Systems and lexical resources from word sense disambiguation (WSD) are often used for the WiC task and WiC dataset construction. In this paper, we establish the exact relationship between WiC and WSD, as well as the related task of target sense verification (TSV). Building upon a novel hypothesis on the equivalence of sense and meaning distinctions, we demonstrate through the application of tools from theoretical computer science that these three semantic classification problems can be pairwise reduced to each other, and therefore are equivalent. The results of experiments that involve systems and datasets for both WiC and WSD provide strong empirical evidence that our problem reductions work in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.179.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--179 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.179.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.179" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.179/>What do tokens know about their characters and how do they know it?</a></strong><br><a href=/people/a/ayush-kaushal/>Ayush Kaushal</a>
|
<a href=/people/k/kyle-mahowald/>Kyle Mahowald</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--179><div class="card-body p-3 small">Pre-trained language models (PLMs) that use subword tokenization schemes can succeed at a variety of language tasks that require character-level information, despite lacking explicit access to the character composition of tokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe), we probe what word pieces encode about character-level information by training classifiers to predict the presence or absence of a particular alphabetical character in a token, based on its embedding (e.g., probing whether the model embedding for “cat” encodes that it contains the character “a”). We find that these models robustly encode character-level information and, in general, larger models perform better at the task. We show that these results generalize to characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic). Then, through a series of experiments and analyses, we investigate the mechanisms through which PLMs acquire English-language character information during training and argue that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.180.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.180.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.180" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.180/><span class=acl-fixed-case>A</span>nswer<span class=acl-fixed-case>S</span>umm: A Manually-Curated Dataset and Pipeline for Answer Summarization</a></strong><br><a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/x/xiaojian-wu/>Xiaojian Wu</a>
|
<a href=/people/s/srini-iyer/>Srini Iyer</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--180><div class="card-body p-3 small">Community Question Answering (CQA) fora such as Stack Overflow and Yahoo! Answers contain a rich resource of answers to a wide range of community-based questions. Each question thread can receive a large number of answers with different perspectives. One goal of answer summarization is to produce a summary that reflects the range of answer perspectives. A major obstacle for this task is the absence of a dataset to provide supervision for producing such summaries. Recent works propose heuristics to create such data, but these are often noisy and do not cover all answer perspectives present. This work introduces a novel dataset of 4,631 CQA threads for answer summarization curated by professional linguists. Our pipeline gathers annotations for all subtasks of answer summarization, including relevant answer sentence selection, grouping these sentences based on perspectives, summarizing each perspective, and producing an overall summary. We analyze and benchmark state-of-the-art models on these subtasks and introduce a novel unsupervised approach for multi-perspective data augmentation that boosts summarization performance according to automatic evaluation. Finally, we propose reinforcement learning rewards to improve factual consistency and answer coverage and analyze areas for improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.181.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.181.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.181" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.181/>Paragraph-based Transformer Pre-training for Multi-Sentence Inference</a></strong><br><a href=/people/l/luca-di-liello/>Luca Di Liello</a>
|
<a href=/people/s/siddhant-garg/>Siddhant Garg</a>
|
<a href=/people/l/luca-soldaini/>Luca Soldaini</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--181><div class="card-body p-3 small">Inference tasks such as answer sentence selection (AS2) or fact verification are typically solved by fine-tuning transformer-based models as individual sentence-pair classifiers. Recent studies show that these tasks benefit from modeling dependencies across multiple candidate sentences jointly. In this paper, we first show that popular pre-trained transformers perform poorly when used for fine-tuning on multi-candidate inference tasks. We then propose a new pre-training objective that models the paragraph-level semantics across multiple input sentences. Our evaluation on three AS2 and one fact verification datasets demonstrates the superiority of our pre-training technique over the traditional ones for transformers used as joint models for multi-candidate inference tasks, as well as when used as cross-encoders for sentence-pair formulations of these tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.182.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.182.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.182.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.182/>Text Style Transfer via Optimal Transport</a></strong><br><a href=/people/n/nasim-nouri/>Nasim Nouri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--182><div class="card-body p-3 small">Text style transfer (TST) is a well-known task whose goal is to convert the style of the text (e.g., from formal to informal) while preserving its content. Recently, it has been shown that both syntactic and semantic similarities between the source and the converted text are important for TST. However, the interaction between these two concepts has not been modeled. In this work, we propose a novel method based on Optimal Transport for TST to simultaneously incorporate syntactic and semantic information into similarity computation between the source and the converted text. We evaluate the proposed method in both supervised and unsupervised settings. Our analysis reveal the superiority of the proposed model in both settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.183.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--183 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.183.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.183/>Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning</a></strong><br><a href=/people/v/vishakh-padmakumar/>Vishakh Padmakumar</a>
|
<a href=/people/l/leonard-lausen/>Leonard Lausen</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/s/sheng-zha/>Sheng Zha</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/g/george-karypis/>George Karypis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--183><div class="card-body p-3 small">Recent work has found that multi-task training with a large number of diverse tasks can uniformly improve downstream performance on unseen target tasks. In contrast, literature on task transferability has established that the choice of intermediate tasks can heavily affect downstream task performance. In this work, we aim to disentangle the effect of scale and relatedness of tasks in multi-task representation learning. We find that, on average, increasing the scale of multi-task learning, in terms of the number of tasks, indeed results in better learned representations than smaller multi-task setups. However, if the target tasks are known ahead of time, then training on a smaller set of related tasks is competitive to the large-scale multi-task training at a reduced computational cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.184.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.184.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.184" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.184/>Interactive Query-Assisted Summarization via Deep Reinforcement Learning</a></strong><br><a href=/people/o/ori-shapira/>Ori Shapira</a>
|
<a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/y/yael-amsterdamer/>Yael Amsterdamer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--184><div class="card-body p-3 small">Interactive summarization is a task that facilitates user-guided exploration of information within a document set. While one would like to employ state of the art neural models to improve the quality of interactive summarization, many such technologies cannot ingest the full document set or cannot operate at sufficient speed for interactivity. To that end, we propose two novel deep reinforcement learning models for the task that address, respectively, the subtask of summarizing salient information that adheres to user queries, and the subtask of listing suggested queries to assist users throughout their exploration. In particular, our models allow encoding the interactive session state and history to refrain from redundancy. Together, these models compose a state of the art solution that addresses all of the task requirements. We compare our solution to a recent interactive summarization system, and show through an experimental study involving real users that our models are able to improve informativeness while preserving positive user experience.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.185.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.185.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.185/>Data Augmentation with Dual Training for Offensive Span Detection</a></strong><br><a href=/people/n/nasim-nouri/>Nasim Nouri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--185><div class="card-body p-3 small">Recognizing offensive text is an important requirement for every content management system, especially for social networks. While the majority of the prior work formulate this problem as text classification, i.e., if a text excerpt is offensive or not, in this work we propose a novel model for offensive span detection (OSD), whose goal is to identify the spans responsible for the offensive tone of the text. One of the challenges to train a model for this novel setting is the lack of enough training data. To address this limitation, in this work we propose a novel method in which the large-scale pre-trained language model GPT-2 is employed to generate synthetic training data for OSD. In particular, we propose to train the GPT-2 model in a dual-training setting using the REINFORCE algorithm to generate in-domain, natural and diverse training samples. Extensive experiments on the benchmark dataset for OSD reveal the effectiveness of the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.186.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--186 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.186.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.186/>Training Mixed-Domain Translation Models via Federated Learning</a></strong><br><a href=/people/p/peyman-passban/>Peyman Passban</a>
|
<a href=/people/t/tanya-roosta/>Tanya Roosta</a>
|
<a href=/people/r/rahul-gupta/>Rahul Gupta</a>
|
<a href=/people/a/ankit-chadha/>Ankit Chadha</a>
|
<a href=/people/c/clement-chung/>Clement Chung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--186><div class="card-body p-3 small">Training mixed-domain translation models is a complex task that demands tailored architec- tures and costly data preparation techniques. In this work, we leverage federated learning (FL) in order to tackle the problem. Our investiga- tion demonstrates that with slight modifications in the training process, neural machine trans- lation (NMT) engines can be easily adapted when an FL-based aggregation is applied to fuse different domains. Experimental results also show that engines built via FL are able to perform on par with state-of-the-art baselines that rely on centralized training techniques. We evaluate our hypothesis in the presence of five datasets with different sizes, from different domains, to translate from German into English and discuss how FL and NMT can mutually benefit from each other. In addition to provid- ing benchmarking results on the union of FL and NMT, we also propose a novel technique to dynamically control the communication band- width by selecting impactful parameters during FL updates. This is a significant achievement considering the large size of NMT engines that need to be exchanged between FL parties.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.187.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--187 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.187.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.187" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.187/><span class=acl-fixed-case>QAF</span>act<span class=acl-fixed-case>E</span>val: Improved <span class=acl-fixed-case>QA</span>-Based Factual Consistency Evaluation for Summarization</a></strong><br><a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/w/wenhao-liu/>Wenhao Liu</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--187><div class="card-body p-3 small">Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the components of a QA-based metric, especially question generation and answerability classification, is critical to performance. Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric. Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.188.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--188 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.188.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.188" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.188/>How Gender Debiasing Affects Internal Model Representations, and Why It Matters</a></strong><br><a href=/people/h/hadas-orgad/>Hadas Orgad</a>
|
<a href=/people/s/seraphina-goldfarb-tarrant/>Seraphina Goldfarb-Tarrant</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--188><div class="card-body p-3 small">Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models’ internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together: we debias a model during downstream fine-tuning, which reduces extrinsic bias, and measure the effect on intrinsic bias, which is operationalized as bias extractability with information-theoretic probing. Through experiments on two tasks and multiple bias metrics, we show that our intrinsic bias metric is a better indicator of debiasing than (a contextual adaptation of) the standard WEAT metric, and can also expose cases of superficial debiasing. Our framework provides a comprehensive perspective on bias in NLP models, which can be applied to deploy NLP systems in a more informed manner. Our code and model checkpoints are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.189.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.189.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.189" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.189/>A Structured Span Selector</a></strong><br><a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/y/yuchen-jiang/>Yuchen Jiang</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--189><div class="card-body p-3 small">Many natural language processing tasks, e.g., coreference resolution and semantic role labeling, require selecting text spans and making decisions about them. A typical approach to such tasks is to score all possible spans and greedily select spans for task-specific downstream processing. This approach, however, does not incorporate any inductive bias about what sort of spans ought to be selected, e.g., that selected spans tend to be syntactic constituents. In this paper, we propose a novel grammar-based structured span selection model which learns to make use of the partial span-level annotation provided for such problems. Compared to previous approaches, our approach gets rid of the heuristic greedy span selection scheme, allowing us to model the downstream task on an optimal set of spans. We evaluate our model on two popular span prediction tasks: coreference resolution and semantic role labeling; and show improvements on both.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.190.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--190 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.190.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.190" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.190/>Unified Semantic Typing with Meaningful Label Inference</a></strong><br><a href=/people/j/james-y-huang/>James Y. Huang</a>
|
<a href=/people/b/bangzheng-li/>Bangzheng Li</a>
|
<a href=/people/j/jiashu-xu/>Jiashu Xu</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--190><div class="card-body p-3 small">Semantic typing aims at classifying tokens or spans of interest in a textual context into semantic categories such as relations, entity types, and event types. The inferred labels of semantic categories meaningfully interpret how machines understand components of text. In this paper, we present UniST, a unified framework for semantic typing that captures label semantics by projecting both inputs and labels into a joint semantic embedding space. To formulate different lexical and relational semantic typing tasks as a unified task, we incorporate task descriptions to be jointly encoded with the input, allowing UniST to be adapted to different tasks without introducing task-specific model components. UniST optimizes a margin ranking loss such that the semantic relatedness of the input and labels is reflected from their embedding similarity. Our experiments demonstrate that UniST achieves strong performance across three semantic typing tasks: entity typing, relation classification and event typing. Meanwhile, UniST effectively transfers semantic knowledge of labels and substantially improves generalizability on inferring rarely seen and unseen types. In addition, multiple semantic typing tasks can be jointly trained within the unified framework, leading to a single compact multi-tasking model that performs comparably to dedicated single-task models, while offering even better transferability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.191.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.191.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.191/>Learning To Retrieve Prompts for In-Context Learning</a></strong><br><a href=/people/o/ohad-rubin/>Ohad Rubin</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--191><div class="card-body p-3 small">In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.192.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--192 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.192.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.192.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.192" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.192/>Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection</a></strong><br><a href=/people/e/esma-balkir/>Esma Balkir</a>
|
<a href=/people/i/isar-nejadgholi/>Isar Nejadgholi</a>
|
<a href=/people/k/kathleen-c-fraser/>Kathleen Fraser</a>
|
<a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--192><div class="card-body p-3 small">We present a novel feature attribution method for explaining text classifiers, and analyze it in the context of hate speech detection. Although feature attribution models usually provide a single importance score for each token, we instead provide two complementary and theoretically-grounded scores – necessity and sufficiency – resulting in more informative explanations. We propose a transparent method that calculates these values by generating explicit perturbations of the input text, allowing the importance scores themselves to be explainable. We employ our method to explain the predictions of different hate speech detection models on the same set of curated examples from a test suite, and show that different values of necessity and sufficiency for identity terms correspond to different kinds of false positive errors, exposing sources of classifier bias against marginalized groups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.193.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--193 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.193.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.193.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.193" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.193/>Learning to Retrieve Passages without Supervision</a></strong><br><a href=/people/o/ori-ram/>Ori Ram</a>
|
<a href=/people/g/gal-shachaf/>Gal Shachaf</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a>
|
<a href=/people/a/amir-globerson/>Amir Globerson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--193><div class="card-body p-3 small">Dense retrievers for open-domain question answering (ODQA) have been shown to achieve impressive performance by training on large datasets of question-passage pairs. In this work we ask whether this dependence on labeled data can be reduced via unsupervised pretraining that is geared towards ODQA. We show this is in fact possible, via a novel pretraining scheme designed for retrieval. Our “recurring span retrieval” approach uses recurring spans across passages in a document to create pseudo examples for contrastive learning. Our pretraining scheme directly controls for term overlap across pseudo queries and relevant passages, thus allowing to model both lexical and semantic relations between them. The resulting model, named Spider, performs surprisingly well without any labeled training examples on a wide range of ODQA datasets. Specifically, it significantly outperforms all other pretrained baselines in a zero-shot setting, and is competitive with BM25, a strong sparse baseline. Moreover, a hybrid retriever over Spider and BM25 improves over both, and is often competitive with DPR models, which are trained on tens of thousands of examples. Last, notable gains are observed when using Spider as an initialization for supervised training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.194.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--194 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.194.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.194" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.194/><span class=acl-fixed-case>R</span>e2<span class=acl-fixed-case>G</span>: Retrieve, Rerank, Generate</a></strong><br><a href=/people/m/michael-glass/>Michael Glass</a>
|
<a href=/people/g/gaetano-rossiello/>Gaetano Rossiello</a>
|
<a href=/people/m/md-faisal-mahbub-chowdhury/>Md Faisal Mahbub Chowdhury</a>
|
<a href=/people/a/ankita-naik/>Ankita Naik</a>
|
<a href=/people/p/pengshan-cai/>Pengshan Cai</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--194><div class="card-body p-3 small">As demonstrated by GPT-3 and T5, transformers grow in capability as parameter spaces become larger and larger. However, for tasks that require a large amount of knowledge, non-parametric memory allows models to grow dramatically with a sub-linear increase in computational cost and GPU memory requirements. Recent models such as RAG and REALM have introduced retrieval into conditional generation. These models incorporate neural initial retrieval from a corpus of passages. We build on this line of research, proposing Re2G, which combines both neural initial retrieval and reranking into a BART-based sequence-to-sequence generation. Our reranking approach also permits merging retrieval results from sources with incomparable scores, enabling an ensemble of BM25 and neural initial retrieval. To train our system end-to-end, we introduce a novel variation of knowledge distillation to train the initial retrieval, reranker and generation using only ground truth on the target sequence output. We find large gains in four diverse tasks: zero-shot slot filling, question answering, fact checking and dialog, with relative gains of 9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make our code available as open source.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.195.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.195.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.195.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.195" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.195/>Don’t sweat the small stuff, classify the rest: Sample Shielding to protect text classifiers against adversarial attacks</a></strong><br><a href=/people/j/jonathan-rusert/>Jonathan Rusert</a>
|
<a href=/people/p/padmini-srinivasan/>Padmini Srinivasan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--195><div class="card-body p-3 small">Deep learning (DL) is being used extensively for text classification. However, researchers have demonstrated the vulnerability of such classifiers to adversarial attacks. Attackers modify the text in a way which misleads the classifier while keeping the original meaning close to intact. State-of-the-art (SOTA) attack algorithms follow the general principle of making minimal changes to the text so as to not jeopardize semantics. Taking advantage of this we propose a novel and intuitive defense strategy called Sample Shielding.It is attacker and classifier agnostic, does not require any reconfiguration of the classifier or external resources and is simple to implement. Essentially, we sample subsets of the input text, classify them and summarize these into a final decision. We shield three popular DL text classifiers with Sample Shielding, test their resilience against four SOTA attackers across three datasets in a realistic threat setting. Even when given the advantage of knowing about our shielding strategy the adversary’s attack success rate is &lt;=10% with only one exception and often &lt; 5%. Additionally, Sample Shielding maintains near original accuracy when applied to original texts. Crucially, we show that the ‘make minimal changes’ approach of SOTA attackers leads to critical vulnerabilities that can be defended against with an intuitive sampling strategy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.196.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--196 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.196.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.196/>Federated Learning with Noisy User Feedback</a></strong><br><a href=/people/r/rahul-sharma/>Rahul Sharma</a>
|
<a href=/people/a/anil-ramakrishna/>Anil Ramakrishna</a>
|
<a href=/people/a/ansel-maclaughlin/>Ansel MacLaughlin</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/j/jimit-majmudar/>Jimit Majmudar</a>
|
<a href=/people/c/clement-chung/>Clement Chung</a>
|
<a href=/people/s/salman-avestimehr/>Salman Avestimehr</a>
|
<a href=/people/r/rahul-gupta/>Rahul Gupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--196><div class="card-body p-3 small">Machine Learning (ML) systems are getting increasingly popular, and drive more and more applications and services in our daily life. Thishas led to growing concerns over user privacy, since human interaction data typically needs to be transmitted to the cloud in order to trainand improve such systems. Federated learning (FL) has recently emerged as a method for training ML models on edge devices using sensitive user data and is seen as a way to mitigate concerns over data privacy. However, since ML models are most commonly trained with label supervision, we need a way to extract labels on edge to make FL viable. In this work, we propose a strategy for training FL models using positive and negative user feedback. We also design a novel framework to study different noise patterns in user feedback, and explore how well standard noise-robust objectives can help mitigate this noise when training models in a federated setting. We evaluate our proposed training setup through detailed experiments on two text classification datasets and analyze the effects of varying levels of user reliability and feedback noise on model performance. We show that our method improves substantially over a self-training baseline, achieving performance closer to models trained with full supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.197.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.197.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.197" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.197/>Gender Bias in Masked Language Models for Multiple Languages</a></strong><br><a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/a/aizhan-imankulova/>Aizhan Imankulova</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--197><div class="card-body p-3 small">Masked Language Models (MLMs) pre-trained by predicting masked tokens on large corpora have been used successfully in natural language processing tasks for a variety of languages. Unfortunately, it was reported that MLMs also learn discriminative biases regarding attributes such as gender and race. Because most studies have focused on MLMs in English, the bias of MLMs in other languages has rarely been investigated. Manual annotation of evaluation data for languages other than English has been challenging due to the cost and difficulty in recruiting annotators. Moreover, the existing bias evaluation methods require the stereotypical sentence pairs consisting of the same context with attribute words (e.g. He/She is a nurse).We propose Multilingual Bias Evaluation (MBE) score, to evaluate bias in various languages using only English attribute word lists and parallel corpora between the target language and English without requiring manually annotated data. We evaluated MLMs in eight languages using the MBE and confirmed that gender-related biases are encoded in MLMs for all those languages. We manually created datasets for gender bias in Japanese and Russian to evaluate the validity of the MBE.The results show that the bias scores reported by the MBE significantly correlates with that computed from the above manually created datasets and the existing English datasets for gender bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.198.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.198.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.198/>Multi-Domain Targeted Sentiment Analysis</a></strong><br><a href=/people/o/orith-toledo-ronen/>Orith Toledo-Ronen</a>
|
<a href=/people/m/matan-orbach/>Matan Orbach</a>
|
<a href=/people/y/yoav-katz/>Yoav Katz</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--198><div class="card-body p-3 small">Targeted Sentiment Analysis (TSA) is a central task for generating insights from consumer reviews. Such content is extremely diverse, with sites like Amazon or Yelp containing reviews on products and businesses from many different domains. A real-world TSA system should gracefully handle that diversity. This can be achieved by a multi-domain model – one that is robust to the domain of the analyzed texts, and performs well on various domains. To address this scenario, we present a multi-domain TSA system based on augmenting a given training set with diverse weak labels from assorted domains. These are obtained through self-training on the Yelp reviews corpus. Extensive experiments with our approach on three evaluation datasets across different domains demonstrate the effectiveness of our solution. We further analyze how restrictions imposed on the available labeled data affect the performance, and compare the proposed method to the costly alternative of manually gathering diverse TSA labeled data. Our results and analysis show that our approach is a promising step towards a practical domain-robust TSA system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.199.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.199.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.199" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.199/>Falsesum: Generating Document-level <span class=acl-fixed-case>NLI</span> Examples for Recognizing Factual Inconsistency in Summarization</a></strong><br><a href=/people/p/prasetya-utama/>Prasetya Utama</a>
|
<a href=/people/j/joshua-bambrick/>Joshua Bambrick</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Moosavi</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--199><div class="card-body p-3 small">Neural abstractive summarization models are prone to generate summaries that are factually inconsistent with their source documents. Previous work has introduced the task of recognizing such factual inconsistency as a downstream application of natural language inference (NLI). However, state-of-the-art NLI models perform poorly in this context due to their inability to generalize to the target task. In this work, we show that NLI models can be effective for this task when the training data is augmented with high-quality task-oriented examples. We introduce Falsesum, a data generation pipeline leveraging a controllable text generation model to perturb human-annotated summaries, introducing varying types of factual inconsistencies. Unlike previously introduced document-level NLI datasets, our generated dataset contains examples that are diverse and inconsistent yet plausible. We show that models trained on a Falsesum-augmented NLI dataset improve the state-of-the-art performance across four benchmarks for detecting factual inconsistency in summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.200.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--200 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.200.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.200/>Dynamic Gazetteer Integration in Multilingual Models for Cross-Lingual and Cross-Domain Named Entity Recognition</a></strong><br><a href=/people/b/besnik-fetahu/>Besnik Fetahu</a>
|
<a href=/people/a/anjie-fang/>Anjie Fang</a>
|
<a href=/people/o/oleg-rokhlenko/>Oleg Rokhlenko</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--200><div class="card-body p-3 small">Named entity recognition (NER) in a real-world setting remains challenging and is impacted by factors like text genre, corpus quality, and data availability. NER models trained on CoNLL do not transfer well to other domains, even within the same language. This is especially the case for multi-lingual models when applied to low-resource languages, and is mainly due to missing entity information. We propose an approach that with limited effort and data, addresses the NER knowledge gap across languages and domains. Our novel approach uses a token-level gating layer to augment pre-trained multilingual transformers with gazetteers containing named entities (NE) from a target language or domain. This approach provides the flexibility to jointly integrate both textual and gazetteer information dynamically: entity knowledge from gazetteers is used only when a token’s textual representation is insufficient for the NER task. Evaluation on several languages and domains demonstrates: (i) a high mismatch of reported NER performance on CoNLL vs. domain specific datasets, (ii) gazetteers significantly improve NER performance across languages and domains, and (iii) gazetteers can be flexibly incorporated to guide knowledge transfer. On cross-lingual transfer we achieve an improvement over the baseline with F1=+17.6%, and with F1=+21.3% for cross-domain transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.201.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--201 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.201.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.201" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.201/><span class=acl-fixed-case>M</span>eta<span class=acl-fixed-case>ICL</span>: Learning to Learn In Context</a></strong><br><a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--201><div class="card-body p-3 small">We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task training data, and outperforms much bigger models with nearly 8x parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.202.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--202 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.202.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.202/>Enhancing Knowledge Selection for Grounded Dialogues via Document Semantic Graphs</a></strong><br><a href=/people/s/sha-li/>Sha Li</a>
|
<a href=/people/m/mahdi-namazifar/>Mahdi Namazifar</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--202><div class="card-body p-3 small">Providing conversation models with background knowledge has been shown to make open-domain dialogues more informative and engaging. Existing models treat knowledge selection as a sentence ranking or classification problem where each sentence is handled individually, ignoring the internal semantic connection between sentences. In this work, we propose to automatically convert the background knowledge documents into document semantic graphs and then perform knowledge selection over such graphs. Our document semantic graphs preserve sentence-level information through the use of sentence nodes and provide concept connections between sentences. We apply multi-task learning to perform sentence-level knowledge selection and concept-level knowledge selection, showing that it improves sentence-level selection. Our experiments show that our semantic graph-based knowledge selection improves over sentence selection baselines for both the knowledge selection task and the end-to-end response generation task on HollE and improves generalization on unseen topics in WoW.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.203.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.203.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.203/>Using Natural Sentence Prompts for Understanding Biases in Language Models</a></strong><br><a href=/people/s/sarah-alnegheimish/>Sarah Alnegheimish</a>
|
<a href=/people/a/alicia-guo/>Alicia Guo</a>
|
<a href=/people/y/yi-sun/>Yi Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--203><div class="card-body p-3 small">Evaluation of biases in language models is often limited to synthetically generated datasets. This dependence traces back to the need of prompt-style dataset to trigger specific behaviors of language models. In this paper, we address this gap by creating a prompt dataset with respect to occupations collected from real-world natural sentences present in Wikipedia.We aim to understand the differences between using template-based prompts and natural sentence prompts when studying gender-occupation biases in language models. We find bias evaluations are very sensitiveto the design choices of template prompts, and we propose using natural sentence prompts as a way of more systematically using real-world sentences to move away from design decisions that may bias the results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.204.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.204.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.204" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.204/>Robust Conversational Agents against Imperceptible Toxicity Triggers</a></strong><br><a href=/people/n/ninareh-mehrabi/>Ninareh Mehrabi</a>
|
<a href=/people/a/ahmad-beirami/>Ahmad Beirami</a>
|
<a href=/people/f/fred-morstatter/>Fred Morstatter</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--204><div class="card-body p-3 small">Warning: this paper contains content that maybe offensive or upsetting. Recent research in Natural Language Processing (NLP) has advanced the development of various toxicity detection models with the intention of identifying and mitigating toxic language from existing systems. Despite the abundance of research in this area, less attention has been given to adversarial attacks that force the system to generate toxic language and the defense against them. Existing work to generate such attacks is either based on human-generated attacks which is costly and not scalable or, in case of automatic attacks, the attack vector does not conform to human-like language, which can be detected using a language model loss. In this work, we propose attacks against conversational agents that are imperceptible, i.e., they fit the conversation in terms of coherency, relevancy, and fluency, while they are effective and scalable, i.e., they can automatically trigger the system into generating toxic language. We then propose a defense mechanism against such attacks which not only mitigates the attack but also attempts to maintain the conversational flow. Through automatic and human evaluations, we show that our defense is effective at avoiding toxic language generation even against imperceptible toxicity triggers while the generated language fits the conversation in terms of coherency and relevancy. Lastly, we establish the generalizability of such a defense mechanism on language generation models beyond conversational agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.205.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.205.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.205.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.205" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.205/>Selective Differential Privacy for Language Modeling</a></strong><br><a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/a/aiqi-cui/>Aiqi Cui</a>
|
<a href=/people/e/evan-li/>Evan Li</a>
|
<a href=/people/r/ruoxi-jia/>Ruoxi Jia</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--205><div class="card-body p-3 small">With the increasing applications of language models, it has become crucial to protect these models from leaking private information. Previous work has attempted to tackle this challenge by training RNN-based language models with differential privacy guarantees. However, applying classical differential privacy to language models leads to poor model performance as the underlying privacy notion is over-pessimistic and provides undifferentiated protection for all tokens in the data. Given that the private information in natural language is sparse (for example, the bulk of an email might not carry personally identifiable information), we propose a new privacy notion, selective differential privacy, to provide rigorous privacy guarantees on the sensitive portion of the data to improve model utility. To realize such a new notion, we develop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based language models. Besides language modeling, we also apply the method to a more concrete application – dialog systems. Experiments on both language modeling and dialog system building show that the proposed privacy-preserving mechanism achieves better utilities while remaining safe under various privacy attacks compared to the baselines. The data and code are released at <a href=https://github.com/wyshi/lm_privacy class=acl-markup-url>https://github.com/wyshi/lm_privacy</a> to facilitate future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.206.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--206 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.206.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.206/>Do Trajectories Encode Verb Meaning?</a></strong><br><a href=/people/d/dylan-ebert/>Dylan Ebert</a>
|
<a href=/people/c/chen-sun/>Chen Sun</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--206><div class="card-body p-3 small">Distributional models learn representations of words from text, but are criticized for their lack of grounding, or the linking of text to the non-linguistic world. Grounded language models have had success in learning to connect concrete categories like nouns and adjectives to the world via images and videos, but can struggle to isolate the meaning of the verbs themselves from the context in which they typically occur. In this paper, we investigate the extent to which trajectories (i.e. the position and rotation of objects over time) naturally encode verb semantics. We build a procedurally generated agent-object-interaction dataset, obtain human annotations for the verbs that occur in this data, and compare several methods for representation learning given the trajectories. We find that trajectories correlate as-is with some verbs (e.g., fall), and that additional abstraction via self-supervised pretraining can further capture nuanced differences in verb meaning (e.g., roll and slide).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.207.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.207.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.207" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.207/>Long Context Question Answering via Supervised Contrastive Learning</a></strong><br><a href=/people/a/avi-caciularu/>Avi Caciularu</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/j/jacob-goldberger/>Jacob Goldberger</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--207><div class="card-body p-3 small">Long-context question answering (QA) tasks require reasoning over a long document or multiple documents. Addressing these tasks often benefits from identifying a set of evidence spans (e.g., sentences), which provide supporting evidence for answering the question. In this work, we propose a novel method for equipping long-context QA models with an additional sequence-level objective for better identification of the supporting evidence. We achieve this via an additional contrastive supervision signal in finetuning, where the model is encouraged to explicitly discriminate supporting evidence sentences from negative ones by maximizing question-evidence similarity. The proposed additional loss exhibits consistent improvements on three different strong long-context transformer models, across two challenging question answering benchmarks – HotpotQA and QAsper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.208.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.208.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.208/>The <span class=acl-fixed-case>USMLE</span>® Step 2 Clinical Skills Patient Note Corpus</a></strong><br><a href=/people/v/victoria-yaneva/>Victoria Yaneva</a>
|
<a href=/people/j/janet-mee/>Janet Mee</a>
|
<a href=/people/l/le-ha/>Le Ha</a>
|
<a href=/people/p/polina-harik/>Polina Harik</a>
|
<a href=/people/m/michael-jodoin/>Michael Jodoin</a>
|
<a href=/people/a/alex-mechaber/>Alex Mechaber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--208><div class="card-body p-3 small">This paper presents a corpus of 43,985 clinical patient notes (PNs) written by 35,156 examinees during the high-stakes USMLE® Step 2 Clinical Skills examination. In this exam, examinees interact with standardized patients - people trained to portray simulated scenarios called clinical cases. For each encounter, an examinee writes a PN, which is then scored by physician raters using a rubric of clinical concepts, expressions of which should be present in the PN. The corpus features PNs from 10 clinical cases, as well as the clinical concepts from the case rubrics. A subset of 2,840 PNs were annotated by 10 physician experts such that all 143 concepts from the case rubrics (e.g., shortness of breath) were mapped to 34,660 PN phrases (e.g., dyspnea, difficulty breathing). The corpus is available via a data sharing agreement with NBME and can be requested at <a href=https://www.nbme.org/services/data-sharing class=acl-markup-url>https://www.nbme.org/services/data-sharing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.209.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--209 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.209.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.209" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.209/>Learning to Borrow– Relation Representation for Without-Mention Entity-Pairs for Knowledge Graph Completion</a></strong><br><a href=/people/h/huda-hakami/>Huda Hakami</a>
|
<a href=/people/m/mona-hakami/>Mona Hakami</a>
|
<a href=/people/a/angrosh-mandya/>Angrosh Mandya</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--209><div class="card-body p-3 small">Prior work on integrating text corpora with knowledge graphs (KGs) to improve Knowledge Graph Embedding (KGE) have obtained good performance for entities that co-occur in sentences in text corpora. Such sentences (textual mentions of entity-pairs) are represented as Lexicalised Dependency Paths (LDPs) between two entities. However, it is not possible to represent relations between entities that do not co-occur in a single sentence using LDPs. In this paper, we propose and evaluate several methods to address this problem, where we <i>borrow</i> LDPs from the entity pairs that co-occur in sentences in the corpus (i.e. <i>with mentions</i> entity pairs) to represent entity pairs that do <i>not</i> co-occur in any sentence in the corpus (i.e. <i>without mention</i> entity pairs). We propose a supervised borrowing method, <i>SuperBorrow</i>, that learns to score the suitability of an LDP to represent a without-mentions entity pair using pre-trained entity embeddings and contextualised LDP representations. Experimental results show that SuperBorrow improves the link prediction performance of multiple widely-used prior KGE methods such as TransE, DistMult, ComplEx and RotatE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.210.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--210 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.210.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.210/>Improving Entity Disambiguation by Reasoning over a Knowledge Base</a></strong><br><a href=/people/t/tom-ayoola/>Tom Ayoola</a>
|
<a href=/people/j/joseph-fisher/>Joseph Fisher</a>
|
<a href=/people/a/andrea-pierleoni/>Andrea Pierleoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--210><div class="card-body p-3 small">Recent work in entity disambiguation (ED) has typically neglected structured knowledge base (KB) facts, and instead relied on a limited subset of KB information, such as entity descriptions or types. This limits the range of contexts in which entities can be disambiguated. To allow the use of all KB facts, as well as descriptions and types, we introduce an ED model which links entities by reasoning over a symbolic knowledge base in a fully differentiable fashion. Our model surpasses state-of-the-art baselines on six well-established ED datasets by 1.3 F1 on average. By allowing access to all KB information, our model is less reliant on popularity-based entity priors, and improves performance on the challenging ShadowLink dataset (which emphasises infrequent and ambiguous entities) by 12.7 F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.211.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--211 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.211.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.211" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.211/>Modal Dependency Parsing via Language Model Priming</a></strong><br><a href=/people/j/jiarui-yao/>Jiarui Yao</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--211><div class="card-body p-3 small">The task of modal dependency parsing aims to parse a text into its modal dependency structure, which is a representation for the factuality of events in the text. We design a modal dependency parser that is based on priming pre-trained language models, and evaluate the parser on two data sets. Compared to baselines, we show an improvement of 2.6% in F-score for English and 4.6% for Chinese. To the best of our knowledge, this is also the first work on Chinese modal dependency parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.212.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--212 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.212.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.212" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.212/>Document-Level Relation Extraction with Sentences Importance Estimation and Focusing</a></strong><br><a href=/people/w/wang-xu/>Wang Xu</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--212><div class="card-body p-3 small">Document-level relation extraction (DocRE) aims to determine the relation between two entities from a document of multiple sentences. Recent studies typically represent the entire document by sequence- or graph-based models to predict the relations of all entity pairs. However, we find that such a model is not robust and exhibits bizarre behaviors: it predicts correctly when an entire test document is fed as input, but errs when non-evidence sentences are removed. To this end, we propose a Sentence Importance Estimation and Focusing (SIEF) framework for DocRE, where we design a sentence importance score and a sentence focusing loss, encouraging DocRE models to focus on evidence sentences. Experimental results on two domains show that our SIEF not only improves overall performance, but also makes DocRE models more robust. Moreover, SIEF is a general framework, shown to be effective when combined with a variety of base DocRE models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.213.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.213.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.213/>Are All the Datasets in Benchmark Necessary? A Pilot Study of Dataset Evaluation for Text Classification</a></strong><br><a href=/people/y/yang-xiao/>Yang Xiao</a>
|
<a href=/people/j/jinlan-fu/>Jinlan Fu</a>
|
<a href=/people/s/see-kiong-ng/>See-Kiong Ng</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--213><div class="card-body p-3 small">In this paper, we ask the research question of whether all the datasets in the benchmark are necessary. We approach this by first characterizing the distinguishability of datasets when comparing different systems. Experiments on 9 datasets and 36 systems show that several existing benchmark datasets contribute little to discriminating top-scoring systems, while those less used datasets exhibit impressive discriminative power. We further, taking the text classification task as a case study, investigate the possibility of predicting dataset discrimination based on its properties (e.g., average sentence length). Our preliminary experiments promisingly show that given a sufficient number of training experimental records, a meaningful predictor can be learned to estimate dataset discrimination over unseen datasets. We released all datasets with features explored in this work on DataLab.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.214.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--214 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.214.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.214" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.214/>Triggerless Backdoor Attack for <span class=acl-fixed-case>NLP</span> Tasks with Clean Labels</a></strong><br><a href=/people/l/leilei-gan/>Leilei Gan</a>
|
<a href=/people/j/jiwei-li/>Jiwei Li</a>
|
<a href=/people/t/tianwei-zhang/>Tianwei Zhang</a>
|
<a href=/people/x/xiaoya-li/>Xiaoya Li</a>
|
<a href=/people/y/yuxian-meng/>Yuxian Meng</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/y/yi-yang/>Yi Yang</a>
|
<a href=/people/s/shangwei-guo/>Shangwei Guo</a>
|
<a href=/people/c/chun-fan/>Chun Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--214><div class="card-body p-3 small">Backdoor attacks pose a new threat to NLP models. A standard strategy to construct poisoned data in backdoor attacks is to insert triggers (e.g., rare words) into selected sentences and alter the original label to a target label. This strategy comes with a severe flaw of being easily detected from both the trigger and the label perspectives: the trigger injected, which is usually a rare word, leads to an abnormal natural language expression, and thus can be easily detected by a defense model; the changed target label leads the example to be mistakenly labeled, and thus can be easily detected by manual inspections. To deal with this issue, in this paper, we propose a new strategy to perform textual backdoor attack which does not require an external trigger and the poisoned samples are correctly labeled. The core idea of the proposed strategy is to construct clean-labeled examples, whose labels are correct but can lead to test label changes when fused with the training set. To generate poisoned clean-labeled examples, we propose a sentence generation model based on the genetic algorithm to cater to the non-differentiable characteristic of text data. Extensive experiments demonstrate that the proposed attacking strategy is not only effective, but more importantly, hard to defend due to its triggerless and clean-labeled nature. Our work marks the first step towards developing triggerless attacking strategies in NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.215.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--215 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.215.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.215" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.215/><span class=acl-fixed-case>PPL-MCTS</span>: <span class=acl-fixed-case>C</span>onstrained Textual Generation Through Discriminator-Guided <span class=acl-fixed-case>MCTS</span> Decoding</a></strong><br><a href=/people/a/antoine-chaffin/>Antoine Chaffin</a>
|
<a href=/people/v/vincent-claveau/>Vincent Claveau</a>
|
<a href=/people/e/ewa-kijak/>Ewa Kijak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--215><div class="card-body p-3 small">Large language models (LM) based on Transformers allow to generate plausible long texts. In this paper, we explore how this generation can be further controlled at decoding time to satisfy certain constraints (e.g. being non-toxic, conveying certain emotions, using a specific writing style, etc.) without fine-tuning the LM.Precisely, we formalize constrained generation as a tree exploration process guided by a discriminator that indicates how well the associated sequence respects the constraint. This approach, in addition to being easier and cheaper to train than fine-tuning the LM, allows to apply the constraint more finely and dynamically. We propose several original methods to search this generation tree, notably the Monte Carlo Tree Search (MCTS) which provides theoretical guarantees on the search efficiency, but also simpler methods based on re-ranking a pool of diverse sequences using the discriminator scores. These methods are evaluated, with automatic and human-based metrics, on two types of constraints and languages: review polarity and emotion control in French and English. We show that discriminator-guided MCTS decoding achieves state-of-the-art results without having to tune the language model, in both tasks and languages. We also demonstrate that other proposed decoding methods based on re-ranking can be really effective when diversity among the generated propositions is encouraged.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.216.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--216 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.216.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.216/>Interpretable Proof Generation via Iterative Backward Reasoning</a></strong><br><a href=/people/h/hanhao-qu/>Hanhao Qu</a>
|
<a href=/people/y/yu-cao/>Yu Cao</a>
|
<a href=/people/j/jun-gao/>Jun Gao</a>
|
<a href=/people/l/liang-ding/>Liang Ding</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--216><div class="card-body p-3 small">We present IBR, an Iterative Backward Reasoning model to solve the proof generation tasks on rule-based Question Answering (QA), where models are required to reason over a series of textual rules and facts to find out the related proof path and derive the final answer. We handle the limitations of existed works in two folds: 1) enhance the interpretability of reasoning procedures with detailed tracking, by predicting nodes and edges in the proof path iteratively backward from the question; 2) promote the efficiency and accuracy via reasoning on the elaborate representations of nodes and history paths, without any intermediate texts that may introduce external noise during proof generation. There are three main modules in IBR, QA and proof strategy prediction to obtain the answer and offer guidance for the following procedure; parent node prediction to determine a node in the existing proof that a new child node will link to; child node prediction to find out which new node will be added to the proof. Experiments on both synthetic and paraphrased datasets demonstrate that IBR has better in-domain performance as well as cross-domain transferability than several strong baselines. Our code and models are available at <a href=https://github.com/find-knowledge/IBR class=acl-markup-url>https://github.com/find-knowledge/IBR</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.217.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.217.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.217/>Domain Confused Contrastive Learning for Unsupervised Domain Adaptation</a></strong><br><a href=/people/q/quanyu-long/>Quanyu Long</a>
|
<a href=/people/t/tianze-luo/>Tianze Luo</a>
|
<a href=/people/w/wenya-wang/>Wenya Wang</a>
|
<a href=/people/s/sinno-pan/>Sinno Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--217><div class="card-body p-3 small">In this work, we study Unsupervised Domain Adaptation (UDA) in a challenging self-supervised approach. One of the difficulties is how to learn task discrimination in the absence of target labels. Unlike previous literature which directly aligns cross-domain distributions or leverages reverse gradient, we propose Domain Confused Contrastive Learning (DCCL), which can bridge the source and target domains via domain puzzles, and retain discriminative representations after adaptation. Technically, DCCL searches for a most domain-challenging direction and exquisitely crafts domain confused augmentations as positive pairs, then it contrastively encourages the model to pull representations towards the other domain, thus learning more stable and effective domain invariances. We also investigate whether contrastive learning necessarily helps with UDA when performing other data augmentations. Extensive experiments demonstrate that DCCL significantly outperforms baselines, further ablation study and analysis also show the effectiveness and availability of DCCL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.218.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--218 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.218.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.218.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.218" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.218/>Incorporating Centering Theory into Neural Coreference Resolution</a></strong><br><a href=/people/h/haixia-chai/>Haixia Chai</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--218><div class="card-body p-3 small">In recent years, transformer-based coreference resolution systems have achieved remarkable improvements on the CoNLL dataset. However, how coreference resolvers can benefit from discourse coherence is still an open question. In this paper, we propose to incorporate centering transitions derived from centering theory in the form of a graph into a neural coreference model. Our method improves the performance over the SOTA baselines, especially on pronoun resolution in long documents, formal well-structured text, and clusters with scattered mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.219.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--219 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.219.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.219" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.219/>Progressive Class Semantic Matching for Semi-supervised Text Classification</a></strong><br><a href=/people/h/haiming-xu/>Haiming Xu</a>
|
<a href=/people/l/lingqiao-liu/>Lingqiao Liu</a>
|
<a href=/people/e/ehsan-abbasnejad/>Ehsan Abbasnejad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--219><div class="card-body p-3 small">Semi-supervised learning is a promising way to reduce the annotation cost for text-classification. Combining with pre-trained language models (PLMs), e.g., BERT, recent semi-supervised learning methods achieved impressive performance. In this work, we further investigate the marriage between semi-supervised learning and a pre-trained language model. Unlike existing approaches that utilize PLMs only for model parameter initialization, we explore the inherent topic matching capability inside PLMs for building a more powerful semi-supervised learning approach. Specifically, we propose a joint semi-supervised learning process that can progressively build a standard <span class=tex-math>K</span>-way classifier and a matching network for the input text and the Class Semantic Representation (CSR). The CSR will be initialized from the given labeled sentences and progressively updated through the training process. By means of extensive experiments, we show that our method can not only bring remarkable improvement to baselines, but also overall be more stable, and achieves state-of-the-art performance in semi-supervised text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.220.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.220.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.220/>Low Resource Style Transfer via Domain Adaptive Meta Learning</a></strong><br><a href=/people/x/xiangyang-li/>Xiangyang Li</a>
|
<a href=/people/x/xiang-long/>Xiang Long</a>
|
<a href=/people/y/yu-xia/>Yu Xia</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--220><div class="card-body p-3 small">Text style transfer (TST) without parallel data has achieved some practical success. However, most of the existing unsupervised text style transfer methods suffer from (i) requiring massive amounts of non-parallel data to guide transferring different text styles. (ii) colossal performance degradation when fine-tuning the model in new domains. In this work, we propose DAML-ATM (Domain Adaptive Meta-Learning with Adversarial Transfer Model), which consists of two parts: DAML and ATM. DAML is a domain adaptive meta-learning approach to learn general knowledge in multiple heterogeneous source domains, capable of adapting to new unseen domains with a small amount of data. Moreover, we propose a new unsupervised TST approach Adversarial Transfer Model (ATM), composed of a sequence-to-sequence pre-trained language model and uses adversarial style training for better content preservation and style transfer. Results on multi-domain datasets demonstrate that our approach generalizes well on unseen low-resource domains, achieving state-of-the-art results against ten strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.221.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--221 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.221.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.221" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.221/>Features or Spurious Artifacts? Data-centric Baselines for Fair and Robust Hate Speech Detection</a></strong><br><a href=/people/a/alan-ramponi/>Alan Ramponi</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--221><div class="card-body p-3 small">Avoiding to rely on dataset artifacts to predict hate speech is at the cornerstone of robust and fair hate speech detection. In this paper we critically analyze lexical biases in hate speech detection via a cross-platform study, disentangling various types of spurious and authentic artifacts and analyzing their impact on out-of-distribution fairness and robustness. We experiment with existing approaches and propose simple yet surprisingly effective data-centric baselines. Our results on English data across four platforms show that distinct spurious artifacts require different treatments to ultimately attain both robustness and fairness in hate speech detection. To encourage research in this direction, we release all baseline models and the code to compute artifacts, pointing it out as a complementary and necessary addition to the data statements practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.222.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--222 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.222.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.222/>Document-Level Event Argument Extraction by Leveraging Redundant Information and Closed Boundary Loss</a></strong><br><a href=/people/h/hanzhang-zhou/>Hanzhang Zhou</a>
|
<a href=/people/k/kezhi-mao/>Kezhi Mao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--222><div class="card-body p-3 small">In document-level event argument extraction, an argument is likely to appear multiple times in different expressions in the document. The redundancy of arguments underlying multiple sentences is beneficial but is often overlooked. In addition, in event argument extraction, most entities are regarded as class “others”, i.e. Universum class, which is defined as a collection of samples that do not belong to any class of interest. Universum class is composed of heterogeneous entities without typical common features. Classifiers trained by cross entropy loss could easily misclassify the Universum class because of their open decision boundary. In this paper, to make use of redundant event information underlying a document, we build an entity coreference graph with the graph2token module to produce a comprehensive and coreference-aware representation for every entity and then build an entity summary graph to merge the multiple extraction results. To better classify Universum class, we propose a new loss function to build classifiers with closed boundaries. Experimental results show that our model outperforms the previous state-of-the-art models by 3.35% in F1-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.223.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--223 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.223.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.223" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.223/>A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for <span class=acl-fixed-case>A</span>frican News Translation</a></strong><br><a href=/people/d/david-adelani/>David Adelani</a>
|
<a href=/people/j/jesujoba-alabi/>Jesujoba Alabi</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/m/machel-reid/>Machel Reid</a>
|
<a href=/people/d/dana-ruiter/>Dana Ruiter</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a>
|
<a href=/people/p/peter-nabende/>Peter Nabende</a>
|
<a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/t/tajuddeen-gwadabe/>Tajuddeen Gwadabe</a>
|
<a href=/people/f/freshia-sackey/>Freshia Sackey</a>
|
<a href=/people/b/bonaventure-f-p-dossou/>Bonaventure F. P. Dossou</a>
|
<a href=/people/c/chris-emezue/>Chris Emezue</a>
|
<a href=/people/c/colin-leong/>Colin Leong</a>
|
<a href=/people/m/michael-beukman/>Michael Beukman</a>
|
<a href=/people/s/shamsuddeen-muhammad/>Shamsuddeen Muhammad</a>
|
<a href=/people/g/guyo-jarso/>Guyo Jarso</a>
|
<a href=/people/o/oreen-yousuf/>Oreen Yousuf</a>
|
<a href=/people/a/andre-niyongabo-rubungo/>Andre Niyongabo Rubungo</a>
|
<a href=/people/g/gilles-hacheme/>Gilles Hacheme</a>
|
<a href=/people/e/eric-peter-wairagala/>Eric Peter Wairagala</a>
|
<a href=/people/m/muhammad-umair-nasir/>Muhammad Umair Nasir</a>
|
<a href=/people/b/benjamin-ajibade/>Benjamin Ajibade</a>
|
<a href=/people/t/tunde-ajayi/>Tunde Ajayi</a>
|
<a href=/people/y/yvonne-gitau/>Yvonne Gitau</a>
|
<a href=/people/j/jade-abbott/>Jade Abbott</a>
|
<a href=/people/m/mohamed-ahmed/>Mohamed Ahmed</a>
|
<a href=/people/m/millicent-ochieng/>Millicent Ochieng</a>
|
<a href=/people/a/anuoluwapo-aremu/>Anuoluwapo Aremu</a>
|
<a href=/people/p/perez-ogayo/>Perez Ogayo</a>
|
<a href=/people/j/jonathan-mukiibi/>Jonathan Mukiibi</a>
|
<a href=/people/f/fatoumata-ouoba-kabore/>Fatoumata Ouoba Kabore</a>
|
<a href=/people/g/godson-kalipe/>Godson Kalipe</a>
|
<a href=/people/d/derguene-mbaye/>Derguene Mbaye</a>
|
<a href=/people/a/allahsera-auguste-tapo/>Allahsera Auguste Tapo</a>
|
<a href=/people/v/victoire-memdjokam-koagne/>Victoire Memdjokam Koagne</a>
|
<a href=/people/e/edwin-munkoh-buabeng/>Edwin Munkoh-Buabeng</a>
|
<a href=/people/v/valencia-wagner/>Valencia Wagner</a>
|
<a href=/people/i/idris-abdulmumin/>Idris Abdulmumin</a>
|
<a href=/people/a/ayodele-awokoya/>Ayodele Awokoya</a>
|
<a href=/people/h/happy-buzaaba/>Happy Buzaaba</a>
|
<a href=/people/b/blessing-sibanda/>Blessing Sibanda</a>
|
<a href=/people/a/andiswa-bukula/>Andiswa Bukula</a>
|
<a href=/people/s/sam-manthalu/>Sam Manthalu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--223><div class="card-body p-3 small">Recent advances in the pre-training for language models leverage large-scale datasets to create multilingual models. However, low-resource languages are mostly left out in these datasets. This is primarily because many widely spoken languages that are not well represented on the web and therefore excluded from the large-scale crawls for datasets. Furthermore, downstream users of these models are restricted to the selection of languages originally chosen for pre-training. This work investigates how to optimally leverage existing pre-trained models to create low-resource translation systems for 16 African languages. We focus on two questions: 1) How can pre-trained models be used for languages not included in the initial pretraining? and 2) How can the resulting translation models effectively transfer to new domains? To answer these questions, we create a novel African news corpus covering 16 languages, of which eight languages are not part of any existing evaluation dataset. We demonstrate that the most effective strategy for transferring both additional languages and additional domains is to leverage small quantities of high-quality translation data to fine-tune large pre-trained models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.224.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.224.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.224/>Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis</a></strong><br><a href=/people/y/yiwei-wang/>Yiwei Wang</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/w/wenxuan-zhou/>Wenxuan Zhou</a>
|
<a href=/people/y/yujun-cai/>Yujun Cai</a>
|
<a href=/people/y/yuxuan-liang/>Yuxuan Liang</a>
|
<a href=/people/d/dayiheng-liu/>Dayiheng Liu</a>
|
<a href=/people/b/baosong-yang/>Baosong Yang</a>
|
<a href=/people/j/juncheng-liu/>Juncheng Liu</a>
|
<a href=/people/b/bryan-hooi/>Bryan Hooi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--224><div class="card-body p-3 small">Recent literature focuses on utilizing the entity information in the sentence-level relation extraction (RE), but this risks leaking superficial and spurious clues of relations. As a result, RE still suffers from unintended entity bias, i.e., the spurious correlation between entity mentions (names) and relations. Entity bias can mislead the RE models to extract the relations that do not exist in the text. To combat this issue, some previous work masks the entity mentions to prevent the RE models from over-fitting entity mentions. However, this strategy degrades the RE performance because it loses the semantic information of entities. In this paper, we propose the CoRE (Counterfactual Analysis based Relation Extraction) debiasing method that guides the RE models to focus on the main effects of textual context without losing the entity information. We first construct a causal graph for RE, which models the dependencies between variables in RE models. Then, we propose to conduct counterfactual analysis on our causal graph to distill and mitigate the entity bias, that captures the causal effects of specific entity mentions in each instance. Note that our CoRE method is model-agnostic to debias existing RE systems during inference without changing their training processes. Extensive experimental results demonstrate that our CoRE yields significant gains on both effectiveness and generalization for RE. The source code is provided at: <a href=https://github.com/vanoracai/CoRE class=acl-markup-url>https://github.com/vanoracai/CoRE</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.225.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--225 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.225.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.225.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.225" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.225/>Analyzing Encoded Concepts in Transformer Language Models</a></strong><br><a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/f/firoj-alam/>Firoj Alam</a>
|
<a href=/people/a/abdul-khan/>Abdul Khan</a>
|
<a href=/people/j/jia-xu/>Jia Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--225><div class="card-body p-3 small">We propose a novel framework ConceptX, to analyze how latent concepts are encoded in representations learned within pre-trained lan-guage models. It uses clustering to discover the encoded concepts and explains them by aligning with a large set of human-defined concepts. Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the model are dominated by lexical concepts (e.g., affixation) and linguistic ontologies (e.g. Word-Net), whereas the core-linguistic concepts (e.g., morphology, syntactic relations) are better represented in the middle and higher layers, iii) some encoded concepts are multi-faceted and cannot be adequately explained using the existing human-defined concepts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.226.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--226 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.226.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.226/>Boosted Dense Retriever</a></strong><br><a href=/people/p/patrick-lewis/>Patrick Lewis</a>
|
<a href=/people/b/barlas-oguz/>Barlas Oguz</a>
|
<a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/f/fabio-petroni/>Fabio Petroni</a>
|
<a href=/people/s/scott-yih/>Scott Yih</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--226><div class="card-body p-3 small">We propose DrBoost, a dense retrieval ensemble inspired by boosting. DrBoost is trained in stages: each component model is learned sequentially and specialized by focusing only on retrieval mistakes made by the current ensemble. The final representation is the concatenation of the output vectors of all the component models, making it a drop-in replacement for standard dense retrievers at test time. DrBoost enjoys several advantages compared to standard dense retrieval models. It produces representations which are 4x more compact, while delivering comparable retrieval results. It also performs surprisingly well under approximate search with coarse quantization, reducing latency and bandwidth needs by another 4x. In practice, this can make the difference between serving indices from disk versus from memory, paving the way for much cheaper deployments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.227.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.227.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.227" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.227/><span class=acl-fixed-case>M</span>u<span class=acl-fixed-case>CGEC</span>: a Multi-Reference Multi-Source Evaluation Dataset for <span class=acl-fixed-case>C</span>hinese Grammatical Error Correction</a></strong><br><a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/z/zuyi-bao/>Zuyi Bao</a>
|
<a href=/people/j/jiacheng-li/>Jiacheng Li</a>
|
<a href=/people/b/bo-zhang/>Bo Zhang</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--227><div class="card-body p-3 small">This paper presents MuCGEC, a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three Chinese-as-a-Second-Language (CSL) learner sources. Each sentence is corrected by three annotators, and their corrections are carefully reviewed by a senior annotator, resulting in 2.3 references per sentence. We conduct experiments with two mainstream CGEC models, i.e., the sequence-to-sequence model and the sequence-to-edit model, both enhanced with large pretrained language models, achieving competitive benchmark performance on previous and our datasets. We also discuss CGEC evaluation methodologies, including the effect of multiple references and using a char-based metric. Our annotation guidelines, data, and code are available at <a href=https://github.com/HillZhang1999/MuCGEC class=acl-markup-url>https://github.com/HillZhang1999/MuCGEC</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.228.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.228.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.228" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.228/><span class=acl-fixed-case>N</span>eu<span class=acl-fixed-case>S</span>: Neutral Multi-News Summarization for Mitigating Framing Bias</a></strong><br><a href=/people/n/nayeon-lee/>Nayeon Lee</a>
|
<a href=/people/y/yejin-bang/>Yejin Bang</a>
|
<a href=/people/t/tiezheng-yu/>Tiezheng Yu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--228><div class="card-body p-3 small">Media news framing bias can increase political polarization and undermine civil society. The need for automatic mitigation methods is therefore growing. We propose a new task, a neutral summary generation from multiple news articles of the varying political leaningsto facilitate balanced and unbiased news reading. In this paper, we first collect a new dataset, illustrate insights about framing bias through a case study, and propose a new effective metric and model (NeuS-Title) for the task. Based on our discovery that title provides a good signal for framing bias, we present NeuS-Title that learns to neutralize news content in hierarchical order from title to article. Our hierarchical multi-task learning is achieved by formatting our hierarchical data pair (title, article) sequentially with identifier-tokens (“TITLE=>”, “ARTICLE=>”) and fine-tuning the auto-regressive decoder with the standard negative log-likelihood objective. We then analyze and point out the remaining challenges and future directions. One of the most interesting observations is that neural NLG models can hallucinate not only factually inaccurate or unverifiable content but also politically biased content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.229.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.229.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.229/>Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation</a></strong><br><a href=/people/s/shumpei-inoue/>Shumpei Inoue</a>
|
<a href=/people/t/tsungwei-liu/>Tsungwei Liu</a>
|
<a href=/people/s/son-nguyen/>Son Nguyen</a>
|
<a href=/people/m/minh-tien-nguyen/>Minh-Tien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--229><div class="card-body p-3 small">This paper introduces a model for incomplete utterance restoration (IUR) called JET (Joint learning token Extraction and Text generation). Different from prior studies that only work on extraction or abstraction datasets, we design a simple but effective model, working for both scenarios of IUR. Our design simulates the nature of IUR, where omitted tokens from the context contribute to restoration. From this, we construct a Picker that identifies the omitted tokens. To support the picker, we design two label creation methods (soft and hard labels), which can work in cases of no annotation data for the omitted tokens. The restoration is done by using a Generator with the help of the Picker on joint learning. Promising results on four benchmark datasets in extraction and abstraction scenarios show that our model is better than the pretrained T5 and non-generative language model methods in both rich and limited training data settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.230.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--230 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.230.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.230/>Efficient Constituency Tree based Encoding for Natural Language to Bash Translation</a></strong><br><a href=/people/s/shikhar-bharadwaj/>Shikhar Bharadwaj</a>
|
<a href=/people/s/shirish-shevade/>Shirish Shevade</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--230><div class="card-body p-3 small">Bash is a Unix command language used for interacting with the Operating System. Recent works on natural language to Bash translation have made significant advances, but none of the previous methods utilize the problem’s inherent structure. We identify this structure andpropose a Segmented Invocation Transformer (SIT) that utilizes the information from the constituency parse tree of the natural language text. Our method is motivated by the alignment between segments in the natural language text and Bash command components. Incorporating the structure in the modelling improves the performance of the model. Since such systems must be universally accessible, we benchmark the inference times on a CPU rather than a GPU. We observe a 1.8x improvement in the inference time and a 5x reduction in model parameters. Attribution analysis using Integrated Gradients reveals that the proposed method can capture the problem structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.231.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--231 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.231.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.231.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.231/>Privacy-Preserving Text Classification on <span class=acl-fixed-case>BERT</span> Embeddings with Homomorphic Encryption</a></strong><br><a href=/people/g/garam-lee/>Garam Lee</a>
|
<a href=/people/m/minsoo-kim/>Minsoo Kim</a>
|
<a href=/people/j/jai-hyun-park/>Jai Hyun Park</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a>
|
<a href=/people/j/jung-hee-cheon/>Jung Hee Cheon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--231><div class="card-body p-3 small">Embeddings, which compress information in raw text into semantics-preserving low-dimensional vectors, have been widely adopted for their efficacy. However, recent research has shown that embeddings can potentially leak private information about sensitive attributes of the text, and in some cases, can be inverted to recover the original input text. To address these growing privacy challenges, we propose a privatization mechanism for embeddings based on homomorphic encryption, to prevent potential leakage of any piece of information in the process of text classification. In particular, our method performs text classification on the encryption of embeddings from state-of-the-art models like BERT, supported by an efficient GPU implementation of CKKS encryption scheme. We show that our method offers encrypted protection of BERT embeddings, while largely preserving their utility on downstream text classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.232.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--232 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.232.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.232" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.232/><span class=acl-fixed-case>ITA</span>: Image-Text Alignments for Multi-Modal Named Entity Recognition</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/m/min-gui/>Min Gui</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/z/zixia-jia/>Zixia Jia</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--232><div class="card-body p-3 small">Recently, Multi-modal Named Entity Recognition (MNER) has attracted a lot of attention. Most of the work utilizes image information through region-level visual representations obtained from a pretrained object detector and relies on an attention mechanism to model the interactions between image and text representations. However, it is difficult to model such interactions as image and text representations are trained separately on the data of their respective modality and are not aligned in the same space. As text representations take the most important role in MNER, in this paper, we propose <b>I</b>mage-<b>t</b>ext <b>A</b>lignments (ITA) to align image features into the textual space, so that the attention mechanism in transformer-based pretrained textual embeddings can be better utilized. ITA first aligns the image into regional object tags, image-level captions and optical characters as visual contexts, concatenates them with the input texts as a new cross-modal input, and then feeds it into a pretrained textual embedding model. This makes it easier for the attention module of a pretrained textual embedding model to model the interaction between the two modalities since they are both represented in the textual space. ITA further aligns the output distributions predicted from the cross-modal input and textual input views so that the MNER model can be more practical in dealing with text-only inputs and robust to noises from images. In our experiments, we show that ITA models can achieve state-of-the-art accuracy on multi-modal Named Entity Recognition datasets, even without image information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.233.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--233 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.233.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.233" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.233/>A Dataset for N-ary Relation Extraction of Drug Combinations</a></strong><br><a href=/people/a/aryeh-tiktinsky/>Aryeh Tiktinsky</a>
|
<a href=/people/v/vijay-viswanathan/>Vijay Viswanathan</a>
|
<a href=/people/d/danna-niezni/>Danna Niezni</a>
|
<a href=/people/d/dana-meron-azagury/>Dana Meron Azagury</a>
|
<a href=/people/y/yosi-shamay/>Yosi Shamay</a>
|
<a href=/people/h/hillel-taub-tabib/>Hillel Taub-Tabib</a>
|
<a href=/people/t/tom-hope/>Tom Hope</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--233><div class="card-body p-3 small">Combination therapies have become the standard of care for diseases such as cancer, tuberculosis, malaria and HIV. However, the combinatorial set of available multi-drug treatments creates a challenge in identifying effective combination therapies available in a situation. To assist medical professionals in identifying beneficial drug-combinations, we construct an expert-annotated dataset for extracting information about the efficacy of drug combinations from the scientific literature. Beyond its practical utility, the dataset also presents a unique NLP challenge, as the first relation extraction dataset consisting of variable-length relations. Furthermore, the relations in this dataset predominantly require language understanding beyond the sentence level, adding to the challenge of this task. We provide a promising baseline model and identify clear areas for further improvement. We release our dataset (<a href=https://huggingface.co/datasets/allenai/drug-combo-extraction class=acl-markup-url>https://huggingface.co/datasets/allenai/drug-combo-extraction</a>), code (<a href=https://github.com/allenai/drug-combo-extraction class=acl-markup-url>https://github.com/allenai/drug-combo-extraction</a>) and baseline models (<a href=https://huggingface.co/allenai/drug-combo-classifier-pubmedbert-dapt class=acl-markup-url>https://huggingface.co/allenai/drug-combo-classifier-pubmedbert-dapt</a>) publicly to encourage the NLP community to participate in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.234.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.234.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.234/>Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding</a></strong><br><a href=/people/z/zeming-chen/>Zeming Chen</a>
|
<a href=/people/q/qiyue-gao/>Qiyue Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--234><div class="card-body p-3 small">In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models’ abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets. In this paper, we introduce Curriculum as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. Curriculum contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. We show that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality. In addition, our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models that may encourage future research on re-designing datasets, model architectures, and learning objectives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.235.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--235 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.235.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.235.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.235/>Neural Language Taskonomy: Which <span class=acl-fixed-case>NLP</span> Tasks are the most Predictive of f<span class=acl-fixed-case>MRI</span> Brain Activity?</a></strong><br><a href=/people/s/subba-reddy-oota/>Subba Reddy Oota</a>
|
<a href=/people/j/jashn-arora/>Jashn Arora</a>
|
<a href=/people/v/veeral-agarwal/>Veeral Agarwal</a>
|
<a href=/people/m/mounika-marreddy/>Mounika Marreddy</a>
|
<a href=/people/m/manish-gupta/>Manish Gupta</a>
|
<a href=/people/b/bapi-surampudi/>Bapi Surampudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--235><div class="card-body p-3 small">Several popular Transformer based language models have been found to be successful for text-driven brain encoding. However, existing literature leverages only pretrained text Transformer models and has not explored the efficacy of task-specific learned Transformer representations. In this work, we explore transfer learning from representations learned for ten popular natural language processing tasks (two syntactic and eight semantic) for predicting brain responses from two diverse datasets: Pereira (subjects reading sentences from paragraphs) and Narratives (subjects listening to the spoken stories). Encoding models based on task features are used to predict activity in different regions across the whole brain. Features from coreference resolution, NER, and shallow syntax parsing explain greater variance for the reading activity. On the other hand, for the listening activity, tasks such as paraphrase generation, summarization, and natural language inference show better encoding performance. Experiments across all 10 task representations provide the following cognitive insights: (i) language left hemisphere has higher predictive brain activity versus language right hemisphere, (ii) posterior medial cortex, temporo-parieto-occipital junction, dorsal frontal lobe have higher correlation versus early auditory and auditory association cortex, (iii) syntactic and semantic tasks display a good predictive performance across brain regions for reading and listening stimuli resp.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.236.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--236 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.236.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.236.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.236" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.236/><span class=acl-fixed-case>F</span>act<span class=acl-fixed-case>G</span>raph: Evaluating Factuality in Summarization with Semantic Graph Representations</a></strong><br><a href=/people/l/leonardo-f-r-ribeiro/>Leonardo F. R. Ribeiro</a>
|
<a href=/people/m/mengwen-liu/>Mengwen Liu</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/m/markus-dreyer/>Markus Dreyer</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--236><div class="card-body p-3 small">Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.237.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.237.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.237/>Unsupervised Paraphrasability Prediction for Compound Nominalizations</a></strong><br><a href=/people/j/john-sie-yuen-lee/>John Sie Yuen Lee</a>
|
<a href=/people/h/ho-hung-lim/>Ho Hung Lim</a>
|
<a href=/people/c/carol-webster/>Carol Webster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--237><div class="card-body p-3 small">Commonly found in academic and formal texts, a nominalization uses a deverbal noun to describe an event associated with its corresponding verb. Nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Automatic generation of clausal paraphrases for nominalizations can help disambiguate their meaning. However, previous work has not identified cases where it is awkward or impossible to paraphrase a compound nominalization. This paper investigates unsupervised prediction of paraphrasability, which determines whether the prenominal modifier of a nominalization can be re-written as a noun or adverb in a clausal paraphrase. We adopt the approach of overgenerating candidate paraphrases followed by candidate ranking with a neural language model. In experiments on an English dataset, we show that features from an Abstract Meaning Representation graph lead to statistically significant improvement in both paraphrasability prediction and paraphrase generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.238.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.238.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.238" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.238/>Global Entity Disambiguation with <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/i/ikuya-yamada/>Ikuya Yamada</a>
|
<a href=/people/k/koki-washio/>Koki Washio</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--238><div class="card-body p-3 small">We propose a global entity disambiguation (ED) model based on BERT. To capture global contextual information for ED, our model treats not only words but also entities as input tokens, and solves the task by sequentially resolving mentions to their referent entities and using resolved entities as inputs at each step. We train the model using a large entity-annotated corpus obtained from Wikipedia. We achieve new state-of-the-art results on five standard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, and WNED-WIKI. The source code and model checkpoint are available at <a href=https://github.com/studio-ousia/luke class=acl-markup-url>https://github.com/studio-ousia/luke</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.239.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--239 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.239.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.239" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.239/>Clues Before Answers: Generation-Enhanced Multiple-Choice <span class=acl-fixed-case>QA</span></a></strong><br><a href=/people/z/zixian-huang/>Zixian Huang</a>
|
<a href=/people/a/ao-wu/>Ao Wu</a>
|
<a href=/people/j/jiaying-zhou/>Jiaying Zhou</a>
|
<a href=/people/y/yu-gu/>Yu Gu</a>
|
<a href=/people/y/yue-zhao/>Yue Zhao</a>
|
<a href=/people/g/gong-cheng/>Gong Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--239><div class="card-body p-3 small">A trending paradigm for multiple-choice question answering (MCQA) is using a text-to-text framework. By unifying data in different tasks into a single text-to-text format, it trains a generative encoder-decoder model which is both powerful and universal. However, a side effect of twisting a generation target to fit the classification nature of MCQA is the under-utilization of the decoder and the knowledge that can be decoded. To exploit the generation capability and underlying knowledge of a pre-trained encoder-decoder model, in this paper, we propose a generation-enhanced MCQA model named GenMC. It generates a clue from the question and then leverages the clue to enhance a reader for MCQA. It outperforms text-to-text models on multiple MCQA datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.240.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--240 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.240.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.240" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.240/>Towards Efficient <span class=acl-fixed-case>NLP</span>: A Standard Evaluation and A Strong Baseline</a></strong><br><a href=/people/x/xiangyang-liu/>Xiangyang Liu</a>
|
<a href=/people/t/tianxiang-sun/>Tianxiang Sun</a>
|
<a href=/people/j/junliang-he/>Junliang He</a>
|
<a href=/people/j/jiawen-wu/>Jiawen Wu</a>
|
<a href=/people/l/lingling-wu/>Lingling Wu</a>
|
<a href=/people/x/xinyu-zhang/>Xinyu Zhang</a>
|
<a href=/people/h/hao-jiang/>Hao Jiang</a>
|
<a href=/people/z/zhao-cao/>Zhao Cao</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--240><div class="card-body p-3 small">Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depicting the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.241.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--241 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.241.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.241/>Stylized Knowledge-Grounded Dialogue Generation via Disentangled Template Rewriting</a></strong><br><a href=/people/q/qingfeng-sun/>Qingfeng Sun</a>
|
<a href=/people/c/can-xu/>Can Xu</a>
|
<a href=/people/h/huang-hu/>Huang Hu</a>
|
<a href=/people/y/yujing-wang/>Yujing Wang</a>
|
<a href=/people/j/jian-miao/>Jian Miao</a>
|
<a href=/people/x/xiubo-geng/>Xiubo Geng</a>
|
<a href=/people/y/yining-chen/>Yining Chen</a>
|
<a href=/people/f/fei-xu/>Fei Xu</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--241><div class="card-body p-3 small">Current Knowledge-Grounded Dialogue Generation (KDG) models specialize in producing rational and factual responses. However, to establish long-term relationships with users, the KDG model needs the capability to generate responses in a desired style or attribute. Thus, we study a new problem: Stylized Knowledge-Grounded Dialogue Generation (SKDG). It presents two challenges: (1) How to train a SKDG model where no &lt;context, knowledge, stylized response> triples are available. (2) How to cohere with context and preserve the knowledge when generating a stylized response. In this paper, we propose a novel disentangled template rewriting (DTR) method which generates responses via combing disentangled style templates (from monolingual stylized corpus) and content templates (from KDG corpus). The entire framework is end-to-end differentiable and learned without supervision. Extensive experiments on two benchmarks indicate that DTR achieves a significant improvement on all evaluation metrics compared with previous state-of-the-art stylized dialogue generation methods. Besides, DTR achieves comparable performance with the state-of-the-art KDG methods in standard KDG evaluation setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.242.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.242.software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.242.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.242" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.242/><span class=acl-fixed-case>LUNA</span>: Learning Slot-Turn Alignment for Dialogue State Tracking</a></strong><br><a href=/people/y/yifan-wang/>Yifan Wang</a>
|
<a href=/people/j/jing-zhao/>Jing Zhao</a>
|
<a href=/people/j/junwei-bao/>Junwei Bao</a>
|
<a href=/people/c/chaoqun-duan/>Chaoqun Duan</a>
|
<a href=/people/y/youzheng-wu/>Youzheng Wu</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--242><div class="card-body p-3 small">Dialogue state tracking (DST) aims to predict the current dialogue state given the dialogue history. Existing methods generally exploit the utterances of all dialogue turns to assign value for each slot. This could lead to suboptimal results due to the information introduced from irrelevant utterances in the dialogue history, which may be useless and can even cause confusion. To address this problem, we propose LUNA, a SLot-TUrN Alignment enhanced approach. It first explicitly aligns each slot with its most relevant utterance, then further predicts the corresponding value based on this aligned utterance instead of all dialogue utterances. Furthermore, we design a slot ranking auxiliary task to learn the temporal correlation among slots which could facilitate the alignment. Comprehensive experiments are conducted on three multi-domain task-oriented dialogue datasets, MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2. The results show that LUNA achieves new state-of-the-art results on these datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.243.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--243 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.243.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.243" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.243/>Crossroads, Buildings and Neighborhoods: A Dataset for Fine-grained Location Recognition</a></strong><br><a href=/people/p/pei-chen/>Pei Chen</a>
|
<a href=/people/h/haotian-xu/>Haotian Xu</a>
|
<a href=/people/c/cheng-zhang/>Cheng Zhang</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--243><div class="card-body p-3 small">General domain Named Entity Recognition (NER) datasets like CoNLL-2003 mostly annotate coarse-grained location entities such as a country or a city. But many applications require identifying fine-grained locations from texts and mapping them precisely to geographic sites, e.g., a crossroad, an apartment building, or a grocery store. In this paper, we introduce a new dataset HarveyNER with fine-grained locations annotated in tweets. This dataset presents unique challenges and characterizes many complex and long location mentions in informal descriptions. We built strong baseline models using Curriculum Learning and experimented with different heuristic curricula to better recognize difficult location mentions. Experimental results show that the simple curricula can improve the system’s performance on hard cases and its overall performance, and outperform several other baseline systems. The dataset and the baseline models can be found at <a href=https://github.com/brickee/HarveyNER class=acl-markup-url>https://github.com/brickee/HarveyNER</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.244.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--244 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.244.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.244/>Tricks for Training Sparse Translation Models</a></strong><br><a href=/people/d/dheeru-dua/>Dheeru Dua</a>
|
<a href=/people/s/shruti-bhosale/>Shruti Bhosale</a>
|
<a href=/people/v/vedanuj-goswami/>Vedanuj Goswami</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--244><div class="card-body p-3 small">Multi-task learning with an unbalanced data distribution skews model learning towards high resource tasks, especially when model capacity is fixed and fully shared across all tasks. Sparse scaling architectures, such as BASELayers, provide flexible mechanisms for different tasks to have a variable number of parameters, which can be useful to counterbalance skewed data distributions. We find that that sparse architectures for multilingual machine translation can perform poorly out of the box and propose two straightforward techniques to mitigate this — a temperature heating mechanism and dense pre-training. Overall, these methods improve performance on two multilingual translation benchmarks compared to standard BASELayers and Dense scaling baselines, and in combination, more than 2x model convergence speed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.245.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--245 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.245.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.245.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.245" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.245/>Persona-Guided Planning for Controlling the Protagonist’s Persona in Story Generation</a></strong><br><a href=/people/z/zhexin-zhang/>Zhexin Zhang</a>
|
<a href=/people/j/jiaxin-wen/>Jiaxin Wen</a>
|
<a href=/people/j/jian-guan/>Jian Guan</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--245><div class="card-body p-3 small">Endowing the protagonist with a specific personality is essential for writing an engaging story. In this paper, we aim to control the protagonist’s persona in story generation, i.e., generating a story from a leading context and a persona description, where the protagonist should exhibit the specified personality through a coherent event sequence. Considering that personas are usually embodied implicitly and sparsely in stories, we propose a planning-based generation model named ConPer to explicitly model the relationship between personas and events. ConPer first plans events of the protagonist’s behavior which are motivated by the specified persona through predicting one target sentence, then plans the plot as a sequence of keywords with the guidance of the predicted persona-related events and commonsense knowledge, and finally generates the whole story. Both automatic and manual evaluation results demonstrate that ConPer outperforms state-of-the-art baselines for generating more coherent and persona-controllable stories. Our code is available at <a href=https://github.com/thu-coai/ConPer class=acl-markup-url>https://github.com/thu-coai/ConPer</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.246.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--246 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.246.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.246" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.246/><span class=acl-fixed-case>CHEF</span>: A Pilot <span class=acl-fixed-case>C</span>hinese Dataset for Evidence-Based Fact-Checking</a></strong><br><a href=/people/x/xuming-hu/>Xuming Hu</a>
|
<a href=/people/z/zhijiang-guo/>Zhijiang Guo</a>
|
<a href=/people/g/guanyu-wu/>GuanYu Wu</a>
|
<a href=/people/a/aiwei-liu/>Aiwei Liu</a>
|
<a href=/people/l/lijie-wen/>Lijie Wen</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--246><div class="card-body p-3 small">The explosion of misinformation spreading in the media ecosystem urges for automated fact-checking. While misinformation spans both geographic and linguistic boundaries, most work in the field has focused on English. Datasets and tools available in other languages, such as Chinese, are limited. In order to bridge this gap, we construct CHEF, the first CHinese Evidence-based Fact-checking dataset of 10K real-world claims. The dataset covers multiple domains, ranging from politics to public health, and provides annotated evidence retrieved from the Internet. Further, we develop established baselines and a novel approach that is able to model the evidence retrieval as a latent variable, allowing jointly training with the veracity prediction model in an end-to-end fashion. Extensive experiments show that CHEF will provide a challenging testbed for the development of fact-checking systems designed to retrieve and reason over non-English claims.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.247.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--247 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.247.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.247/><span class=acl-fixed-case>VGNMN</span>: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems</a></strong><br><a href=/people/h/hung-le/>Hung Le</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a>
|
<a href=/people/s/steven-hoi/>Steven Hoi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--247><div class="card-body p-3 small">Neural module networks (NMN) have achieved success in image-grounded tasks such as Visual Question Answering (VQA) on synthetic images. However, very limited work on NMN has been studied in the video-grounded dialogue tasks. These tasks extend the complexity of traditional visual tasks with the additional visual temporal variance and language cross-turn dependencies. Motivated by recent NMN approaches on image-grounded tasks, we introduce Video-grounded Neural Module Network (VGNMN) to model the information retrieval process in video-grounded language tasks as a pipeline of neural modules. VGNMN first decomposes all language components in dialogues to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Our experiments show that VGNMN can achieve promising performance on a challenging video-grounded dialogue benchmark as well as a video QA benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.248.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--248 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.248.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.248" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.248/>Multimodal Dialogue State Tracking</a></strong><br><a href=/people/h/hung-le/>Hung Le</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a>
|
<a href=/people/s/steven-hoi/>Steven Hoi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--248><div class="card-body p-3 small">Designed for tracking user goals in dialogues, a dialogue state tracker is an essential component in a dialogue system. However, the research of dialogue state tracking has largely been limited to unimodality, in which slots and slot values are limited by knowledge domains (e.g. restaurant domain with slots of restaurant name and price range) and are defined by specific database schema. In this paper, we propose to extend the definition of dialogue state tracking to multimodality. Specifically, we introduce a novel dialogue state tracking task to track the information of visual objects that are mentioned in video-grounded dialogues. Each new dialogue utterance may introduce a new video segment, new visual objects, or new object attributes and a state tracker is required to update these information slots accordingly. We created a new synthetic benchmark and designed a novel baseline, Video-Dialogue Transformer Network (VDTN), for this task. VDTN combines both object-level features and segment-level features and learns contextual dependencies between videos and dialogues to generate multimodal dialogue states. We optimized VDTN for a state generation task as well as a self-supervised video understanding task which recovers video segment or object representations. Finally, we trained VDTN to use the decoded states in a response prediction task. Together with comprehensive ablation and qualitative analysis, we discovered interesting insights towards building more capable multimodal dialogue systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.249.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--249 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.249.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.249" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.249/>On the Use of Bert for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation</a></strong><br><a href=/people/y/yongjie-wang/>Yongjie Wang</a>
|
<a href=/people/c/chuang-wang/>Chuang Wang</a>
|
<a href=/people/r/ruobing-li/>Ruobing Li</a>
|
<a href=/people/h/hui-lin/>Hui Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--249><div class="card-body p-3 small">In recent years, pre-trained models have become dominant in most natural language processing (NLP) tasks. However, in the area of Automated Essay Scoring (AES), pre-trained models such as BERT have not been properly used to outperform other deep learning models such as LSTM. In this paper, we introduce a novel multi-scale essay representation for BERT that can be jointly learned. We also employ multiple losses and transfer learning from out-of-domain essays to further improve the performance. Experiment results show that our approach derives much benefit from joint learning of multi-scale essay representation and obtains almost the state-of-the-art result among all deep learning models in the ASAP task. Our multi-scale essay representation also generalizes well to CommonLit Readability Prize data set, which suggests that the novel text representation proposed in this paper may be a new and effective choice for long-text tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.250.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--250 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.250.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.250" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.250/>Recognition of They/Them as Singular Personal Pronouns in Coreference Resolution</a></strong><br><a href=/people/c/connor-baumler/>Connor Baumler</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--250><div class="card-body p-3 small">As using they/them as personal pronouns becomes increasingly common in English, it is important that coreference resolution systems work as well for individuals who use personal “they” as they do for those who use gendered personal pronouns. We introduce a new benchmark for coreference resolution systems which evaluates singular personal “they” recognition. Using these WinoNB schemas, we evaluate a number of publicly available coreference resolution systems and confirm their bias toward resolving “they” pronouns as plural.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.251.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--251 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.251.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.251/><span class=acl-fixed-case>TWEETSPIN</span>: Fine-grained Propaganda Detection in Social Media Using Multi-View Representations</a></strong><br><a href=/people/p/prashanth-vijayaraghavan/>Prashanth Vijayaraghavan</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--251><div class="card-body p-3 small">Recently, several studies on propaganda detection have involved document and fragment-level analyses of news articles. However, there are significant data and modeling challenges dealing with fine-grained detection of propaganda on social media. In this work, we present TWEETSPIN, a dataset containing tweets that are weakly annotated with different fine-grained propaganda techniques, and propose a neural approach to detect and categorize propaganda tweets across those fine-grained categories. These categories include specific rhetorical and psychological techniques, ranging from leveraging emotions to using logical fallacies. Our model relies on multi-view representations of the input tweet data to (a) extract different aspects of the input text including the context, entities, their relationships, and external knowledge; (b) model their mutual interplay; and (c) effectively speed up the learning process by requiring fewer training examples. Our method allows for representation enrichment leading to better detection and categorization of propaganda on social media. We verify the effectiveness of our proposed method on TWEETSPIN and further probe how the implicit relations between the views impact the performance. Our experiments show that our model is able to outperform several benchmark methods and transfer the knowledge to relatively low-resource news domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.252.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--252 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.252.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.252/><span class=acl-fixed-case>U</span>ser<span class=acl-fixed-case>I</span>dentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis</a></strong><br><a href=/people/f/fatemehsadat-mireshghallah/>Fatemehsadat Mireshghallah</a>
|
<a href=/people/v/vaishnavi-shrivastava/>Vaishnavi Shrivastava</a>
|
<a href=/people/m/milad-shokouhi/>Milad Shokouhi</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a>
|
<a href=/people/r/robert-sim/>Robert Sim</a>
|
<a href=/people/d/dimitrios-dimitriadis/>Dimitrios Dimitriadis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--252><div class="card-body p-3 small">Global models are typically trained to be as generalizable as possible. Invariance to the specific user is considered desirable since models are shared across multitudes of users. However, these models are often unable to produce personalized responses for individual users, based on their data. Contrary to widely-used personalization techniques based on few-shot and meta-learning, we propose UserIdentifier, a novel scheme for training a single shared model for all users. Our approach produces personalized responses by prepending a fixed, user-specific non-trainable string (called “user identifier”) to each user’s input text. Unlike prior work, this method doesn’t need any additional model parameters, any extra rounds of personal few-shot learning or any change made to the vocabulary. We empirically study different types of user identifiers (numeric, alphanumeric, and also randomly generated) and demonstrate that, surprisingly, randomly generated user identifiers outperform the prefix-tuning based state-of-the-art approach by up to 13, on a suite of sentiment analysis datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.253.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.253.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--253 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.253 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.253.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.253/>Improving Neural Models for Radiology Report Retrieval with Lexicon-based Automated Annotation</a></strong><br><a href=/people/l/luyao-shi/>Luyao Shi</a>
|
<a href=/people/t/tanveer-syeda-mahmood/>Tanveer Syeda-mahmood</a>
|
<a href=/people/t/tyler-baldwin/>Tyler Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--253><div class="card-body p-3 small">Many clinical informatics tasks that are based on electronic health records (EHR) need relevant patient cohorts to be selected based on findings, symptoms and diseases. Frequently, these conditions are described in radiology reports which can be retrieved using information retrieval (IR) methods. The latest of these techniques utilize neural IR models such as BERT trained on clinical text. However, these methods still lack semantic understanding of the underlying clinical conditions as well as ruled out findings, resulting in poor precision during retrieval. In this paper we combine clinical finding detection with supervised query match learning. Specifically, we use lexicon-driven concept detection to detect relevant findings in sentences. These findings are used as queries to train a Sentence-BERT (SBERT) model using triplet loss on matched and unmatched query-sentence pairs. We show that the proposed supervised training task remarkably improves the retrieval performance of SBERT. The trained model generalizes well to unseen queries and reports from different collections.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.254.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--254 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.254.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.254" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.254/>Transparent Human Evaluation for Image Captioning</a></strong><br><a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/l/lavinia-dunagan/>Lavinia Dunagan</a>
|
<a href=/people/j/jacob-morrison/>Jacob Morrison</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--254><div class="card-body p-3 small">We establish THumB, a rubric-based human evaluation protocol for image captioning models. Our scoring rubrics and their definitions are carefully developed based on machine- and human-generated captions on the MSCOCO dataset. Each caption is evaluated along two main dimensions in a tradeoff (precision and recall) as well as other aspects that measure the text quality (fluency, conciseness, and inclusive language). Our evaluations demonstrate several critical problems of the current evaluation practice. Human-generated captions show substantially higher quality than machine-generated ones, especially in coverage of salient information (i.e., recall), while most automatic metrics say the opposite. Our rubric-based results reveal that CLIPScore, a recent metric that uses image features, better correlates with human judgments than conventional text-only metrics because it is more sensitive to recall. We hope that this work will promote a more transparent evaluation protocol for image captioning and its automatic metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.255.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.255.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.255/>Lifting the Curse of Multilinguality by Pre-training Modular Transformers</a></strong><br><a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/n/naman-goyal/>Naman Goyal</a>
|
<a href=/people/x/xi-lin/>Xi Lin</a>
|
<a href=/people/x/xian-li/>Xian Li</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/m/mikel-artetxe/>Mikel Artetxe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--255><div class="card-body p-3 small">Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-Mod) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.256.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--256 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.256.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.256" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.256/><span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>AMR</span>: Multi-Sentence <span class=acl-fixed-case>AMR</span> Representation and Evaluation</a></strong><br><a href=/people/t/tahira-naseem/>Tahira Naseem</a>
|
<a href=/people/a/austin-blodgett/>Austin Blodgett</a>
|
<a href=/people/s/sadhana-kumaravel/>Sadhana Kumaravel</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/y/young-suk-lee/>Young-Suk Lee</a>
|
<a href=/people/j/jeffrey-flanigan/>Jeffrey Flanigan</a>
|
<a href=/people/r/ramon-fernandez-astudillo/>Ramón Astudillo</a>
|
<a href=/people/r/radu-florian/>Radu Florian</a>
|
<a href=/people/s/salim-roukos/>Salim Roukos</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--256><div class="card-body p-3 small">Despite extensive research on parsing of English sentences into Abstract Meaning Representation (AMR) graphs, which are compared to gold graphs via the Smatch metric, full-document parsing into a unified graph representation lacks well-defined representation and evaluation. Taking advantage of a super-sentential level of coreference annotation from previous work, we introduce a simple algorithm for deriving a unified graph representation, avoiding the pitfalls of information loss from over-merging and lack of coherence from under merging. Next, we describe improvements to the Smatch metric to make it tractable for comparing document-level graphs and use it to re-evaluate the best published document-level AMR parser. We also present a pipeline approach combining the top-performing AMR parser and coreference resolution systems, providing a strong baseline for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.257.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--257 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.257.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.257" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.257/>Learning to Transfer Prompts for Text Generation</a></strong><br><a href=/people/j/junyi-li/>Junyi Li</a>
|
<a href=/people/t/tianyi-tang/>Tianyi Tang</a>
|
<a href=/people/j/jian-yun-nie/>Jian-Yun Nie</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a>
|
<a href=/people/w/wayne-xin-zhao/>Xin Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--257><div class="card-body p-3 small">Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text generation in a transferable setting. First, PTG learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks. To consider both task- and instance-level information, we design an adaptive attention mechanism to derive the target prompts. For each data instance, PTG learns a specific target prompt by attending to highly relevant source prompts. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We release our source prompts as an open resource, where users can add or reuse them to improve new text generation tasks for future research. Code and data can be available at <a href=https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation class=acl-markup-url>https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.258.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--258 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.258.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.258" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.258/><span class=acl-fixed-case>E</span>lite<span class=acl-fixed-case>PLM</span>: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models</a></strong><br><a href=/people/j/junyi-li/>Junyi Li</a>
|
<a href=/people/t/tianyi-tang/>Tianyi Tang</a>
|
<a href=/people/z/zheng-gong/>Zheng Gong</a>
|
<a href=/people/l/lixin-yang/>Lixin Yang</a>
|
<a href=/people/z/zhuohao-yu/>Zhuohao Yu</a>
|
<a href=/people/z/zhipeng-chen/>Zhipeng Chen</a>
|
<a href=/people/j/jingyuan-wang/>Jingyuan Wang</a>
|
<a href=/people/w/wayne-xin-zhao/>Xin Zhao</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--258><div class="card-body p-3 small">Nowadays, pretrained language models (PLMs) have dominated the majority of NLP tasks. While, little research has been conducted on systematically evaluating the language abilities of PLMs. In this paper, we present a large-scale empirical study on general language ability evaluation of PLMs (ElitePLM). In our study, we design four evaluation dimensions, memory, comprehension, reasoning, and composition, to measure ten widely-used PLMs within five categories. Our empirical results demonstrate that: (1) PLMs with varying training objectives and strategies are good at different ability tests; (2) fine-tuning PLMs in downstream tasks is usually sensitive to the data size and distribution; (3) PLMs have excellent transferability between similar tasks. Moreover, the prediction results of PLMs in our experiments are released as an open resource for more deep and detailed analysis on the language abilities of PLMs. This paper can guide the future work to select, apply, and design PLMs for specific tasks. We have made all the details of experiments publicly available at <a href=https://github.com/RUCAIBox/ElitePLM class=acl-markup-url>https://github.com/RUCAIBox/ElitePLM</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.259.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--259 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.259.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.259" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.259/>Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand</a></strong><br><a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/l/lavinia-dunagan/>Lavinia Dunagan</a>
|
<a href=/people/j/jacob-morrison/>Jacob Morrison</a>
|
<a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--259><div class="card-body p-3 small">Natural language processing researchers have identified limitations of evaluation methodology for generation tasks, with new questions raised about the validity of automatic metrics and of crowdworker judgments. Meanwhile, efforts to improve generation models tend to depend on simple n-gram overlap metrics (e.g., BLEU, ROUGE). We argue that new advances on models and metrics should each more directly benefit and inform the other. We therefore propose a generalization of leaderboards, bidimensional leaderboards (Billboards), that simultaneously tracks progress in language generation models and metrics for their evaluation. Unlike conventional unidimensional leaderboards that sort submitted systems by predetermined metrics, a Billboard accepts both generators and evaluation metrics as competing entries. A Billboard automatically creates an ensemble metric that selects and linearly combines a few metrics based on a global analysis across generators. Further, metrics are ranked based on their correlation with human judgments. We release four Billboards for machine translation, summarization, and image captioning. We demonstrate that a linear ensemble of a few diverse metrics sometimes substantially outperforms existing metrics in isolation. Our mixed-effects model analysis shows that most automatic metrics, especially the reference-based ones, overrate machine over human generation, demonstrating the importance of updating metrics as generation models become stronger (and perhaps more similar to humans) in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.260.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--260 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.260.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.260/>Improving In-Context Few-Shot Learning via Self-Supervised Training</a></strong><br><a href=/people/m/mingda-chen/>Mingda Chen</a>
|
<a href=/people/j/jingfei-du/>Jingfei Du</a>
|
<a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/t/todor-mihaylov/>Todor Mihaylov</a>
|
<a href=/people/s/srini-iyer/>Srini Iyer</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--260><div class="card-body p-3 small">Self-supervised pretraining has made few-shot learning possible for many NLP tasks. But the pretraining objectives are not typically adapted specifically for in-context few-shot learning. In this paper, we propose to use self-supervision in an intermediate training stage between pretraining and downstream few-shot usage with the goal to teach the model to perform in-context few shot learning. We propose and evaluate four self-supervised objectives on two benchmarks. We find that the intermediate self-supervision stage produces models that outperform strong baselines. Ablation study shows that several factors affect the downstream performance, such as the amount of training data and the diversity of the self-supervised objectives. Human-annotated cross-task supervision and self-supervision are complementary. Qualitative analysis suggests that the self-supervised-trained models are better at following task requirements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.261.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--261 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.261.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.261" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.261/>Exposing the Limits of Video-Text Models through Contrast Sets</a></strong><br><a href=/people/j/jae-sung-park/>Jae Sung Park</a>
|
<a href=/people/s/sheng-shen/>Sheng Shen</a>
|
<a href=/people/a/ali-farhadi/>Ali Farhadi</a>
|
<a href=/people/t/trevor-darrell/>Trevor Darrell</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/a/anna-rohrbach/>Anna Rohrbach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--261><div class="card-body p-3 small">Recent video-text models can retrieve relevant videos based on text with a high accuracy, but to what extent do they comprehend the semantics of the text? Can they discriminate between similar entities and actions? To answer this, we propose an evaluation framework that probes video-text models with hard negatives. We automatically build contrast sets, where true textual descriptions are manipulated in ways that change their semantics while maintaining plausibility. Specifically, we leverage a pre-trained language model and a set of heuristics to create verb and person entity focused contrast sets. We apply these in the multiple choice video to-text classification setting. We test the robustness of recent methods on the proposed automatic contrast sets, and compare them to additionally collected human-generated counterparts, to assess their effectiveness. We see that model performance suffers across all methods, erasing the gap between recent CLIP-based methods vs. the earlier methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.262.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.262.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.262" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.262/>Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features</a></strong><br><a href=/people/y/yufei-tian/>Yufei Tian</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--262><div class="card-body p-3 small">Poetry generation, and creative language generation in general, usually suffers from the lack of large training data. In this paper, we present a novel framework to generate sonnets that does not require training on poems. We design a hierarchical framework which plans the poem sketch before decoding. Specifically, a content planning module is trained on non-poetic texts to obtain discourse-level coherence; then a rhyme module generates rhyme words and a polishing module introduces imagery and similes for aesthetics purposes. Finally, we design a constrained decoding algorithm to impose the meter-and-rhyme constraint of the generated sonnets. Automatic and human evaluation show that our multi-stage approach without training on poem corpora generates more coherent, poetic, and creative sonnets than several strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.263.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--263 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.263.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.263.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.263" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.263/>Benchmarking Intersectional Biases in <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/j/john-p-lalor/>John Lalor</a>
|
<a href=/people/y/yi-yang/>Yi Yang</a>
|
<a href=/people/k/kendall-smith/>Kendall Smith</a>
|
<a href=/people/n/nicole-forsgren/>Nicole Forsgren</a>
|
<a href=/people/a/ahmed-abbasi/>Ahmed Abbasi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--263><div class="card-body p-3 small">There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.264.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.264.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.264" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.264/>When is <span class=acl-fixed-case>BERT</span> Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer</a></strong><br><a href=/people/a/ameet-deshpande/>Ameet Deshpande</a>
|
<a href=/people/p/partha-talukdar/>Partha Talukdar</a>
|
<a href=/people/k/karthik-narasimhan/>Karthik Narasimhan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--264><div class="card-body p-3 small">While recent work on multilingual language models has demonstrated their capacity for cross-lingual zero-shot transfer on downstream tasks, there is a lack of consensus in the community as to what shared properties between languages enable such transfer. Analyses involving pairs of natural languages are often inconclusive and contradictory since languages simultaneously differ in many linguistic aspects. In this paper, we perform a large-scale empirical study to isolate the effects of various linguistic properties by measuring zero-shot transfer between four diverse natural languages and their counterparts constructed by modifying aspects such as the script, word order, and syntax. Among other things, our experiments show that the absence of sub-word overlap significantly affects zero-shot transfer when languages differ in their word order, and there is a strong correlation between transfer performance and word embedding alignment between languages (e.g., <span class=tex-math>𝜌<sub>s</sub>=0.94</span> on the task of NLI). Our results call for focus in multilingual models on explicitly improving word embedding alignment between languages rather than relying on its implicit emergence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.265.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.265.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--265 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.265 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.265.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.265" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.265/>How Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns</a></strong><br><a href=/people/s/stephanie-brandl/>Stephanie Brandl</a>
|
<a href=/people/r/ruixiang-cui/>Ruixiang Cui</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--265><div class="card-body p-3 small">Gender-neutral pronouns have recently been introduced in many languages to a) include non-binary people and b) as a generic singular. Recent results from psycholinguistics suggest that gender-neutral pronouns (in Swedish) are not associated with human processing difficulties. This, we show, is in sharp contrast with automated processing. We show that gender-neutral pronouns in Danish, English, and Swedish are associated with higher perplexity, more dispersed attention patterns, and worse downstream performance. We argue that such conservativity in language models may limit widespread adoption of gender-neutral pronouns and must therefore be resolved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.266.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--266 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.266.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.266" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.266/>Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts</a></strong><br><a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/x/xinxi-lyu/>Xinxi Lyu</a>
|
<a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/l/lianhui-qin/>Lianhui Qin</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a>
|
<a href=/people/s/sean-welleck/>Sean Welleck</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--266><div class="card-body p-3 small">Fine-tuning continuous prompts for target tasks has recently emerged as a compact alternative to full model fine-tuning. Motivated by these promising results, we investigate the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve. In practice, we observe a “wayward” behavior between the task solved by continuous prompts and their nearest neighbor discrete projections: We can find continuous prompts that solve a task while being projected to an arbitrary text (e.g., definition of a different or even a contradictory task), while being within a very small (2%) margin of the best continuous prompt of the same size for the task. We provide intuitions behind this odd and surprising behavior, as well as extensive empirical analyses quantifying the effect of various parameters. For instance, for larger model sizes we observe higher waywardness, i.e, we can find prompts that more closely map to any arbitrary text with a smaller drop in accuracy. These findings have important implications relating to the difficulty of faithfully interpreting continuous prompts and their generalization across models and tasks, providing guidance for future progress in prompting language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.267.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.267.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.267/>Contrastive Representation Learning for Cross-Document Coreference Resolution of Events and Entities</a></strong><br><a href=/people/b/benjamin-hsu/>Benjamin Hsu</a>
|
<a href=/people/g/graham-horwood/>Graham Horwood</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--267><div class="card-body p-3 small">Identifying related entities and events within and across documents is fundamental to natural language understanding. We present an approach to entity and event coreference resolution utilizing contrastive representation learning. Earlier state-of-the-art methods have formulated this problem as a binary classification problem and leveraged large transformers in a cross-encoder architecture to achieve their results. For large collections of documents and corresponding set of <span class=tex-math>n</span> mentions, the necessity of performing <span class=tex-math>n<sup>2</sup></span> transformer computations in these earlier approaches can be computationally intensive. We show that it is possible to reduce this burden by applying contrastive learning techniques that only require <span class=tex-math>n</span> transformer computations at inference time. Our method achieves state-of-the-art results on a number of key metrics on the ECB+ corpus and is competitive on others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.268.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.268.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--268 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.268 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.268.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.268/>Learning the Ordering of Coordinate Compounds and Elaborate Expressions in <span class=acl-fixed-case>H</span>mong, <span class=acl-fixed-case>L</span>ahu, and <span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/c/chenxuan-cui/>Chenxuan Cui</a>
|
<a href=/people/k/katherine-j-zhang/>Katherine J. Zhang</a>
|
<a href=/people/d/david-r-mortensen/>David Mortensen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--268><div class="card-body p-3 small">Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate constructions common in languages of East and Southeast Asia. Mortensen (2006) claims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese can be predicted via phonological hierarchies and (2) that these phonological hierarchies lack a clear phonetic rationale. These claims are significant because morphosyntax has often been seen as in a feed-forward relationship with phonology, and phonological generalizations have often been assumed to be phonetically “natural”. We investigate whether the ordering of CCs and EEs can be learned empirically and whether computational models (classifiers and sequence-labeling models) learn unnatural hierarchies similar to those posited by Mortensen (2006). We find that decision trees and SVMs learn to predict the order of CCs/EEs on the basis of phonology, beating strong baselines for all three languages, with DTs learning hierarchies strikingly similar to those proposed by Mortensen. However, we also find that a neural sequence labeling model is able to learn the ordering of elaborate expressions in Hmong very effectively without using any phonological information. We argue that EE ordering can be learned through two independent routes: phonology and lexical distribution, presenting a more nuanced picture than previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.269.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--269 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best new task paper (tied)"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.269.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.269/><span class=acl-fixed-case>FRUIT</span>: Faithfully Reflecting Updated Information in Text</a></strong><br><a href=/people/r/robert-iv/>Robert Iv</a>
|
<a href=/people/a/alexandre-passos/>Alexandre Passos</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--269><div class="card-body p-3 small">Textual knowledge bases such as Wikipedia require considerable effort to keep up to date and consistent. While automated writing assistants could potentially ease this burden, the problem of suggesting edits grounded in external knowledge has been under-explored. In this paper, we introduce the novel generation task of *faithfully reflecting updated information in text* (FRUIT) where the goal is to update an existing article given new evidence. We release the FRUIT-WIKI dataset, a collection of over 170K distantly supervised data produced from pairs of Wikipedia snapshots, along with our data generation pipeline and a gold evaluation set of 914 instances whose edits are guaranteed to be supported by the evidence. We provide benchmark results for popular generation systems as well as EDIT5 – a T5-based approach tailored to editing we introduce that establishes the state of the art. Our analysis shows that developing models that can update articles faithfully requires new capabilities for neural generation models, and opens doors to many new applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.270.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--270 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.270.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.270" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.270/><span class=acl-fixed-case>M</span>ulti2<span class=acl-fixed-case>WOZ</span>: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog</a></strong><br><a href=/people/c/chia-chien-hung/>Chia-Chien Hung</a>
|
<a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--270><div class="card-body p-3 small">Research on (multi-domain) task-oriented dialog (TOD) has predominantly focused on the English language, primarily due to the shortage of robust TOD datasets in other languages, preventing the systematic investigation of cross-lingual transfer for this crucial NLP application area. In this work, we introduce Multi2WOZ, a new multilingual multi-domain TOD dataset, derived from the well-established English dataset MultiWOZ, that spans four typologically diverse languages: Chinese, German, Arabic, and Russian. In contrast to concurrent efforts, Multi2WOZ contains gold-standard dialogs in target languages that are directly comparable with development and test portions of the English dataset, enabling reliable and comparative estimates of cross-lingual transfer performance for TOD. We then introduce a new framework for multilingual conversational specialization of pretrained language models (PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream TOD tasks. Using such conversational PrLMs specialized for concrete target languages, we systematically benchmark a number of zero-shot and few-shot cross-lingual transfer approaches on two standard TOD tasks: Dialog State Tracking and Response Retrieval. Our experiments show that, in most setups, the best performance entails the combination of (i) conversational specialization in the target language and (ii) few-shot transfer for the concrete TOD task. Most importantly, we show that our conversational specialization in the target language allows for an exceptionally sample-efficient few-shot transfer for downstream TOD tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.271.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--271 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.271.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.271" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.271/><span class=acl-fixed-case>C</span>hapter<span class=acl-fixed-case>B</span>reak: A Challenge Dataset for Long-Range Language Models</a></strong><br><a href=/people/s/simeng-sun/>Simeng Sun</a>
|
<a href=/people/k/katherine-thai/>Katherine Thai</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--271><div class="card-body p-3 small">While numerous architectures for long-range language models (LRLMs) have recently been proposed, a meaningful evaluation of their discourse-level language understanding capabilities has not yet followed. To this end, we introduce ChapterBreak, a challenge dataset that provides an LRLM with a long segment from a narrative that ends at a chapter boundary and asks it to distinguish the beginning of the ground-truth next chapter from a set of negative segments sampled from the same narrative. A fine-grained human annotation reveals that our dataset contains many complex types of chapter transitions (e.g., parallel narratives, cliffhanger endings) that require processing global context to comprehend. Experiments on ChapterBreak show that existing LRLMs fail to effectively leverage long-range context, substantially underperforming a segment-level model trained directly for this task. We publicly release our ChapterBreak dataset to spur more principled future research into LRLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.272.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.272.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--272 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.272 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.272.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.272" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.272/><span class=acl-fixed-case>C</span>ol<span class=acl-fixed-case>BERT</span>v2: Effective and Efficient Retrieval via Lightweight Late Interaction</a></strong><br><a href=/people/k/keshav-santhanam/>Keshav Santhanam</a>
|
<a href=/people/o/omar-khattab/>Omar Khattab</a>
|
<a href=/people/j/jon-saad-falcon/>Jon Saad-Falcon</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/m/matei-zaharia/>Matei Zaharia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--272><div class="card-body p-3 small">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6–10x.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.273.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--273 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.273.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.273" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.273/>Quantifying Language Variation Acoustically with Few Resources</a></strong><br><a href=/people/m/martijn-bartelds/>Martijn Bartelds</a>
|
<a href=/people/m/martijn-wieling/>Martijn Wieling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--273><div class="card-body p-3 small">Deep acoustic models represent linguistic information based on massive amounts of data. Unfortunately, for regional languages and dialects such resources are mostly not available. However, deep acoustic models might have learned linguistic information that transfers to low-resource languages. In this study, we evaluate whether this is the case through the task of distinguishing low-resource (Dutch) regional varieties. By extracting embeddings from the hidden layers of various wav2vec 2.0 models (including new models which are pre-trained and/or fine-tuned on Dutch) and using dynamic time warping, we compute pairwise pronunciation differences averaged over 10 words for over 100 individual dialects from four (regional) languages. We then cluster the resulting difference matrix in four groups and compare these to a gold standard, and a partitioning on the basis of comparing phonetic transcriptions. Our results show that acoustic models outperform the (traditional) transcription-based approach without requiring phonetic transcriptions, with the best performance achieved by the multilingual XLSR-53 model fine-tuned on Dutch. On the basis of only six seconds of speech, the resulting clustering closely matches the gold standard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.274.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.274.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--274 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.274 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.274.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.274.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.274" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.274/>Adaptable Adapters</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Moosavi</a>
|
<a href=/people/q/quentin-delfosse/>Quentin Delfosse</a>
|
<a href=/people/k/kristian-kersting/>Kristian Kersting</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--274><div class="card-body p-3 small">State-of-the-art pretrained NLP models contain a hundred million to trillion parameters. Adapters provide a parameter-efficient alternative for the full finetuning in which we can only finetune lightweight neural network layers on top of pretrained weights. Adapter layers are initialized randomly. However, existing work uses the same adapter architecture—i.e., the same adapter layer on top of each layer of the pretrained model—for every dataset, regardless of the properties of the dataset or the amount of available training data. In this work, we introduce adaptable adapters that contain (1) learning different activation functions for different layers and different input data, and (2) a learnable switch to select and only use the beneficial adapter layers. We show that adaptable adapters achieve on-par performances with the standard adapter architecture while using a considerably smaller number of adapter layers. In addition, we show that the selected adapter architecture by adaptable adapters transfers well across different data settings and similar tasks. We propose to use adaptable adapters for designing efficient and effective adapter architectures. The resulting adapters (a) contain about 50% of the learning parameters of the standard adapter and are therefore more efficient at training and inference, and require less storage space, and (b) achieve considerably higher performances in low-data settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.275.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--275 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.275.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.275/>Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants</a></strong><br><a href=/people/m/max-bartolo/>Max Bartolo</a>
|
<a href=/people/t/tristan-thrush/>Tristan Thrush</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--275><div class="card-body p-3 small">In Dynamic Adversarial Data Collection (DADC), human annotators are tasked with finding examples that models struggle to predict correctly. Models trained on DADC-collected training data have been shown to be more robust in adversarial and out-of-domain settings, and are considerably harder for humans to fool. However, DADC is more time-consuming than traditional data collection and thus more costly per annotated example. In this work, we examine whether we can maintain the advantages of DADC, without incurring the additional cost. To that end, we introduce Generative Annotation Assistants (GAAs), generator-in-the-loop models that provide real-time suggestions that annotators can either approve, modify, or reject entirely. We collect training datasets in twenty experimental settings and perform a detailed analysis of this approach for the task of extractive question answering (QA) for both standard and adversarial data collection. We demonstrate that GAAs provide significant efficiency benefits with over a 30% annotation speed-up, while leading to over a 5x improvement in model fooling rates. In addition, we find that using GAA-assisted training data leads to higher downstream model performance on a variety of question answering tasks over adversarial data collection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.276.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.276.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--276 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.276 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.276.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.276/><span class=acl-fixed-case>GMN</span>: Generative Multi-modal Network for Practical Document Information Extraction</a></strong><br><a href=/people/h/haoyu-cao/>Haoyu Cao</a>
|
<a href=/people/j/jiefeng-ma/>Jiefeng Ma</a>
|
<a href=/people/a/antai-guo/>Antai Guo</a>
|
<a href=/people/y/yiqing-hu/>Yiqing Hu</a>
|
<a href=/people/h/hao-liu/>Hao Liu</a>
|
<a href=/people/d/deqiang-jiang/>Deqiang Jiang</a>
|
<a href=/people/y/yinsong-liu/>Yinsong Liu</a>
|
<a href=/people/b/bo-ren/>Bo Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--276><div class="card-body p-3 small">Document Information Extraction (DIE) has attracted increasing attention due to its various advanced applications in the real world. Although recent literature has already achieved competitive results, these approaches usually fail when dealing with complex documents with noisy OCR results or mutative layouts. This paper proposes Generative Multi-modal Network (GMN) for real-world scenarios to address these problems, which is a robust multi-modal generation method without predefined label categories. With the carefully designed spatial encoder and modal-aware mask module, GMN can deal with complex documents that are hard to serialized into sequential order. Moreover, GMN tolerates errors in OCR results and requires no character-level annotation, which is vital because fine-grained annotation of numerous documents is laborious and even requires annotators with specialized domain knowledge. Extensive experiments show that GMN achieves new state-of-the-art performance on several public DIE datasets and surpasses other methods by a large margin, especially in realistic scenes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.277.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--277 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.277.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.277" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.277/>One Reference Is Not Enough: Diverse Distillation with Reference Selection for Non-Autoregressive Translation</a></strong><br><a href=/people/c/chenze-shao/>Chenze Shao</a>
|
<a href=/people/x/xuanfu-wu/>Xuanfu Wu</a>
|
<a href=/people/y/yang-feng/>Yang Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--277><div class="card-body p-3 small">Non-autoregressive neural machine translation (NAT) suffers from the multi-modality problem: the source sentence may have multiple correct translations, but the loss function is calculated only according to the reference sentence. Sequence-level knowledge distillation makes the target more deterministic by replacing the target with the output from an autoregressive model. However, the multi-modality problem in the distilled dataset is still nonnegligible. Furthermore, learning from a specific teacher limits the upper bound of the model capability, restricting the potential of NAT models. In this paper, we argue that one reference is not enough and propose diverse distillation with reference selection (DDRS) for NAT. Specifically, we first propose a method called SeedDiv for diverse machine translation, which enables us to generate a dataset containing multiple high-quality reference translations for each source sentence. During the training, we compare the NAT output with all references and select the one that best fits the NAT output to train the model. Experiments on widely-used machine translation benchmarks demonstrate the effectiveness of DDRS, which achieves 29.82 BLEU with only one decoding pass on WMT14 En-De, improving the state-of-the-art performance for NAT by over 1 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.278.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--278 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.278.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.278" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.278/>Can Rationalization Improve Robustness?</a></strong><br><a href=/people/h/howard-chen/>Howard Chen</a>
|
<a href=/people/j/jacqueline-he/>Jacqueline He</a>
|
<a href=/people/k/karthik-narasimhan/>Karthik Narasimhan</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--278><div class="card-body p-3 small">A growing line of work has investigated the development of neural NLP models that can produce rationales–subsets of input that can explain their model predictions. In this paper, we ask whether such rationale models can provide robustness to adversarial attacks in addition to their interpretable nature. Since these models need to first generate rationales (“rationalizer”) before making predictions (“predictor”), they have the potential to ignore noise or adversarially added text by simply masking it out of the generated rationale. To this end, we systematically generate various types of ‘AddText’ attacks for both token and sentence-level rationalization tasks and perform an extensive empirical evaluation of state-of-the-art rationale models across five different tasks. Our experiments reveal that the rationale models promise to improve robustness over AddText attacks while they struggle in certain scenarios–when the rationalizer is sensitive to position bias or lexical choices of attack text. Further, leveraging human rationale as supervision does not always translate to better performance. Our study is a first step towards exploring the interplay between interpretability and robustness in the rationalize-then-predict framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.279.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.279.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--279 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.279 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.279.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.279" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.279/>On the Effectiveness of Sentence Encoding for Intent Detection Meta-Learning</a></strong><br><a href=/people/t/tingting-ma/>Tingting Ma</a>
|
<a href=/people/q/qianhui-wu/>Qianhui Wu</a>
|
<a href=/people/z/zhiwei-yu/>Zhiwei Yu</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--279><div class="card-body p-3 small">Recent studies on few-shot intent detection have attempted to formulate the task as a meta-learning problem, where a meta-learning model is trained with a certain capability to quickly adapt to newly specified few-shot tasks with potentially unseen intent categories. Prototypical networks have been commonly used in this setting, with the hope that good prototypical representations could be learned to capture the semantic similarity between the query and a few labeled instances. This intuition naturally leaves a question of whether or not a good sentence representation scheme could suffice for the task without further domain-specific adaptation. In this paper, we conduct empirical studies on a number of general-purpose sentence embedding schemes, showing that good sentence embeddings without any fine-tuning on intent detection data could produce a non-trivially strong performance. Inspired by the results from our qualitative analysis, we propose a frustratingly easy modification, which leads to consistent improvements over all sentence encoding schemes, including those from the state-of-the-art prototypical network variants with task-specific fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.280.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--280 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.280.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.280" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.280/>A Computational Acquisition Model for Multimodal Word Categorization</a></strong><br><a href=/people/u/uri-berger/>Uri Berger</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--280><div class="card-body p-3 small">Recent advances in self-supervised modeling of text and images open new opportunities for computational models of child language acquisition, which is believed to rely heavily on cross-modal signals. However, prior studies has been limited by their reliance on vision models trained on large image datasets annotated with a pre-defined set of depicted object categories. This is (a) not faithful to the information children receive and (b) prohibits the evaluation of such models with respect to category learning tasks, due to the pre-imposed category structure. We address this gap, and present a cognitively-inspired, multimodal acquisition model, trained from image-caption pairs on naturalistic data using cross-modal self-supervision. We show that the model learns word categories and object recognition abilities, and presents trends reminiscent of ones reported in the developmental literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.281.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--281 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.281.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.281" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.281/>Residue-Based Natural Language Adversarial Attack Detection</a></strong><br><a href=/people/v/vyas-raina/>Vyas Raina</a>
|
<a href=/people/m/mark-gales/>Mark Gales</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--281><div class="card-body p-3 small">Deep learning based systems are susceptible to adversarial attacks, where a small, imperceptible change at the input alters the model prediction. However, to date the majority of the approaches to detect these attacks have been designed for image processing systems. Many popular image adversarial detection approaches are able to identify adversarial examples from embedding feature spaces, whilst in the NLP domain existing state of the art detection approaches solely focus on input text features, without consideration of model embedding spaces. This work examines what differences result when porting these image designed strategies to Natural Language Processing (NLP) tasks - these detectors are found to not port over well. This is expected as NLP systems have a very different form of input: discrete and sequential in nature, rather than the continuous and fixed size inputs for images. As an equivalent model-focused NLP detection approach, this work proposes a simple sentence-embedding “residue” based detector to identify adversarial examples. On many tasks, it out-performs ported image domain detectors and recent state of the art NLP specific detectors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.282.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.282.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--282 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.282 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.282.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.282/>Does it Really Generalize Well on Unseen Data? Systematic Evaluation of Relational Triple Extraction Methods</a></strong><br><a href=/people/j/juhyuk-lee/>Juhyuk Lee</a>
|
<a href=/people/m/min-joong-lee/>Min-Joong Lee</a>
|
<a href=/people/j/june-yong-yang/>June Yong Yang</a>
|
<a href=/people/e/eunho-yang/>Eunho Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--282><div class="card-body p-3 small">The ability to extract entities and their relations from unstructured text is essential for the automated maintenance of large-scale knowledge graphs. To keep a knowledge graph up-to-date, an extractor needs not only the ability to recall the triples it encountered during training, but also the ability to extract the new triples from the context that it has never seen before. In this paper, we show that although existing extraction models are able to easily memorize and recall already seen triples, they cannot generalize effectively for unseen triples. This alarming observation was previously unknown due to the composition of the test sets of the go-to benchmark datasets, which turns out to contain only 2% unseen data, rendering them incapable to measure the generalization performance. To separately measure the generalization performance from the memorization performance, we emphasize unseen data by rearranging datasets, sifting out training instances, or augmenting test sets. In addition to that, we present a simple yet effective augmentation technique to promote generalization of existing extraction models, and experimentally confirm that the proposed method can significantly increase the generalization performance of existing models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.283.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.283.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.283.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.283/>From spoken dialogue to formal summary: An utterance rewriting for dialogue summarization</a></strong><br><a href=/people/y/yue-fang/>Yue Fang</a>
|
<a href=/people/h/hainan-zhang/>Hainan Zhang</a>
|
<a href=/people/h/hongshen-chen/>Hongshen Chen</a>
|
<a href=/people/z/zhuoye-ding/>Zhuoye Ding</a>
|
<a href=/people/b/bo-long/>Bo Long</a>
|
<a href=/people/y/yanyan-lan/>Yanyan Lan</a>
|
<a href=/people/y/yanquan-zhou/>Yanquan Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--283><div class="card-body p-3 small">Due to the dialogue characteristics of unstructured contexts and multi-parties with first-person perspective, many successful text summarization works have failed when dealing with dialogue summarization. In dialogue summarization task, the input dialogue is usually spoken style with ellipsis and co-references but the output summaries are more formal and complete. Therefore, the dialogue summarization model should be able to complete the ellipsis content and co-reference information and then produce a suitable summary accordingly. However, the current state-of-the-art models pay more attention on the topic or structure of summary, rather than the consistency of dialogue summary with its input dialogue context, which may suffer from the personal and logical inconsistency problem. In this paper, we propose a new model, named ReWriteSum, to tackle this problem. Firstly, an utterance rewriter is conducted to complete the ellipsis content of dialogue content and then obtain the rewriting utterances. Then, the co-reference data augmentation mechanism is utilized to replace the referential person name with its specific name to enhance the personal information. Finally, the rewriting utterances and the co-reference replacement data are used in the standard BART model. Experimental results on both SAMSum and DialSum datasets show that our ReWriteSum significantly outperforms baseline models, in terms of both metric-based and human evaluations. Further analysis on multi-speakers also shows that ReWriteSum can obtain relatively higher improvement with more speakers, validating the correctness and property of ReWriteSum.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.284.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.284.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--284 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.284 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.284.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.284" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.284/><span class=acl-fixed-case>EASE</span>: Entity-Aware Contrastive Learning of Sentence Embedding</a></strong><br><a href=/people/s/sosuke-nishikawa/>Sosuke Nishikawa</a>
|
<a href=/people/r/ryokan-ri/>Ryokan Ri</a>
|
<a href=/people/i/ikuya-yamada/>Ikuya Yamada</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a>
|
<a href=/people/i/isao-echizen/>Isao Echizen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--284><div class="card-body p-3 small">We present EASE, a novel method for learning sentence embeddings via contrastive learning between sentences and their related entities. The advantage of using entity supervision is twofold: (1) entities have been shown to be a strong indicator of text semantics and thus should provide rich training signals for sentence embeddings; (2) entities are defined independently of languages and thus offer useful cross-lingual alignment supervision. We evaluate EASE against other unsupervised models both in monolingual and multilingual settings. We show that EASE exhibits competitive or better performance in English semantic textual similarity (STS) and short text clustering (STC) tasks and it significantly outperforms baseline methods in multilingual settings on a variety of tasks. Our source code, pre-trained models, and newly constructed multi-lingual STC dataset are available at <a href=https://github.com/studio-ousia/ease class=acl-markup-url>https://github.com/studio-ousia/ease</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.285.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--285 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.285.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.285" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.285/>Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics</a></strong><br><a href=/people/z/zihan-zhang/>Zihan Zhang</a>
|
<a href=/people/m/meng-fang/>Meng Fang</a>
|
<a href=/people/l/ling-chen/>Ling Chen</a>
|
<a href=/people/m/mohammad-reza-namazi-rad/>Mohammad Reza Namazi Rad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--285><div class="card-body p-3 small">Recent work incorporates pre-trained word embeddings such as BERT embeddings into Neural Topic Models (NTMs), generating highly coherent topics. However, with high-quality contextualized document representations, do we really need sophisticated neural models to obtain coherent and interpretable topics? In this paper, we conduct thorough experiments showing that directly clustering high-quality sentence embeddings with an appropriate word selecting method can generate more coherent and diverse topics than NTMs, achieving also higher efficiency and simplicity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.286.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--286 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.286.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.286/>Dynamic Multistep Reasoning based on Video Scene Graph for Video Question Answering</a></strong><br><a href=/people/j/jianguo-mao/>Jianguo Mao</a>
|
<a href=/people/w/wenbin-jiang/>Wenbin Jiang</a>
|
<a href=/people/x/xiangdong-wang/>Xiangdong Wang</a>
|
<a href=/people/z/zhifan-feng/>Zhifan Feng</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/h/hong-liu/>Hong Liu</a>
|
<a href=/people/y/yong-zhu/>Yong Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--286><div class="card-body p-3 small">Existing video question answering (video QA) models lack the capacity for deep video understanding and flexible multistep reasoning. We propose for video QA a novel model which performs dynamic multistep reasoning between questions and videos. It creates video semantic representation based on the video scene graph composed of semantic elements of the video and semantic relations among these elements. Then, it performs multistep reasoning for better answer decision between the representations of the question and the video, and dynamically integrate the reasoning results. Experiments show the significant advantage of the proposed model against previous methods in accuracy and interpretability. Against the existing state-of-the-art model, the proposed model dramatically improves more than <span class=tex-math>4%/3.1%/2%</span> on the three widely used video QA datasets, MSRVTT-QA, MSRVTT multi-choice, and TGIF-QA, and displays better interpretability by backtracing along with the attention mechanisms to the video scene graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.287.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--287 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.287" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-main.287/><span class=acl-fixed-case>TRUE</span>: Re-evaluating Factual Consistency Evaluation</a></strong><br><a href=/people/o/or-honovich/>Or Honovich</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/h/hagai-taitelbaum/>Hagai Taitelbaum</a>
|
<a href=/people/d/doron-kukliansy/>Doron Kukliansy</a>
|
<a href=/people/v/vered-cohen/>Vered Cohen</a>
|
<a href=/people/t/thomas-scialom/>Thomas Scialom</a>
|
<a href=/people/i/idan-szpektor/>Idan Szpektor</a>
|
<a href=/people/a/avinatan-hassidim/>Avinatan Hassidim</a>
|
<a href=/people/y/yossi-matias/>Yossi Matias</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--287><div class="card-body p-3 small">Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real-world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data. While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption. Moreover, previous meta-evaluation protocols focused on system-level correlations with human annotations, which leave the example-level accuracy of such metrics unclear. In this work, we introduce TRUE: a comprehensive survey and assessment of factual consistency metrics on a standardized collection of existing texts from diverse tasks, manually annotated for factual consistency. Our standardization enables an example-level meta-evaluation protocol that is more actionable and interpretable than previously reported correlations, yielding clearer quality measures. Across diverse state-of-the-art metrics and 11 datasets we find that large-scale NLI and question generation-and-answering-based approaches achieve strong and complementary results. We recommend those methods as a starting point for model and metric developers, and hope TRUE will foster progress towards even better evaluation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.288.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--288 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.288.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.288" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.288/>Knowledge Inheritance for Pre-trained Language Models</a></strong><br><a href=/people/y/yujia-qin/>Yujia Qin</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/j/jing-yi/>Jing Yi</a>
|
<a href=/people/j/jiajie-zhang/>Jiajie Zhang</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zhengyan-zhang/>Zhengyan Zhang</a>
|
<a href=/people/y/yusheng-su/>Yusheng Su</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--288><div class="card-body p-3 small">Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing PLMs benefit training large-scale PLMs in future. Specifically, we introduce a pre-training framework named “knowledge inheritance” (KI) and explore how could knowledge distillation serve as auxiliary supervision during pre-training to efficiently learn larger PLMs. Experimental results demonstrate the superiority of KI in training efficiency. We also conduct empirical analyses to explore the effects of teacher PLMs’ pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI could be applied to domain adaptation and knowledge transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.289.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--289 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.289.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.289" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.289/><span class=acl-fixed-case>B</span>i-<span class=acl-fixed-case>S</span>im<span class=acl-fixed-case>C</span>ut: A Simple Strategy for Boosting Neural Machine Translation</a></strong><br><a href=/people/p/pengzhi-gao/>Pengzhi Gao</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--289><div class="card-body p-3 small">We introduce Bi-SimCut: a simple but effective training strategy to boost neural machine translation (NMT) performance. It consists of two procedures: bidirectional pretraining and unidirectional finetuning. Both procedures utilize SimCut, a simple regularization method that forces the consistency between the output distributions of the original and the cutoff sentence pairs. Without leveraging extra dataset via back-translation or integrating large-scale pretrained model, Bi-SimCut achieves strong translation performance across five translation benchmarks (data sizes range from 160K to 20.2M): BLEU scores of 31.16 for <span class=tex-math><span class=text-monospace>en</span>→<span class=text-monospace>de</span></span> and 38.37 for <span class=tex-math><span class=text-monospace>de</span>→<span class=text-monospace>en</span></span> on the IWSLT14 dataset, 30.78 for <span class=tex-math><span class=text-monospace>en</span>→<span class=text-monospace>de</span></span> and 35.15 for <span class=tex-math><span class=text-monospace>de</span>→<span class=text-monospace>en</span></span> on the WMT14 dataset, and 27.17 for <span class=tex-math><span class=text-monospace>zh</span>→<span class=text-monospace>en</span></span> on the WMT17 dataset. SimCut is not a new method, but a version of Cutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be considered as a perturbation-based method. Given the universality and simplicity of Bi-SimCut and SimCut, we believe they can serve as strong baselines for future NMT research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.290.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--290 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.290.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.290" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.290/>On Transferability of Prompt Tuning for Natural Language Processing</a></strong><br><a href=/people/y/yusheng-su/>Yusheng Su</a>
|
<a href=/people/x/xiaozhi-wang/>Xiaozhi Wang</a>
|
<a href=/people/y/yujia-qin/>Yujia Qin</a>
|
<a href=/people/c/chi-min-chan/>Chi-Min Chan</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/h/huadong-wang/>Huadong Wang</a>
|
<a href=/people/k/kaiyue-wen/>Kaiyue Wen</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/p/peng-li/>Peng Li</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--290><div class="card-body p-3 small">Prompt tuning (PT) is a promising parameter-efficient method to utilize extremely large pre-trained language models (PLMs), which can achieve comparable performance to full-parameter fine-tuning by only tuning a few soft prompts. However, PT requires much more training time than fine-tuning. Intuitively, knowledge transfer can help to improve the efficiency. To explore whether we can improve PT via prompt transfer, we empirically investigate the transferability of soft prompts across different downstream tasks and PLMs in this work. We find that (1) in zero-shot setting, trained soft prompts can effectively transfer to similar tasks on the same PLM and also to other PLMs with a cross-model projector trained on similar tasks; (2) when used as initialization, trained soft prompts of similar tasks and projected prompts of other PLMs can significantly accelerate training and also improve the performance of PT. Moreover, to explore what decides prompt transferability, we investigate various transferability indicators and find that the overlapping rate of activated neurons strongly reflects the transferability, which suggests how the prompts stimulate PLMs is essential. Our findings show that prompt transfer is promising for improving PT, and further research shall focus more on prompts’ stimulation to PLMs. The source code can be obtained from <a href=https://github.com/thunlp/Prompt-Transferability class=acl-markup-url>https://github.com/thunlp/Prompt-Transferability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.291.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.291.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--291 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.291 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.291.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.291" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.291/><span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>EE</span>: A Large-Scale and Fine-grained Benchmark for Document-level Event Extraction</a></strong><br><a href=/people/m/meihan-tong/>MeiHan Tong</a>
|
<a href=/people/b/bin-xu/>Bin Xu</a>
|
<a href=/people/s/shuai-wang/>Shuai Wang</a>
|
<a href=/people/m/meihuan-han/>Meihuan Han</a>
|
<a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/j/jiangqi-zhu/>Jiangqi Zhu</a>
|
<a href=/people/s/siyu-chen/>Siyu Chen</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--291><div class="card-body p-3 small">Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present DocEE, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. We highlight three features: large-scale manual annotations, fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41% Vs 85% in F1 score), indicating that DocEE is an open issue. DocEE is now available at <a href=https://github.com/tongmeihan1995/DocEE.git class=acl-markup-url>https://github.com/tongmeihan1995/DocEE.git</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.292.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.292.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--292 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.292 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.292.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.292" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.292/>Towards Debiasing Translation Artifacts</a></strong><br><a href=/people/k/koel-dutta-chowdhury/>Koel Dutta Chowdhury</a>
|
<a href=/people/r/rricha-jalota/>Rricha Jalota</a>
|
<a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a>
|
<a href=/people/j/josef-genabith/>Josef Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--292><div class="card-body p-3 small">Cross-lingual natural language processing relies on translation, either by humans or machines, at different levels, from translating training data to translating test sets. However, compared to original texts in the same language, translations possess distinct qualities referred to as translationese. Previous research has shown that these translation artifacts influence the performance of a variety of cross-lingual tasks. In this work, we propose a novel approach to reducing translationese by extending an established bias-removal technique. We use the Iterative Null-space Projection (INLP) algorithm, and show by measuring classification accuracy before and after debiasing, that translationese is reduced at both sentence and word level. We evaluate the utility of debiasing translationese on a natural language inference (NLI) task, and show that by reducing this bias, NLI accuracy improves. To the best of our knowledge, this is the first study to debias translationese as represented in latent embedding space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.293.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.293.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--293 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.293 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.293.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.293" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.293/><span class=acl-fixed-case>WECHSEL</span>: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models</a></strong><br><a href=/people/b/benjamin-minixhofer/>Benjamin Minixhofer</a>
|
<a href=/people/f/fabian-paischer/>Fabian Paischer</a>
|
<a href=/people/n/navid-rekabsaz/>Navid Rekabsaz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--293><div class="card-body p-3 small">Large pretrained language models (LMs) have become the central building block of many NLP applications. Training these models requires ever more computational resources and most of the existing models are trained on English text only. It is exceedingly expensive to train these models in other languages. To alleviate this problem, we introduce a novel method – called WECHSEL – to efficiently and effectively transfer pretrained LMs to new languages. WECHSEL can be applied to any model which uses subword-based tokenization and learns an embedding for each subword. The tokenizer of the source model (in English) is replaced with a tokenizer in the target language and token embeddings are initialized such that they are semantically similar to the English tokens by utilizing multilingual static word embeddings covering English and the target language. We use WECHSEL to transfer the English RoBERTa and GPT-2 models to four languages (French, German, Chinese and Swahili). We also study the benefits of our method on very low-resource languages. WECHSEL improves over proposed methods for cross-lingual parameter transfer and outperforms models of comparable size trained from scratch with up to 64x less training effort. Our method makes training large language models for new languages more accessible and less damaging to the environment. We make our code and models publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.294.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--294 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.294.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.294/>A New Concept of Knowledge based Question Answering (<span class=acl-fixed-case>KBQA</span>) System for Multi-hop Reasoning</a></strong><br><a href=/people/y/yu-wang/>Yu Wang</a>
|
<a href=/people/v/v-srinivasan-samsung-com-v-srinivasan-samsung-com/>V.srinivasan@samsung.com V.srinivasan@samsung.com</a>
|
<a href=/people/h/hongxia-jin/>Hongxia Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--294><div class="card-body p-3 small">Knowledge based question answering (KBQA) is a complex task for natural language understanding. Many KBQA approaches have been proposed in recent years, and most of them are trained based on labeled reasoning path. This hinders the system’s performance as many correct reasoning paths are not labeled as ground truth, and thus they cannot be learned. In this paper, we introduce a new concept of KBQA system which can leverage multiple reasoning paths’ information and only requires labeled answer as supervision. We name it as <b>M</b>utliple <b>R</b>easoning <b>P</b>aths KB<b>QA</b> System (MRP-QA). We conduct experiments on several benchmark datasets containing both single-hop simple questions as well as muti-hop complex questions, including WebQuestionSP (WQSP), ComplexWebQuestion-1.1 (CWQ), and PathQuestion-Large (PQL), and demonstrate strong performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.295.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--295 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.295.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.295/>Bilingual Tabular Inference: A Case Study on <span class=acl-fixed-case>I</span>ndic Languages</a></strong><br><a href=/people/c/chaitanya-agarwal/>Chaitanya Agarwal</a>
|
<a href=/people/v/vivek-gupta/>Vivek Gupta</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--295><div class="card-body p-3 small">Existing research on Tabular Natural Language Inference (TNLI) exclusively examines the task in a monolingual setting where the tabular premise and hypothesis are in the same language. However, due to the uneven distribution of text resources on the web across languages, it is common to have the tabular premise in a high resource language and the hypothesis in a low resource language. As a result, we present the challenging task of bilingual Tabular Natural Language Inference (bTNLI), in which the tabular premise and a hypothesis over it are in two separate languages. We construct EI-InfoTabS: an English-Indic bTNLI dataset by translating the textual hypotheses of the English TNLI dataset InfoTabS into eleven major Indian languages. We thoroughly investigate how pre-trained multilingual models learn and perform on EI-InfoTabS. Our study shows that the performance on bTNLI can be close to its monolingual counterpart, with translate-train, translate-test and unified-train being strongly competitive baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.296.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.296.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--296 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.296 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.296.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.296" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.296/>Generative Biomedical Entity Linking via Knowledge Base-Guided Pre-training and Synonyms-Aware Fine-tuning</a></strong><br><a href=/people/h/hongyi-yuan/>Hongyi Yuan</a>
|
<a href=/people/z/zheng-yuan/>Zheng Yuan</a>
|
<a href=/people/s/sheng-yu/>Sheng Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--296><div class="card-body p-3 small">Entities lie in the heart of biomedical natural language understanding, and the biomedical entity linking (EL) task remains challenging due to the fine-grained and diversiform concept names. Generative methods achieve remarkable performances in general domain EL with less memory usage while requiring expensive pre-training. Previous biomedical EL methods leverage synonyms from knowledge bases (KB) which is not trivial to inject into a generative method. In this work, we use a generative approach to model biomedical EL and propose to inject synonyms knowledge in it. We propose KB-guided pre-training by constructing synthetic samples with synonyms and definitions from KB and require the model to recover concept names. We also propose synonyms-aware fine-tuning to select concept names for training, and propose decoder prompt and multi-synonyms constrained prefix tree for inference. Our method achieves state-of-the-art results on several biomedical EL tasks without candidate selection which displays the effectiveness of proposed pre-training and fine-tuning strategies. The source code is available at <a href=https://github.com/Yuanhy1997/GenBioEL class=acl-markup-url>https://github.com/Yuanhy1997/GenBioEL</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.297.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--297 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.297.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.297.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.297" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.297/>Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting</a></strong><br><a href=/people/l/linzhi-wu/>Linzhi Wu</a>
|
<a href=/people/p/pengjun-xie/>Pengjun Xie</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/m/meishan-zhang/>Meishan Zhang</a>
|
<a href=/people/m/ma-chunping/>Ma Chunping</a>
|
<a href=/people/g/guangwei-xu/>Guangwei Xu</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--297><div class="card-body p-3 small">Self-augmentation has received increasing research interest recently to improve named entity recognition (NER) performance in low-resource scenarios. Token substitution and mixup are two feasible heterogeneous self-augmentation techniques for NER that can achieve effective performance with certain specialized efforts. Noticeably, self-augmentation may introduce potentially noisy augmented data. Prior research has mainly resorted to heuristic rule-based constraints to reduce the noise for specific self-augmentation methods individually. In this paper, we revisit these two typical self-augmentation methods for NER, and propose a unified meta-reweighting strategy for them to achieve a natural integration. Our method is easily extensible, imposing little effort on a specific self-augmentation method. Experiments on different Chinese and English NER benchmarks show that our token substitution and mixup method, as well as their integration, can achieve effective performance improvement. Based on the meta-reweighting mechanism, we can enhance the advantages of the self-augmentation techniques without much extra effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.298.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.298.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--298 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.298 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.298.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.298" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.298/>Unsupervised Stem-based Cross-lingual Part-of-Speech Tagging for Morphologically Rich Low-Resource Languages</a></strong><br><a href=/people/r/ramy-eskander/>Ramy Eskander</a>
|
<a href=/people/c/cass-lowry/>Cass Lowry</a>
|
<a href=/people/s/sujay-khandagale/>Sujay Khandagale</a>
|
<a href=/people/j/judith-l-klavans/>Judith Klavans</a>
|
<a href=/people/m/maria-polinsky/>Maria Polinsky</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--298><div class="card-body p-3 small">Unsupervised cross-lingual projection for part-of-speech (POS) tagging relies on the use of parallel data to project POS tags from a source language for which a POS tagger is available onto a target language across word-level alignments. The projected tags then form the basis for learning a POS model for the target language. However, languages with rich morphology often yield sparse word alignments because words corresponding to the same citation form do not align well. We hypothesize that for morphologically complex languages, it is more efficient to use the stem rather than the word as the core unit of abstraction. Our contributions are: 1) we propose an unsupervised stem-based cross-lingual approach for POS tagging for low-resource languages of rich morphology; 2) we further investigate morpheme-level alignment and projection; and 3) we examine whether the use of linguistic priors for morphological segmentation improves POS tagging. We conduct experiments using six source languages and eight morphologically complex target languages of diverse typologies. Our results show that the stem-based approach improves the POS models for all the target languages, with an average relative error reduction of 10.3% in accuracy per target language, and outperforms the word-based approach that operates on three-times more data for about two thirds of the language pairs we consider. Moreover, we show that morpheme-level alignment and projection and the use of linguistic priors for morphological segmentation further improve POS tagging.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.299.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--299 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.299.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.299" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.299/>Optimising Equal Opportunity Fairness in Model Training</a></strong><br><a href=/people/a/aili-shen/>Aili Shen</a>
|
<a href=/people/x/xudong-han/>Xudong Han</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--299><div class="card-body p-3 small">Real-world datasets often encode stereotypes and societal biases. Such biases can be implicitly captured by trained models, leading to biased predictions and exacerbating existing societal preconceptions. Existing debiasing methods, such as adversarial training and removing protected information from representations, have been shown to reduce bias. However, a disconnect between fairness criteria and training objectives makes it difficult to reason theoretically about the effectiveness of different techniques. In this work, we propose two novel training objectives which directly optimise for the widely-used criterion of <i>equal opportunity</i>, and show that they are effective in reducing bias while maintaining high performance over two classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.300.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--300 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.300.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.300.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.300" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.300/>Leaner and Faster: Two-Stage Model Compression for Lightweight Text-Image Retrieval</a></strong><br><a href=/people/s/siyu-ren/>Siyu Ren</a>
|
<a href=/people/k/kenny-zhu/>Kenny Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--300><div class="card-body p-3 small">Current text-image approaches (e.g., CLIP) typically adopt dual-encoder architecture using pre-trained vision-language representation. However, these models still pose non-trivial memory requirements and substantial incremental indexing time, which makes them less practical on mobile devices. In this paper, we present an effective two-stage framework to compress large pre-trained dual-encoder for lightweight text-image retrieval. The resulting model is smaller (39% of the original), faster (1.6x/2.9x for processing image/text respectively), yet performs on par with or better than the original full model on Flickr30K and MSCOCO benchmarks. We also open-source an accompanying realistic mobile image search application.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.301.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--301 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.301.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.301/>Joint Learning-based Heterogeneous Graph Attention Network for Timeline Summarization</a></strong><br><a href=/people/j/jingyi-you/>Jingyi You</a>
|
<a href=/people/d/dongyuan-li/>Dongyuan Li</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/k/kotaro-funakoshi/>Kotaro Funakoshi</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--301><div class="card-body p-3 small">Previous studies on the timeline summarization (TLS) task ignored the information interaction between sentences and dates, and adopted pre-defined unlearnable representations for them. They also considered date selection and event detection as two independent tasks, which makes it impossible to integrate their advantages and obtain a globally optimal summary. In this paper, we present a <i>joint learning-based heterogeneous graph attention network for TLS</i> (HeterTls), in which date selection and event detection are combined into a unified framework to improve the extraction accuracy and remove redundant sentences simultaneously. Our heterogeneous graph involves multiple types of nodes, the representations of which are iteratively learned across the heterogeneous graph attention layer. We evaluated our model on four datasets, and found that it significantly outperformed the current state-of-the-art baselines with regard to ROUGE scores and date selection metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.302.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--302 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.302.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.302/><span class=acl-fixed-case>E</span>arly Rumor Detection Using Neural <span class=acl-fixed-case>H</span>awkes Process with a New Benchmark Dataset</a></strong><br><a href=/people/f/fengzhu-zeng/>Fengzhu Zeng</a>
|
<a href=/people/w/wei-gao/>Wei Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--302><div class="card-body p-3 small">Little attention has been paid on EArly Rumor Detection (EARD), and EARD performance was evaluated inappropriately on a few datasets where the actual early-stage information is largely missing. To reverse such situation, we construct BEARD, a new Benchmark dataset for EARD, based on claims from fact-checking websites by trying to gather as many early relevant posts as possible. We also propose HEARD, a novel model based on neural Hawkes process for EARD, which can guide a generic rumor detection model to make timely, accurate and stable predictions. Experiments show that HEARD achieves effective EARD performance on two commonly used general rumor detection datasets and our BEARD dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.303.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--303 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.303.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.303/>Emp-<span class=acl-fixed-case>RFT</span>: Empathetic Response Generation via Recognizing Feature Transitions between Utterances</a></strong><br><a href=/people/w/wongyu-kim/>Wongyu Kim</a>
|
<a href=/people/y/youbin-ahn/>Youbin Ahn</a>
|
<a href=/people/d/donghyun-kim/>Donghyun Kim</a>
|
<a href=/people/k/kyong-ho-lee/>Kyong-Ho Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--303><div class="card-body p-3 small">Each utterance in multi-turn empathetic dialogues has features such as emotion, keywords, and utterance-level meaning. Feature transitions between utterances occur naturally. However, existing approaches fail to perceive the transitions because they extract features for the context at the coarse-grained level. To solve the above issue, we propose a novel approach of recognizing feature transitions between utterances, which helps understand the dialogue flow and better grasp the features of utterance that needs attention. Also, we introduce a response generation strategy to help focus on emotion and keywords related to appropriate features when generating responses. Experimental results show that our approach outperforms baselines and especially, achieves significant improvements on multi-turn dialogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.304.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--304 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.304.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.304" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.304/><span class=acl-fixed-case>KCD</span>: Knowledge Walks and Textual Cues Enhanced Political Perspective Detection in News Media</a></strong><br><a href=/people/w/wenqian-zhang/>Wenqian Zhang</a>
|
<a href=/people/s/shangbin-feng/>Shangbin Feng</a>
|
<a href=/people/z/zilong-chen/>Zilong Chen</a>
|
<a href=/people/z/zhenyu-lei/>Zhenyu Lei</a>
|
<a href=/people/j/jundong-li/>Jundong Li</a>
|
<a href=/people/m/minnan-luo/>Minnan Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--304><div class="card-body p-3 small">Political perspective detection has become an increasingly important task that can help combat echo chambers and political polarization. Previous approaches generally focus on leveraging textual content to identify stances, while they fail to reason with background knowledge or leverage the rich semantic and syntactic textual labels in news articles. In light of these limitations, we propose KCD, a political perspective detection approach to enable multi-hop knowledge reasoning and incorporate textual cues as paragraph-level labels. Specifically, we firstly generate random walks on external knowledge graphs and infuse them with news text representations. We then construct a heterogeneous information network to jointly model news content as well as semantic, syntactic and entity cues in news articles. Finally, we adopt relational graph neural networks for graph-level representation learning and conduct political perspective detection. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods on two benchmark datasets. We further examine the effect of knowledge walks and textual cues and how they contribute to our approach’s data efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.305.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--305 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.305.software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.305.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.305" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.305/>Collective Relevance Labeling for Passage Retrieval</a></strong><br><a href=/people/j/jihyuk-kim/>Jihyuk Kim</a>
|
<a href=/people/m/minsoo-kim/>Minsoo Kim</a>
|
<a href=/people/s/seung-won-hwang/>Seung-won Hwang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--305><div class="card-body p-3 small">Deep learning for Information Retrieval (IR) requires a large amount of high-quality query-document relevance labels, but such labels are inherently sparse. Label smoothing redistributes some observed probability mass over unobserved instances, often uniformly, uninformed of the true distribution. In contrast, we propose knowledge distillation for informed labeling, without incurring high computation overheads at evaluation time. Our contribution is designing a simple but efficient teacher model which utilizes collective knowledge, to outperform state-of-the-arts distilled from a more complex teacher model. Specifically, we train up to <span class=tex-math>×8</span> faster than the state-of-the-art teacher, while distilling the rankings better. Our code is publicly available at <a href=https://github.com/jihyukkim-nlp/CollectiveKD class=acl-markup-url>https://github.com/jihyukkim-nlp/CollectiveKD</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.306.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--306 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.306.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.306.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.306" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.306/><span class=acl-fixed-case>COGMEN</span>: <span class=acl-fixed-case>CO</span>ntextualized <span class=acl-fixed-case>GNN</span> based Multimodal Emotion recognitio<span class=acl-fixed-case>N</span></a></strong><br><a href=/people/a/abhinav-joshi/>Abhinav Joshi</a>
|
<a href=/people/a/ashwani-bhat/>Ashwani Bhat</a>
|
<a href=/people/a/ayush-jain/>Ayush Jain</a>
|
<a href=/people/a/atin-singh/>Atin Singh</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--306><div class="card-body p-3 small">Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person’s emotions are influenced by the other speaker’s utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi- modal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-the- art (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.307.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--307 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.307.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.307" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.307/>Revisit Overconfidence for <span class=acl-fixed-case>OOD</span> Detection: Reassigned Contrastive Learning with Adaptive Class-dependent Threshold</a></strong><br><a href=/people/y/yanan-wu/>Yanan Wu</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/q/qixiang-gao/>QiXiang Gao</a>
|
<a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/f/fujia-zheng/>Fujia Zheng</a>
|
<a href=/people/l/lulu-zhao/>Lulu Zhao</a>
|
<a href=/people/h/huixing-jiang/>Huixing Jiang</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--307><div class="card-body p-3 small">Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task-oriented dialog system. A key challenge of OOD detection is the overconfidence of neural models. In this paper, we comprehensively analyze overconfidence and classify it into two perspectives: over-confident OOD and in-domain (IND). Then according to intrinsic reasons, we respectively propose a novel reassigned contrastive learning (RCL) to discriminate IND intents for over-confident OOD and an adaptive class-dependent local threshold mechanism to separate similar IND and OOD intents for over-confident IND. Experiments and analyses show the effectiveness of our proposed method for both aspects of overconfidence issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.308.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--308 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.308.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.308/><span class=acl-fixed-case>AISFG</span>: Abundant Information Slot Filling Generator</a></strong><br><a href=/people/y/yang-yan/>Yang Yan</a>
|
<a href=/people/j/junda-ye/>Junda Ye</a>
|
<a href=/people/z/zhongbao-zhang/>Zhongbao Zhang</a>
|
<a href=/people/l/liwen-wang/>Liwen Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--308><div class="card-body p-3 small">As an essential component of task-oriented dialogue systems, slot filling requires enormous labeled training data in a certain domain. However, in most cases, there is little or no target domain training data is available in the training stage. Thus, cross-domain slot filling has to cope with the data scarcity problem by zero/few-shot learning. Previous researches on zero/few-shot cross-domain slot filling focus on slot descriptions and examples while ignoring the slot type ambiguity and example ambiguity issues. To address these problems, we propose Abundant Information Slot Filling Generator (AISFG), a generative model with a novel query template that incorporates domain descriptions, slot descriptions, and examples with context. Experimental results show that our model outperforms state-of-the-art approaches in zero/few-shot slot filling task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.309.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--309 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.309.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.309/>Improving negation detection with negation-focused pre-training</a></strong><br><a href=/people/t/thinh-truong/>Thinh Truong</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--309><div class="card-body p-3 small">Negation is a common linguistic feature that is crucial in many language understanding tasks, yet it remains a hard problem due to diversity in its expression in different types of text. Recent works show that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains. We propose a new negation-focused pre-training strategy, involving targeted data augmentation and negation masking, to better incorporate negation information into language models. Extensive experiments on common benchmarks show that our proposed approach improves negation detection performance and generalizability over the strong baseline NegBERT (Khandelwal and Sawant, 2020).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.310.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--310 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.310.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.310.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.310" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.310/>Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers</a></strong><br><a href=/people/v/vivek-kumar/>Vivek Kumar</a>
|
<a href=/people/r/rishabh-maheshwary/>Rishabh Maheshwary</a>
|
<a href=/people/v/vikram-pudi/>Vikram Pudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--310><div class="card-body p-3 small">Existing Math Word Problem (MWP) solvers have achieved high accuracy on benchmark datasets. However, prior works have shown that such solvers do not generalize well and rely on superficial cues to achieve high performance. In this paper, we first conduct experiments to showcase that this behaviour is mainly associated with the limited size and diversity present in existing MWP datasets. Next, we propose several data augmentation techniques broadly categorized into Substitution and Paraphrasing based methods. By deploying these methods we increase the size of existing datasets by five folds. Extensive experiments on two benchmark datasets across three state-of-the-art MWP solvers shows that proposed methods increase the generalization and robustness of existing solvers. On average, proposed methods significantly increase the state-of-the-art results by over five percentage points on benchmark datasets. Further, the solvers trained on the augmented dataset performs comparatively better on the challenge test set. We also show the effectiveness of proposed techniques through ablation studies and verify the quality of augmented samples through human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.311.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--311 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.311.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.311" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.311/><span class=acl-fixed-case>D</span>iff<span class=acl-fixed-case>CSE</span>: Difference-based Contrastive Learning for Sentence Embeddings</a></strong><br><a href=/people/y/yung-sung-chuang/>Yung-Sung Chuang</a>
|
<a href=/people/r/rumen-dangovski/>Rumen Dangovski</a>
|
<a href=/people/h/hongyin-luo/>Hongyin Luo</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a>
|
<a href=/people/s/shiyu-chang/>Shiyu Chang</a>
|
<a href=/people/m/marin-soljacic/>Marin Soljacic</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/s/scott-yih/>Scott Yih</a>
|
<a href=/people/y/yoon-kim/>Yoon Kim</a>
|
<a href=/people/j/james-glass/>James Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--311><div class="card-body p-3 small">We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning, which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other “harmful” types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic textual similarity tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.312.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--312 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.312.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.312" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.312/>Generative Cross-Domain Data Augmentation for Aspect and Opinion Co-Extraction</a></strong><br><a href=/people/j/junjie-li/>Junjie Li</a>
|
<a href=/people/j/jianfei-yu/>Jianfei Yu</a>
|
<a href=/people/r/rui-xia/>Rui Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--312><div class="card-body p-3 small">As a fundamental task in opinion mining, aspect and opinion co-extraction aims to identify the aspect terms and opinion terms in reviews. However, due to the lack of fine-grained annotated resources, it is hard to train a robust model for many domains. To alleviate this issue, unsupervised domain adaptation is proposed to transfer knowledge from a labeled source domain to an unlabeled target domain. In this paper, we propose a new Generative Cross-Domain Data Augmentation framework for unsupervised domain adaptation. The proposed framework is aimed to generate target-domain data with fine-grained annotation by exploiting the labeled data in the source domain. Specifically, we remove the domain-specific segments in a source-domain labeled sentence, and then use this as input to a pre-trained sequence-to-sequence model BART to simultaneously generate a target-domain sentence and predict the corresponding label for each word. Experimental results on three datasets demonstrate that our approach is more effective than previous domain adaptation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.313.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--313 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.313.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.313" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.313/><span class=acl-fixed-case>P</span>ro<span class=acl-fixed-case>QA</span>: Structural Prompt-based Pre-training for Unified Question Answering</a></strong><br><a href=/people/w/wanjun-zhong/>Wanjun Zhong</a>
|
<a href=/people/y/yifan-gao/>Yifan Gao</a>
|
<a href=/people/n/ning-ding/>Ning Ding</a>
|
<a href=/people/y/yujia-qin/>Yujia Qin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/j/jiahai-wang/>Jiahai Wang</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--313><div class="card-body p-3 small">Question Answering (QA) is a longstanding challenge in natural language processing. Existing QA works mostly focus on specific question types, knowledge domains, or reasoning skills. The specialty in QA research hinders systems from modeling commonalities between tasks and generalization for wider applications. To address this issue, we present ProQA, a unified QA paradigm that solves various tasks through a single model. ProQA takes a unified structural prompt as the bridge and improves the QA-centric ability by structural prompt-based pre-training. Through a structurally designed prompt-based input schema, ProQA concurrently models the knowledge generalization for all QA tasks while keeping the knowledge customization for every specific QA task. Furthermore, ProQA is pre-trained with structural prompt-formatted large-scale synthesized corpus, which empowers the model with the commonly-required QA ability. Experimental results on 11 QA benchmarks demonstrate that ProQA consistently boosts performance on both full data fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore, ProQA exhibits strong ability in both continual learning and transfer learning by taking the advantages of the structural prompt.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.314.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--314 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.314.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.314.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.314/>A Data Cartography based <span class=acl-fixed-case>M</span>ix<span class=acl-fixed-case>U</span>p for Pre-trained Language Models</a></strong><br><a href=/people/s/seo-yeon-park/>Seo Yeon Park</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--314><div class="card-body p-3 small">MixUp is a data augmentation strategy where additional samples are generated during training by combining random pairs of training samples and their labels. However, selecting random pairs is not potentially an optimal choice. In this work, we propose TDMixUp, a novel MixUp strategy that leverages Training Dynamics and allows more informative samples to be combined for generating new data samples. Our proposed TDMixUp first measures confidence, variability, (Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al., 2020) to identify the characteristics of training samples (e.g., as easy-to-learn or ambiguous samples), and then interpolates these characterized samples. We empirically validate that our method not only achieves competitive performance using a smaller subset of the training data compared with strong baselines, but also yields lower expected calibration error on the pre-trained language model, BERT, on both in-domain and out-of-domain settings in a wide range of NLP tasks. We publicly release our code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.315.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--315 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.315.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.315.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.315/>Grapheme-to-Phoneme Conversion for <span class=acl-fixed-case>T</span>hai using Neural Regression Models</a></strong><br><a href=/people/t/tomohiro-yamasaki/>Tomohiro Yamasaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--315><div class="card-body p-3 small">We propose a novel Thai grapheme-to-phoneme conversion method based on a neural regression model that is trained using neural networks to predict the similarity between a candidate and the correct pronunciation. After generating a set of candidates for an input word or phrase using the orthography rules, this model selects the best-similarity pronunciation from the candidates. This method can be applied to languages other than Thai simply by preparing enough orthography rules, and can reduce the mistakes that neural network models often make. We show that the accuracy of the proposed method is .931, which is comparable to that of encoder-decoder sequence models. We also demonstrate that the proposed method is superior in terms of the difference between correct and predicted pronunciations because incorrect, strange output sometimes occurs when using encoder-decoder sequence models but the error is within the expected range when using the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.316.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--316 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.316.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.316" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.316/>Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation</a></strong><br><a href=/people/s/siyu-lai/>Siyu Lai</a>
|
<a href=/people/z/zhen-yang/>Zhen Yang</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/x/xue-zhang/>Xue Zhang</a>
|
<a href=/people/y/yufeng-chen/>Yufeng Chen</a>
|
<a href=/people/j/jinan-xu/>Jinan Xu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--316><div class="card-body p-3 small">Generating adversarial examples for Neural Machine Translation (NMT) with single Round-Trip Translation (RTT) has achieved promising results by releasing the meaning-preserving restriction. However, a potential pitfall for this approach is that we cannot decide whether the generated examples are adversarial to the target NMT model or the auxiliary backward one, as the reconstruction error through the RTT can be related to either. To remedy this problem, we propose a new definition for NMT adversarial examples based on the Doubly Round-Trip Translation (DRTT). Specifically, apart from the source-target-source RTT, we also consider the target-source-target one, which is utilized to pick out the authentic adversarial examples for the target NMT model. Additionally, to enhance the robustness of the NMT model, we introduce the masked language models to construct bilingual adversarial pairs based on DRTT, which are used to train the NMT model directly. Extensive experiments on both the clean and noisy test sets (including the artificial and natural noise) show that our approach substantially improves the robustness of NMT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.317.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--317 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.317.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.317" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.317/><span class=acl-fixed-case>TVS</span>how<span class=acl-fixed-case>G</span>uess: Character Comprehension in Stories as Speaker Guessing</a></strong><br><a href=/people/y/yisi-sang/>Yisi Sang</a>
|
<a href=/people/x/xiangyang-mou/>Xiangyang Mou</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/s/shunyu-yao/>Shunyu Yao</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/j/jeffrey-stanton/>Jeffrey Stanton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--317><div class="card-body p-3 small">We propose a new task for assessing machines’ skills of understanding fictional characters in narrative stories. The task, TVShowGuess, builds on the scripts of TV series and takes the form of guessing the anonymous main characters based on the backgrounds of the scenes and the dialogues. Our human study supports that this form of task covers comprehension of multiple types of character persona, including understanding characters’ personalities, facts and memories of personal experience, which are well aligned with the psychological and literary theories about the theory of mind (ToM) of human beings on understanding fictional characters during reading. We further propose new model architectures to support the contextualized encoding of long scene texts. Experiments show that our proposed approaches significantly outperform baselines, yet still largely lag behind the (nearly perfect) human performance. Our work serves as a first step toward the goal of narrative character comprehension.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.318.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--318 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.318.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.318" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.318/>Causal Distillation for Language Models</a></strong><br><a href=/people/z/zhengxuan-wu/>Zhengxuan Wu</a>
|
<a href=/people/a/atticus-geiger/>Atticus Geiger</a>
|
<a href=/people/j/joshua-rozner/>Joshua Rozner</a>
|
<a href=/people/e/elisa-kreiss/>Elisa Kreiss</a>
|
<a href=/people/h/hanson-lu/>Hanson Lu</a>
|
<a href=/people/t/thomas-icard/>Thomas Icard</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/n/noah-goodman/>Noah Goodman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--318><div class="card-body p-3 small">Distillation efforts have led to language models that are more compact and efficient without serious drops in performance. The standard approach to distillation trains a student model against two objectives: a task-specific objective (e.g., language modeling) and an imitation objective that encourages the hidden states of the student model to be similar to those of the larger teacher model. In this paper, we show that it is beneficial to augment distillation with a third objective that encourages the student to imitate the <i>causal</i> dynamics of the teacher through a distillation interchange intervention training objective (DIITO). DIITO pushes the student model to become a <i>causal abstraction</i> of the teacher model – a faithful model with simpler causal structure. DIITO is fully differentiable, easily implemented, and combines flexibly with other objectives. Compared against standard distillation with the same setting, DIITO results in lower perplexity on the WikiText-103M corpus (masked language modeling) and marked improvements on the GLUE benchmark (natural language understanding), SQuAD (question answering), and CoNLL-2003 (named entity recognition).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.319.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--319 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best efficient NLP paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.319.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.319" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.319/><span class=acl-fixed-case>FN</span>et: Mixing Tokens with <span class=acl-fixed-case>F</span>ourier Transforms</a></strong><br><a href=/people/j/james-lee-thorp/>James Lee-Thorp</a>
|
<a href=/people/j/joshua-ainslie/>Joshua Ainslie</a>
|
<a href=/people/i/ilya-eckstein/>Ilya Eckstein</a>
|
<a href=/people/s/santiago-ontanon/>Santiago Ontanon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--319><div class="card-body p-3 small">We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that “mix” input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the “efficient Transformers” on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.320.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--320 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.320.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.320" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.320/>Answer Consolidation: Formulation and Benchmarking</a></strong><br><a href=/people/w/wenxuan-zhou/>Wenxuan Zhou</a>
|
<a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/h/heba-elfardy/>Heba Elfardy</a>
|
<a href=/people/k/kevin-small/>Kevin Small</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--320><div class="card-body p-3 small">Current question answering (QA) systems primarily consider the single-answer scenario, where each question is assumed to be paired with one correct answer. However, in many real-world QA applications, multiple answer scenarios arise where consolidating answers into a comprehensive and non-redundant set of answers is a more efficient user interface. In this paper, we formulate the problem of answer consolidation, where answers are partitioned into multiple groups, each representing different aspects of the answer set. Then, given this partitioning, a comprehensive and non-redundant set of answers can be constructed by picking one answer from each group. To initiate research on answer consolidation, we construct a dataset consisting of 4,699 questions and 24,006 sentences and evaluate multiple models. Despite a promising performance achieved by the best-performing supervised models, we still believe this task has room for further improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.321.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--321 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.321.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.321/>Informativeness and Invariance: Two Perspectives on Spurious Correlations in Natural Language</a></strong><br><a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--321><div class="card-body p-3 small">Spurious correlations are a threat to the trustworthiness of natural language processing systems, motivating research into methods for identifying and eliminating them. However, addressing the problem of spurious correlations requires more clarity on what they are and how they arise in language data. Gardner et al (2021) argue that the compositional nature of language implies that <i>all</i> correlations between labels and individual “input features” are spurious. This paper analyzes this proposal in the context of a toy example, demonstrating three distinct conditions that can give rise to feature-label correlations in a simple PCFG. Linking the toy example to a structured causal model shows that (1) feature-label correlations can arise even when the label is invariant to interventions on the feature, and (2) feature-label correlations may be absent even when the label is sensitive to interventions on the feature. Because input features will be individually correlated with labels in all but very rare circumstances, domain knowledge must be applied to identify spurious correlations that pose genuine robustness threats.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.322.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--322 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.322.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.322" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.322/><span class=acl-fixed-case>FOAM</span>: A Follower-aware Speaker Model For Vision-and-Language Navigation</a></strong><br><a href=/people/z/zi-yi-dou/>Zi-Yi Dou</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--322><div class="card-body p-3 small">The speaker-follower models have proven to be effective in vision-and-language navigation, where a speaker model is used to synthesize new instructions to augment the training data for a follower navigation model. However, in previous work, the speaker model is follower-agnostic and fails to take the state of the follower into consideration. In this paper, we present FOAM, a FOllower-Aware speaker Model that is constantly updated given the follower feedback, so that the generated instructions can be more suitable to the current learning state of the follower. Specifically, we optimize the speaker using a bi-level optimization framework and obtain its training signals by evaluating the follower on labeled data. Experimental results on the Room-to-Room and Room-across-Room datasets demonstrate that our methods can outperform strong baseline models across settings. Analyses also reveal that our generated instructions are of higher quality than the baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.323.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--323 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.323.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.323" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.323/>Improving Compositional Generalization with Latent Structure and Data Augmentation</a></strong><br><a href=/people/l/linlu-qiu/>Linlu Qiu</a>
|
<a href=/people/p/peter-shaw/>Peter Shaw</a>
|
<a href=/people/p/panupong-pasupat/>Panupong Pasupat</a>
|
<a href=/people/p/pawel-nowak/>Pawel Nowak</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/f/fei-sha/>Fei Sha</a>
|
<a href=/people/k/kristina-toutanova/>Kristina Toutanova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--323><div class="card-body p-3 small">Generic unstructured neural networks have been shown to struggle on out-of-distribution compositional generalization. Compositional data augmentation via example recombination has transferred some prior knowledge about compositionality to such black-box neural models for several semantic parsing tasks, but this often required task-specific engineering or provided limited gains. We present a more powerful data recombination method using a model called Compositional Structure Learner (CSL). CSL is a generative model with a quasi-synchronous context-free grammar backbone, which we induce from the training data. We sample recombined examples from CSL and add them to the fine-tuning data of a pre-trained sequence-to-sequence model (T5). This procedure effectively transfers most of CSL’s compositional bias to T5 for diagnostic tasks, and results in a model even stronger than a T5-CSL ensemble on two real world compositional generalization tasks. This results in new state-of-the-art performance for these challenging semantic parsing tasks requiring generalization to both natural language variation and novel compositions of elements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.324.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--324 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.324.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.324/>Joint Extraction of Entities, Relations, and Events via Modeling Inter-Instance and Inter-Label Dependencies</a></strong><br><a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/thien-nguyen/>Thien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--324><div class="card-body p-3 small">Event trigger detection, entity mention recognition, event argument extraction, and relation extraction are the four important tasks in information extraction that have been performed jointly (Joint Information Extraction - JointIE) to avoid error propagation and leverage dependencies between the task instances (i.e., event triggers, entity mentions, relations, and event arguments). However, previous JointIE models often assume heuristic manually-designed dependency between the task instances and mean-field factorization for the joint distribution of instance labels, thus unable to capture optimal dependencies among instances and labels to improve representation learning and IE performance. To overcome these limitations, we propose to induce a dependency graph among task instances from data to boost representation learning. To better capture dependencies between instance labels, we propose to directly estimate their joint distribution via Conditional Random Fields. Noise Contrastive Estimation is introduced to address the maximization of the intractable joint likelihood for model training. Finally, to improve the decoding with greedy or beam search in prior work, we present Simulated Annealing to better find the globally optimal assignment for instance labels at decoding time. Experimental results show that our proposed model outperforms previous models on multiple IE tasks across 5 datasets and 2 languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.325.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--325 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.325.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.325" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.325/>Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling</a></strong><br><a href=/people/j/jakob-prange/>Jakob Prange</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/l/lingpeng-kong/>Lingpeng Kong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--325><div class="card-body p-3 small">We examine the extent to which, in principle, different syntactic and semantic graph representations can complement and improve neural language modeling. Specifically, by conditioning on a subgraph encapsulating the locally relevant sentence history, can a model make better next-word predictions than a pretrained sequential language model alone? With an ensemble setup consisting of GPT-2 and ground-truth graphs from one of 7 different formalisms, we find that the graph information indeed improves perplexity and other metrics. Moreover, this architecture provides a new way to compare different frameworks of linguistic representation. In our oracle graph setup, training and evaluating on English WSJ, semantic constituency structures prove most useful to language modeling performance—outpacing syntactic constituency structures as well as syntactic and semantic dependency structures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.326.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--326 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.326.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.326" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.326/>Imagination-Augmented Natural Language Understanding</a></strong><br><a href=/people/y/yujie-lu/>Yujie Lu</a>
|
<a href=/people/w/wanrong-zhu/>Wanrong Zhu</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/m/miguel-eckstein/>Miguel Eckstein</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--326><div class="card-body p-3 small">Human brains integrate linguistic and perceptual information simultaneously to understand natural language, and hold the critical ability to render imaginations. Such abilities enable us to construct new abstract concepts or concrete objects, and are essential in involving practical knowledge to solve problems in low-resource scenarios. However, most existing methods for Natural Language Understanding (NLU) are mainly focused on textual signals. They do not simulate human visual imagination ability, which hinders models from inferring and learning efficiently from limited data samples. Therefore, we introduce an Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language understanding tasks from a novel learning perspective—imagination-augmented cross-modal understanding. iACE enables visual imagination with external knowledge transferred from the powerful generative and pre-trained vision-and-language models. Extensive experiments on GLUE and SWAG show that iACE achieves consistent improvement over visually-supervised pre-trained models. More importantly, results in extreme and normal few-shot settings validate the effectiveness of iACE in low-resource natural language understanding circumstances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.327.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--327 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.327.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.327/>What company do words keep? Revisiting the distributional semantics of <span class=acl-fixed-case>J</span>.<span class=acl-fixed-case>R</span>. Firth & Zellig <span class=acl-fixed-case>H</span>arris</a></strong><br><a href=/people/m/mikael-brunila/>Mikael Brunila</a>
|
<a href=/people/j/jack-laviolette/>Jack LaViolette</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--327><div class="card-body p-3 small">The power of word embeddings is attributed to the linguistic theory that similar words will appear in similar contexts. This idea is specifically invoked by noting that “you shall know a word by the company it keeps,” a quote from British linguist J.R. Firth who, along with his American colleague Zellig Harris, is often credited with the invention of “distributional semantics.” While both Firth and Harris are cited in all major NLP textbooks and many foundational papers, the content and differences between their theories is seldom discussed. Engaging in a close reading of their work, we discover two distinct and in many ways divergent theories of meaning. One focuses exclusively on the internal workings of linguistic forms, while the other invites us to consider words in new company—not just with other linguistic elements, but also in a broader cultural and situational context. Contrasting these theories from the perspective of current debates in NLP, we discover in Firth a figure who could guide the field towards a more culturally grounded notion of semantics. We consider how an expanded notion of “context” might be modeled in practice through two different strategies: comparative stratification and syntagmatic extension.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.328.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--328 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.328.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.328" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.328/>Compositional Task-Oriented Parsing as Abstractive Question Answering</a></strong><br><a href=/people/w/wenting-zhao/>Wenting Zhao</a>
|
<a href=/people/k/konstantine-arkoudas/>Konstantine Arkoudas</a>
|
<a href=/people/w/weiqi-sun/>Weiqi Sun</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--328><div class="card-body p-3 small">Task-oriented parsing (TOP) aims to convert natural language into machine-readable representations of specific tasks, such as setting an alarm. A popular approach to TOP is to apply seq2seq models to generate linearized parse trees. A more recent line of work argues that pretrained seq2seq2 models are better at generating outputs that are themselves natural language, so they replace linearized parse trees with canonical natural-language paraphrases that can then be easily translated into parse trees, resulting in so-called naturalized parsers. In this work we continue to explore naturalized semantic parsing by presenting a general reduction of TOP to abstractive question answering that overcomes some limitations of canonical paraphrasing. Experimental results show that our QA-based technique outperforms state-of-the-art methods in full-data settings while achieving dramatic improvements in few-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.329.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--329 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.329.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.329" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.329/>Learning Cross-Lingual <span class=acl-fixed-case>IR</span> from an <span class=acl-fixed-case>E</span>nglish Retriever</a></strong><br><a href=/people/y/yulong-li/>Yulong Li</a>
|
<a href=/people/m/martin-franz/>Martin Franz</a>
|
<a href=/people/m/md-arafat-sultan/>Md Arafat Sultan</a>
|
<a href=/people/b/bhavani-iyer/>Bhavani Iyer</a>
|
<a href=/people/y/young-suk-lee/>Young-Suk Lee</a>
|
<a href=/people/a/avirup-sil/>Avirup Sil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--329><div class="card-body p-3 small">We present DR.DECR (Dense Retrieval with Distillation-Enhanced Cross-Lingual Representation), a new cross-lingual information retrieval (CLIR) system trained using multi-stage knowledge distillation (KD). The teacher of DR.DECR relies on a highly effective but computationally expensive two-stage inference process consisting of query translation and monolingual IR, while the student, DR.DECR, executes a single CLIR step. We teach DR.DECR powerful multilingual representations as well as CLIR by optimizing two corresponding KD objectives. Learning useful representations of non-English text from an English-only retriever is accomplished through a cross-lingual token alignment algorithm that relies on the representation capabilities of the underlying multilingual encoders. In both in-domain and zero-shot out-of-domain evaluation, DR.DECR demonstrates far superior accuracy over direct fine-tuning with labeled CLIR data. It is also the best single-model retriever on the XOR-TyDi benchmark at the time of this writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.330.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--330 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.330.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.330" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.330/>Testing the Ability of Language Models to Interpret Figurative Language</a></strong><br><a href=/people/e/emmy-liu/>Emmy Liu</a>
|
<a href=/people/c/chenxuan-cui/>Chenxuan Cui</a>
|
<a href=/people/k/kenneth-zheng/>Kenneth Zheng</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--330><div class="card-body p-3 small">Figurative and metaphorical language are commonplace in discourse, and figurative expressions play an important role in communication and cognition. However, figurative language has been a relatively under-studied area in NLP, and it remains an open question to what extent modern language models can interpret nonliteral phrases. To address this question, we introduce Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings. We evaluate the performance of several state-of-the-art language models on this task, and find that although language models achieve performance significantly over chance, they still fall short of human performance, particularly in zero- or few-shot settings. This suggests that further work is needed to improve the nonliteral reasoning capabilities of language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.331.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--331 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.331.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.331" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.331/>Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity</a></strong><br><a href=/people/s/sheshera-mysore/>Sheshera Mysore</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/t/tom-hope/>Tom Hope</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--331><div class="card-body p-3 small">We present a new scientific document similarity model based on matching fine-grained aspects of texts. To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations). Such co-citations not only reflect close paper relatedness, but also provide textual descriptions of how the co-cited papers are related. This novel form of textual supervision is used for learning to match aspects across papers. We develop multi-vector representations where vectors correspond to sentence-level aspects of documents, and present two methods for aspect matching: (1) A fast method that only matches single aspects, and (2) a method that makes sparse multiple matches with an Optimal Transport mechanism that computes an Earth Mover’s Distance between aspects. Our approach improves performance on document similarity tasks in four datasets. Further, our fast single-match method achieves competitive results, paving the way for applying fine-grained similarity to large scientific corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.332.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--332 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-main.332/><span class=acl-fixed-case>CHAI</span>: A <span class=acl-fixed-case>CH</span>atbot <span class=acl-fixed-case>AI</span> for Task-Oriented Dialogue with Offline Reinforcement Learning</a></strong><br><a href=/people/s/siddharth-verma/>Siddharth Verma</a>
|
<a href=/people/j/justin-fu/>Justin Fu</a>
|
<a href=/people/s/sherry-yang/>Sherry Yang</a>
|
<a href=/people/s/sergey-levine/>Sergey Levine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--332><div class="card-body p-3 small">Conventionally, generation of natural language for dialogue agents may be viewed as a statistical learning problem: determine the patterns in human-provided data and generate appropriate responses with similar statistical properties. However, dialogue can also be regarded as a goal directed process, where speakers attempt to accomplish a specific task. Reinforcement learning (RL) algorithms are designed specifically for solving such goal-directed problems, but the most direct way to apply RL, through trial-and-error learning in human conversations, is costly. In this paper, we study how offline reinforcement learning can instead be used to train dialogue agents entirely using static datasets collected from human speakers. Our experiments show that recently developed offline RL methods can be combined with language models to yield realistic dialogue agents that better accomplish task goals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.333.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--333 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.333.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.333" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.333/>Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer</a></strong><br><a href=/people/y/yanpeng-zhao/>Yanpeng Zhao</a>
|
<a href=/people/j/jack-hessel/>Jack Hessel</a>
|
<a href=/people/y/youngjae-yu/>Youngjae Yu</a>
|
<a href=/people/x/ximing-lu/>Ximing Lu</a>
|
<a href=/people/r/rowan-zellers/>Rowan Zellers</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--333><div class="card-body p-3 small">Machines that can represent and describe environmental soundscapes have practical potential, e.g., for audio tagging and captioning. Prevailing learning paradigms of audio-text connections have been relying on parallel audio-text data, which is, however, scarcely available on the web. We propose VIP-ANT that induces Audio-Text alignment without using any parallel audio-text data. Our key idea is to share the image modality between bi-modal image-text representations and bi-modal image-audio representations; the image modality functions as a pivot and connects audio and text in a tri-modal embedding space implicitly. In a difficult zero-shot setting with no paired audio-text data, our model demonstrates state-of-the-art zero-shot performance on the ESC50 and US8K audio classification tasks, and even surpasses the supervised state of the art for Clotho caption retrieval (with audio queries) by 2.2% R@1. We further investigate cases of minimal audio-text supervision, finding that, e.g., just a few hundred supervised audio-text pairs increase the zero-shot audio classification accuracy by 8% on US8K. However, to match human parity on some zero-shot tasks, our empirical scaling experiments suggest that we would need about <span class=tex-math>2<sup>21</sup> ≈ 2<span class=font-weight-normal>M</span></span> supervised audio-caption pairs. Our work opens up new avenues for learning audio-text connections with little to no parallel audio-text data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.334.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.334.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--334 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.334 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.334.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.334/><span class=acl-fixed-case>SURF</span>: Semantic-level Unsupervised Reward Function for Machine Translation</a></strong><br><a href=/people/a/atijit-anuchitanukul/>Atijit Anuchitanukul</a>
|
<a href=/people/j/julia-ive/>Julia Ive</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--334><div class="card-body p-3 small">The performance of Reinforcement Learning (RL) for natural language tasks including Machine Translation (MT) is crucially dependent on the reward formulation. This is due to the intrinsic difficulty of the task in the high-dimensional discrete action space as well as the sparseness of the standard reward functions defined for limited set of ground-truth sequences biased towards singular lexical choices. To address this issue, we formulate SURF, a maximally dense semantic-level unsupervised reward function which mimics human evaluation by considering both sentence fluency and semantic similarity. We demonstrate the strong potential of SURF to leverage a family of Actor-Critic Transformer-based Architectures with synchronous and asynchronous multi-agent variants. To tackle the problem of large action-state spaces, each agent is equipped with unique exploration strategies, promoting diversity during its exploration of the hypothesis space. When BLEU scores are compared, our dense unsupervised reward outperforms the standard sparse reward by 2% on average for in- and out-of-domain settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.335.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.335.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.335.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.335" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.335/>Disentangling Categorization in Multi-agent Emergent Communication</a></strong><br><a href=/people/w/washington-garcia/>Washington Garcia</a>
|
<a href=/people/h/hamilton-clouse/>Hamilton Clouse</a>
|
<a href=/people/k/kevin-butler/>Kevin Butler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--335><div class="card-body p-3 small">The emergence of language between artificial agents is a recent focus of computational linguistics, as it offers a synthetic substrate for reasoning about human language evolution. From the perspective of cognitive science, sophisticated categorization in humans is thought to enable reasoning about novel observations, and thus compose old information to describe new phenomena. Unfortunately, the literature to date has not managed to isolate the effect of categorization power in artificial agents on their inter-communication ability, particularly on novel, unseen objects. In this work, we propose the use of disentangled representations from representation learning to quantify the categorization power of agents, enabling a differential analysis between combinations of heterogeneous systems, e.g., pairs of agents which learn to communicate despite mismatched concept realization. Through this approach, we observe that agent heterogeneity can cut signaling accuracy by up to 40%, despite encouraging compositionality in the artificial language. We conclude that the reasoning process of agents plays a key role in their communication, with unexpected benefits arising from their mixing, such as better language compositionality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.336.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--336 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.336.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.336/>Show, Don’t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue</a></strong><br><a href=/people/r/raghav-gupta/>Raghav Gupta</a>
|
<a href=/people/h/harrison-lee/>Harrison Lee</a>
|
<a href=/people/j/jeffrey-zhao/>Jeffrey Zhao</a>
|
<a href=/people/y/yuan-cao/>Yuan Cao</a>
|
<a href=/people/a/abhinav-rastogi/>Abhinav Rastogi</a>
|
<a href=/people/y/yonghui-wu/>Yonghui Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--336><div class="card-body p-3 small">Building universal dialogue systems that operate across multiple domains/APIs and generalize to new ones with minimal overhead is a critical challenge. Recent works have leveraged natural language descriptions of schema elements to enable such systems; however, descriptions only indirectly convey schema semantics. In this work, we propose Show, Don’t Tell, which prompts seq2seq models with a labeled example dialogue to show the semantics of schema elements rather than tell the model through descriptions. While requiring similar effort from service developers as generating descriptions, we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks designed to measure zero-shot generalization - the Schema-Guided Dialogue dataset and the MultiWOZ leave-one-out benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.337.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.337.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--337 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.337 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.337.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.337.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.337/>Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge</a></strong><br><a href=/people/i/ian-porada/>Ian Porada</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--337><div class="card-body p-3 small">Transformer models pre-trained with a masked-language-modeling objective (e.g., BERT) encode commonsense knowledge as evidenced by behavioral probes; however, the extent to which this knowledge is acquired by systematic inference over the semantics of the pre-training corpora is an open question. To answer this question, we selectively inject verbalized knowledge into the pre-training minibatches of BERT and evaluate how well the model generalizes to supported inferences after pre-training on the injected knowledge. We find generalization does not improve over the course of pre-training BERT from scratch, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.338.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.338.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--338 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.338 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.338.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.338/>Using Paraphrases to Study Properties of Contextual Embeddings</a></strong><br><a href=/people/l/laura-burdick/>Laura Burdick</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--338><div class="card-body p-3 small">We use paraphrases as a unique source of data to analyze contextualized embeddings, with a particular focus on BERT. Because paraphrases naturally encode consistent word and phrase semantics, they provide a unique lens for investigating properties of embeddings. Using the Paraphrase Database’s alignments, we study words within paraphrases as well as phrase representations. We find that contextual embeddings effectively handle polysemous words, but give synonyms surprisingly different representations in many cases. We confirm previous findings that BERT is sensitive to word order, but find slightly different patterns than prior work in terms of the level of contextualization across BERT’s layers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.339.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--339 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.339.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.339/>Measure and Improve Robustness in <span class=acl-fixed-case>NLP</span> Models: A Survey</a></strong><br><a href=/people/x/xuezhi-wang/>Xuezhi Wang</a>
|
<a href=/people/h/haohan-wang/>Haohan Wang</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--339><div class="card-body p-3 small">As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research. In this paper, we aim to provide a unifying survey of how to define, measure and improve robustness in NLP. We first connect multiple definitions of robustness, then unify various lines of work on identifying robustness failures and evaluating models’ robustness. Correspondingly, we present mitigation strategies that are data-driven, model-driven, and inductive-prior-based, with a more systematic view of how to effectively improve robustness in NLP models. Finally, we conclude by outlining open challenges and future directions to motivate further research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.340.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--340 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.340.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.340/>Learning to Generate Examples for Semantic Processing Tasks</a></strong><br><a href=/people/d/danilo-croce/>Danilo Croce</a>
|
<a href=/people/s/simone-filice/>Simone Filice</a>
|
<a href=/people/g/giuseppe-castellucci/>Giuseppe Castellucci</a>
|
<a href=/people/r/roberto-basili/>Roberto Basili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--340><div class="card-body p-3 small">Even if recent Transformer-based architectures, such as BERT, achieved impressive results in semantic processing tasks, their fine-tuning stage still requires large scale training resources. Usually, Data Augmentation (DA) techniques can help to deal with low resource settings. In Text Classification tasks, the objective of DA is the generation of well-formed sentences that i) represent the desired task category and ii) are novel with respect to existing sentences. In this paper, we propose a neural approach to automatically learn to generate new examples using a pre-trained sequence-to-sequence model. We first learn a task-oriented similarity function that we use to pair similar examples. Then, we use these example pairs to train a model to generate examples. Experiments in low resource settings show that augmenting the training material with the proposed strategy systematically improves the results on text classification and natural language inference tasks by up to 10% accuracy, outperforming existing DA approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.341.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--341 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.341.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.341" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.341/>Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</a></strong><br><a href=/people/p/peter-west/>Peter West</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/j/jack-hessel/>Jack Hessel</a>
|
<a href=/people/j/jena-hwang/>Jena Hwang</a>
|
<a href=/people/l/liwei-jiang/>Liwei Jiang</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/x/ximing-lu/>Ximing Lu</a>
|
<a href=/people/s/sean-welleck/>Sean Welleck</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--341><div class="card-body p-3 small">The common practice for training commonsense models has gone from–human–to–corpus–to–machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from–machine–to–corpus–to–machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically–as text–in addition to the neural model. We distill only one aspect–the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model’s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.342.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--342 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.342.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.342" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.342/><span class=acl-fixed-case>G</span>en<span class=acl-fixed-case>IE</span>: Generative Information Extraction</a></strong><br><a href=/people/m/martin-josifoski/>Martin Josifoski</a>
|
<a href=/people/n/nicola-de-cao/>Nicola De Cao</a>
|
<a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/f/fabio-petroni/>Fabio Petroni</a>
|
<a href=/people/r/robert-west/>Robert West</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--342><div class="card-body p-3 small">Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.343.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--343 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.343.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.343" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.343/>Entity Linking via Explicit Mention-Mention Coreference Modeling</a></strong><br><a href=/people/d/dhruv-agarwal/>Dhruv Agarwal</a>
|
<a href=/people/r/rico-angell/>Rico Angell</a>
|
<a href=/people/n/nicholas-monath/>Nicholas Monath</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--343><div class="card-body p-3 small">Learning representations of entity mentions is a core component of modern entity linking systems for both candidate generation and making linking predictions. In this paper, we present and empirically analyze a novel training approach for learning mention and entity representations that is based on building minimum spanning arborescences (i.e., directed spanning trees) over mentions and entities across documents to explicitly model mention coreference relationships. We demonstrate the efficacy of our approach by showing significant improvements in both candidate generation recall and linking accuracy on the Zero-Shot Entity Linking dataset and MedMentions, the largest publicly available biomedical dataset. In addition, we show that our improvements in candidate generation yield higher quality re-ranking models downstream, setting a new SOTA result in linking accuracy on MedMentions. Finally, we demonstrate that our improved mention representations are also effective for the discovery of new entities via cross-document coreference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.344.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--344 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.344.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.344" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.344/>Massive-scale Decoding for Text Generation using Lattices</a></strong><br><a href=/people/j/jiacheng-xu/>Jiacheng Xu</a>
|
<a href=/people/s/siddhartha-jonnalagadda/>Siddhartha Jonnalagadda</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--344><div class="card-body p-3 small">Conditional neural text generation models generate high-quality outputs, but often concentrate around a mode when what we really want is a diverse set of options. We present a search algorithm to construct lattices encoding a massive number of generation options. First, we restructure decoding as a best-first search, which explores the space differently than beam search and improves efficiency by avoiding pruning paths. Second, we revisit the idea of hypothesis recombination: we can identify pairs of similar generation candidates during search and merge them as an approximation. On both summarization and machine translation, we show that our algorithm encodes thousands of diverse options that remain grammatical and high-quality into one lattice. This algorithm provides a foundation for building downstream generation applications on top of massive-scale diverse outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.345.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.345.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--345 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.345 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.345.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.345" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.345/>Disentangling Indirect Answers to Yes-No Questions in Real Conversations</a></strong><br><a href=/people/k/krishna-sanagavarapu/>Krishna Sanagavarapu</a>
|
<a href=/people/j/jathin-singaraju/>Jathin Singaraju</a>
|
<a href=/people/a/anusha-kakileti/>Anusha Kakileti</a>
|
<a href=/people/a/anirudh-kaza/>Anirudh Kaza</a>
|
<a href=/people/a/aaron-mathews/>Aaron Mathews</a>
|
<a href=/people/h/helen-li/>Helen Li</a>
|
<a href=/people/n/nathan-brito/>Nathan Brito</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--345><div class="card-body p-3 small">In this paper, we explore the task of determining indirect answers to yes-no questions in real conversations. We work with transcripts of phone conversations in the Switchboard Dialog Act (SwDA) corpus and create SwDA-IndirectAnswers (SwDA-IA), a subset of SwDA consisting of all conversations containing a yes-no question with an indirect answer. We annotate the underlying direct answers to the yes-no questions (yes, probably yes, middle, probably no, or no). We show that doing so requires taking into account conversation context: the indirect answer alone is insufficient to determine the ground truth. Experimental results also show that taking into account context is beneficial. More importantly, our results demonstrate that existing corpora with synthetic indirect answers to yes-no questions are not beneficial when working with real conversations. Our best models outperform the majority baseline by a substantial margin, but the task remains a challenge (F1: 0.46).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.346.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.346.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--346 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.346 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.346.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.346" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.346/>Quantifying Adaptability in Pre-trained Language Models with 500 Tasks</a></strong><br><a href=/people/b/belinda-z-li/>Belinda Li</a>
|
<a href=/people/j/jane-yu/>Jane Yu</a>
|
<a href=/people/m/madian-khabsa/>Madian Khabsa</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/a/alon-halevy/>Alon Halevy</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--346><div class="card-body p-3 small">When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.347.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--347 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.347.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.347.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.347/>Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection</a></strong><br><a href=/people/i/indira-sen/>Indira Sen</a>
|
<a href=/people/m/mattia-samory/>Mattia Samory</a>
|
<a href=/people/c/claudia-wagner/>Claudia Wagner</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--347><div class="card-body p-3 small">Counterfactually Augmented Data (CAD) aims to improve out-of-domain generalizability, an indicator of model robustness. The improvement is credited to promoting core features of the construct over spurious artifacts that happen to correlate with it. Yet, over-relying on core features may lead to unintended model bias. Especially, construct-driven CAD—perturbations of core features—may induce models to ignore the context in which core features are used. Here, we test models for sexism and hate speech detection on challenging data: non-hate and non-sexist usage of identity and gendered terms. On these hard cases, models trained on CAD, especially construct-driven CAD, show higher false positive rates than models trained on the original, unperturbed data. Using a diverse set of CAD—construct-driven and construct-agnostic—reduces such unintended bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.348.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.348.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--348 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.348 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.348.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.348" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.348/>A Study of the Attention Abnormality in Trojaned <span class=acl-fixed-case>BERT</span>s</a></strong><br><a href=/people/w/weimin-lyu/>Weimin Lyu</a>
|
<a href=/people/s/songzhu-zheng/>Songzhu Zheng</a>
|
<a href=/people/t/tengfei-ma/>Tengfei Ma</a>
|
<a href=/people/c/chao-chen/>Chao Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--348><div class="card-body p-3 small">Trojan attacks raise serious security concerns. In this paper, we investigate the underlying mechanism of Trojaned BERT models. We observe the attention focus drifting behavior of Trojaned models, i.e., when encountering an poisoned input, the trigger token hijacks the attention focus regardless of the context. We provide a thorough qualitative and quantitative analysis of this phenomenon, revealing insights into the Trojan mechanism. Based on the observation, we propose an attention-based Trojan detector to distinguish Trojaned models from clean ones. To the best of our knowledge, we are the first to analyze the Trojan mechanism and develop a Trojan detector based on the transformer’s attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.349.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--349 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.349.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.349.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.349" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.349/><span class=acl-fixed-case>EP</span>i<span class=acl-fixed-case>DA</span>: An Easy Plug-in Data Augmentation Framework for High Performance Text Classification</a></strong><br><a href=/people/m/minyi-zhao/>Minyi Zhao</a>
|
<a href=/people/l/lu-zhang/>Lu Zhang</a>
|
<a href=/people/y/yi-xu/>Yi Xu</a>
|
<a href=/people/j/jiandong-ding/>Jiandong Ding</a>
|
<a href=/people/j/jihong-guan/>Jihong Guan</a>
|
<a href=/people/s/shuigeng-zhou/>Shuigeng Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--349><div class="card-body p-3 small">Recent works have empirically shown the effectiveness of data augmentation (DA) in NLP tasks, especially for those suffering from data scarcity. Intuitively, given the size of generated data, their diversity and quality are crucial to the performance of targeted tasks. However, to the best of our knowledge, most existing methods consider only either the diversity or the quality of augmented data, thus cannot fully mine the potential of DA for NLP. In this paper, we present an easy and plug-in data augmentation framework EPiDA to support effective text classification. EPiDA employs two mechanisms: relative entropy maximization (REM) and conditional entropy minimization (CEM) to control data generation, where REM is designed to enhance the diversity of augmented data while CEM is exploited to ensure their semantic consistency. EPiDA can support efficient and continuous data generation for effective classifier training. Extensive experiments show that EPiDA outperforms existing SOTA methods in most cases, though not using any agent networks or pre-trained generation networks, and it works well with various DA algorithms and classification models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.350.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.350.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--350 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.350 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.350.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.350" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.350/>Partial-input baselines show that <span class=acl-fixed-case>NLI</span> models can ignore context, but they don’t.</a></strong><br><a href=/people/n/neha-srikanth/>Neha Srikanth</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--350><div class="card-body p-3 small">When strong partial-input baselines reveal artifacts in crowdsourced NLI datasets, the performance of full-input models trained on such datasets is often dismissed as reliance on spurious correlations. We investigate whether state-of-the-art NLI models are capable of overriding default inferences made by a partial-input baseline. We introduce an evaluation set of 600 examples consisting of perturbed premises to examine a RoBERTa model’s sensitivity to edited contexts. Our results indicate that NLI models are still capable of learning to condition on context—a necessary component of inferential reasoning—despite being trained on artifact-ridden datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.351.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.351.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--351 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.351 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.351.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.351/>Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora</a></strong><br><a href=/people/x/xisen-jin/>Xisen Jin</a>
|
<a href=/people/d/dejiao-zhang/>Dejiao Zhang</a>
|
<a href=/people/h/henghui-zhu/>Henghui Zhu</a>
|
<a href=/people/w/wei-xiao/>Wei Xiao</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/x/xiaokai-wei/>Xiaokai Wei</a>
|
<a href=/people/a/andrew-arnold/>Andrew Arnold</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--351><div class="card-body p-3 small">Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data distributions that deviates from what the PTLM was initially trained on. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and a chronologically-ordered tweet stream, we incrementally pretrain a PTLM with different continual learning algorithms, and keep track of the downstream task performance (after fine-tuning). We evaluate PTLM’s ability to adapt to new corpora while retaining learned knowledge in earlier corpora. Our experiments show distillation-based approaches to be most effective in retaining downstream performance in earlier domains. The algorithms also improve knowledge transfer, allowing models to achieve better downstream performance over latest data, and improve temporal generalization when distribution gaps exist between training and evaluation because of time. We believe our problem formulation, methods, and analysis will inspire future studies towards continual pretraining of language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.352.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--352 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.352.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.352" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.352/>Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition</a></strong><br><a href=/people/p/pengshan-cai/>Pengshan Cai</a>
|
<a href=/people/h/hui-wan/>Hui Wan</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a>
|
<a href=/people/s/sachindra-joshi/>Sachindra Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--352><div class="card-body p-3 small">We propose novel AI-empowered chat bots for learning as conversation where a user does not read a passage but gains information and knowledge through conversation with a teacher bot. Our information acquisition-oriented dialogue system employs a novel adaptation of reinforced self-play so that the system can be transferred to various domains without in-domain dialogue data, and can carry out conversations both informative and attentive to users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.353.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.353.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--353 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.353 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.353.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.353" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.353/>Dynamic Programming in Rank Space: Scaling Structured Inference with Low-Rank <span class=acl-fixed-case>HMM</span>s and <span class=acl-fixed-case>PCFG</span>s</a></strong><br><a href=/people/s/songlin-yang/>Songlin Yang</a>
|
<a href=/people/w/wei-liu/>Wei Liu</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--353><div class="card-body p-3 small">Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) are widely used structured models, both of which can be represented as factor graph grammars (FGGs), a powerful formalism capable of describing a wide range of models. Recent research found it beneficial to use large state spaces for HMMs and PCFGs. However, inference with large state spaces is computationally demanding, especially for PCFGs. To tackle this challenge, we leverage tensor rank decomposition (aka. CPD) to decrease inference computational complexities for a subset of FGGs subsuming HMMs and PCFGs. We apply CPD on the factors of an FGG and then construct a new FGG defined in the rank space. Inference with the new FGG produces the same result but has a lower time complexity when the rank size is smaller than the state size. We conduct experiments on HMM language modeling and unsupervised PCFG parsing, showing better performance than previous work. Our code is publicly available at <a href=https://github.com/VPeterV/RankSpace-Models class=acl-markup-url>https://github.com/VPeterV/RankSpace-Models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.354.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.354.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--354 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.354 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.354.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.354" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.354/>What Factors Should Paper-Reviewer Assignments Rely On? Community Perspectives on Issues and Ideals in Conference Peer-Review</a></strong><br><a href=/people/t/terne-thorn-jakobsen/>Terne Thorn Jakobsen</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--354><div class="card-body p-3 small">Both scientific progress and individual researcher careers depend on the quality of peer review, which in turn depends on paper-reviewer matching. Surprisingly, this problem has been mostly approached as an automated recommendation problem rather than as a matter where different stakeholders (area chairs, reviewers, authors) have accumulated experience worth taking into account. We present the results of the first survey of the NLP community, identifying common issues and perspectives on what factors should be considered by paper-reviewer matching systems. This study contributes actionable recommendations for improving future NLP conferences, and desiderata for interpretable peer review assignments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.355.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--355 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.355.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.355.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.355/>Reducing Disambiguation Biases in <span class=acl-fixed-case>NMT</span> by Leveraging Explicit Word Sense Information</a></strong><br><a href=/people/n/niccolo-campolungo/>Niccolò Campolungo</a>
|
<a href=/people/t/tommaso-pasini/>Tommaso Pasini</a>
|
<a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--355><div class="card-body p-3 small">Recent studies have shed some light on a common pitfall of Neural Machine Translation (NMT) models, stemming from their struggle to disambiguate polysemous words without lapsing into their most frequently occurring senses in the training corpus. In this paper, we first provide a novel approach for automatically creating high-precision sense-annotated parallel corpora, and then put forward a specifically tailored fine-tuning strategy for exploiting these sense annotations during training without introducing any additional requirement at inference time. The use of explicit senses proved to be beneficial to reduce the disambiguation bias of a baseline NMT model, while, at the same time, leading our system to attain higher BLEU scores than its vanilla counterpart in 3 language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.356.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.356.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.356/>Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting</a></strong><br><a href=/people/s/shuzheng-si/>Shuzheng Si</a>
|
<a href=/people/s/shuang-zeng/>Shuang Zeng</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--356><div class="card-body p-3 small">Incomplete utterance rewriting has recently raised wide attention. However, previous works do not consider the semantic structural information between incomplete utterance and rewritten utterance or model the semantic structure implicitly and insufficiently. To address this problem, we propose a QUEry-Enhanced Network(QUEEN) to solve this problem. Firstly, our proposed query template explicitly brings guided semantic structural knowledge between the incomplete utterance and the rewritten utterance making model perceive where to refer back to or recover omitted tokens. Then, we adopt a fast and effective edit operation scoring network to model the relation between two tokens. Benefiting from extra information and the well-designed network, QUEEN achieves state-of-the-art performance on several public datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.357.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--357 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.357.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.357" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.357/>Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization</a></strong><br><a href=/people/l/lulu-zhao/>Lulu Zhao</a>
|
<a href=/people/f/fujia-zheng/>Fujia Zheng</a>
|
<a href=/people/w/weihao-zeng/>Weihao Zeng</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a>
|
<a href=/people/h/huixing-jiang/>Huixing Jiang</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/y/yanan-wu/>Yanan Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--357><div class="card-body p-3 small">The most advanced abstractive dialogue summarizers lack generalization ability on new domains and the existing researches for domain adaptation in summarization generally rely on large-scale pre-trainings. To explore the lightweight fine-tuning methods for domain adaptation of dialogue summarization, in this paper, we propose an efficient and generalizable Domain-Oriented Prefix-tuning model, which utilizes a domain word initialized prefix module to alleviate domain entanglement and adopts discrete prompts to guide the model to focus on key contents of dialogues and enhance model generalization. We conduct zero-shot experiments and build domain adaptation benchmarks on two multi-domain dialogue summarization datasets, TODSum and QMSum. Adequate experiments and qualitative analysis prove the effectiveness of our methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.358.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--358 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.358.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.358.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.358" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.358/>Interactive Symbol Grounding with Complex Referential Expressions</a></strong><br><a href=/people/r/rimvydas-rubavicius/>Rimvydas Rubavicius</a>
|
<a href=/people/a/alex-lascarides/>Alex Lascarides</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--358><div class="card-body p-3 small">We present a procedure for learning to ground symbols from a sequence of stimuli consisting of an arbitrarily complex noun phrase (e.g. “all but one green square above both red circles.”) and its designation in the visual scene. Our distinctive approach combines: a) lazy few-shot learning to relate open-class words like green and above to their visual percepts; and b) symbolic reasoning with closed-class word categories like quantifiers and negation. We use this combination to estimate new training examples for grounding symbols that occur <i>within</i> a noun phrase but aren’t designated by that noun phase (e.g, red in the above example), thereby potentially gaining data efficiency. We evaluate the approach in a visual reference resolution task, in which the learner starts out unaware of concepts that are part of the domain model and how they relate to visual percepts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.359.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.359.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--359 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.359 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-main.359/>Generalized Quantifiers as a Source of Error in Multilingual <span class=acl-fixed-case>NLU</span> Benchmarks</a></strong><br><a href=/people/r/ruixiang-cui/>Ruixiang Cui</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--359><div class="card-body p-3 small">Logical approaches to representing language have developed and evaluated computational models of quantifier words since the 19th century, but today’s NLU models still struggle to capture their semantics. We rely on Generalized Quantifier Theory for language-independent representations of the semantics of quantifier words, to quantify their contribution to the errors of NLU models. We find that quantifiers are pervasive in NLU benchmarks, and their occurrence at test time is associated with performance drops. Multilingual models also exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse for non-English languages. To facilitate directly-targeted probing, we present an adversarial generalized quantifier NLI task (GQNLI) and show that pre-trained language models have a clear lack of robustness in generalized quantifier reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.360.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--360 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.360" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-main.360/>Exact Paired-Permutation Testing for Structured Test Statistics</a></strong><br><a href=/people/r/ran-zmigrod/>Ran Zmigrod</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--360><div class="card-body p-3 small">Significance testing—especially the paired-permutation test—has played a vital role in developing NLP systems to provide confidence that the difference in performance between two systems (i.e., the test statistic) is not due to luck. However, practitioners rely on Monte Carlo approximation to perform this test due to a lack of a suitable exact algorithm. In this paper, we provide an efficient exact algorithm for the paired-permutation test for a family of structured test statistics. Our algorithm runs in <span class=tex-math>𝒪(G N (<span class=tex-math-function>log</span> GN )(<span class=tex-math-function>log</span> N))</span> time where <span class=tex-math>N</span> is the dataset size and <span class=tex-math>G</span> is the range of the test statistic. We found that our exact algorithm was 10x faster than the Monte Carlo approximation with 20000 samples on a common dataset</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.361.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.361.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--361 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.361 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Honorable mention for contribution to methods"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.361.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.361" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.361/>A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank</a></strong><br><a href=/people/d/dan-malkin/>Dan Malkin</a>
|
<a href=/people/t/tomasz-limisiewicz/>Tomasz Limisiewicz</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--361><div class="card-body p-3 small">We show that the choice of pretraining languages affects downstream cross-lingual transfer for BERT-based models. We inspect zero-shot performance in balanced data conditions to mitigate data size confounds, classifying pretraining languages that improve downstream performance as donors, and languages that are improved in zero-shot performance as recipients. We develop a method of quadratic time complexity in the number of languages to estimate these relations, instead of an exponential exhaustive computation of all possible combinations. We find that our method is effective on a diverse set of languages spanning different linguistic features and two downstream tasks. Our findings can inform developers of large-scale multilingual language models in choosing better pretraining configurations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.362.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.362.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--362 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.362 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.362.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.362" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.362/><span class=acl-fixed-case>SSEGCN</span>: Syntactic and Semantic Enhanced Graph Convolutional Network for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/z/zili-zhou/>Zili Zhou</a>
|
<a href=/people/y/yanna-wang/>Yanna Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--362><div class="card-body p-3 small">Aspect-based Sentiment Analysis (ABSA) aims to predict the sentiment polarity towards a particular aspect in a sentence. Recently, graph neural networks based on dependency tree convey rich structural information which is proven to be utility for ABSA. However, how to effectively harness the semantic and syntactic structure information from the dependency tree remains a challenging research question. In this paper, we propose a novel Syntactic and Semantic Enhanced Graph Convolutional Network (SSEGCN) model for ABSA task. Specifically, we propose an aspect-aware attention mechanism combined with self-attention to obtain attention score matrices of a sentence, which can not only learn the aspect-related semantic correlations, but also learn the global semantics of the sentence. In order to obtain comprehensive syntactic structure information, we construct syntactic mask matrices of the sentence according to the different syntactic distances between words. Furthermore, to combine syntactic structure and semantic information, we equip the attention score matrices by syntactic mask matrices. Finally, we enhance the node representations with graph convolutional network over attention score matrices for ABSA. Experimental results on benchmark datasets illustrate that our proposed model outperforms state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.363.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.363.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--363 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.363 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.363.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.363/>Mitigating Toxic Degeneration with Empathetic Data: Exploring the Relationship Between Toxicity and Empathy</a></strong><br><a href=/people/a/allison-lahnala/>Allison Lahnala</a>
|
<a href=/people/c/charles-welch/>Charles Welch</a>
|
<a href=/people/b/bela-neuendorf/>Béla Neuendorf</a>
|
<a href=/people/l/lucie-flek/>Lucie Flek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--363><div class="card-body p-3 small">Large pre-trained neural language models have supported the effectiveness of many NLP tasks, yet are still prone to generating toxic language hindering the safety of their use. Using empathetic data, we improve over recent work on controllable text generation that aims to reduce the toxicity of generated text. We find we are able to dramatically reduce the size of fine-tuning data to 7.5-30k samples while at the same time making significant improvements over state-of-the-art toxicity mitigation of up to 3.4% absolute reduction (26% relative) from the original work on 2.3m samples, by strategically sampling data based on empathy scores. We observe that the degree of improvements is subject to specific communication components of empathy. In particular, the more cognitive components of empathy significantly beat the original dataset in almost all experiments, while emotional empathy was tied to less improvement and even underperforming random samples of the original data. This is a particularly implicative insight for NLP work concerning empathy as until recently the research and resources built for it have exclusively considered empathy as an emotional concept.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.364.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--364 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.364.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.364.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.364" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.364/><span class=acl-fixed-case>DUCK</span>: Rumour Detection on Social Media by Modelling User and Comment Propagation Networks</a></strong><br><a href=/people/l/lin-tian/>Lin Tian</a>
|
<a href=/people/x/xiuzhen-jenny-zhang/>Xiuzhen Zhang</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--364><div class="card-body p-3 small">Social media rumours, a form of misinformation, can mislead the public and cause significant economic and social disruption. Motivated by the observation that the user network — which captures <span class=tex-math><em>who</em></span> engage with a story — and the comment network — which captures <span class=tex-math><em>how</em></span> they react to it — provide complementary signals for rumour detection, in this paper, we propose DUCK (rumour <span class=tex-math> ̲d</span>etection with <span class=tex-math> ̲u</span>ser and <span class=tex-math> ̲c</span>omment networ<span class=tex-math> ̲k</span>s) for rumour detection on social media. We study how to leverage transformers and graph attention networks to jointly model the contents and structure of social media conversations, as well as the network of users who engaged in these conversations. Over four widely used benchmark rumour datasets in English and Chinese, we show that DUCK produces superior performance for detecting rumours, creating a new state-of-the-art. Source code for DUCK is available at: <a href=https://github.com/ltian678/DUCK-code class=acl-markup-url>https://github.com/ltian678/DUCK-code</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.365.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--365 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.365.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.365/>Jam or Cream First? Modeling Ambiguity in Neural Machine Translation with <span class=acl-fixed-case>SCONES</span></a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/s/shankar-kumar/>Shankar Kumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--365><div class="card-body p-3 small">The softmax layer in neural machine translation is designed to model the distribution over mutually exclusive tokens. Machine translation, however, is intrinsically uncertain: the same source sentence can have multiple semantically equivalent translations. Therefore, we propose to replace the softmax activation with a multi-label classification layer that can model ambiguity more effectively. We call our loss function Single-label Contrastive Objective for Non-Exclusive Sequences (SCONES). We show that the multi-label output layer can still be trained on single reference training data using the SCONES loss function. SCONES yields consistent BLEU score gains across six translation directions, particularly for medium-resource language pairs and small beam sizes. By using smaller beam sizes we can speed up inference by a factor of 3.9x and still match or improve the BLEU score obtained using softmax. Furthermore, we demonstrate that SCONES can be used to train NMT models that assign the highest probability to adequate translations, thus mitigating the “beam search curse”. Additional experiments on synthetic language pairs with varying levels of uncertainty suggest that the improvements from SCONES can be attributed to better handling of ambiguity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.366.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.366.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.366/><span class=acl-fixed-case>S</span>kill<span class=acl-fixed-case>S</span>pan: Hard and Soft Skill Extraction from <span class=acl-fixed-case>E</span>nglish Job Postings</a></strong><br><a href=/people/m/mike-zhang/>Mike Zhang</a>
|
<a href=/people/k/kristian-jensen/>Kristian Jensen</a>
|
<a href=/people/s/sif-sonniks/>Sif Sonniks</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--366><div class="card-body p-3 small">Skill Extraction (SE) is an important and widely-studied task useful to gain insights into labor market dynamics. However, there is a lacuna of datasets and annotation guidelines; available datasets are few and contain crowd-sourced labels on the span-level or labels from a predefined skill inventory. To address this gap, we introduce SKILLSPAN, a novel SE dataset consisting of 14.5K sentences and over 12.5K annotated spans. We release its respective guidelines created over three different sources annotated for hard and soft skills by domain experts. We introduce a BERT baseline (Devlin et al., 2019). To improve upon this baseline, we experiment with language models that are optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al., 2020), and multi-task learning (Caruana, 1997). Our results show that the domain-adapted models significantly outperform their non-adapted counterparts, and single-task outperforms multi-task learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.367.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.367.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--367 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.367 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.367.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.367" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.367/><span class=acl-fixed-case>RAAT</span>: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction</a></strong><br><a href=/people/y/yuan-liang/>Yuan Liang</a>
|
<a href=/people/z/zhuoxuan-jiang/>Zhuoxuan Jiang</a>
|
<a href=/people/d/di-yin/>Di Yin</a>
|
<a href=/people/b/bo-ren/>Bo Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--367><div class="card-body p-3 small">In document-level event extraction (DEE) task, event arguments always scatter across sentences (across-sentence issue) and multipleevents may lie in one document (multi-event issue). In this paper, we argue that the relation information of event arguments is of greatsignificance for addressing the above two issues, and propose a new DEE framework which can model the relation dependencies, calledRelation-augmented Document-level Event Extraction (ReDEE). More specifically, this framework features a novel and tailored transformer,named as Relation-augmented Attention Transformer (RAAT). RAAT is scalable to capture multi-scale and multi-amount argument relations. To further leverage relation information, we introduce a separate event relation prediction task and adopt multi-task learning method to explicitly enhance event extraction performance. Extensive experiments demonstrate the effectiveness of the proposed method, which can achieve state-of-the-art performance on two public datasets. Our code is available at <a href=https://github.com/TencentYoutuResearch/RAAT class=acl-markup-url>https://github.com/TencentYoutuResearch/RAAT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.368.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.368.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--368 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.368 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.368.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.368" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.368/>A Double-Graph Based Framework for Frame Semantic Parsing</a></strong><br><a href=/people/c/ce-zheng/>Ce Zheng</a>
|
<a href=/people/x/xudong-chen/>Xudong Chen</a>
|
<a href=/people/r/runxin-xu/>Runxin Xu</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--368><div class="card-body p-3 small">Frame semantic parsing is a fundamental NLP task, which consists of three subtasks: frame identification, argument identification and role classification. Most previous studies tend to neglect relations between different subtasks and arguments and pay little attention to ontological frame knowledge defined in FrameNet. In this paper, we propose a Knowledge-guided Incremental semantic parser with Double-graph (KID). We first introduce Frame Knowledge Graph (FKG), a heterogeneous graph containing both frames and FEs (Frame Elements) built on the frame knowledge so that we can derive knowledge-enhanced representations for frames and FEs. Besides, we propose Frame Semantic Graph (FSG) to represent frame semantic structures extracted from the text with graph structures. In this way, we can transform frame semantic parsing into an incremental graph construction problem to strengthen interactions between subtasks and relations between arguments. Our experiments show that KID outperforms the previous state-of-the-art method by up to 1.7 F1-score on two FrameNet datasets. Our code is availavle at <a href=https://github.com/PKUnlp-icler/KID class=acl-markup-url>https://github.com/PKUnlp-icler/KID</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.369.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.369.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--369 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.369 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.369.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.369.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.369" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.369/>An Enhanced Span-based Decomposition Method for Few-Shot Sequence Labeling</a></strong><br><a href=/people/p/peiyi-wang/>Peiyi Wang</a>
|
<a href=/people/r/runxin-xu/>Runxin Xu</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/q/qingyu-zhou/>Qingyu Zhou</a>
|
<a href=/people/y/yunbo-cao/>Yunbo Cao</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--369><div class="card-body p-3 small">Few-Shot Sequence Labeling (FSSL) is a canonical paradigm for the tagging models, e.g., named entity recognition and slot filling, to generalize on an emerging, resource-scarce domain. Recently, the metric-based meta-learning framework has been recognized as a promising approach for FSSL. However, most prior works assign a label to each token based on the token-level similarities, which ignores the integrality of named entities or slots. To this end, in this paper, we propose ESD, an Enhanced Span-based Decomposition method for FSSL. ESD formulates FSSL as a span-level matching problem between test query and supporting instances. Specifically, ESD decomposes the span matching problem into a series of span-level procedures, mainly including enhanced span representation, class prototype aggregation and span conflicts resolution. Extensive experiments show that ESD achieves the new state-of-the-art results on two popular FSSL benchmarks, FewNERD and SNIPS, and is proven to be more robust in the noisy and nested tagging scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.370.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.370.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--370 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.370 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.370.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.370" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.370/>A Two-Stream <span class=acl-fixed-case>AMR</span>-enhanced Model for Document-level Event Argument Extraction</a></strong><br><a href=/people/r/runxin-xu/>Runxin Xu</a>
|
<a href=/people/p/peiyi-wang/>Peiyi Wang</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/s/shuang-zeng/>Shuang Zeng</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--370><div class="card-body p-3 small">Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a <b>T</b>wo-<b>S</b>tream <b>A</b>bstract meaning <b>R</b>epresentation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in <a href=https://github.com/PKUnlp-icler/TSAR class=acl-markup-url>https://github.com/PKUnlp-icler/TSAR</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.371.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.371.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--371 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.371 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.371.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.371" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.371/>Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning</a></strong><br><a href=/people/f/fei-wang/>Fei Wang</a>
|
<a href=/people/z/zhewei-xu/>Zhewei Xu</a>
|
<a href=/people/p/pedro-szekely/>Pedro Szekely</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--371><div class="card-body p-3 small">Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly, we propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column, and it differentiates between relevant cells and irrelevant cells from the structural perspective. Our framework also modifies the positional encoding mechanism to preserve the relative position of tokens in the same cell but enforce position invariance among different cells. Our technology is free to be plugged into existing table-to-text generation models, and has improved T5-based models to offer better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo, we preserve promising performance, while previous SOTA systems, even with transformation-based data augmentation, have seen significant performance drops.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.372.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.372.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--372 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.372 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.372.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.372" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.372/><span class=acl-fixed-case>J</span>oint<span class=acl-fixed-case>LK</span>: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering</a></strong><br><a href=/people/y/yueqing-sun/>Yueqing Sun</a>
|
<a href=/people/q/qi-shi/>Qi Shi</a>
|
<a href=/people/l/le-qi/>Le Qi</a>
|
<a href=/people/y/yu-zhang/>Yu Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--372><div class="card-body p-3 small">Existing KG-augmented models for commonsense question answering primarily focus on designing elaborate Graph Neural Networks (GNNs) to model knowledge graphs (KGs). However, they ignore (i) the effectively fusing and reasoning over question context representations and the KG representations, and (ii) automatically selecting relevant nodes from the noisy KGs during reasoning. In this paper, we propose a novel model, JointLK, which solves the above limitations through the joint reasoning of LM and GNN and the dynamic KGs pruning mechanism. Specifically, JointLK performs joint reasoning between LM and GNN through a novel dense bidirectional attention module, in which each question token attends on KG nodes and each KG node attends on question tokens, and the two modal representations fuse and update mutually by multi-step interactions. Then, the dynamic pruning module uses the attention weights generated by joint reasoning to prune irrelevant KG nodes recursively. We evaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate its improvements to the existing LM and LM+KG models, as well as its capability to perform interpretable reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.373.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.373.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--373 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.373 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.373.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.373" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.373/>Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens</a></strong><br><a href=/people/i/itay-itzhak/>Itay Itzhak</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--373><div class="card-body p-3 small">Standard pretrained language models operate on sequences of subword tokens without direct access to the characters that compose each token’s string representation. We probe the embedding layer of pretrained language models and show that models learn the internal character composition of whole word and subword tokens to a surprising extent, without ever seeing the characters coupled with the tokens. Our results show that the embedding layers of RoBERTa and GPT2 each hold enough information to accurately spell up to a third of the vocabulary and reach high character <span class=tex-math>n</span>gram overlap across all token types. We further test whether enriching subword models with character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment. Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.374.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.374.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--374 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.374 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.374.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.374.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.374" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.374/>A Corpus for Understanding and Generating Moral Stories</a></strong><br><a href=/people/j/jian-guan/>Jian Guan</a>
|
<a href=/people/z/ziqi-liu/>Ziqi Liu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--374><div class="card-body p-3 small">Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to assess these abilities of machines. We present STORAL, a new dataset of Chinese and English human-written moral stories. We show the difficulty of the proposed tasks by testing various models with automatic and manual evaluation on STORAL. Furthermore, we present a retrieval-augmented algorithm that effectively exploits related concepts or events in training sets as additional guidance to improve performance on these tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.375.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.375.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--375 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.375 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.375.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.375" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.375/>Modeling Multi-Granularity Hierarchical Features for Relation Extraction</a></strong><br><a href=/people/x/xinnian-liang/>Xinnian Liang</a>
|
<a href=/people/s/shuangzhi-wu/>Shuangzhi Wu</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--375><div class="card-body p-3 small">Relation extraction is a key task in Natural Language Processing (NLP), which aims to extract relations between entity pairs from given texts. Recently, relation extraction (RE) has achieved remarkable progress with the development of deep neural networks. Most existing research focuses on constructing explicit structured features using external knowledge such as knowledge graph and dependency tree. In this paper, we propose a novel method to extract multi-granularity features based solely on the original input sentences. We show that effective structured features can be attained even without external knowledge. Three kinds of features based on the input sentences are fully exploited, which are in entity mention level, segment level, and sentence level. All the three are jointly and hierarchically modeled. We evaluate our method on three public benchmarks: SemEval 2010 Task 8, Tacred, and Tacred Revisited. To verify the effectiveness, we apply our method to different encoders such as LSTM and BERT. Experimental results show that our method significantly outperforms existing state-of-the-art models that even use external knowledge. Extensive analyses demonstrate that the performance of our model is contributed by the capture of multi-granularity features and the model of their hierarchical structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.376.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--376 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.376.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.376" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.376/>Cross-modal Contrastive Learning for Speech Translation</a></strong><br><a href=/people/r/rong-ye/>Rong Ye</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--376><div class="card-body p-3 small">How can we learn unified representations for spoken utterances and their written text? Learning similar representations for semantically similar speech and text is important for speech translation. To this end, we propose ConST, a cross-modal contrastive learning method for end-to-end speech-to-text translation. We evaluate ConST and a variety of previous baselines on a popular benchmark MuST-C. Experiments show that the proposed ConST consistently outperforms the previous methods, and achieves an average BLEU of 29.4. The analysis further verifies that ConST indeed closes the representation gap of different modalities — its learned representation improves the accuracy of cross-modal speech-text retrieval from 4% to 88%. Code and models are available at <a href=https://github.com/ReneeYe/ConST class=acl-markup-url>https://github.com/ReneeYe/ConST</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.377.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.377.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--377 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.377 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.377.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.377" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.377/>Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances</a></strong><br><a href=/people/s/seungju-han/>Seungju Han</a>
|
<a href=/people/b/beomsu-kim/>Beomsu Kim</a>
|
<a href=/people/j/jin-yong-yoo/>Jin Yong Yoo</a>
|
<a href=/people/s/seokjun-seo/>Seokjun Seo</a>
|
<a href=/people/s/sangbum-kim/>Sangbum Kim</a>
|
<a href=/people/e/enkhbayar-erdenee/>Enkhbayar Erdenee</a>
|
<a href=/people/b/buru-chang/>Buru Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--377><div class="card-body p-3 small">In this paper, we consider mimicking fictional characters as a promising direction for building engaging conversation models. To this end, we present a new practical task where only a few utterances of each fictional character are available to generate responses mimicking them. Furthermore, we propose a new method named Pseudo Dialog Prompting (PDP) that generates responses by leveraging the power of large-scale language models with prompts containing the target character’s utterances. To better reflect the style of the character, PDP builds the prompts in the form of dialog that includes the character’s utterances as dialog history. Since only utterances of the characters are available in the proposed task, PDP matches each utterance with an appropriate pseudo-context from a predefined set of context candidates using a retrieval model. Through human and automatic evaluation, we show that PDP generates responses that better reflect the style of fictional characters than baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.378.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--378 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.378.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.378/><span class=acl-fixed-case>D</span>ynamic<span class=acl-fixed-case>TOC</span>: Persona-based Table of Contents for Consumption of Long Documents</a></strong><br><a href=/people/h/himanshu-maheshwari/>Himanshu Maheshwari</a>
|
<a href=/people/n/nethraa-sivakumar/>Nethraa Sivakumar</a>
|
<a href=/people/s/shelly-jain/>Shelly Jain</a>
|
<a href=/people/t/tanvi-karandikar/>Tanvi Karandikar</a>
|
<a href=/people/v/vinay-aggarwal/>Vinay Aggarwal</a>
|
<a href=/people/n/navita-goyal/>Navita Goyal</a>
|
<a href=/people/s/sumit-shekhar/>Sumit Shekhar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--378><div class="card-body p-3 small">Long documents like contracts, financial documents, etc., are often tedious to read through. Linearly consuming (via scrolling or navigation through default table of content) these documents is time-consuming and challenging. These documents are also authored to be consumed by varied entities (referred to as persona in the paper) interested in only certain parts of the document. In this work, we describe DynamicToC, a dynamic table of content-based navigator, to aid in the task of non-linear, persona-based document consumption. DynamicToC highlights sections of interest in the document as per the aspects relevant to different personas. DynamicToC is augmented with short questions to assist the users in understanding underlying content. This uses a novel deep-reinforcement learning technique to generate questions on these persona-clustered paragraphs. Human and automatic evaluations suggest the efficacy of both end-to-end pipeline and different components of DynamicToC.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.379.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--379 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.379.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.379" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.379/><span class=acl-fixed-case>KALA:</span> Knowledge-Augmented Language Model Adaptation</a></strong><br><a href=/people/m/minki-kang/>Minki Kang</a>
|
<a href=/people/j/jinheon-baek/>Jinheon Baek</a>
|
<a href=/people/s/sung-ju-hwang/>Sung Ju Hwang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--379><div class="card-body p-3 small">Pre-trained language models (PLMs) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM’s performance on the downstream task by causing catastrophic forgetting of its general knowledge. To overcome such limitations of adaptive pre-training for PLM adaption, we propose a novel domain adaption framework for PLMs coined as Knowledge-Augmented Language model Adaptation (KALA), which modulates the intermediate hidden representations of PLMs with domain knowledge, consisting of entities and their relational facts. We validate the performance of our KALA on question answering and named entity recognition tasks on multiple datasets across various domains. The results show that, despite being computationally efficient, our KALA largely outperforms adaptive pre-training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.380.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.380.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--380 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.380 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.380.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.380/>On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model</a></strong><br><a href=/people/s/seongjin-shin/>Seongjin Shin</a>
|
<a href=/people/s/sang-woo-lee/>Sang-Woo Lee</a>
|
<a href=/people/h/hwijeen-ahn/>Hwijeen Ahn</a>
|
<a href=/people/s/sungdong-kim/>Sungdong Kim</a>
|
<a href=/people/h/hyoungseok-kim/>HyoungSeok Kim</a>
|
<a href=/people/b/boseop-kim/>Boseop Kim</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/g/gichang-lee/>Gichang Lee</a>
|
<a href=/people/w/woomyoung-park/>Woomyoung Park</a>
|
<a href=/people/j/jung-woo-ha/>Jung-Woo Ha</a>
|
<a href=/people/n/nako-sung/>Nako Sung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--380><div class="card-body p-3 small">Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.381.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--381 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.381.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.381.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.381" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.381/>Sketching as a Tool for Understanding and Accelerating Self-attention for Long Sequences</a></strong><br><a href=/people/y/yifan-chen/>Yifan Chen</a>
|
<a href=/people/q/qi-zeng/>Qi Zeng</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/y/yun-yang/>Yun Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--381><div class="card-body p-3 small">Transformer-based models are not efficient in processing long sequences due to the quadratic space and time complexity of the self-attention modules. To address this limitation, Linformer and Informer reduce the quadratic complexity to linear (modulo logarithmic factors) via low-dimensional projection and row selection, respectively. These two models are intrinsically connected, and to understand their connection we introduce a theoretical framework of matrix sketching. Based on the theoretical analysis, we propose Skeinformer to accelerate self-attention and further improve the accuracy of matrix approximation to self-attention with column sampling, adaptive row normalization and pilot sampling reutilization. Experiments on the Long Range Arena benchmark demonstrate that our methods outperform alternatives with a consistently smaller time/space footprint.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.382.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.382.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--382 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.382 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.382.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.382.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.382/>Partner Personas Generation for Dialogue Response Generation</a></strong><br><a href=/people/h/hongyuan-lu/>Hongyuan Lu</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a>
|
<a href=/people/h/hong-cheng/>Hong Cheng</a>
|
<a href=/people/h/helen-meng/>Helen Meng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--382><div class="card-body p-3 small">Incorporating personas information allows diverse and engaging responses in dialogue response generation. Unfortunately, prior works have primarily focused on self personas and have overlooked the value of partner personas. Moreover, in practical applications, the availability of the gold partner personas is often not the case. This paper attempts to tackle these issues by offering a novel framework that leverages automatic partner personas generation to enhance the succeeding dialogue response generation. Our framework employs reinforcement learning with a dedicatedly designed critic network for reward judgement. Experimental results from automatic and human evaluations indicate that our framework is capable of generating relevant, interesting, coherent and informative partner personas, even compared to the ground truth partner personas. This enhances the succeeding dialogue response generation, which surpasses our competitive baselines that condition on the ground truth partner personas.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.383.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.383.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--383 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.383 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.383.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.383" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.383/>Semantically Informed Slang Interpretation</a></strong><br><a href=/people/z/zhewei-sun/>Zhewei Sun</a>
|
<a href=/people/r/richard-zemel/>Richard Zemel</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--383><div class="card-body p-3 small">Slang is a predominant form of informal language making flexible and extended use of words that is notoriously hard for natural language processing systems to interpret. Existing approaches to slang interpretation tend to rely on context but ignore semantic extensions common in slang word usage. We propose a semantically informed slang interpretation (SSI) framework that considers jointly the contextual and semantic appropriateness of a candidate interpretation for a query slang. We perform rigorous evaluation on two large-scale online slang dictionaries and show that our approach not only achieves state-of-the-art accuracy for slang interpretation in English, but also does so in zero-shot and few-shot scenarios where training data is sparse. Furthermore, we show how the same framework can be applied to enhancing machine translation of slang from English to other languages. Our work creates opportunities for the automated interpretation and translation of informal language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.384.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--384 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.384.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.384/>Dual-Channel Evidence Fusion for Fact Verification over Texts and Tables</a></strong><br><a href=/people/n/nan-hu/>Nan Hu</a>
|
<a href=/people/z/zirui-wu/>Zirui Wu</a>
|
<a href=/people/y/yuxuan-lai/>Yuxuan Lai</a>
|
<a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--384><div class="card-body p-3 small">Different from previous fact extraction and verification tasks that only consider evidence of a single format, FEVEROUS brings further challenges by extending the evidence format to both plain text and tables. Existing works convert all candidate evidence into either sentences or tables, thus often failing to fully capture the rich context in their original format from the converted evidence, let alone the context information lost during conversion. In this paper, we propose a Dual Channel Unified Format fact verification model (DCUF), which unifies various evidence into parallel streams, i.e., natural language sentences and a global evidence table, simultaneously. With carefully-designed evidence conversion and organization methods, DCUF makes the most of pre-trained table/language models to encourage each evidence piece to perform early and thorough interactions with other pieces in its original format. Experiments show that our model can make better use of existing pre-trained models to absorb evidence of two formats, thus outperforming previous works by a large margin. Our code and models are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.385.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--385 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.385.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.385" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.385/><span class=acl-fixed-case>T</span>ree<span class=acl-fixed-case>M</span>ix: Compositional Constituency-based Data Augmentation for Natural Language Understanding</a></strong><br><a href=/people/l/le-zhang/>Le Zhang</a>
|
<a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--385><div class="card-body p-3 small">Data augmentation is an effective approach to tackle over-fitting. Many previous works have proposed different data augmentations strategies for NLP, such as noise injection, word replacement, back-translation etc. Though effective, they missed one important characteristic of language–compositionality, meaning of a complex expression is built from its sub-parts. Motivated by this, we propose a compositional data augmentation approach for natural language understanding called TreeMix. Specifically, TreeMix leverages constituency parsing tree to decompose sentences into constituent sub-structures and the Mixup data augmentation technique to recombine them to generate new sentences. Compared with previous approaches, TreeMix introduces greater diversity to the samples generated and encourages models to learn compositionality of NLP data. Extensive experiments on text classification and SCAN demonstrate that TreeMix outperforms current state-of-the-art data augmentation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.386.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.386.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--386 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.386 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.386.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.386.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.386" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.386/><span class=acl-fixed-case>S</span>yn2<span class=acl-fixed-case>V</span>ec: Synset Colexification Graphs for Lexical Semantic Similarity</a></strong><br><a href=/people/j/john-harvill/>John Harvill</a>
|
<a href=/people/r/roxana-girju/>Roxana Girju</a>
|
<a href=/people/m/mark-hasegawa-johnson/>Mark Hasegawa-Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--386><div class="card-body p-3 small">In this paper we focus on patterns of colexification (co-expressions of form-meaning mapping in the lexicon) as an aspect of lexical-semantic organization, and use them to build large scale synset graphs across BabelNet’s typologically diverse set of 499 world languages. We introduce and compare several approaches: monolingual and cross-lingual colexification graphs, popular distributional models, and fusion approaches. The models are evaluated against human judgments on a semantic similarity task for nine languages. Our strong empirical findings also point to the importance of universality of our graph synset embedding representations with no need for any language-specific adaptation when evaluated on the lexical similarity task. The insights of our exploratory investigation of large-scale colexification graphs could inspire significant advances in NLP across languages, especially for tasks involving languages which lack dedicated lexical resources, and can benefit from language transfer from large shared cross-lingual semantic spaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.387.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.387.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--387 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.387 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.387.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.387" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.387/>On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?</a></strong><br><a href=/people/n/nouha-dziri/>Nouha Dziri</a>
|
<a href=/people/s/sivan-milton/>Sivan Milton</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/o/osmar-r-zaiane/>Osmar Zaiane</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--387><div class="card-body p-3 small">Knowledge-grounded conversational models are known to suffer from producing factually invalid statements, a phenomenon commonly called hallucination. In this work, we investigate the underlying causes of this phenomenon: is hallucination due to the training data, or to the models? We conduct a comprehensive human study on both existing knowledge-grounded conversational benchmarks and several state-of-the-art models. Our study reveals that the standard benchmarks consist of > 60% hallucinated responses, leading to models that not only hallucinate but even amplify hallucinations. Our findings raise important questions on the quality of existing datasets and models trained using them. We make our annotations publicly available for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.388.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.388.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--388 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.388 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.388.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.388/>Is “My Favorite New Movie” My Favorite Movie? Probing the Understanding of Recursive Noun Phrases</a></strong><br><a href=/people/q/qing-lyu/>Qing Lyu</a>
|
<a href=/people/z/zheng-hua/>Zheng Hua</a>
|
<a href=/people/d/daoxin-li/>Daoxin Li</a>
|
<a href=/people/l/li-zhang/>Li Zhang</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--388><div class="card-body p-3 small">Recursive noun phrases (NPs) have interesting semantic properties. For example, “my favorite new movie” is not necessarily my favorite movie, whereas “my new favorite movie” is. This is common sense to humans, yet it is unknown whether language models have such knowledge. We introduce the Recursive Noun Phrase Challenge (RNPC), a dataset of three textual inference tasks involving textual entailment and event plausibility comparison, precisely targeting the understanding of recursive NPs. When evaluated on RNPC, state-of-the-art Transformer models only perform around chance. Still, we show that such knowledge is learnable with appropriate data. We further probe the models for relevant linguistic features that can be learned from our tasks, including modifier semantic category and modifier scope. Finally, models trained on RNPC achieve strong zero-shot performance on an extrinsic Harm Detection evaluation task, showing the usefulness of the understanding of recursive NPs in downstream applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.389.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.389.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--389 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.389 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.389.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.389" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.389/>Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance</a></strong><br><a href=/people/j/jingwei-ni/>Jingwei Ni</a>
|
<a href=/people/z/zhijing-jin/>Zhijing Jin</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a>
|
<a href=/people/b/bernhard-scholkopf/>Bernhard Schölkopf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--389><div class="card-body p-3 small">Human-translated text displays distinct features from naturally written text in the same language. This phenomena, known as translationese, has been argued to confound the machine translation (MT) evaluation. Yet, we find that existing work on translationese neglects some important factors and the conclusions are mostly correlational but not causal. In this work, we collect CausalMT, a dataset where the MT training data are also labeled with the human translation directions. We inspect two critical factors, the train-test direction match (whether the human translation directions in the training and test sets are aligned), and data-model direction match (whether the model learns in the same direction as the human translation direction in the dataset). We show that these two factors have a large causal effect on the MT performance, in addition to the test-model direction mismatch highlighted by existing work on the impact of translationese. In light of our findings, we provide a set of suggestions for MT training and evaluation. Our code and data are at <a href=https://github.com/EdisonNi-hku/CausalMT class=acl-markup-url>https://github.com/EdisonNi-hku/CausalMT</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.390.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--390 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.390.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.390" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.390/>Visual Commonsense in Pretrained Unimodal and Multimodal Models</a></strong><br><a href=/people/c/chenyu-zhang/>Chenyu Zhang</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/z/zhuowan-li/>Zhuowan Li</a>
|
<a href=/people/e/elias-stengel-eskin/>Elias Stengel-Eskin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--390><div class="card-body p-3 small">Our commonsense knowledge about objects includes their typical visual attributes; we know that bananas are typically yellow or green, and not purple. Text and image corpora, being subject to reporting bias, represent this world-knowledge to varying degrees of faithfulness. In this paper, we investigate to what degree unimodal (language-only) and multimodal (image and language) models capture a broad range of visually salient attributes. To that end, we create the Visual Commonsense Tests (ViComTe) dataset covering 5 property types (color, shape, material, size, and visual co-occurrence) for over 5000 subjects. We validate this dataset by showing that our grounded color data correlates much better than ungrounded text-only data with crowdsourced color judgments provided by Paik et al. (2021). We then use our dataset to evaluate pretrained unimodal models and multimodal models. Our results indicate that multimodal models better reconstruct attribute distributions, but are still subject to reporting bias. Moreover, increasing model size does not enhance performance, suggesting that the key to visual commonsense lies in the data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.391.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--391 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.391.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.391" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.391/><span class=acl-fixed-case>Q</span>u<span class=acl-fixed-case>ALITY</span>: Question Answering with Long Input Texts, Yes!</a></strong><br><a href=/people/r/richard-yuanzhe-pang/>Richard Yuanzhe Pang</a>
|
<a href=/people/a/alicia-parrish/>Alicia Parrish</a>
|
<a href=/people/n/nitish-joshi/>Nitish Joshi</a>
|
<a href=/people/n/nikita-nangia/>Nikita Nangia</a>
|
<a href=/people/j/jason-phang/>Jason Phang</a>
|
<a href=/people/a/angelica-chen/>Angelica Chen</a>
|
<a href=/people/v/vishakh-padmakumar/>Vishakh Padmakumar</a>
|
<a href=/people/j/johnny-ma/>Johnny Ma</a>
|
<a href=/people/j/jana-thompson/>Jana Thompson</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--391><div class="card-body p-3 small">To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Our baseline models perform poorly on this task (55.4%) and significantly lag behind human performance (93.5%).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.392.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.392.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--392 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.392 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.392.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.392" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.392/><span class=acl-fixed-case>ExSum</span>: <span class=acl-fixed-case>F</span>rom Local Explanations to Model Understanding</a></strong><br><a href=/people/y/yilun-zhou/>Yilun Zhou</a>
|
<a href=/people/m/marco-tulio-ribeiro/>Marco Tulio Ribeiro</a>
|
<a href=/people/j/julie-shah/>Julie Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--392><div class="card-body p-3 small">Interpretability methods are developed to understand the working mechanisms of black-box models, which is crucial to their responsible deployment. Fulfilling this goal requires both that the explanations generated by these methods are correct and that people can easily and reliably understand them. While the former has been addressed in prior work, the latter is often overlooked, resulting in informal model understanding derived from a handful of local explanations. In this paper, we introduce explanation summary (ExSum), a mathematical framework for quantifying model understanding, and propose metrics for its quality assessment. On two domains, ExSum highlights various limitations in the current practice, helps develop accurate model understanding, and reveals easily overlooked properties of the model. We also connect understandability to other properties of explanations such as human alignment, robustness, and counterfactual similarity and plausibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.393.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--393 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.393.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.393" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.393/>Maximum <span class=acl-fixed-case>B</span>ayes <span class=acl-fixed-case>S</span>match Ensemble Distillation for <span class=acl-fixed-case>AMR</span> Parsing</a></strong><br><a href=/people/y/young-suk-lee/>Young-Suk Lee</a>
|
<a href=/people/r/ramon-fernandez-astudillo/>Ramón Astudillo</a>
|
<a href=/people/h/hoang-thanh-lam/>Hoang Thanh Lam</a>
|
<a href=/people/t/tahira-naseem/>Tahira Naseem</a>
|
<a href=/people/r/radu-florian/>Radu Florian</a>
|
<a href=/people/s/salim-roukos/>Salim Roukos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--393><div class="card-body p-3 small">AMR parsing has experienced an unprecendented increase in performance in the last three years, due to a mixture of effects including architecture improvements and transfer learning. Self-learning techniques have also played a role in pushing performance forward. However, for most recent high performant parsers, the effect of self-learning and silver data augmentation seems to be fading. In this paper we propose to overcome this diminishing returns of silver data by combining Smatch-based ensembling techniques with ensemble distillation. In an extensive experimental setup, we push single model English parser performance to a new state-of-the-art, 85.9 (AMR2.0) and 84.3 (AMR3.0), and return to substantial gains from silver data augmentation. We also attain a new state-of-the-art for cross-lingual AMR parsing for Chinese, German, Italian and Spanish. Finally we explore the impact of the proposed technique on domain adaptation, and show that it can produce gains rivaling those of human annotated data for QALD-9 and achieve a new state-of-the-art for BioAMR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.394.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.394.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--394 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.394 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.394.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.394" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.394/>When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes</a></strong><br><a href=/people/m/mycal-tucker/>Mycal Tucker</a>
|
<a href=/people/t/tiwalayo-eisape/>Tiwalayo Eisape</a>
|
<a href=/people/p/peng-qian/>Peng Qian</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a>
|
<a href=/people/j/julie-shah/>Julie Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--394><div class="card-body p-3 small">Recent causal probing literature reveals when language models and syntactic probes use similar representations. Such techniques may yield “false negative” causality results: models may use representations of syntax, but probes may have learned to use redundant encodings of the same syntactic information. We demonstrate that models do encode syntactic information redundantly and introduce a new probe design that guides probes to consider all syntactic information present in embeddings. Using these probes, we find evidence for the use of syntax in models where prior methods did not, allowing us to boost model performance by injecting syntactic information into representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.395.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.395.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--395 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.395 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.395.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.395.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.395/>Modeling Task Interactions in Document-Level Joint Entity and Relation Extraction</a></strong><br><a href=/people/l/liyan-xu/>Liyan Xu</a>
|
<a href=/people/j/jinho-d-choi/>Jinho Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--395><div class="card-body p-3 small">We target on the document-level relation extraction in an end-to-end setting, where the model needs to jointly perform mention extraction, coreference resolution (COREF) and relation extraction (RE) at once, and gets evaluated in an entity-centric way. Especially, we address the two-way interaction between COREF and RE that has not been the focus by previous work, and propose to introduce explicit interaction namely Graph Compatibility (GC) that is specifically designed to leverage task characteristics, bridging decisions of two tasks for direct task interference. Our experiments are conducted on DocRED and DWIE; in addition to GC, we implement and compare different multi-task settings commonly adopted in previous work, including pipeline, shared encoders, graph propagation, to examine the effectiveness of different interactions. The result shows that GC achieves the best performance by up to 2.3/5.1 F1 improvement over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.396.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.396.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--396 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.396 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.396.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.396/>Few-Shot Semantic Parsing with Language Models Trained on Code</a></strong><br><a href=/people/r/richard-shin/>Richard Shin</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--396><div class="card-body p-3 small">Large language models can perform semantic parsing with little training data, when prompted with in-context examples. It has been shown that this can be improved by formulating the problem as paraphrasing into canonical utterances, which casts the underlying meaning representation into a controlled natural language-like representation. Intuitively, such models can more easily output canonical utterances as they are closer to the natural language used for pre-training. Recently, models also pre-trained on code, like OpenAI Codex, have risen in prominence. For semantic parsing tasks where we map natural language into code, such models may prove more adept at it. In this paper, we test this hypothesis and find that Codex performs better on such tasks than equivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.397.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.397.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--397 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.397 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.397.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.397.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.397" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.397/><span class=acl-fixed-case>CORWA</span>: A Citation-Oriented Related Work Annotation Dataset</a></strong><br><a href=/people/x/xiangci-li/>Xiangci Li</a>
|
<a href=/people/b/biswadip-mandal/>Biswadip Mandal</a>
|
<a href=/people/j/jessica-ouyang/>Jessica Ouyang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--397><div class="card-body p-3 small">Academic research is an exploratory activity to discover new solutions to problems. By this nature, academic research works perform literature reviews to distinguish their novelties from prior work. In natural language processing, this literature review is usually conducted under the “Related Work” section. The task of related work generation aims to automatically generate the related work section given the rest of the research paper and a list of papers to cite. Prior work on this task has focused on the sentence as the basic unit of generation, neglecting the fact that related work sections consist of variable length text fragments derived from different information sources. As a first step toward a linguistically-motivated related work generation framework, we present a Citation Oriented Related Work Annotation (CORWA) dataset that labels different types of citation text fragments from different information sources. We train a strong baseline model that automatically tags the CORWA labels on massive unlabeled related work section texts. We further suggest a novel framework for human-in-the-loop, iterative, abstractive related work generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.398.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.398.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.398/>Overcoming Catastrophic Forgetting During Domain Adaptation of Seq2seq Language Generation</a></strong><br><a href=/people/d/dingcheng-li/>Dingcheng Li</a>
|
<a href=/people/z/zheng-chen/>Zheng Chen</a>
|
<a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a>
|
<a href=/people/x/xiaohu-liu/>Xiaohu Liu</a>
|
<a href=/people/f/fan-xing/>Fan Xing</a>
|
<a href=/people/c/chenlei-guo/>Chenlei Guo</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--398><div class="card-body p-3 small">Seq2seq language generation models that are trained offline with multiple domains in a sequential fashion often suffer from catastrophic forgetting. Lifelong learning has been proposed to handle this problem. However, existing work such as experience replay or elastic weighted consolidation requires incremental memory space. In this work, we propose an innovative framework, RMR_DSEthat leverages a recall optimization mechanism to selectively memorize important parameters of previous tasks via regularization, and uses a domain drift estimation algorithm to compensate the drift between different do-mains in the embedding space. These designs enable the model to be trained on the current task while keep-ing the memory of previous tasks, and avoid much additional data storage. Furthermore, RMR_DSE can be combined with existing lifelong learning approaches. Our experiments on two seq2seq language generation tasks, paraphrase and dialog response generation, show thatRMR_DSE outperforms SOTA models by a considerable margin and reduces forgetting greatly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.399.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.399.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--399 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.399 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.399.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.399" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.399/>Extreme <span class=acl-fixed-case>Z</span>ero-<span class=acl-fixed-case>S</span>hot Learning for Extreme Text Classification</a></strong><br><a href=/people/y/yuanhao-xiong/>Yuanhao Xiong</a>
|
<a href=/people/w/wei-cheng-chang/>Wei-Cheng Chang</a>
|
<a href=/people/c/cho-jui-hsieh/>Cho-Jui Hsieh</a>
|
<a href=/people/h/hsiang-fu-yu/>Hsiang-Fu Yu</a>
|
<a href=/people/i/inderjit-dhillon/>Inderjit Dhillon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--399><div class="card-body p-3 small">The eXtreme Multi-label text Classification (XMC) problem concerns finding most relevant labels for an input text instance from a large label set. However, the XMC setup faces two challenges: (1) it is not generalizable to predict unseen labels in dynamic environments, and (2) it requires a large amount of supervised (instance, label) pairs, which can be difficult to obtain for emerging domains. In this paper, we consider a more practical scenario called Extreme Zero-Shot XMC (EZ-XMC), in which no supervision is needed and merely raw text of instances and labels are accessible. Few-Shot XMC (FS-XMC), an extension to EZ-XMC with limited supervision is also investigated. To learn the semantic embeddings of instances and labels with raw text, we propose to pre-train Transformer-based encoders with self-supervised contrastive losses. Specifically, we develop a pre-training method <span class=tex-math><strong>MACLR</strong></span>, which thoroughly leverages the raw text with techniques including <span class=tex-math><strong>M</strong></span>ulti-scale <span class=tex-math><strong>A</strong></span>daptive <span class=tex-math><strong>C</strong></span>lustering, <span class=tex-math><strong>L</strong></span>abel <span class=tex-math><strong>R</strong></span>egularization, and self-training with pseudo positive pairs. Experimental results on four public EZ-XMC datasets demonstrate that MACLR achieves superior performance compared to all other leading baseline methods, in particular with approximately 5-10% improvement in precision and recall on average. Moreover, we show that our pre-trained encoder can be further improved on FS-XMC when there are a limited number of ground-truth positive pairs in training. Our code is available at <a href=https://github.com/amzn/pecos/tree/mainline/examples/MACLR class=acl-markup-url>https://github.com/amzn/pecos/tree/mainline/examples/MACLR</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.400.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--400 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.400 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.400.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.400" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.400/><span class=acl-fixed-case>C</span>onfli<span class=acl-fixed-case>BERT</span>: A Pre-trained Language Model for Political Conflict and Violence</a></strong><br><a href=/people/y/yibo-hu/>Yibo Hu</a>
|
<a href=/people/m/mohammadsaleh-hosseini/>MohammadSaleh Hosseini</a>
|
<a href=/people/e/erick-skorupa-parolin/>Erick Skorupa Parolin</a>
|
<a href=/people/j/javier-osorio/>Javier Osorio</a>
|
<a href=/people/l/latifur-khan/>Latifur Khan</a>
|
<a href=/people/p/patrick-brandt/>Patrick Brandt</a>
|
<a href=/people/v/vito-dorazio/>Vito D’Orazio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--400><div class="card-body p-3 small">Analyzing conflicts and political violence around the world is a persistent challenge in the political science and policy communities due in large part to the vast volumes of specialized text needed to monitor conflict and violence on a global scale. To help advance research in political science, we introduce ConfliBERT, a domain-specific pre-trained language model for conflict and political violence. We first gather a large domain-specific text corpus for language modeling from various sources. We then build ConfliBERT using two approaches: pre-training from scratch and continual pre-training. To evaluate ConfliBERT, we collect 12 datasets and implement 18 tasks to assess the models’ practical application in conflict research. Finally, we evaluate several versions of ConfliBERT in multiple experiments. Results consistently show that ConfliBERT outperforms BERT when analyzing political violence and conflict.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.401.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--401 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.401.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.401" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.401/>Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification</a></strong><br><a href=/people/h/han-wang/>Han Wang</a>
|
<a href=/people/c/canwen-xu/>Canwen Xu</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--401><div class="card-body p-3 small">Prompt-based learning (i.e., prompting) is an emerging paradigm for exploiting knowledge learned by a pretrained language model. In this paper, we propose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method to automatically select label mappings for few-shot text classification with prompting. Our method exploits one-to-many label mappings and a statistics-based algorithm to select label mappings given a prompt template. Our experiments demonstrate that AMuLaP achieves competitive performance on the GLUE benchmark without human effort or external resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.402.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--402 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.402.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.402/>Few-shot Subgoal Planning with Language Models</a></strong><br><a href=/people/l/lajanugen-logeswaran/>Lajanugen Logeswaran</a>
|
<a href=/people/y/yao-fu/>Yao Fu</a>
|
<a href=/people/m/moontae-lee/>Moontae Lee</a>
|
<a href=/people/h/honglak-lee/>Honglak Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--402><div class="card-body p-3 small">Pre-trained language models have shown successful progress in many text understanding benchmarks. This work explores the capability of these models to predict actionable plans in real-world environments. Given a text instruction, we show that language priors encoded in pre-trained models allow us to infer fine-grained subgoal sequences. In contrast to recent methods which make strong assumptions about subgoal supervision, our experiments show that language models can infer detailed subgoal sequences from few training sequences without any fine-tuning. We further propose a simple strategy to re-rank language model predictions based on interaction and feedback from the environment. Combined with pre-trained navigation and visual reasoning components, our approach demonstrates competitive performance on subgoal prediction and task completion in the ALFRED benchmark compared to prior methods that assume more subgoal supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.403.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--403 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.403.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.403/><span class=acl-fixed-case>IDPG</span>: An Instance-Dependent Prompt Generation Method</a></strong><br><a href=/people/z/zhuofeng-wu/>Zhuofeng Wu</a>
|
<a href=/people/s/sinong-wang/>Sinong Wang</a>
|
<a href=/people/j/jiatao-gu/>Jiatao Gu</a>
|
<a href=/people/r/rui-hou/>Rui Hou</a>
|
<a href=/people/y/yuxiao-dong/>Yuxiao Dong</a>
|
<a href=/people/v/v-g-vinod-vydiswaran/>V.G.Vinod Vydiswaran</a>
|
<a href=/people/h/hao-ma/>Hao Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--403><div class="card-body p-3 small">Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a task-specific prompt in each input instance during the model training stage. It freezes the pre-trained language model and only optimizes a few task-specific prompts. In this paper, we propose a conditional prompt generation method to generate prompts for each input instance, referred to as the Instance-Dependent Prompt Generation (IDPG). Unlike traditional prompt tuning methods that use a fixed prompt, IDPG introduces a lightweight and trainable component to generate prompts based on each input sentence. Extensive experiments on ten natural language understanding (NLU) tasks show that the proposed strategy consistently outperforms various prompt tuning baselines and is on par with other efficient transfer learning methods such as Compacter while tuning far fewer model parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.404.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--404 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.404.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.404" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.404/>Embedding Hallucination for Few-shot Language Fine-tuning</a></strong><br><a href=/people/y/yiren-jian/>Yiren Jian</a>
|
<a href=/people/c/chongyang-gao/>Chongyang Gao</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--404><div class="card-body p-3 small">Few-shot language learners adapt knowledge from a pre-trained model to recognize novel classes from a few-labeled sentences. In such settings, fine-tuning a pre-trained language model can cause severe over-fitting. In this paper, we propose an Embedding Hallucination (EmbedHalluc) method, which generates auxiliary embedding-label pairs to expand the fine-tuning dataset. The hallucinator is trained by playing an adversarial game with the discriminator, such that the hallucinated embedding is indiscriminative to the real ones in the fine-tuning dataset. By training with the extended dataset, the language learner effectively learns from the diverse hallucinated embeddings to overcome the over-fitting issue. Experiments demonstrate that our proposed method is effective in a wide range of language tasks, outperforming current fine-tuning methods. Further, we show that EmbedHalluc outperforms other methods that address this over-fitting problem, such as common data augmentation, semi-supervised pseudo-labeling, and regularization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.405.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--405 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.405.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.405.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.405" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.405/>Cryptocurrency Bubble Detection: A New Stock Market Dataset, Financial Task & Hyperbolic Models</a></strong><br><a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/s/shivam-agarwal/>Shivam Agarwal</a>
|
<a href=/people/v/vivek-mittal/>Vivek Mittal</a>
|
<a href=/people/p/paolo-rosso/>Paolo Rosso</a>
|
<a href=/people/v/vikram-nanda/>Vikram Nanda</a>
|
<a href=/people/s/sudheer-chava/>Sudheer Chava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--405><div class="card-body p-3 small">The rapid spread of information over social media influences quantitative trading and investments. The growing popularity of speculative trading of highly volatile assets such as cryptocurrencies and meme stocks presents a fresh challenge in the financial realm. Investigating such “bubbles” - periods of sudden anomalous behavior of markets are critical in better understanding investor behavior and market dynamics. However, high volatility coupled with massive volumes of chaotic social media texts, especially for underexplored assets like cryptocoins pose a challenge to existing methods. Taking the first step towards NLP for cryptocoins, we present and publicly release CryptoBubbles, a novel multi- span identification task for bubble detection, and a dataset of more than 400 cryptocoins from 9 exchanges over five years spanning over two million tweets. Further, we develop a set of sequence-to-sequence hyperbolic models suited to this multi-span identification task based on the power-law dynamics of cryptocurrencies and user behavior on social media. We further test the effectiveness of our models under zero-shot settings on a test set of Reddit posts pertaining to 29 “meme stocks”, which see an increase in trade volume due to social media hype. Through quantitative, qualitative, and zero-shot analyses on Reddit and Twitter spanning cryptocoins and meme-stocks, we show the practical applicability of CryptoBubbles and hyperbolic models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.406.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--406 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.406.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.406" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.406/>Nearest Neighbor Knowledge Distillation for Neural Machine Translation</a></strong><br><a href=/people/z/zhixian-yang/>Zhixian Yang</a>
|
<a href=/people/r/renliang-sun/>Renliang Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--406><div class="card-body p-3 small">k-nearest-neighbor machine translation (<span class=tex-math>k</span>NN-MT), proposed by Khandelwal et al. (2021), has achieved many state-of-the-art results in machine translation tasks. Although effective, <span class=tex-math>k</span>NN-MT requires conducting <span class=tex-math>k</span>NN searches through the large datastore for each decoding step during inference, prohibitively increasing the decoding cost and thus leading to the difficulty for the deployment in real-world applications. In this paper, we propose to move the time-consuming <span class=tex-math>k</span>NN search forward to the preprocessing phase, and then introduce <span class=tex-math>k</span> Nearest Neighbor Knowledge Distillation (<span class=tex-math>k</span>NN-KD) that trains the base NMT model to directly learn the knowledge of <span class=tex-math>k</span>NN. Distilling knowledge retrieved by <span class=tex-math>k</span>NN can encourage the NMT model to take more reasonable target tokens into consideration, thus addressing the overcorrection problem. Extensive experimental results show that, the proposed method achieves consistent improvement over the state-of-the-art baselines including <span class=tex-math>k</span>NN-MT, while maintaining the same training and decoding speed as the standard NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.407.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--407 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.407.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.407" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.407/><span class=acl-fixed-case>DEM</span>ix Layers: Disentangling Domains for Modular Language Modeling</a></strong><br><a href=/people/s/suchin-gururangan/>Suchin Gururangan</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/a/ari-holtzman/>Ari Holtzman</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--407><div class="card-body p-3 small">We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data), increase training efficiency, and enable rapid adaptation. Mixing experts during inference, using a parameter-free weighted ensemble, enables better generalization to heterogeneous or unseen domains. We also show it is possible to add experts to adapt to new domains without forgetting older ones, and remove experts to restrict access to unwanted domains. Overall, these results demonstrate benefits of domain modularity in language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.408.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--408 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.408.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.408" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.408/>Contrastive Learning for Prompt-based Few-shot Language Learners</a></strong><br><a href=/people/y/yiren-jian/>Yiren Jian</a>
|
<a href=/people/c/chongyang-gao/>Chongyang Gao</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--408><div class="card-body p-3 small">The impressive performance of GPT-3 using natural language prompts and in-context learning has inspired work on better fine-tuning of moderately-sized models under this paradigm. Following this line of work, we present a contrastive learning framework that clusters inputs from the same class for better generality of models trained with only limited examples. Specifically, we propose a supervised contrastive framework that clusters inputs from the same class under different augmented “views” and repel the ones from different classes. We create different “views” of an example by appending it with different language prompts and contextual demonstrations. Combining a contrastive loss with the standard masked language modeling (MLM) loss in prompt-based few-shot learners, the experimental results show that our method can improve over the state-of-the-art methods in a diverse set of 15 language tasks. Our framework makes minimal assumptions on the task or the base model, and can be applied to many recent methods with little modification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.409.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--409 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.409.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.409/>Cross-Lingual Event Detection via Optimized Adversarial Training</a></strong><br><a href=/people/l/luis-guzman-nateras/>Luis Guzman-Nateras</a>
|
<a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/t/thien-nguyen/>Thien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--409><div class="card-body p-3 small">In this work, we focus on Cross-Lingual Event Detection where a model is trained on data from a <span class=tex-math><em>source</em></span> language but its performance is evaluated on data from a second, <span class=tex-math><em>target</em></span>, language. Most recent works in this area have harnessed the language-invariant qualities displayed by pre-trained Multi-lingual Language Models. Their performance, however, reveals there is room for improvement as the cross-lingual setting entails particular challenges. We employ Adversarial Language Adaptation to train a Language Discriminator to discern between the source and target languages using unlabeled data. The discriminator is trained in an adversarial manner so that the encoder learns to produce refined, language-invariant representations that lead to improved performance. More importantly, we optimize the adversarial training process by only presenting the discriminator with the most informative samples. We base our intuition about what makes a sample informative on two disparate metrics: sample similarity and event presence. Thus, we propose leveraging Optimal Transport as a solution to naturally combine these two distinct information sources into the selection process. Extensive experiments on 8 different language pairs, using 4 languages from unrelated families, show the flexibility and effectiveness of our model that achieves state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.410.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--410 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.410.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.410" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.410/>Identifying Implicitly Abusive Remarks about Identity Groups using a Linguistically Informed Approach</a></strong><br><a href=/people/m/michael-wiegand/>Michael Wiegand</a>
|
<a href=/people/e/elisabeth-eder/>Elisabeth Eder</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--410><div class="card-body p-3 small">We address the task of distinguishing implicitly abusive sentences on identity groups (“Muslims contaminate our planet”) from other group-related negative polar sentences (“Muslims despise terrorism”). Implicitly abusive language are utterances not conveyed by abusive words (e.g. “bimbo” or “scum”). So far, the detection of such utterances could not be properly addressed since existing datasets displaying a high degree of implicit abuse are fairly biased. Following the recently-proposed strategy to solve implicit abuse by separately addressing its different subtypes, we present a new focused and less biased dataset that consists of the subtype of atomic negative sentences about identity groups. For that task, we model components that each address one facet of such implicit abuse, i.e. depiction as perpetrators, aspectual classification and non-conformist views. The approach generalizes across different identity groups and languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.411.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--411 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.411.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.411" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.411/>Label Definitions Improve Semantic Role Labeling</a></strong><br><a href=/people/l/li-zhang/>Li Zhang</a>
|
<a href=/people/i/ishan-jindal/>Ishan Jindal</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--411><div class="card-body p-3 small">Argument classification is at the core of Semantic Role Labeling. Given a sentence and the predicate, a semantic role label is assigned to each argument of the predicate. While semantic roles come with meaningful definitions, existing work has treated them as symbolic. Learning symbolic labels usually requires ample training data, which is frequently unavailable due to the cost of annotation. We instead propose to retrieve and leverage the definitions of these labels from the annotation guidelines. For example, the verb predicate “work” has arguments defined as “worker”, “job”, “employer”, etc. Our model achieves state-of-the-art performance on the CoNLL09 dataset injected with label definitions given the predicate senses. The performance improvement is even more pronounced in low-resource settings when training data is scarce.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.412.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--412 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.412.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.412.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.412/>Shedding New Light on the Language of the Dark Web</a></strong><br><a href=/people/y/youngjin-jin/>Youngjin Jin</a>
|
<a href=/people/e/eugene-jang/>Eugene Jang</a>
|
<a href=/people/y/yongjae-lee/>Yongjae Lee</a>
|
<a href=/people/s/seungwon-shin/>Seungwon Shin</a>
|
<a href=/people/j/jin-woo-chung/>Jin-Woo Chung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--412><div class="card-body p-3 small">The hidden nature and the limited accessibility of the Dark Web, combined with the lack of public datasets in this domain, make it difficult to study its inherent characteristics such as linguistic properties. Previous works on text classification of Dark Web domain have suggested that the use of deep neural models may be ineffective, potentially due to the linguistic differences between the Dark and Surface Webs. However, not much work has been done to uncover the linguistic characteristics of the Dark Web. This paper introduces CoDA, a publicly available Dark Web dataset consisting of 10000 web documents tailored towards text-based Dark Web analysis. By leveraging CoDA, we conduct a thorough linguistic analysis of the Dark Web and examine the textual differences between the Dark Web and the Surface Web. We also assess the performance of various methods of Dark Web page classification. Finally, we compare CoDA with an existing public Dark Web dataset and evaluate their suitability for various use cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.413.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--413 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.413.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.413/>Conceptualizing Treatment Leakage in Text-based Causal Inference</a></strong><br><a href=/people/a/adel-daoud/>Adel Daoud</a>
|
<a href=/people/c/connor-jerzak/>Connor Jerzak</a>
|
<a href=/people/r/richard-johansson/>Richard Johansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--413><div class="card-body p-3 small">Causal inference methods that control for text-based confounders are becoming increasingly important in the social sciences and other disciplines where text is readily available. However, these methods rely on a critical assumption that there is no treatment leakage: that is, the text only contains information about the confounder and no information about treatment assignment. When this assumption does not hold, methods that control for text to adjust for confounders face the problem of post-treatment (collider) bias. However, the assumption that there is no treatment leakage may be unrealistic in real-world situations involving text, as human language is rich and flexible. Language appearing in a public policy document or health records may refer to the future and the past simultaneously, and thereby reveal information about the treatment assignment. In this article, we define the treatment-leakage problem, and discuss the identification as well as the estimation challenges it raises. Second, we delineate the conditions under which leakage can be addressed by removing the treatment-related signal from the text in a pre-processing step we define as text distillation. Lastly, using simulation, we show how treatment leakage introduces a bias in estimates of the average treatment effect (ATE) and how text distillation can mitigate this bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.414.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--414 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.414.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.414.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.414" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.414/>Consistency Training with Virtual Adversarial Discrete Perturbation</a></strong><br><a href=/people/j/jungsoo-park/>Jungsoo Park</a>
|
<a href=/people/g/gyuwan-kim/>Gyuwan Kim</a>
|
<a href=/people/j/jaewoo-kang/>Jaewoo Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--414><div class="card-body p-3 small">Consistency training regularizes a model by enforcing predictions of original and perturbed inputs to be similar. Previous studies have proposed various augmentation methods for the perturbation but are limited in that they are agnostic to the training model. Thus, the perturbed samples may not aid in regularization due to their ease of classification from the model. In this context, we propose an augmentation method of adding a discrete noise that would incur the highest divergence between predictions. This virtual adversarial discrete noise obtained by replacing a small portion of tokens while keeping original semantics as much as possible efficiently pushes a training model’s decision boundary. Experimental results show that our proposed method outperforms other consistency training baselines with text editing, paraphrasing, or a continuous noise on semi-supervised text classification tasks and a robustness benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.415.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--415 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.415.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.415/><span class=acl-fixed-case>CONFIT</span>: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning</a></strong><br><a href=/people/x/xiangru-tang/>Xiangru Tang</a>
|
<a href=/people/a/arjun-nair/>Arjun Nair</a>
|
<a href=/people/b/borui-wang/>Borui Wang</a>
|
<a href=/people/b/bingyao-wang/>Bingyao Wang</a>
|
<a href=/people/j/jai-desai/>Jai Desai</a>
|
<a href=/people/a/aaron-wade/>Aaron Wade</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--415><div class="card-body p-3 small">Factual inconsistencies in generated summaries severely limit the practical applications of abstractive dialogue summarization. Although significant progress has been achieved by using pre-trained neural language models, substantial amounts of hallucinated content are found during the human evaluation. In this work, we first devised a typology of factual errors to better understand the types of hallucinations generated by current models and conducted human evaluation on popular dialog summarization dataset. We further propose a training strategy that improves the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called CONFIT. To tackle top factual errors from our annotation, we introduce additional contrastive loss with carefully designed hard negative samples and self-supervised dialogue-specific loss to capture the key information between speakers. We show that our model significantly reduces all kinds of factual errors on both SAMSum dialogue summarization and AMI meeting summarization. On both datasets, we achieve significant improvements over state-of-the-art baselines using both automatic metrics, ROUGE and BARTScore, and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.416.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.416.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.416" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.416/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>MPM</span>: Context Modeling with Speaker’s Pre-trained Memory Tracking for Emotion Recognition in Conversation</a></strong><br><a href=/people/j/joosung-lee/>Joosung Lee</a>
|
<a href=/people/w/wooin-lee/>Wooin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--416><div class="card-body p-3 small">As the use of interactive machines grow, the task of Emotion Recognition in Conversation (ERC) became more important. If the machine-generated sentences reflect emotion, more human-like sympathetic conversations are possible. Since emotion recognition in conversation is inaccurate if the previous utterances are not taken into account, many studies reflect the dialogue context to improve the performances. Many recent approaches show performance improvement by combining knowledge into modules learned from external structured data. However, structured data is difficult to access in non-English languages, making it difficult to extend to other languages. Therefore, we extract the pre-trained memory using the pre-trained language model as an extractor of external knowledge. We introduce CoMPM, which combines the speaker’s pre-trained memory with the context model, and find that the pre-trained memory significantly improves the performance of the context model. CoMPM achieves the first or second performance on all data and is state-of-the-art among systems that do not leverage structured data. In addition, our method shows that it can be extended to other languages because structured knowledge is not required, unlike previous methods. Our code is available on github .</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.417.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--417 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-main.417/>Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries</a></strong><br><a href=/people/x/xiangru-tang/>Xiangru Tang</a>
|
<a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/z/ziming-mao/>Ziming Mao</a>
|
<a href=/people/g/griffin-adams/>Griffin Adams</a>
|
<a href=/people/b/borui-wang/>Borui Wang</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--417><div class="card-body p-3 small">Current pre-trained models applied for summarization are prone to factual inconsistencies that misrepresent the source text. Evaluating the factual consistency of summaries is thus necessary to develop better models. However, the human evaluation setup for evaluating factual consistency has not been standardized. To determine the factors that affect the reliability of the human evaluation, we crowdsource evaluations for factual consistency across state-of-the-art models on two news summarization datasets using the rating-based Likert Scale and ranking-based Best-Worst Scaling. Our analysis reveals that the ranking-based Best-Worst Scaling offers a more reliable measure of summary quality across datasets and that the reliability of Likert ratings highly depends on the target dataset and the evaluation design. To improve crowdsourcing reliability, we extend the scale of the Likert rating and present a scoring algorithm for Best-Worst Scaling that we call value learning. Our crowdsourcing guidelines will be publicly available to facilitate future work on factual consistency in summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.418.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--418 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.418.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.418/><span class=acl-fixed-case>D</span>ial<span class=acl-fixed-case>S</span>umm<span class=acl-fixed-case>E</span>val: Revisiting Summarization Evaluation for Dialogues</a></strong><br><a href=/people/m/mingqi-gao/>Mingqi Gao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--418><div class="card-body p-3 small">Dialogue summarization is receiving increasing attention from researchers due to its extraordinary difficulty and unique application value. We observe that current dialogue summarization models have flaws that may not be well exposed by frequently used metrics such as ROUGE. In our paper, we re-evaluate 18 categories of metrics in terms of four dimensions: coherence, consistency, fluency and relevance, as well as a unified human evaluation of various models for the first time. Some noteworthy trends which are different from the conventional summarization tasks are identified. We will release DialSummEval, a multi-faceted dataset of human judgments containing the outputs of 14 models on SAMSum.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.419.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--419 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.419.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.419" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.419/>Hyperbolic Relevance Matching for Neural Keyphrase Extraction</a></strong><br><a href=/people/m/mingyang-song/>Mingyang Song</a>
|
<a href=/people/y/yi-feng/>Yi Feng</a>
|
<a href=/people/l/liping-jing/>Liping Jing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--419><div class="card-body p-3 small">Keyphrase extraction is a fundamental task in natural language processing that aims to extract a set of phrases with important information from a source document. Identifying important keyphrases is the central component of keyphrase extraction, and its main challenge is learning to represent information comprehensively and discriminate importance accurately. In this paper, to address the above issues, we design a new hyperbolic matching model (HyperMatch) to explore keyphrase extraction in hyperbolic space. Concretely, to represent information comprehensively, HyperMatch first takes advantage of the hidden representations in the middle layers of RoBERTa and integrates them as the word embeddings via an adaptive mixing layer to capture the hierarchical syntactic and semantic structures. Then, considering the latent structure information hidden in natural languages, HyperMatch embeds candidate phrases and documents in the same hyperbolic space via a hyperbolic phrase encoder and a hyperbolic document encoder. To discriminate importance accurately, HyperMatch estimates the importance of each candidate phrase by explicitly modeling the phrase-document relevance via the Poincaré distance and optimizes the whole model by minimizing the hyperbolic margin-based triplet loss. Extensive experiments are conducted on six benchmark datasets and demonstrate that HyperMatch outperforms the recent state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.420.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.420.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.420.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.420/>Template-free Prompt Tuning for Few-shot <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/r/ruotian-ma/>Ruotian Ma</a>
|
<a href=/people/x/xin-zhou/>Xin Zhou</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/y/yiding-tan/>Yiding Tan</a>
|
<a href=/people/l/linyang-li/>Linyang Li</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--420><div class="card-body p-3 small">Prompt-based methods have been successfully applied in sentence-level few-shot learning tasks, mostly owing to the sophisticated design of templates and label words. However, when applied to token-level labeling tasks such as NER, it would be time-consuming to enumerate the template queries over all potential entity spans. In this work, we propose a more elegant method to reformulate NER tasks as LM problems without any templates. Specifically, we discard the template construction process while maintaining the word prediction paradigm of pre-training models to predict a class-related pivot word (or label word) at the entity position. Meanwhile, we also explore principled ways to automatically search for appropriate label words that the pre-trained models can easily adapt to. While avoiding the complicated template-based process, the proposed LM objective also reduces the gap between different objectives used in pre-training and fine-tuning, thus it can better benefit the few-shot performance. Experimental results demonstrate the effectiveness of the proposed method over bert-tagger and template-based method under few-shot settings. Moreover, the decoding speed of the proposed method is up to 1930.12 times faster than the template-based method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.421.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--421 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.421.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.421" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.421/>Few-Shot Document-Level Relation Extraction</a></strong><br><a href=/people/n/nicholas-popovic/>Nicholas Popovic</a>
|
<a href=/people/m/michael-farber/>Michael Färber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--421><div class="card-body p-3 small">We present FREDo, a few-shot document-level relation extraction (FSDLRE) benchmark. As opposed to existing benchmarks which are built on sentence-level relation extraction corpora, we argue that document-level corpora provide more realism, particularly regarding none-of-the-above (NOTA) distributions. Therefore, we propose a set of FSDLRE tasks and construct a benchmark based on two existing supervised learning data sets, DocRED and sciERC. We adapt the state-of-the-art sentence-level method MNAV to the document-level and develop it further for improved domain adaptation. We find FSDLRE to be a challenging setting with interesting new characteristics such as the ability to sample NOTA instances from the support set. The data, code, and trained models are available online (<a href=https://github.com/nicpopovic/FREDo class=acl-markup-url>https://github.com/nicpopovic/FREDo</a>).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.422.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--422 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.422.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.422" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.422/><span class=acl-fixed-case>L</span>a<span class=acl-fixed-case>M</span>emo: Language Modeling with Look-Ahead Memory</a></strong><br><a href=/people/h/haozhe-ji/>Haozhe Ji</a>
|
<a href=/people/r/rongsheng-zhang/>Rongsheng Zhang</a>
|
<a href=/people/z/zhenyu-yang/>Zhenyu Yang</a>
|
<a href=/people/z/zhipeng-hu/>Zhipeng Hu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--422><div class="card-body p-3 small">Although Transformers with fully connected self-attentions are powerful to model long-term dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides up-to-date information for token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo) that enhances the recurrence memory by incrementally attending to the right-side tokens and interpolating with the old memory states to maintain long-term information in the history. LaMemo embraces bi-directional attention and segment recurrence with an additional computation overhead only linearly proportional to the memory length. Experiments on widely used language modeling benchmarks demonstrate its superiority over the baselines equipped with different types of memory mechanisms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.423.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.423.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--423 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.423 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.423.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.423" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.423/>Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with <span class=acl-fixed-case>VAE</span>s</a></strong><br><a href=/people/g/ghazi-felhi/>Ghazi Felhi</a>
|
<a href=/people/j/joseph-le-roux/>Joseph Le Roux</a>
|
<a href=/people/d/djame-seddah/>Djamé Seddah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--423><div class="card-body p-3 small">We propose a generative model for text generation, which exhibits disentangled latent representations of syntax and semantics. Contrary to previous work, this model does not need syntactic information such as constituency parses, or semantic information such as paraphrase pairs. Our model relies solely on the inductive bias found in attention-based architectures such as Transformers. In the attention of Transformers, <span class=tex-math>keys</span> handle information selection while <span class=tex-math>values</span> specify what information is conveyed. Our model, dubbed QKVAE, uses Attention in its decoder to read latent variables where one latent variable infers keys while another infers values. We run experiments on latent representations and experiments on syntax/semantics transfer which show that QKVAE displays clear signs of disentangled syntax and semantics. We also show that our model displays competitive syntax transfer capabilities when compared to supervised models and that comparable supervised models need a fairly large amount of data (more than 50K samples) to outperform it on both syntactic and semantic transfer. The code for our experiments is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.424.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--424 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.424.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.424" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.424/>Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints</a></strong><br><a href=/people/c/chun-zeng/>Chun Zeng</a>
|
<a href=/people/j/jiangjie-chen/>Jiangjie Chen</a>
|
<a href=/people/t/tianyi-zhuang/>Tianyi Zhuang</a>
|
<a href=/people/r/rui-xu/>Rui Xu</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/q/qin-ying/>Qin Ying</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/y/yanghua-xiao/>Yanghua Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--424><div class="card-body p-3 small">Lexically constrained neural machine translation (NMT) draws much industrial attention for its practical usage in specific domains. However, current autoregressive approaches suffer from high latency. In this paper, we focus on non-autoregressive translation (NAT) for this problem for its efficiency advantage. We identify that current constrained NAT models, which are based on iterative editing, do not handle low-frequency constraints well. To this end, we propose a plug-in algorithm for this line of work, i.e., Aligned Constrained Training (ACT), which alleviates this problem by familiarizing the model with the source-side context of the constraints. Experiments on the general and domain datasets show that our model improves over the backbone constrained NAT model in constraint preservation and translation quality, especially for rare constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.425.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--425 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.425.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.425/>What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured</a></strong><br><a href=/people/a/alexander-henlein/>Alexander Henlein</a>
|
<a href=/people/a/alexander-mehler/>Alexander Mehler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--425><div class="card-body p-3 small">Transformer-based models are now predominant in NLP.They outperform approaches based on static models in many respects. This success has in turn prompted research that reveals a number of biases in the language models generated by transformers. In this paper we utilize this research on biases to investigate to what extent transformer-based language models allow for extracting knowledge about object relations (X occurs in Y; X consists of Z; action A involves using X).To this end, we compare contextualized models with their static counterparts. We make this comparison dependent on the application of a number of similarity measures and classifiers. Our results are threefold:Firstly, we show that the models combined with the different similarity measures differ greatly in terms of the amount of knowledge they allow for extracting. Secondly, our results suggest that similarity measures perform much worse than classifier-based approaches. Thirdly, we show that, surprisingly, static models perform almost as well as contextualized models – in some cases even better.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.426.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--426 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.426.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.426/>Less is More: Learning to Refine Dialogue History for Personalized Dialogue Generation</a></strong><br><a href=/people/h/hanxun-zhong/>Hanxun Zhong</a>
|
<a href=/people/z/zhicheng-dou/>Zhicheng Dou</a>
|
<a href=/people/y/yutao-zhu/>Yutao Zhu</a>
|
<a href=/people/h/hongjin-qian/>Hongjin Qian</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--426><div class="card-body p-3 small">Personalized dialogue systems explore the problem of generating responses that are consistent with the user’s personality, which has raised much attention in recent years. Existing personalized dialogue systems have tried to extract user profiles from dialogue history to guide personalized response generation. Since the dialogue history is usually long and noisy, most existing methods truncate the dialogue history to model the user’s personality. Such methods can generate some personalized responses, but a large part of dialogue history is wasted, leading to sub-optimal performance of personalized response generation. In this work, we propose to refine the user dialogue history on a large scale, based on which we can handle more dialogue history and obtain more abundant and accurate persona information. Specifically, we design an MSP model which consists of three personal information refiners and a personalized response generator. With these multi-level refiners, we can sparsely extract the most valuable information (tokens) from the dialogue history and leverage other similar users’ data to enhance personalization. Experimental results on two real-world datasets demonstrate the superiority of our model in generating more informative and personalized responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.427.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--427 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.427.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.427" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.427/>A Holistic Framework for Analyzing the <span class=acl-fixed-case>COVID</span>-19 Vaccine Debate</a></strong><br><a href=/people/m/maria-leonor-pacheco/>Maria Leonor Pacheco</a>
|
<a href=/people/t/tunazzina-islam/>Tunazzina Islam</a>
|
<a href=/people/m/monal-mahajan/>Monal Mahajan</a>
|
<a href=/people/a/andrey-shor/>Andrey Shor</a>
|
<a href=/people/m/ming-yin/>Ming Yin</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--427><div class="card-body p-3 small">The Covid-19 pandemic has led to infodemic of low quality information leading to poor health decisions. Combating the outcomes of this infodemic is not only a question of identifying false claims, but also reasoning about the decisions individuals make. In this work we propose a holistic analysis framework connecting stance and reason analysis, and fine-grained entity level moral sentiment analysis. We study how to model the dependencies between the different level of analysis and incorporate human insights into the learning process. Experiments show that our framework provides reliable predictions even in the low-supervision settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.428.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--428 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.428.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.428" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.428/>Learning to Win Lottery Tickets in <span class=acl-fixed-case>BERT</span> Transfer via Task-agnostic Mask Training</a></strong><br><a href=/people/y/yuanxin-liu/>Yuanxin Liu</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/z/zheng-lin/>Zheng Lin</a>
|
<a href=/people/p/peng-fu/>Peng Fu</a>
|
<a href=/people/y/yanan-cao/>Yanan Cao</a>
|
<a href=/people/w/weiping-wang/>Weiping Wang</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--428><div class="card-body p-3 small">Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained language models (PLMs) like BERT contain matching subnetworks that have similar transfer learning performance as the original PLM. These subnetworks are found using magnitude-based pruning. In this paper, we find that the BERT subnetworks have even more potential than these studies have shown. Firstly, we discover that the success of magnitude pruning can be attributed to the preserved pre-training performance, which correlates with the downstream transferability. Inspired by this, we propose to directly optimize the subnetwork structure towards the pre-training objectives, which can better preserve the pre-training performance. Specifically, we train binary masks over model weights on the pre-training tasks, with the aim of preserving the universal transferability of the subnetwork, which is agnostic to any specific downstream tasks. We then fine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The results show that, compared with magnitude pruning, mask training can effectively find BERT subnetworks with improved overall performance on downstream tasks. Moreover, our method is also more efficient in searching subnetworks and more advantageous when fine-tuning within a certain range of data scarcity. Our code is available at <a href=https://github.com/llyx97/TAMT class=acl-markup-url>https://github.com/llyx97/TAMT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.429.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--429 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.429.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.429.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.429" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.429/>You Don’t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers’ Private Personas</a></strong><br><a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a>
|
<a href=/people/l/lixin-fan/>Lixin Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--429><div class="card-body p-3 small">Social chatbots, also known as chit-chat chatbots, evolve rapidly with large pretrained language models. Despite the huge progress, privacy concerns have arisen recently: training data of large language models can be extracted via model inversion attacks. On the other hand, the datasets used for training chatbots contain many private conversations between two individuals. In this work, we further investigate the privacy leakage of the hidden states of chatbots trained by language modeling which has not been well studied yet. We show that speakers’ personas can be inferred through a simple neural network with high accuracy. To this end, we propose effective defense objectives to protect persona leakage from hidden states. We conduct extensive experiments to demonstrate that our proposed defense objectives can greatly reduce the attack accuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve language models’ powerful generation ability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.430.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--430 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.430 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.430.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.430/>Explaining Dialogue Evaluation Metrics using Adversarial Behavioral Analysis</a></strong><br><a href=/people/b/baber-khalid/>Baber Khalid</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--430><div class="card-body p-3 small">There is an increasing trend in using neural methods for dialogue model evaluation. Lack of a framework to investigate these metrics can cause dialogue models to reflect their biases and cause unforeseen problems during interactions. In this work, we propose an adversarial test-suite which generates problematic variations of various dialogue aspects, e.g. logical entailment, using automatic heuristics. We show that dialogue metrics for both open-domain and task-oriented settings are biased in their assessments of different conversation behaviors and fail to properly penalize problematic conversations, by analyzing their assessments of these problematic examples. We conclude that variability in training methodologies and data-induced biases are some of the main causes of these problems. We also conduct an investigation into the metric behaviors using a black-box interpretability model which corroborates our findings and provides evidence that metrics pay attention to the problematic conversational constructs signaling a misunderstanding of different conversation semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.431.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--431 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.431.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.431/>Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection</a></strong><br><a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/l/laura-vianna/>Laura Vianna</a>
|
<a href=/people/x/xuhui-zhou/>Xuhui Zhou</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--431><div class="card-body p-3 small">The perceived toxicity of language can vary based on someone’s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system’s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.432.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--432 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.432.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.432.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.432/>Non-Autoregressive <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>ASR</span> Error Correction with Phonological Training</a></strong><br><a href=/people/z/zheng-fang/>Zheng Fang</a>
|
<a href=/people/r/ruiqing-zhang/>Ruiqing Zhang</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/y/yanan-cao/>Yanan Cao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--432><div class="card-body p-3 small">Automatic Speech Recognition (ASR) is an efficient and widely used input method that transcribes speech signals into text. As the errors introduced by ASR systems will impair the performance of downstream tasks, we introduce a post-processing error correction method, PhVEC, to correct errors in text space. For the errors in ASR result, existing works mainly focus on fixed-length corrections, modifying each wrong token to a correct one (one-to-one correction), but rarely consider the variable-length correction (one-to-many or many-to-one correction). In this paper, we propose an efficient non-autoregressive (NAR) method for Chinese ASR error correction for both cases. Instead of conventionally predicting the sentence length in NAR methods, we propose a novel approach that uses phonological tokens to extend the source sentence for variable-length correction, enabling our model to generate phonetically similar corrections. Experimental results on datasets of different domains show that our method achieves significant improvement in word error rate reduction and speeds up the inference by 6.2 times compared with the autoregressive model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.433.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--433 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.433.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.433" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.433/>Hate Speech and Counter Speech Detection: Conversational Context Does Matter</a></strong><br><a href=/people/x/xinchen-yu/>Xinchen Yu</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a>
|
<a href=/people/l/lingzi-hong/>Lingzi Hong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--433><div class="card-body p-3 small">Hate speech is plaguing the cyberspace along with user-generated content. Adding counter speech has become an effective way to combat hate speech online. Existing datasets and models target either (a) hate speech or (b) hate and counter speech but disregard the context. This paper investigates the role of context in the annotation and detection of online hate and counter speech, where context is defined as the preceding comment in a conversation thread. We created a context-aware dataset for a 3-way classification task on Reddit comments: hate speech, counter speech, or neutral. Our analyses indicate that context is critical to identify hate and counter speech: human judgments change for most comments depending on whether we show annotators the context. A linguistic analysis draws insights into the language people use to express hate and counter speech. Experimental results show that neural networks obtain significantly better results if context is taken into account. We also present qualitative error analyses shedding light into (a) when and why context is beneficial and (b) the remaining errors made by our best model when context is taken into account.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.434.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--434 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.434.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.434/><span class=acl-fixed-case>DACSA</span>: A large-scale Dataset for Automatic summarization of <span class=acl-fixed-case>C</span>atalan and <span class=acl-fixed-case>S</span>panish newspaper Articles</a></strong><br><a href=/people/e/encarnacion-segarra-soriano/>Encarnación Segarra Soriano</a>
|
<a href=/people/v/vicent-ahuir/>Vicent Ahuir</a>
|
<a href=/people/l/lluis-f-hurtado/>Lluís-F. Hurtado</a>
|
<a href=/people/j/jose-gonzalez/>José González</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--434><div class="card-body p-3 small">The application of supervised methods to automatic summarization requires the availability of adequate corpora consisting of a set of document-summary pairs. As in most Natural Language Processing tasks, the great majority of available datasets for summarization are in English, making it difficult to develop automatic summarization models for other languages. Although Spanish is gradually forming part of some recent summarization corpora, it is not the same for minority languages such as Catalan. In this work, we describe the construction of a corpus of Catalan and Spanish newspapers, the Dataset for Automatic summarization of Catalan and Spanish newspaper Articles (DACSA) corpus. It is a high-quality large-scale corpus that can be used to train summarization models for Catalan and Spanish.We have carried out an analysis of the corpus, both in terms of the style of the summaries and the difficulty of the summarization task. In particular, we have used a set of well-known metrics in the summarization field in order to characterize the corpus. Additionally, for benchmarking purposes, we have evaluated the performances of some extractive and abstractive summarization systems on the DACSA corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.435.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--435 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.435.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.435/>Time Waits for No One! Analysis and Challenges of Temporal Misalignment</a></strong><br><a href=/people/k/kelvin-luu/>Kelvin Luu</a>
|
<a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/s/suchin-gururangan/>Suchin Gururangan</a>
|
<a href=/people/k/karishma-mandyam/>Karishma Mandyam</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--435><div class="card-body p-3 small">When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain-specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal misalignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously reported. We also find that, while temporal adaptation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.436.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--436 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.436.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.436" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.436/><span class=acl-fixed-case>MCSE</span>: <span class=acl-fixed-case>M</span>ultimodal Contrastive Learning of Sentence Embeddings</a></strong><br><a href=/people/m/miaoran-zhang/>Miaoran Zhang</a>
|
<a href=/people/m/marius-mosbach/>Marius Mosbach</a>
|
<a href=/people/d/david-adelani/>David Adelani</a>
|
<a href=/people/m/michael-hedderich/>Michael Hedderich</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--436><div class="card-body p-3 small">Learning semantically meaningful sentence embeddings is an open problem in natural language processing. In this work, we propose a sentence embedding learning approach that exploits both visual and textual information via a multimodal contrastive objective. Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. In particular, combining a small amount of multimodal data with a large text-only corpus, we improve the state-of-the-art average Spearman’s correlation by 1.7%. By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.437.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--437 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.naacl-main.437.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.437.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.437" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.437/><span class=acl-fixed-case>H</span>i<span class=acl-fixed-case>URE</span>: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction</a></strong><br><a href=/people/s/shuliang-liu/>Shuliang Liu</a>
|
<a href=/people/x/xuming-hu/>Xuming Hu</a>
|
<a href=/people/c/chenwei-zhang/>Chenwei Zhang</a>
|
<a href=/people/s/shuang-li/>Shu’ang Li</a>
|
<a href=/people/l/lijie-wen/>Lijie Wen</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--437><div class="card-body p-3 small">Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar. To overcome these defects, we propose a novel contrastive learning framework named HiURE, which has the capability to derive hierarchical signals from relational feature space using cross hierarchy attention and effectively optimize relation representation of sentences under exemplar-wise contrastive learning. Experimental results on two public datasets demonstrate the advanced effectiveness and robustness of HiURE on unsupervised relation extraction when compared with state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.438.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--438 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.438.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.438" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.438/>Diagnosing Vision-and-Language Navigation: What Really Matters</a></strong><br><a href=/people/w/wanrong-zhu/>Wanrong Zhu</a>
|
<a href=/people/y/yuankai-qi/>Yuankai Qi</a>
|
<a href=/people/p/pradyumna-narayana/>Pradyumna Narayana</a>
|
<a href=/people/k/kazoo-sone/>Kazoo Sone</a>
|
<a href=/people/s/sugato-basu/>Sugato Basu</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/q/qi-wu/>Qi Wu</a>
|
<a href=/people/m/miguel-eckstein/>Miguel Eckstein</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--438><div class="card-body p-3 small">Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, there still exist non-negligible gaps between machines’ performance and human benchmarks. Moreover, the agents’ inner mechanisms for navigation decisions remain unclear. To the best of our knowledge, how the agents perceive the multimodal input is under-studied and needs investigation. In this work, we conduct a series of diagnostic experiments to unveil agents’ focus during navigation. Results show that indoor navigation agents refer to both object and direction tokens when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and poorly understand the object tokens. Transformer-based agents acquire a better cross-modal understanding of objects and display strong numerical reasoning ability than non-Transformer-based agents. When it comes to vision-and-language alignments, many models claim that they can align object tokens with specific visual targets. We find unbalanced attention on the vision and text input and doubt the reliability of such cross-modal alignments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.439.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--439 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.439.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.439/>Aligning to Social Norms and Values in Interactive Narratives</a></strong><br><a href=/people/p/prithviraj-ammanabrolu/>Prithviraj Ammanabrolu</a>
|
<a href=/people/l/liwei-jiang/>Liwei Jiang</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--439><div class="card-body p-3 small">We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games—environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms—causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of people—in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others. We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.440.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.440.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--440 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.440 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.440.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.440" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.440/><span class=acl-fixed-case>MOVER</span>: Mask, Over-generate and Rank for Hyperbole Generation</a></strong><br><a href=/people/y/yunxiang-zhang/>Yunxiang Zhang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--440><div class="card-body p-3 small">Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale English hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole generation that does not require parallel literal-hyperbole pairs. During training, we fine-tune BART to infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of an input literal sentence and over-generate multiple possible hyperbolic versions. Then a BERT-based ranker selects the best candidate by hyperbolicity and paraphrase quality. Automatic and human evaluation results show that our model is effective at generating hyperbolic paraphrase sentences and outperforms several baseline systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.441.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--441 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.441.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-main.441" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.441/>Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference</a></strong><br><a href=/people/e/emils-kadikis/>Emīls Kadiķis</a>
|
<a href=/people/v/vaibhav-srivastav/>Vaibhav Srivastav</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--441><div class="card-body p-3 small">The task of natural language inference (NLI), to decide if a hypothesis entails or contradicts a premise, received considerable attention in recent years. All competitive systems build on top of contextualized representations and make use of transformer architectures for learning an NLI model. When somebody is faced with a particular NLI task, they need to select the best model that is available. This is a time-consuming and resource-intense endeavour. To solve this practical problem, we propose a simple method for predicting the performance without actually fine-tuning the model. We do this by testing how well the pre-trained models perform on the aNLI task when just comparing sentence embeddings with cosine similarity to what kind of performance is achieved when training a classifier on top of these embeddings. We show that the accuracy of the cosine similarity approach correlates strongly with the accuracy of the classification approach with a Pearson correlation coefficient of 0.65. Since the similarity is orders of magnitude faster to compute on a given dataset (less than a minute vs. hours), our method can lead to significant time savings in the process of model selection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-main.442.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-main--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-main.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-main.442.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-main.442/>Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics</a></strong><br><a href=/people/d/daniel-deutsch/>Daniel Deutsch</a>
|
<a href=/people/r/rotem-dror/>Rotem Dror</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-main--442><div class="card-body p-3 small">How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice. This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small.</div></div></div><hr><div id=2022naacl-srw><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.naacl-srw.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.naacl-srw/>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.0/>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop</a></strong><br><a href=/people/d/daphne-ippolito/>Daphne Ippolito</a>
|
<a href=/people/l/liunian-harold-li/>Liunian Harold Li</a>
|
<a href=/people/m/maria-leonor-pacheco/>Maria Leonor Pacheco</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.1/>Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention</a></strong><br><a href=/people/a/ayush-k-chakravarthy/>Ayush K Chakravarthy</a>
|
<a href=/people/j/jacob-labe-russin/>Jacob Labe Russin</a>
|
<a href=/people/r/randall-oreilly/>Randall O’Reilly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--1><div class="card-body p-3 small">Systematicity is thought to be a key inductive bias possessed by humans that is lacking in standard natural language processing systems such as those utilizing transformers. In this work, we investigate the extent to which the failure of transformers on systematic generalization tests can be attributed to a lack of linguistic abstraction in its attention mechanism. We develop a novel modification to the transformer by implementing two separate input streams: a role stream controls the attention distributions (i.e., queries and keys) at each layer, and a filler stream determines the values. Our results show that when abstract role labels are assigned to input sequences and provided to the role stream, systematic generalization is improved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.2/>Grounding in social media: An approach to building a chit-chat dialogue model</a></strong><br><a href=/people/r/ritvik-choudhary/>Ritvik Choudhary</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--2><div class="card-body p-3 small">Building open-domain dialogue systems capable of rich human-like conversational ability is one of the fundamental challenges in language generation. However, even with recent advancements in the field, existing open-domain generative models fail to capture and utilize external knowledge, leading to repetitive or generic responses to unseen utterances. Current work on knowledge-grounded dialogue generation primarily focuses on persona incorporation or searching a fact-based structured knowledge source such as Wikipedia. Our method takes a broader and simpler approach, which aims to improve the raw conversation ability of the system by mimicking the human response behavior through casual interactions found on social media. Utilizing a joint retriever-generator setup, the model queries a large set of filtered comment data from Reddit to act as additional context for the seq2seq generator. Automatic and human evaluations on open-domain dialogue datasets demonstrate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.3/><span class=acl-fixed-case>E</span>xtra<span class=acl-fixed-case>P</span>hrase: Efficient Data Augmentation for Abstractive Summarization</a></strong><br><a href=/people/m/mengsay-loem/>Mengsay Loem</a>
|
<a href=/people/s/sho-takase/>Sho Takase</a>
|
<a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--3><div class="card-body p-3 small">Neural models trained with large amount of parallel data have achieved impressive performance in abstractive summarization tasks. However, large-scale parallel corpora are expensive and challenging to construct. In this work, we introduce a low-cost and effective strategy, ExtraPhrase, to augment training data for abstractive summarization tasks. ExtraPhrase constructs pseudo training data in two steps: extractive summarization and paraphrasing. We extract major parts of an input text in the extractive summarization step and obtain its diverse expressions with the paraphrasing step. Through experiments, we show that ExtraPhrase improves the performance of abstractive summarization tasks by more than 0.50 points in ROUGE scores compared to the setting without data augmentation. ExtraPhrase also outperforms existing methods such as back-translation and self-training. We also show that ExtraPhrase is significantly effective when the amount of genuine training data is remarkably small, i.e., a low-resource setting. Moreover, ExtraPhrase is more cost-efficient than the existing approaches</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.4/>Regularized Training of Nearest Neighbor Language Models</a></strong><br><a href=/people/j/jean-francois-ton/>Jean-Francois Ton</a>
|
<a href=/people/w/walter-talbott/>Walter Talbott</a>
|
<a href=/people/s/shuangfei-zhai/>Shuangfei Zhai</a>
|
<a href=/people/j/joshua-m-susskind/>Joshua M. Susskind</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--4><div class="card-body p-3 small">Including memory banks in a natural language processing architecture increases model capacity by equipping it with additional data at inference time. In this paper, we build upon <span class=tex-math>k</span>NN-LM (CITATION), which uses a pre-trained language model together with an exhaustive <span class=tex-math>k</span>NN search through the training data (memory bank) to achieve state-of-the-art results. We investigate whether we can improve the <span class=tex-math>k</span>NN-LM performance by instead training a LM with the knowledge that we will be using a <span class=tex-math>k</span>NN post-hoc. We achieved significant improvement using our method on language modeling tasks on WIKI-2 and WIKI-103. The main phenomenon that we encounter is that adding a simple L2 regularization on the activations (not weights) of the model, a transformer, improves the post-hoc <span class=tex-math>k</span>NN classification performance. We explore some possible reasons for this improvement. In particular, we find that the added L2 regularization seems to improve the performance for high-frequency words without deteriorating the performance for low frequency ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.5/>“Again, Dozens of Refugees Drowned”: A Computational Study of Political Framing Evoked by Presuppositions</a></strong><br><a href=/people/q/qi-yu/>Qi Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--5><div class="card-body p-3 small">Earlier NLP studies on framing in political discourse have focused heavily on shallow classification of issue framing, while framing effect arising from pragmatic cues remains neglected. We put forward this latter type of framing as “pragmatic framing”. To bridge this gap, we take presupposition-triggering adverbs such as ‘again’ as a study case, and quantitatively investigate how different German newspapers use them to covertly evoke different attitudinal subtexts in their report on the event “European Refugee Crisis” (2014-2018). Our study demonstrates the crucial role of presuppositions in framing, and emphasizes the necessity of more attention on pragmatic framing in the research of automated framing detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.6/>Methods for Estimating and Improving Robustness of Language Models</a></strong><br><a href=/people/m/michal-stefanik/>Michal Stefanik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--6><div class="card-body p-3 small">Despite their outstanding performance, large language models (LLMs) suffer notorious flaws related to their preference for shallow textual relations over full semantic complexity of the problem. This proposal investigates a common denominator of this problem in their weak ability to generalise outside of the training domain. We survey diverse research directions providing estimations of model generalisation ability and find that incorporating some of these measures in the training objectives leads to enhanced distributional robustness of neural models. Based on these findings, we present future research directions enhancing the robustness of LLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.7/>Retrieval-augmented Generation across Heterogeneous Knowledge</a></strong><br><a href=/people/w/wenhao-yu/>Wenhao Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--7><div class="card-body p-3 small">Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.8/>Neural Retriever and Go Beyond: A Thesis Proposal</a></strong><br><a href=/people/m/man-luo/>Man Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--8><div class="card-body p-3 small">Information Retriever (IR) aims to find the relevant documents (e.g. snippets, passages, and articles) to a given query at large scale. IR plays an important role in many tasks such as open domain question answering and dialogue systems, where external knowledge is needed. In the past, searching algorithms based on term matching have been widely used. Recently, neural-based algorithms (termed as neural retrievers) have gained more attention which can mitigate the limitations of traditional methods. Regardless of the success achieved by neural retrievers, they still face many challenges, e.g. suffering from a small amount of training data and failing to answer simple entity-centric questions. Furthermore, most of the existing neural retrievers are developed for pure-text query. This prevents them from handling multi-modality queries (i.e. the query is composed of textual description and images). This proposal has two goals. First, we introduce methods to address the abovementioned issues of neural retrievers from three angles, new model architectures, IR-oriented pretraining tasks, and generating large scale training data. Second, we identify the future research direction and propose potential corresponding solution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.9/>Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation</a></strong><br><a href=/people/x/xiruo-ding/>Xiruo Ding</a>
|
<a href=/people/k/kevin-lybarger/>Kevin Lybarger</a>
|
<a href=/people/j/justin-tauscher/>Justin Tauscher</a>
|
<a href=/people/t/trevor-cohen/>Trevor Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--9><div class="card-body p-3 small">Cognitive distortions are counterproductive patterns of thinking that are one of the targets of cognitive behavioral therapy (CBT). These can be challenging for clinicians to detect, especially those without extensive CBT training or supervision. Text classification methods can approximate expert clinician judgment in the detection of frequently occurring cognitive distortions in text-based therapy messages. However, performance with infrequent distortions is relatively poor. In this study, we address this sparsity problem with two approaches: Data Augmentation and Domain-Specific Model. The first approach includes Easy Data Augmentation, back translation, and mixup techniques. The second approach utilizes a domain-specific pretrained language model, MentalBERT. To examine the viability of different data augmentation methods, we utilized a real-world dataset of texts between therapists and clients diagnosed with serious mental illness that was annotated for distorted thinking. We found that with optimized parameter settings, mixup was helpful for rare classes. Performance improvements with an augmented model, MentalBERT, exceed those obtained with data augmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.10/>Generate, Evaluate, and Select: A Dialogue System with a Response Evaluator for Diversity-Aware Response Generation</a></strong><br><a href=/people/r/ryoma-sakaeda/>Ryoma Sakaeda</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--10><div class="card-body p-3 small">We aim to overcome the lack of diversity in responses of current dialogue systems and to develop a dialogue system that is engaging as a conversational partner. We propose a generator-evaluator model that evaluates multiple responses generated by a response generator and selects the best response by an evaluator. By generating multiple responses, we obtain diverse responses. We conduct human evaluations to compare the output of the proposed system with that of a baseline system. The results of the human evaluations showed that the proposed system’s responses were often judged to be better than the baseline system’s, and indicated the effectiveness of the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.11.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-srw.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.11/>Impact of Training Instance Selection on Domain-Specific Entity Extraction using <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/e/eileen-salhofer/>Eileen Salhofer</a>
|
<a href=/people/x/xing-lan-liu/>Xing Lan Liu</a>
|
<a href=/people/r/roman-kern/>Roman Kern</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--11><div class="card-body p-3 small">State of the art performances for entity extraction tasks are achieved by supervised learning, specifically, by fine-tuning pretrained language models such as BERT. As a result, annotating application specific data is the first step in many use cases. However, no practical guidelines are available for annotation requirements. This work supports practitioners by empirically answering the frequently asked questions (1) how many training samples to annotate? (2) which examples to annotate? We found that BERT achieves up to 80% F1 when fine-tuned on only 70 training examples, especially on biomedical domain. The key features for guiding the selection of high performing training instances are identified to be pseudo-perplexity and sentence-length. The best training dataset constructed using our proposed selection strategy shows F1 score that is equivalent to a random selection with twice the sample size. The requirement of only a small number of training data implies cheaper implementations and opens door to wider range of applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.12/>Analysing the Correlation between Lexical Ambiguity and Translation Quality in a Multimodal Setting using <span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/a/ali-hatami/>Ali Hatami</a>
|
<a href=/people/p/paul-buitelaar/>Paul Buitelaar</a>
|
<a href=/people/m/mihael-arcan/>Mihael Arcan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--12><div class="card-body p-3 small">Multimodal Neural Machine Translation is focusing on using visual information to translate sentences in the source language into the target language. The main idea is to utilise information from visual modalities to promote the output quality of the text-based translation model. Although the recent multimodal strategies extract the most relevant visual information in images, the effectiveness of using visual information on translation quality changes based on the text dataset. Due to this, this work studies the impact of leveraging visual information in multimodal translation models of ambiguous sentences. Our experiments analyse the Multi30k evaluation dataset and calculate ambiguity scores of sentences based on the WordNet hierarchical structure. To calculate the ambiguity of a sentence, we extract the ambiguity scores for all nouns based on the number of senses in WordNet. The main goal is to find in which sentences, visual content can improve the text-based translation model. We report the correlation between the ambiguity scores and translation quality extracted for all sentences in the English-German dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.13.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.13/>Building a Personalized Dialogue System with Prompt-Tuning</a></strong><br><a href=/people/t/tomohito-kasahara/>Tomohito Kasahara</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/n/nguyen-tung/>Nguyen Tung</a>
|
<a href=/people/s/shengzhe-li/>Shengzhe Li</a>
|
<a href=/people/k/kenta-shinzato/>Kenta Shinzato</a>
|
<a href=/people/t/toshinori-sato/>Toshinori Sato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--13><div class="card-body p-3 small">Dialogue systems without consistent responses are not attractive. In this study, we build a dialogue system that can respond based on a given character setting (persona) to bring consistency. Considering the trend of the rapidly increasing scale of language models, we propose an approach that uses prompt-tuning, which has low learning costs, on pre-trained large-scale language models. The results of the automatic and manual evaluations in English and Japanese show that it is possible to build a dialogue system with more natural and personalized responses with less computational resources than fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.14.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-srw.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.14/><span class=acl-fixed-case>MM</span>-<span class=acl-fixed-case>GATBT</span>: Enriching Multimodal Representation Using Graph Attention Network</a></strong><br><a href=/people/s/seung-byum-seo/>Seung Byum Seo</a>
|
<a href=/people/h/hyoungwook-nam/>Hyoungwook Nam</a>
|
<a href=/people/p/payam-delgosha/>Payam Delgosha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--14><div class="card-body p-3 small">While there have been advances in Natural Language Processing (NLP), their success is mainly gained by applying a self-attention mechanism into single or multi-modalities. While this approach has brought significant improvements in multiple downstream tasks, it fails to capture the interaction between different entities. Therefore, we propose MM-GATBT, a multimodal graph representation learning model that captures not only the relational semantics within one modality but also the interactions between different modalities. Specifically, the proposed method constructs image-based node embedding which contains relational semantics of entities. Our empirical results show that MM-GATBT achieves state-of-the-art results among all published papers on the MM-IMDb dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.15.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.15/>Simulating Feature Structures with Simple Types</a></strong><br><a href=/people/v/valentin-d-richard/>Valentin D. Richard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--15><div class="card-body p-3 small">Feature structures have been several times considered to enrich categorial grammars in order to build fine-grained grammars. Most attempts to unify both frameworks either model categorial types as feature structures or add feature structures on top of categorial types. We pursue a different approach: using feature structure as categorial atomic types. In this article, we present a procedure to create, from a simplified HPSG grammar, an equivalent abstract categorial grammar (ACG). We represent a feature structure by the enumeration of its totally well-typed upper bounds, so that unification can be simulated as intersection. We implement this idea as a meta-ACG preprocessor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.16.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.16/>Dr. Livingstone, <span class=acl-fixed-case>I</span> presume? Polishing of foreign character identification in literary texts</a></strong><br><a href=/people/a/aleksandra-konovalova/>Aleksandra Konovalova</a>
|
<a href=/people/a/antonio-toral/>Antonio Toral</a>
|
<a href=/people/k/kristiina-taivalkoski-shilov/>Kristiina Taivalkoski-Shilov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--16><div class="card-body p-3 small">Character identification is a key element for many narrative-related tasks. To implement it, the baseform of the name of the character (or lemma) needs to be identified, so different appearances of the same character in the narrative could be aligned. In this paper we tackle this problem in translated texts (English–Finnish translation direction), where the challenge regarding lemmatizing foreign names in an agglutinative language appears. To solve this problem, we present and compare several methods. The results show that the method based on a search for the shortest version of the name proves to be the easiest, best performing (83.4% F1), and most resource-independent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.17.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.17/>Zuo Zhuan <span class=acl-fixed-case>A</span>ncient <span class=acl-fixed-case>C</span>hinese Dataset for Word Sense Disambiguation</a></strong><br><a href=/people/x/xiaomeng-pan/>Xiaomeng Pan</a>
|
<a href=/people/h/hongfei-wang/>Hongfei Wang</a>
|
<a href=/people/t/teruaki-oka/>Teruaki Oka</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--17><div class="card-body p-3 small">Word Sense Disambiguation (WSD) is a core task in Natural Language Processing (NLP). Ancient Chinese has rarely been used in WSD tasks, however, as no public dataset for ancient Chinese WSD tasks exists. Creation of an ancient Chinese dataset is considered a significant challenge because determining the most appropriate sense in a context is difficult and time-consuming owing to the different usages in ancient and modern Chinese. Actually, no public dataset for ancient Chinese WSD tasks exists. To solve the problem of ancient Chinese WSD, we annotate part of Pre-Qin (221 BC) text <i>Zuo Zhuan</i> using a copyright-free dictionary to create a public sense-tagged dataset. Then, we apply a simple Nearest Neighbors (k-NN) method using a pre-trained language model to the dataset. Our code and dataset will be available on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.18.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-srw.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.18/><span class=acl-fixed-case>V</span>i<span class=acl-fixed-case>T</span>5: Pretrained Text-to-Text Transformer for <span class=acl-fixed-case>V</span>ietnamese Language Generation</a></strong><br><a href=/people/l/long-phan/>Long Phan</a>
|
<a href=/people/h/hieu-tran/>Hieu Tran</a>
|
<a href=/people/h/hieu-nguyen/>Hieu Nguyen</a>
|
<a href=/people/t/trieu-h-trinh/>Trieu H. Trinh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--18><div class="card-body p-3 small">We present ViT5, a pretrained Transformer-based encoder-decoder model for the Vietnamese language. With T5-style self-supervised pretraining, ViT5 is trained on a large corpus of high-quality and diverse Vietnamese texts. We benchmark ViT5 on two downstream text generation tasks, Abstractive Text Summarization and Named Entity Recognition. Although Abstractive Text Summarization has been widely studied for the English language thanks to its rich and large source of data, there has been minimal research into the same task in Vietnamese, a much lower resource language. In this work, we perform exhaustive experiments on both Vietnamese Abstractive Summarization and Named Entity Recognition, validating the performance of ViT5 against many other pretrained Transformer-based encoder-decoder models. Our experiments show that ViT5 significantly outperforms existing models and achieves state-of-the-art results on Vietnamese Text Summarization. On the task of Named Entity Recognition, ViT5 is competitive against previous best results from pretrained encoder-based Transformer models. Further analysis shows the importance of context length during the self-supervised pretraining on downstream performance across different settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.19.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-srw.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.19/>Compositional Generalization in Grounded Language Learning via Induced Model Sparsity</a></strong><br><a href=/people/s/sam-spilsbury/>Sam Spilsbury</a>
|
<a href=/people/a/alexander-ilin/>Alexander Ilin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--19><div class="card-body p-3 small">We provide a study of how induced model sparsity can help achieve compositional generalization and better sample efficiency in grounded language learning problems. We consider simple language-conditioned navigation problems in a grid world environment with disentangled observations. We show that standard neural architectures do not always yield compositional generalization. To address this, we design an agent that contains a goal identification module that encourages sparse correlations between words in the instruction and attributes of objects, composing them together to find the goal. The output of the goal identification module is the input to a value iteration network planner. Our agent maintains a high level of performance on goals containing novel combinations of properties even when learning from a handful of demonstrations. We examine the internal representations of our agent and find the correct correspondences between words in its dictionary and attributes in the environment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.20.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.20/>How do people talk about images? A study on open-domain conversations with images.</a></strong><br><a href=/people/y/yi-pei-chen/>Yi-Pei Chen</a>
|
<a href=/people/n/nobuyuki-shimizu/>Nobuyuki Shimizu</a>
|
<a href=/people/t/takashi-miyazaki/>Takashi Miyazaki</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--20><div class="card-body p-3 small">This paper explores how humans conduct conversations with images by investigating an open-domain image conversation dataset, ImageChat. We examined the conversations with images from the perspectives of <span class=tex-math><em>image relevancy</em></span> and <span class=tex-math><em>image information</em></span>. We found that utterances/conversations are not always related to the given image, and conversation topics diverge within three turns about half of the time. Besides image objects, more comprehensive non-object image information is also indispensable. After inspecting the causes, we suggested that understanding the overall scenario of image and connecting objects based on their high-level attributes might be very helpful to generate more engaging open-domain conversations when an image is presented. We proposed enriching the image information with image caption and object tags based on our analysis. With our proposed <span class=tex-math><em>image</em><sup>+</sup></span> features, we improved automatic metrics including BLEU and Bert Score, and increased the diversity and image-relevancy of generated responses to the strong baseline. The result verifies that our analysis provides valuable insights and could facilitate future research on open-domain conversations with images.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.21.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.21/>Text Style Transfer for Bias Mitigation using Masked Language Modeling</a></strong><br><a href=/people/e/ewoenam-kwaku-tokpo/>Ewoenam Kwaku Tokpo</a>
|
<a href=/people/t/toon-calders/>Toon Calders</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--21><div class="card-body p-3 small">It is well known that textual data on the internet and other digital platforms contain significant levels of bias and stereotypes. Various research findings have concluded that biased texts have significant effects on target demographic groups. For instance, masculine-worded job advertisements tend to be less appealing to female applicants. In this paper, we present a text-style transfer model that can be trained on non-parallel data and be used to automatically mitigate bias in textual data. Our style transfer model improves on the limitations of many existing text style transfer techniques such as the loss of content information. Our model solves such issues by combining latent content encoding with explicit keyword replacement. We will show that this technique produces better content preservation whilst maintaining good style transfer accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.22.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.22/>Differentially Private Instance Encoding against Privacy Attacks</a></strong><br><a href=/people/s/shangyu-xie/>Shangyu Xie</a>
|
<a href=/people/y/yuan-hong/>Yuan Hong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--22><div class="card-body p-3 small">TextHide was recently proposed to protect the training data via instance encoding in natural language domain. Due to the lack of theoretic privacy guarantee, such instance encoding scheme has been shown to be vulnerable against privacy attacks, e.g., reconstruction attack. To address such limitation, we revise the instance encoding scheme with differential privacy and thus provide a provable guarantee against privacy attacks. The experimental results also show that the proposed scheme can defend against privacy attacks while ensuring learning utility (as a trade-off).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.23.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.23/>A Simple Approach to Jointly Rank Passages and Select Relevant Sentences in the <span class=acl-fixed-case>OBQA</span> Context</a></strong><br><a href=/people/m/man-luo/>Man Luo</a>
|
<a href=/people/s/shuguang-chen/>Shuguang Chen</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--23><div class="card-body p-3 small">In the open book question answering (OBQA) task, selecting the relevant passages and sentences from distracting information is crucial to reason the answer to a question. HotpotQA dataset is designed to teach and evaluate systems to do both passage ranking and sentence selection. Many existing frameworks use separate models to select relevant passages and sentences respectively. Such systems not only have high complexity in terms of the parameters of models but also fail to take the advantage of training these two tasks together since one task can be beneficial for the other one. In this work, we present a simple yet effective framework to address these limitations by jointly ranking passages and selecting sentences. Furthermore, we propose consistency and similarity constraints to promote the correlation and interaction between passage ranking and sentence selection. The experiments demonstrate that our framework can achieve competitive results with previous systems and outperform the baseline by 28% in terms of exact matching of relevant sentences on the HotpotQA dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.24.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.24/>Multimodal Modeling of Task-Mediated Confusion</a></strong><br><a href=/people/c/camille-mince/>Camille Mince</a>
|
<a href=/people/s/skye-rhomberg/>Skye Rhomberg</a>
|
<a href=/people/c/cecilia-alm/>Cecilia Alm</a>
|
<a href=/people/r/reynold-bailey/>Reynold Bailey</a>
|
<a href=/people/a/alex-ororbia/>Alex Ororbia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--24><div class="card-body p-3 small">In order to build more human-like cognitive agents, systems capable of detecting various human emotions must be designed to respond appropriately. Confusion, the combination of an emotional and cognitive state, is under-explored. In this paper, we build upon prior work to develop models that detect confusion from three modalities: video (facial features), audio (prosodic features), and text (transcribed speech features). Our research improves the data collection process by allowing for continuous (as opposed to discrete) annotation of confusion levels. We also craft models based on recurrent neural networks (RNNs) given their ability to predict sequential data. In our experiments, we find that text and video modalities are the most important in predicting confusion while the explored audio features are relatively unimportant predictors of confusion in our data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.25.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.25/>Probe-Less Probing of <span class=acl-fixed-case>BERT</span>’s Layer-Wise Linguistic Knowledge with Masked Word Prediction</a></strong><br><a href=/people/t/tatsuya-aoyama/>Tatsuya Aoyama</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--25><div class="card-body p-3 small">The current study quantitatively (and qualitatively for an illustrative purpose) analyzes BERT’s layer-wise masked word prediction on an English corpus, and finds that (1) the layerwise localization of linguistic knowledge primarily shown in probing studies is replicated in a behavior-based design and (2) that syntactic and semantic information is encoded at different layers for words of different syntactic categories. Hypothesizing that the above results are correlated with the number of likely potential candidates of the masked word prediction, we also investigate how the results differ for tokens within multiword expressions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.26.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.26.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.26/>Multimodal large language models for inclusive collaboration learning tasks</a></strong><br><a href=/people/a/armanda-lewis/>Armanda Lewis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--26><div class="card-body p-3 small">This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.27.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.27.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.27/>Neural Networks in a Product of Hyperbolic Spaces</a></strong><br><a href=/people/j/jun-takeuchi/>Jun Takeuchi</a>
|
<a href=/people/n/noriki-nishida/>Noriki Nishida</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--27><div class="card-body p-3 small">Machine learning in hyperbolic spaces has attracted much attention in natural language processing and many other fields. In particular, Hyperbolic Neural Networks (HNNs) have improved a wide variety of tasks, from machine translation to knowledge graph embedding. Although some studies have reported the effectiveness of embedding into the product of multiple hyperbolic spaces, HNNs have mainly been constructed in a single hyperbolic space, and their extension to product spaces has not been sufficiently studied. Therefore, we propose a novel method to extend a given HNN in a single space to a product of hyperbolic spaces. We apply our method to Hyperbolic Graph Convolutional Networks (HGCNs), extending several HNNs. Our model improved the graph node classification accuracy especially on datasets with tree-like structures. The results suggest that neural networks in a product of hyperbolic spaces can be more effective than in a single space in representing structural data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.28.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.28.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.28/>Explicit Use of Topicality in Dialogue Response Generation</a></strong><br><a href=/people/t/takumi-yoshikoshi/>Takumi Yoshikoshi</a>
|
<a href=/people/h/hayato-atarashi/>Hayato Atarashi</a>
|
<a href=/people/t/takashi-kodama/>Takashi Kodama</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--28><div class="card-body p-3 small">The current chat dialogue systems implicitly consider the topic given the context, but not explicitly. As a result, these systems often generate inconsistent responses with the topic of the moment. In this study, we propose a dialogue system that responds appropriately following the topic by selecting the entity with the highest “topicality.” In topicality estimation, the model is trained through self-supervised learning that regards entities that appear in both context and response as the topic entities. In response generation, the model is trained to generate topic-relevant responses based on the estimated topicality. Experimental results show that our proposed system can follow the topic more than the existing dialogue system that considers only the context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.29.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.29.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.29/>Automating Human Evaluation of Dialogue Systems</a></strong><br><a href=/people/s/sujan-reddy-a/>Sujan Reddy A</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--29><div class="card-body p-3 small">Automated metrics to evaluate dialogue systems like BLEU, METEOR, etc., weakly correlate with human judgments. Thus, human evaluation is often used to supplement these metrics for system evaluation. However, human evaluation is time-consuming as well as expensive. This paper provides an alternative approach to human evaluation with respect to three aspects: naturalness, informativeness, and quality in dialogue systems. I propose an approach based on fine-tuning the BERT model with three prediction heads, to predict whether the system-generated output is natural, fluent, and informative. I observe that the proposed model achieves an average accuracy of around 77% over these 3 labels. I also design a baseline approach that uses three different BERT models to make the predictions. Based on experimental analysis, I find that using a shared model to compute the three labels performs better than three separate models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.30.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.30.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-srw.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.30/>Strong Heuristics for Named Entity Linking</a></strong><br><a href=/people/m/marko-culjak/>Marko Čuljak</a>
|
<a href=/people/a/andreas-spitz/>Andreas Spitz</a>
|
<a href=/people/r/robert-west/>Robert West</a>
|
<a href=/people/a/akhil-arora/>Akhil Arora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--30><div class="card-body p-3 small">Named entity linking (NEL) in news is a challenging endeavour due to the frequency of unseen and emerging entities, which necessitates the use of unsupervised or zero-shot methods. However, such methods tend to come with caveats, such as no integration of suitable knowledge bases (like Wikidata) for emerging entities, a lack of scalability, and poor interpretability. Here, we consider person disambiguation in Quotebank, a massive corpus of speaker-attributed quotations from the news, and investigate the suitability of intuitive, lightweight, and scalable heuristics for NEL in web-scale corpora. Our best performing heuristic disambiguates 94% and 63% of the mentions on Quotebank and the AIDA-CoNLL benchmark, respectively. Additionally, the proposed heuristics compare favourably to the state-of-the-art unsupervised and zero-shot methods, Eigenthemes and mGENRE, respectively, thereby serving as strong baselines for unsupervised and zero-shot entity linking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.31.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.31.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.31/>Static and Dynamic Speaker Modeling based on Graph Neural Network for Emotion Recognition in Conversation</a></strong><br><a href=/people/p/prakhar-saxena/>Prakhar Saxena</a>
|
<a href=/people/y/yin-jou-huang/>Yin Jou Huang</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--31><div class="card-body p-3 small">Each person has a unique personality which affects how they feel and convey emotions. Hence, speaker modeling is important for the task of emotion recognition in conversation (ERC). In this paper, we propose a novel graph-based ERC model which considers both conversational context and speaker personality. We model the internal state of the speaker (personality) as Static and Dynamic speaker state, where the Dynamic speaker state is modeled with a graph neural network based encoder. Experiments on benchmark dataset shows the effectiveness of our model. Our model outperforms baseline and other graph-based methods. Analysis of results also show the importance of explicit speaker modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.32.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.32.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.32/>Few-shot fine-tuning <span class=acl-fixed-case>SOTA</span> summarization models for medical dialogues</a></strong><br><a href=/people/d/david-fraile-navarro/>David Fraile Navarro</a>
|
<a href=/people/m/mark-dras/>Mark Dras</a>
|
<a href=/people/s/shlomo-berkovsky/>Shlomo Berkovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--32><div class="card-body p-3 small">Abstractive summarization of medical dialogues presents a challenge for standard training approaches, given the paucity of suitable datasets. We explore the performance of state-of-the-art models with zero-shot and few-shot learning strategies and measure the impact of pretraining with general domain and dialogue-specific text on the summarization performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.33.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.33.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.33/>Unifying Parsing and Tree-Structured Models for Generating Sentence Semantic Representations</a></strong><br><a href=/people/a/antoine-simoulin/>Antoine Simoulin</a>
|
<a href=/people/b/benoit-crabbe/>Benoit Crabbé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--33><div class="card-body p-3 small">We introduce a novel tree-based model that learns its composition function together with its structure. The architecture produces sentence embeddings by composing words according to an induced syntactic tree. The parsing and the composition functions are explicitly connected and, therefore, learned jointly. As a result, the sentence embedding is computed according to an interpretable linguistic pattern and may be used on any downstream task. We evaluate our encoder on downstream tasks, and we observe that it outperforms tree-based models relying on external parsers. In some configurations, it is even competitive with Bert base model. Our model is capable of supporting multiple parser architectures. We exploit this property to conduct an ablation study by comparing different parser initializations. We explore to which extent the trees produced by our model compare with linguistic structures and how this initialization impacts downstream performances. We empirically observe that downstream supervision troubles producing stable parses and preserving linguistically relevant structures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.34.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.34.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.34/>Multiformer: A Head-Configurable Transformer-Based Model for Direct Speech Translation</a></strong><br><a href=/people/g/gerard-sant/>Gerard Sant</a>
|
<a href=/people/g/gerard-i-gallego/>Gerard I. Gállego</a>
|
<a href=/people/b/belen-alastruey/>Belen Alastruey</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta Ruiz Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--34><div class="card-body p-3 small">Transformer-based models have been achieving state-of-the-art results in several fields of Natural Language Processing. However, its direct application to speech tasks is not trivial. The nature of this sequences carries problems such as long sequence lengths and redundancy between adjacent tokens. Therefore, we believe that regular self-attention mechanism might not be well suited for it. Different approaches have been proposed to overcome these problems, such as the use of efficient attention mechanisms. However, the use of these methods usually comes with a cost, which is a performance reduction caused by information loss. In this study, we present the Multiformer, a Transformer-based model which allows the use of different attention mechanisms on each head. By doing this, the model is able to bias the self-attention towards the extraction of more diverse token interactions, and the information loss is reduced. Finally, we perform an analysis of the head contributions, and we observe that those architectures where all heads relevance is uniformly distributed obtain better results. Our results show that mixing attention patterns along the different heads and layers outperforms our baseline by up to 0.7 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.35.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.35.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.35/>Defending Compositionality in Emergent Languages</a></strong><br><a href=/people/m/michal-auersperger/>Michal Auersperger</a>
|
<a href=/people/p/pavel-pecina/>Pavel Pecina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--35><div class="card-body p-3 small">Compositionality has traditionally been understood as a major factor in productivity of language and, more broadly, human cognition. Yet, recently some research started to question its status showing that artificial neural networks are good at generalization even without noticeable compositional behavior. We argue some of these conclusions are too strong and/or incomplete. In the context of a two-agent communication game, we show that compositionality indeed seems essential for successful generalization when the evaluation is done on a suitable dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-srw.36.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-srw--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-srw.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-srw.36.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-srw.36/>Exploring the Effect of Dialect Mismatched Language Models in <span class=acl-fixed-case>T</span>elugu Automatic Speech Recognition</a></strong><br><a href=/people/a/aditya-yadavalli/>Aditya Yadavalli</a>
|
<a href=/people/g/ganesh-sai-mirishkar/>Ganesh Sai Mirishkar</a>
|
<a href=/people/a/anil-vuppala/>Anil Vuppala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-srw--36><div class="card-body p-3 small">Previous research has found that Acoustic Models (AM) of an Automatic Speech Recognition (ASR) system are susceptible to dialect variations within a language, thereby adversely affecting the ASR. To counter this, researchers have proposed to build a dialect-specific AM while keeping the Language Model (LM) constant for all the dialects. This study explores the effect of dialect mismatched LM by considering three different Telugu regional dialects: Telangana, Coastal Andhra, and Rayalaseema. We show that dialect variations that surface in the form of a different lexicon, grammar, and occasionally semantics can significantly degrade the performance of the LM under mismatched conditions. Therefore, this degradation has an adverse effect on the ASR even when dialect-specific AM is used. We show a degradation of up to 13.13 perplexity points when LM is used under mismatched conditions. Furthermore, we show a degradation of over 9% and over 15% in Character Error Rate (CER) and Word Error Rate (WER), respectively, in the ASR systems when using mismatched LMs over matched LMs.</div></div></div><hr><div id=2022naacl-demo><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.naacl-demo.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.naacl-demo/>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.0/>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations</a></strong><br><a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/a/avirup-sil/>Avi Sil</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-demo.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.1/>textless-lib: a Library for Textless Spoken Language Processing</a></strong><br><a href=/people/e/eugene-kharitonov/>Eugene Kharitonov</a>
|
<a href=/people/j/jade-copet/>Jade Copet</a>
|
<a href=/people/k/kushal-lakhotia/>Kushal Lakhotia</a>
|
<a href=/people/t/tu-anh-nguyen/>Tu Anh Nguyen</a>
|
<a href=/people/p/paden-tomasello/>Paden Tomasello</a>
|
<a href=/people/a/ann-lee/>Ann Lee</a>
|
<a href=/people/a/ali-elkahky/>Ali Elkahky</a>
|
<a href=/people/w/wei-ning-hsu/>Wei-Ning Hsu</a>
|
<a href=/people/a/abdelrahman-mohamed/>Abdelrahman Mohamed</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a>
|
<a href=/people/y/yossi-adi/>Yossi Adi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--1><div class="card-body p-3 small">Textless spoken language processing is an exciting area of research that promises to extend applicability of the standard NLP toolset onto spoken language and languages with few or no textual resources. Here, we introduce textless-lib, a PyTorch-based library aimed to facilitate research in the area. We describe the building blocks that the library provides and demonstrate its usability by discuss three different use-case examples: (i) speaker probing, (ii) speech resynthesis and compression, and (iii) speech continuation. We believe that textless-lib substantially simplifies research the textless setting and will be handful not only for speech researchers but also for the NLP community at large.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.2/>Web-based Annotation Interface for Derivational Morphology</a></strong><br><a href=/people/l/lukas-kyjanek/>Lukáš Kyjánek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--2><div class="card-body p-3 small">The paper presents a visual interface for manual annotation of language resources for derivational morphology. The interface is web-based and created using relatively simple programming techniques, and yet it rapidly facilitates and speeds up the annotation process, especially in languages with rich derivational morphology. As such, it can reduce the cost of the process. After introducing manual annotation tasks in derivational morphology, the paper describes the new visual interface and a case study that compares the current annotation method to the annotation using the interface. In addition, it also demonstrates the opportunity to use the interface for manual annotation of syntactic trees. The source codes are freely available under the MIT License on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-demo.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.3/><span class=acl-fixed-case>T</span>urkish<span class=acl-fixed-case>D</span>elight<span class=acl-fixed-case>NLP</span>: A Neural <span class=acl-fixed-case>T</span>urkish <span class=acl-fixed-case>NLP</span> Toolkit</a></strong><br><a href=/people/h/huseyin-alecakir/>Huseyin Alecakir</a>
|
<a href=/people/n/necva-bolucu/>Necva Bölücü</a>
|
<a href=/people/b/burcu-can/>Burcu Can</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--3><div class="card-body p-3 small">We introduce a neural Turkish NLP toolkit called TurkishDelightNLP that performs computational linguistic analyses from morphological level to semantic level that involves tasks such as stemming, morphological segmentation, morphological tagging, part-of-speech tagging, dependency parsing, and semantic parsing, as well as high-level NLP tasks such as named entity recognition. We publicly share the open-source Turkish NLP toolkit through a web interface that allows an input text to be analysed in real-time, as well as the open source implementation of the components provided in the toolkit, an API, and several annotated datasets such as word similarity test set to evaluate word embeddings and UCCA-based semantic annotation in Turkish. This will be the first open-source Turkish NLP toolkit that involves a range of NLP tasks in all levels. We believe that it will be useful for other researchers in Turkish NLP and will be also beneficial for other high-level NLP tasks in Turkish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-demo.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.4/><span class=acl-fixed-case>ZS</span>4<span class=acl-fixed-case>IE</span>: A toolkit for Zero-Shot Information Extraction with simple Verbalizations</a></strong><br><a href=/people/o/oscar-sainz/>Oscar Sainz</a>
|
<a href=/people/h/haoling-qiu/>Haoling Qiu</a>
|
<a href=/people/o/oier-lopez-de-lacalle/>Oier Lopez de Lacalle</a>
|
<a href=/people/e/eneko-agirre/>Eneko Agirre</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--4><div class="card-body p-3 small">The current workflow for Information Extraction (IE) analysts involves the definition of the entities/relations of interest and a training corpus with annotated examples. In this demonstration we introduce a new workflow where the analyst directly verbalizes the entities/relations, which are then used by a Textual Entailment model to perform zero-shot IE. We present the design and implementation of a toolkit with a user interface, as well as experiments on four IE tasks that show that the system achieves very good performance at zero-shot learning using only 5–15 minutes per type of a user’s effort. Our demonstration system is open-sourced at <a href=https://github.com/BBN-E/ZS4IE class=acl-markup-url>https://github.com/BBN-E/ZS4IE</a>. A demonstration video is available at <a href=https://vimeo.com/676138340 class=acl-markup-url>https://vimeo.com/676138340</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.5/>Flowstorm: Open-Source Platform with Hybrid Dialogue Architecture</a></strong><br><a href=/people/j/jan-pichl/>Jan Pichl</a>
|
<a href=/people/p/petr-marek/>Petr Marek</a>
|
<a href=/people/j/jakub-konrad/>Jakub Konrád</a>
|
<a href=/people/p/petr-lorenc/>Petr Lorenc</a>
|
<a href=/people/o/ondrej-kobza/>Ondrej Kobza</a>
|
<a href=/people/t/tomas-zajicek/>Tomáš Zajíček</a>
|
<a href=/people/j/jan-sedivy/>Jan Šedivý</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--5><div class="card-body p-3 small">This paper presents a conversational AI platform called Flowstorm. Flowstorm is an open-source SaaS project suitable for creating, running, and analyzing conversational applications. Thanks to the fast and fully automated build process, the dialogues created within the platform can be executed in seconds. Furthermore, we propose a novel dialogue architecture that uses a combination of tree structures with generative models. The tree structures are also used for training NLU models suitable for specific dialogue scenarios. However, the generative models are globally used across applications and extend the functionality of the dialogue trees. Moreover, the platform functionality benefits from out-of-the-box components, such as the one responsible for extracting data from utterances or working with crawled data. Additionally, it can be extended using a custom code directly in the platform. One of the essential features of the platform is the possibility to reuse the created assets across applications. There is a library of prepared assets where each developer can contribute. All of the features are available through a user-friendly visual editor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.6/>Contrastive Explanations of Text Classifiers as a Service</a></strong><br><a href=/people/l/lorenzo-malandri/>Lorenzo Malandri</a>
|
<a href=/people/f/fabio-mercorio/>Fabio Mercorio</a>
|
<a href=/people/m/mario-mezzanzanica/>Mario Mezzanzanica</a>
|
<a href=/people/n/navid-nobani/>Navid Nobani</a>
|
<a href=/people/a/andrea-seveso/>Andrea Seveso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--6><div class="card-body p-3 small">The recent growth of black-box machine-learning methods in data analysis has increased the demand for explanation methods and tools to understand their behaviour and assist human-ML model cooperation. In this paper, we demonstrate ContrXT, a novel approach that uses natural language explanations to help users to comprehend how a back-box model works. ContrXT provides time contrastive (t-contrast) explanations by computing the differences in the classification logic of two different trained models and then reasoning on their symbolic representations through Binary Decision Diagrams. ContrXT is publicly available at ContrXT.ai as a python pip package.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-demo.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.7/><span class=acl-fixed-case>RESIN</span>-11: Schema-guided Event Prediction for 11 Newsworthy Scenarios</a></strong><br><a href=/people/x/xinya-du/>Xinya Du</a>
|
<a href=/people/z/zixuan-zhang/>Zixuan Zhang</a>
|
<a href=/people/s/sha-li/>Sha Li</a>
|
<a href=/people/p/pengfei-yu/>Pengfei Yu</a>
|
<a href=/people/h/hongwei-wang/>Hongwei Wang</a>
|
<a href=/people/t/tuan-lai/>Tuan Lai</a>
|
<a href=/people/x/xudong-lin/>Xudong Lin</a>
|
<a href=/people/z/ziqi-wang/>Ziqi Wang</a>
|
<a href=/people/i/iris-liu/>Iris Liu</a>
|
<a href=/people/b/ben-zhou/>Ben Zhou</a>
|
<a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/m/manling-li/>Manling Li</a>
|
<a href=/people/d/darryl-hannan/>Darryl Hannan</a>
|
<a href=/people/j/jie-lei/>Jie Lei</a>
|
<a href=/people/h/hyounghun-kim/>Hyounghun Kim</a>
|
<a href=/people/r/rotem-dror/>Rotem Dror</a>
|
<a href=/people/h/haoyu-wang/>Haoyu Wang</a>
|
<a href=/people/m/michael-regan/>Michael Regan</a>
|
<a href=/people/q/qi-zeng/>Qi Zeng</a>
|
<a href=/people/q/qing-lyu/>Qing Lyu</a>
|
<a href=/people/c/charles-yu/>Charles Yu</a>
|
<a href=/people/c/carl-edwards/>Carl Edwards</a>
|
<a href=/people/x/xiaomeng-jin/>Xiaomeng Jin</a>
|
<a href=/people/y/yizhu-jiao/>Yizhu Jiao</a>
|
<a href=/people/g/ghazaleh-kazeminejad/>Ghazaleh Kazeminejad</a>
|
<a href=/people/z/zhenhailong-wang/>Zhenhailong Wang</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/c/carl-vondrick/>Carl Vondrick</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/s/shih-fu-chang/>Shih-Fu Chang</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--7><div class="card-body p-3 small">We introduce RESIN-11, a new schema-guided event extraction&amp;prediction framework that can be applied to a large variety of newsworthy scenarios. The framework consists of two parts: (1) an open-domain end-to-end multimedia multilingual information extraction system with weak-supervision and zero-shot learningbased techniques. (2) schema matching and schema-guided event prediction based on our curated schema library. We build a demo website based on our dockerized system and schema library publicly available for installation (<a href=https://github.com/RESIN-KAIROS/RESIN-11 class=acl-markup-url>https://github.com/RESIN-KAIROS/RESIN-11</a>). We also include a video demonstrating the system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.8/>A Human-machine Interface for Few-shot Rule Synthesis for Information Extraction</a></strong><br><a href=/people/r/robert-vacareanu/>Robert Vacareanu</a>
|
<a href=/people/g/george-c-g-barbosa/>George C.G. Barbosa</a>
|
<a href=/people/e/enrique-noriega-atala/>Enrique Noriega-Atala</a>
|
<a href=/people/g/gus-hahn-powell/>Gus Hahn-Powell</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/marco-a-valenzuela-escarcega/>Marco A. Valenzuela-Escárcega</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--8><div class="card-body p-3 small">We propose a system that assists a user in constructing transparent information extraction models, consisting of patterns (or rules) written in a declarative language, through program synthesis. Users of our system can specify their requirements through the use of examples,which are collected with a search interface. The rule-synthesis system proposes rule candidates and the results of applying them on a textual corpus; the user has the option to accept the candidate, request another option, or adjust the examples provided to the system. Through an interactive evaluation, we show that our approach generates high-precision rules even in a 1-shot setting. On a second evaluation on a widely-used relation extraction dataset (TACRED), our method generates rules that outperform considerably manually written patterns. Our code, demo, and documentation is available at <a href=https://clulab.github.io/odinsynth class=acl-markup-url>https://clulab.github.io/odinsynth</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-demo.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.9/><span class=acl-fixed-case>SETS</span>um: Summarization and Visualization of Student Evaluations of Teaching</a></strong><br><a href=/people/y/yinuo-hu/>Yinuo Hu</a>
|
<a href=/people/s/shiyue-zhang/>Shiyue Zhang</a>
|
<a href=/people/v/viji-sathy/>Viji Sathy</a>
|
<a href=/people/a/abigail-panter/>Abigail Panter</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--9><div class="card-body p-3 small">Student Evaluations of Teaching (SETs) are widely used in colleges and universities. Typically SET results are summarized for instructors in a static PDF report. The report often includes summary statistics for quantitative ratings and an unsorted list of open-ended student comments. The lack of organization and summarization of the raw comments hinders those interpreting the reports from fully utilizing informative feedback, making accurate inferences, and designing appropriate instructional improvements. In this work, we introduce a novel system, SETSUM, that leverages sentiment analysis, aspect extraction, summarization, and visualization techniques to provide organized illustrations of SET findings to instructors and other reviewers. Ten university professors from diverse departments serve as evaluators of the system and all agree that SETSUM help them interpret SET results more efficiently; and 6 out of 10 instructors prefer our system over the standard static PDF report (while the remaining 4 would like to have both). This demonstrates that our work holds the potential of reforming the SET reporting conventions in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.10/>Towards Open-Domain Topic Classification</a></strong><br><a href=/people/h/hantian-ding/>Hantian Ding</a>
|
<a href=/people/j/jinrui-yang/>Jinrui Yang</a>
|
<a href=/people/y/yuqian-deng/>Yuqian Deng</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--10><div class="card-body p-3 small">We introduce an open-domain topic classification system that accepts user-defined taxonomy in real time. Users will be able to classify a text snippet with respect to any candidate labels they want, and get instant response from our web interface. To obtain such flexibility, we build the backend model in a zero-shot way. By training on a new dataset constructed from Wikipedia, our label-aware text classifier can effectively utilize implicit knowledge in the pretrained language model to handle labels it has never seen before. We evaluate our model across four datasets from various domains with different label sets. Experiments show that the model significantly improves over existing zero-shot baselines in open-domain scenarios, and performs competitively with weakly-supervised models trained on in-domain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.11/><span class=acl-fixed-case>S</span>ent<span class=acl-fixed-case>S</span>pace: Large-Scale Benchmarking and Evaluation of Text using Cognitively Motivated Lexical, Syntactic, and Semantic Features</a></strong><br><a href=/people/g/greta-tuckute/>Greta Tuckute</a>
|
<a href=/people/a/aalok-sathe/>Aalok Sathe</a>
|
<a href=/people/m/mingye-wang/>Mingye Wang</a>
|
<a href=/people/h/harley-yoder/>Harley Yoder</a>
|
<a href=/people/c/cory-shain/>Cory Shain</a>
|
<a href=/people/e/evelina-fedorenko/>Evelina Fedorenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--11><div class="card-body p-3 small">SentSpace is a modular framework for streamlined evaluation of text. SentSpacecharacterizes textual input using diverse lexical, syntactic, and semantic features derivedfrom corpora and psycholinguistic experiments. Core sentence features fall into three primaryfeature spaces: 1) Lexical, 2) Contextual, and 3) Embeddings. To aid in the analysis of computed features, SentSpace provides a web interface for interactive visualization and comparison with text from large corpora. The modular design of SentSpace allows researchersto easily integrate their own feature computation into the pipeline while benefiting from acommon framework for evaluation and visualization. In this manuscript we will describe thedesign of SentSpace, its core feature spaces, and demonstrate an example use case by comparing human-written and machine-generated (GPT2-XL) sentences to each other. We findthat while GPT2-XL-generated text appears fluent at the surface level, psycholinguistic normsand measures of syntactic processing reveal key differences between text produced by humansand machines. Thus, SentSpace provides a broad set of cognitively motivated linguisticfeatures for evaluation of text within natural language processing, cognitive science, as wellas the social sciences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-demo.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.12/><span class=acl-fixed-case>P</span>addle<span class=acl-fixed-case>S</span>peech: An Easy-to-Use All-in-One Speech Toolkit</a></strong><br><a href=/people/h/hui-zhang/>Hui Zhang</a>
|
<a href=/people/t/tian-yuan/>Tian Yuan</a>
|
<a href=/people/j/junkun-chen/>Junkun Chen</a>
|
<a href=/people/x/xintong-li/>Xintong Li</a>
|
<a href=/people/r/renjie-zheng/>Renjie Zheng</a>
|
<a href=/people/y/yuxin-huang/>Yuxin Huang</a>
|
<a href=/people/x/xiaojie-chen/>Xiaojie Chen</a>
|
<a href=/people/e/enlei-gong/>Enlei Gong</a>
|
<a href=/people/z/zeyu-chen/>Zeyu Chen</a>
|
<a href=/people/x/xiaoguang-hu/>Xiaoguang Hu</a>
|
<a href=/people/d/dianhai-yu/>Dianhai Yu</a>
|
<a href=/people/y/yanjun-ma/>Yanjun Ma</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--12><div class="card-body p-3 small">PaddleSpeech is an open-source all-in-one speech toolkit. It aims at facilitating the development and research of speech processing technologies by providing an easy-to-use command-line interface and a simple code structure. This paper describes the design philosophy and core architecture of PaddleSpeech to support several essential speech-to-text and text-to-speech tasks. PaddleSpeech achieves competitive or state-of-the-art performance on various speech datasets and implements the most popular methods. It also provides recipes and pretrained models to quickly reproduce the experimental results in this paper. PaddleSpeech is publicly avaiable at <a href=https://github.com/PaddlePaddle/PaddleSpeech class=acl-markup-url>https://github.com/PaddlePaddle/PaddleSpeech</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.13.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-demo.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.13/><span class=acl-fixed-case>D</span>adma<span class=acl-fixed-case>T</span>ools: Natural Language Processing Toolkit for <span class=acl-fixed-case>P</span>ersian Language</a></strong><br><a href=/people/r/romina-etezadi/>Romina Etezadi</a>
|
<a href=/people/m/mohammad-karrabi/>Mohammad Karrabi</a>
|
<a href=/people/n/najmeh-zare/>Najmeh Zare</a>
|
<a href=/people/m/mohamad-bagher-sajadi/>Mohamad Bagher Sajadi</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--13><div class="card-body p-3 small">We introduce DadmaTools, an open-source Python Natural Language Processing toolkit for the Persian language. The toolkit is a neural pipeline based on spaCy for several text processing tasks, including normalization, tokenization, lemmatization, part-of-speech, dependency parsing, constituency parsing, chunking, and ezafe detecting. DadmaTools relies on fine-tuning of ParsBERT using the PerDT dataset for most of the tasks. Dataset module and embedding module are included in DadmaTools that support different Persian datasets, embeddings, and commonly used functions for them. Our evaluations show that DadmaTools can attain state-of-the-art performance on multiple NLP tasks. The source code is freely available at <a href=https://github.com/Dadmatech/DadmaTools class=acl-markup-url>https://github.com/Dadmatech/DadmaTools</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-demo.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-demo--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-demo.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-demo.14.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-demo.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-demo.14/><span class=acl-fixed-case>FAMIE</span>: A Fast Active Learning Framework for Multilingual Information Extraction</a></strong><br><a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/n/nghia-ngo/>Nghia Ngo</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/t/thien-nguyen/>Thien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-demo--14><div class="card-body p-3 small">This paper presents FAMIE, a comprehensive and efficient active learning (AL) toolkit for multilingual information extraction. FAMIE is designed to address a fundamental problem in existing AL frameworks where annotators need to wait for a long time between annotation batches due to the time-consuming nature of model training and data selection at each AL iteration. This hinders the engagement, productivity, and efficiency of annotators. Based on the idea of using a small proxy network for fast data selection, we introduce a novel knowledge distillation mechanism to synchronize the proxy network with the main large model (i.e., BERT-based) to ensure the appropriateness of the selected annotation examples for the main model. Our AL framework can support multiple languages. The experiments demonstrate the advantages of FAMIE in terms of competitive performance and time efficiency for sequence labeling with AL. We publicly release our code (<a href=https://github.com/nlp-uoregon/famie class=acl-markup-url>https://github.com/nlp-uoregon/famie</a>) and demo website (<a href=http://nlp.uoregon.edu:9000/ class=acl-markup-url>http://nlp.uoregon.edu:9000/</a>). A demo video for FAMIE is provided at: <a href=https://youtu.be/I2i8n_jAyrY class=acl-markup-url>https://youtu.be/I2i8n_jAyrY</a></div></div></div><hr><div id=2022naacl-tutorials><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-tutorials.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.naacl-tutorials.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.naacl-tutorials/>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-tutorials.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-tutorials.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-tutorials.0/>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts</a></strong><br><a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/c/cecilia-ovesdotter-alm/>Cecilia O. Alm</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-tutorials.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-tutorials.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-tutorials--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-tutorials.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-tutorials.1/>Text Generation with Text-Editing Models</a></strong><br><a href=/people/e/eric-malmi/>Eric Malmi</a>
|
<a href=/people/y/yue-dong/>Yue Dong</a>
|
<a href=/people/j/jonathan-mallinson/>Jonathan Mallinson</a>
|
<a href=/people/a/aleksandr-chuklin/>Aleksandr Chuklin</a>
|
<a href=/people/j/jakub-adamek/>Jakub Adamek</a>
|
<a href=/people/d/daniil-mirylenka/>Daniil Mirylenka</a>
|
<a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/s/sebastian-krause/>Sebastian Krause</a>
|
<a href=/people/s/shankar-kumar/>Shankar Kumar</a>
|
<a href=/people/a/aliaksei-severyn/>Aliaksei Severyn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-tutorials--1><div class="card-body p-3 small">Text-editing models have recently become a prominent alternative to seq2seq models for monolingual text-generation tasks such as grammatical error correction, text simplification, and style transfer. These tasks share a common trait – they exhibit a large amount of textual overlap between the source and target texts. Text-editing models take advantage of this observation and learn to generate the output by predicting edit operations applied to the source sequence. In contrast, seq2seq models generate outputs word-by-word from scratch thus making them slow at inference time. Text-editing models provide several benefits over seq2seq models including faster inference speed, higher sample efficiency, and better control and interpretability of the outputs. This tutorial provides a comprehensive overview of the text-edit based models and current state-of-the-art approaches analyzing their pros and cons. We discuss challenges related to deployment and how these models help to mitigate hallucination and bias, both pressing challenges in the field of text generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-tutorials.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-tutorials.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-tutorials--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-tutorials.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-tutorials.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-tutorials.2/>Self-supervised Representation Learning for Speech Processing</a></strong><br><a href=/people/h/hung-yi-lee/>Hung-yi Lee</a>
|
<a href=/people/a/abdelrahman-mohamed/>Abdelrahman Mohamed</a>
|
<a href=/people/s/shinji-watanabe/>Shinji Watanabe</a>
|
<a href=/people/t/tara-sainath/>Tara Sainath</a>
|
<a href=/people/k/karen-livescu/>Karen Livescu</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/s/shu-wen-yang/>Shu-wen Yang</a>
|
<a href=/people/k/katrin-kirchhoff/>Katrin Kirchhoff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-tutorials--2><div class="card-body p-3 small">There is a trend in the machine learning community to adopt self-supervised approaches to pre-train deep networks. Self-supervised representation learning (SSL) utilizes proxy supervised learning tasks, for example, distinguishing parts of the input signal from distractors, or generating masked input segments conditioned on the unmasked ones, to obtain training data from unlabeled corpora. BERT and GPT in NLP and SimCLR and BYOL in CV are famous examples in this direction. These approaches make it possible to use a tremendous amount of unlabeled data available on the web to train large networks and solve complicated tasks. Thus, SSL has the potential to scale up current machine learning technologies, especially for low-resourced, under-represented use cases, and democratize the technologies. Recently self-supervised approaches for speech processing are also gaining popularity. There are several workshops in relevant topics hosted at ICML 2020 (<a href=https://icml-sas.gitlab.io/ class=acl-markup-url>https://icml-sas.gitlab.io/</a>), NeurIPS 2020 (<a href=https://neurips-sas-2020.github.io/ class=acl-markup-url>https://neurips-sas-2020.github.io/</a>), and AAAI 2022 (<a href=https://aaai-sas-2022.github.io/ class=acl-markup-url>https://aaai-sas-2022.github.io/</a>). However, there is no previous tutorial about a similar topic based on the authors’ best knowledge. Due to the growing popularity of SSL, and the shared mission of the areas in bringing speech and language technologies to more use cases with better quality and scaling the technologies for under-represented languages, we propose this tutorial to systematically survey the latest SSL techniques, tools, datasets, and performance achievement in speech processing. The proposed tutorial is highly relevant to the special theme of ACL about language diversity. One of the main focuses of the tutorial is leveraging SSL to reduce the dependence of speech technologies on labeled data, and to scale up the technologies especially for under-represented languages and use cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-tutorials.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-tutorials.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-tutorials--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-tutorials.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-tutorials.3/>New Frontiers of Information Extraction</a></strong><br><a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/m/manling-li/>Manling Li</a>
|
<a href=/people/b/ben-zhou/>Ben Zhou</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-tutorials--3><div class="card-body p-3 small">This tutorial targets researchers and practitioners who are interested in AI and ML technologies for structural information extraction (IE) from unstructured textual sources. Particularly, this tutorial will provide audience with a systematic introduction to recent advances of IE, by answering several important research questions. These questions include (i) how to develop an robust IE system from noisy, insufficient training data, while ensuring the reliability of its prediction? (ii) how to foster the generalizability of IE through enhancing the system’s cross-lingual, cross-domain, cross-task and cross-modal transferability? (iii) how to precisely support extracting structural information with extremely fine-grained, diverse and boundless labels? (iv) how to further improve IE by leveraging indirect supervision from other NLP tasks, such as NLI, QA or summarization, and pre-trained language models? (v) how to acquire knowledge to guide the inference of IE systems? We will discuss several lines of frontier research that tackle those challenges, and will conclude the tutorial by outlining directions for further investigation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-tutorials.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-tutorials.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-tutorials--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-tutorials.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-tutorials.4/>Human-Centered Evaluation of Explanations</a></strong><br><a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/s/samuel-carton/>Samuel Carton</a>
|
<a href=/people/s/shi-feng/>Shi Feng</a>
|
<a href=/people/q/q-vera-liao/>Q. Vera Liao</a>
|
<a href=/people/t/tania-lombrozo/>Tania Lombrozo</a>
|
<a href=/people/a/alison-smith-renner/>Alison Smith-Renner</a>
|
<a href=/people/c/chenhao-tan/>Chenhao Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-tutorials--4><div class="card-body p-3 small">The NLP community are increasingly interested in providing explanations for NLP models to help people make sense of model behavior and potentially improve human interaction with models. In addition to computational challenges in generating these explanations, evaluations of the generated explanations require human-centered perspectives and approaches. This tutorial will provide an overview of human-centered evaluations of explanations. First, we will give a brief introduction to the psychological foundation of explanations as well as types of NLP model explanations and their corresponding presentation, to provide the necessary background. We will then present a taxonomy of human-centered evaluation of explanations and dive into depth in the two categories: 1) evaluation based on human-annotated explanations; 2) evaluation with human-subjects studies. We will conclude by discussing future directions. We will also adopt a flipped format to maximize the in- teractive components for the live audience.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-tutorials.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-tutorials.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-tutorials--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-tutorials.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-tutorials.5/>Tutorial on Multimodal Machine Learning</a></strong><br><a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/a/amir-zadeh/>Amir Zadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-tutorials--5><div class="card-body p-3 small">Multimodal machine learning involves integrating and modeling information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, HCI, and healthcare. This tutorial, building upon a new edition of a survey paper on multimodal ML as well as previously-given tutorials and academic courses, will describe an updated taxonomy on multimodal machine learning synthesizing its core technical challenges and major directions for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-tutorials.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-tutorials.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-tutorials--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-tutorials.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-tutorials.6/>Contrastive Data and Learning for Natural Language Processing</a></strong><br><a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/y/yangfeng-ji/>Yangfeng Ji</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/r/rebecca-j-passonneau/>Rebecca J. Passonneau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-tutorials--6><div class="card-body p-3 small">Current NLP models heavily rely on effective representation learning algorithms. Contrastive learning is one such technique to learn an embedding space such that similar data sample pairs have close representations while dissimilar samples stay far apart from each other. It can be used in supervised or unsupervised settings using different loss functions to produce task-specific or general-purpose representations. While it has originally enabled the success for vision tasks, recent years have seen a growing number of publications in contrastive NLP. This first line of works not only delivers promising performance improvements in various NLP tasks, but also provides desired characteristics such as task-agnostic sentence representation, faithful text generation, data-efficient learning in zero-shot and few-shot settings, interpretability and explainability. In this tutorial, we aim to provide a gentle introduction to the fundamentals of contrastive learning approaches and the theory behind them. We then survey the benefits and the best practices of contrastive learning for various downstream NLP applications including Text Classification, Question Answering, Summarization, Text Generation, Interpretability and Explainability, Commonsense Knowledge and Reasoning, Vision-and-Language.This tutorial intends to help researchers in the NLP and computational linguistics community to understand this emerging topic and promote future research directions of using contrastive learning for NLP applications.</div></div></div><hr><div id=2022naacl-industry><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.naacl-industry.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.naacl-industry/>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.0/>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track</a></strong><br><a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/r/rashmi-gangadharaiah/>Rashmi Gangadharaiah</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.1/>Scalable and Robust Self-Learning for Skill Routing in Large-Scale Conversational <span class=acl-fixed-case>AI</span> Systems</a></strong><br><a href=/people/m/mohammad-kachuee/>Mohammad Kachuee</a>
|
<a href=/people/j/jinseok-nam/>Jinseok Nam</a>
|
<a href=/people/s/sarthak-ahuja/>Sarthak Ahuja</a>
|
<a href=/people/j/jin-myung-won/>Jin-Myung Won</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--1><div class="card-body p-3 small">Skill routing is an important component in large-scale conversational systems. In contrast to traditional rule-based skill routing, state-of-the-art systems use a model-based approach to enable natural conversations. To provide supervision signal required to train such models, ideas such as human annotation, replication of a rule-based system, relabeling based on user paraphrases, and bandit-based learning were suggested. However, these approaches: (a) do not scale in terms of the number of skills and skill on-boarding, (b) require a very costly expert annotation/rule-design, (c) introduce risks in the user experience with each model update. In this paper, we present a scalable self-learning approach to explore routing alternatives without causing abrupt policy changes that break the user experience, learn from the user interaction, and incrementally improve the routing via frequent model refreshes. To enable such robust frequent model updates, we suggest a simple and effective approach that ensures controlled policy updates for individual domains, followed by an off-policy evaluation for making deployment decisions without any need for lengthy A/B experimentation. We conduct various offline and online A/B experiments on a commercial large-scale conversational system to demonstrate the effectiveness of the proposed method in real-world production settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.2/><span class=acl-fixed-case>CREATER</span>: <span class=acl-fixed-case>CTR</span>-driven Advertising Text Generation with Controlled Pre-Training and Contrastive Fine-Tuning</a></strong><br><a href=/people/p/penghui-wei/>Penghui Wei</a>
|
<a href=/people/x/xuanhua-yang/>Xuanhua Yang</a>
|
<a href=/people/s/shaoguo-liu/>ShaoGuo Liu</a>
|
<a href=/people/l/liang-wang/>Liang Wang</a>
|
<a href=/people/b/bo-zheng/>Bo Zheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--2><div class="card-body p-3 small">This paper focuses on automatically generating the text of an ad, and the goal is that the generated text can capture user interest for achieving higher click-through rate (CTR). We propose CREATER, a CTR-driven advertising text generation approach, to generate ad texts based on high-quality user reviews. To incorporate CTR objective, our model learns from online A/B test data with contrastive learning, which encourages the model to generate ad texts that obtain higher CTR. To make use of large-scale unpaired reviews, we design a customized self-supervised objective reducing the gap between pre-training and fine-tuning. Experiments on industrial datasets show that CREATER significantly outperforms current approaches. It has been deployed online in a leading advertising platform and brings uplift on core online metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.3/>Augmenting Poetry Composition with <span class=acl-fixed-case>V</span>erse by <span class=acl-fixed-case>V</span>erse</a></strong><br><a href=/people/d/david-c-uthus/>David Uthus</a>
|
<a href=/people/m/maria-voitovich/>Maria Voitovich</a>
|
<a href=/people/r/r-j-mical/>R.J. Mical</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--3><div class="card-body p-3 small">We describe Verse by Verse, our experiment in augmenting the creative process of writing poetry with an AI. We have created a group of AI poets, styled after various American classic poets, that are able to offer as suggestions generated lines of verse while a user is composing a poem. In this paper, we describe the underlying system to offer these suggestions. This includes a generative model, which is tasked with generating a large corpus of lines of verse offline and which are then stored in an index, and a dual-encoder model that is tasked with recommending the next possible set of verses from our index given the previous line of verse.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.4/><span class=acl-fixed-case>AB</span>/<span class=acl-fixed-case>BA</span> analysis: A framework for estimating keyword spotting recall improvement while maintaining audio privacy</a></strong><br><a href=/people/r/raphael-petegrosso/>Raphael Petegrosso</a>
|
<a href=/people/v/vasistakrishna-baderdinnni/>VasistaKrishna Baderdinnni</a>
|
<a href=/people/t/thibaud-senechal/>Thibaud Senechal</a>
|
<a href=/people/b/benjamin-bullough/>Benjamin Bullough</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--4><div class="card-body p-3 small">Evaluation of keyword spotting (KWS) systems that detect keywords in speech is a challenging task under realistic privacy constraints. The KWS is designed to only collect data when the keyword is present, limiting the availability of hard samples that may contain false negatives, and preventing direct estimation of model recall from production data. Alternatively, complementary data collected from other sources may not be fully representative of the real application. In this work, we propose an evaluation technique which we call AB/BA analysis. Our framework evaluates a candidate KWS model B against a baseline model A, using cross-dataset offline decoding for relative recall estimation, without requiring negative examples. Moreover, we propose a formulation with assumptions that allow estimation of relative false positive rate between models with low variance even when the number of false positives is small. Finally, we propose to leverage machine-generated soft labels, in a technique we call Semi-Supervised AB/BA analysis, that improves the analysis time, privacy, and cost. Experiments with both simulation and real data show that AB/BA analysis is successful at measuring recall improvement in conjunction with the trade-off in relative false positive rate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.5/>Temporal Generalization for Spoken Language Understanding</a></strong><br><a href=/people/j/judith-gaspers/>Judith Gaspers</a>
|
<a href=/people/a/anoop-kumar/>Anoop Kumar</a>
|
<a href=/people/g/greg-ver-steeg/>Greg Ver Steeg</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--5><div class="card-body p-3 small">Spoken Language Understanding (SLU) models in industry applications are usually trained offline on historic data, but have to perform well on incoming user requests after deployment. Since the application data is not available at training time, this is formally similar to the domain generalization problem, where domains correspond to different temporal segments of the data, and the goal is to build a model that performs well on unseen domains, e.g., upcoming data. In this paper, we explore different strategies for achieving good temporal generalization, including instance weighting, temporal fine-tuning, learning temporal features and building a temporally-invariant model. Our results on data of large-scale SLU systems show that temporal information can be leveraged to improve temporal generalization for SLU models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.6/>An End-to-End Dialogue Summarization System for Sales Calls</a></strong><br><a href=/people/a/abedelkadir-asi/>Abedelkadir Asi</a>
|
<a href=/people/s/song-wang/>Song Wang</a>
|
<a href=/people/r/roy-eisenstadt/>Roy Eisenstadt</a>
|
<a href=/people/d/dean-geckt/>Dean Geckt</a>
|
<a href=/people/y/yarin-kuper/>Yarin Kuper</a>
|
<a href=/people/y/yi-mao/>Yi Mao</a>
|
<a href=/people/r/royi-ronen/>Royi Ronen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--6><div class="card-body p-3 small">Summarizing sales calls is a routine task performed manually by salespeople. We present a production system which combines generative models fine-tuned for customer-agent setting, with a human-in-the-loop user experience for an interactive summary curation process. We address challenging aspects of dialogue summarization task in a real-world setting including long input dialogues, content validation, lack of labeled data and quality evaluation. We show how GPT-3 can be leveraged as an offline data labeler to handle training data scarcity and accommodate privacy constraints in an industrial setting. Experiments show significant improvements by our models in tackling the summarization and content validation tasks on public datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.7/>Controlled Data Generation via Insertion Operations for <span class=acl-fixed-case>NLU</span></a></strong><br><a href=/people/m/manoj-kumar/>Manoj Kumar</a>
|
<a href=/people/y/yuval-merhav/>Yuval Merhav</a>
|
<a href=/people/h/haidar-khan/>Haidar Khan</a>
|
<a href=/people/r/rahul-gupta/>Rahul Gupta</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--7><div class="card-body p-3 small">Use of synthetic data is rapidly emerging as a realistic alternative to manually annotating live traffic for industry-scale model building. Manual data annotation is slow, expensive and not preferred for meeting customer privacy expectations. Further, commercial natural language applications are required to support continuously evolving features as well as newly added experiences. To address these requirements, we propose a targeted synthetic data generation technique by inserting tokens into a given semantic signature. The generated data are used as additional training samples in the tasks of intent classification and named entity recognition. We evaluate on a real-world voice assistant dataset, and using only 33% of the available training set, we achieve the same accuracy as training with all available data. Further, we analyze the effects of data generation across varied real-world applications and propose heuristics that improve the task performance further.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.8/>Easy and Efficient Transformer: Scalable Inference Solution For Large <span class=acl-fixed-case>NLP</span> Model</a></strong><br><a href=/people/g/gongzheng-li/>Gongzheng Li</a>
|
<a href=/people/y/yadong-xi/>Yadong Xi</a>
|
<a href=/people/j/jingzhen-ding/>Jingzhen Ding</a>
|
<a href=/people/d/duan-wang/>Duan Wang</a>
|
<a href=/people/z/ziyang-luo/>Ziyang Luo</a>
|
<a href=/people/r/rongsheng-zhang/>Rongsheng Zhang</a>
|
<a href=/people/b/bai-liu/>Bai Liu</a>
|
<a href=/people/c/changjie-fan/>Changjie Fan</a>
|
<a href=/people/x/xiaoxi-mao/>Xiaoxi Mao</a>
|
<a href=/people/z/zeng-zhao/>Zeng Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--8><div class="card-body p-3 small">Recently, large-scale transformer-based models have been proven to be effective over various tasks across many domains. Nevertheless, applying them in industrial production requires tedious and heavy works to reduce inference costs. To fill such a gap, we introduce a scalable inference solution: <b>Easy and Efficient Transformer (EET)</b>, including a series of transformer inference optimization at the algorithm and implementation levels. First, we design highly optimized kernels for long inputs and large hidden sizes. Second, we propose a flexible CUDA memory manager to reduce the memory footprint when deploying a large model. Compared with the state-of-the-art transformer inference library (Faster Transformer v4.0), EET can achieve an average of 1.40-4.20x speedup on the transformer decoder layer with an A100 GPU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.9/>Aspect-based Analysis of Advertising Appeals for Search Engine Advertising</a></strong><br><a href=/people/s/soichiro-murakami/>Soichiro Murakami</a>
|
<a href=/people/p/peinan-zhang/>Peinan Zhang</a>
|
<a href=/people/s/sho-hoshino/>Sho Hoshino</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--9><div class="card-body p-3 small">Writing an ad text that attracts people and persuades them to click or act is essential for the success of search engine advertising. Therefore, ad creators must consider various aspects of advertising appeals (A<span class=tex-math><sup>3</sup></span>) such as the price, product features, and quality. However, products and services exhibit unique effective A<span class=tex-math><sup>3</sup></span> for different industries. In this work, we focus on exploring the effective A<span class=tex-math><sup>3</sup></span> for different industries with the aim of assisting the ad creation process. To this end, we created a dataset of advertising appeals and used an existing model that detects various aspects for ad texts. Our experiments demonstrated %through correlation analysis that different industries have their own effective A<span class=tex-math><sup>3</sup></span> and that the identification of the A<span class=tex-math><sup>3</sup></span> contributes to the estimation of advertising performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.10/>Self-supervised Product Title Rewrite for Product Listing Ads</a></strong><br><a href=/people/x/xue-zhao/>Xue Zhao</a>
|
<a href=/people/d/dayiheng-liu/>Dayiheng Liu</a>
|
<a href=/people/j/junwei-ding/>Junwei Ding</a>
|
<a href=/people/l/liang-yao/>Liang Yao</a>
|
<a href=/people/m/mahone-yan/>Mahone Yan</a>
|
<a href=/people/h/huibo-wang/>Huibo Wang</a>
|
<a href=/people/w/wenqing-yao/>Wenqing Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--10><div class="card-body p-3 small">Product Listing Ads (PLAs) are primary online advertisements merchants pay to attract more customers. However, merchants prefer to stack various attributes to the title and neglect the fluency and information priority. These seller-created titles are not suitable for PLAs as they fail to highlight the core information in the visible part in PLAs titles. In this work, we present a title rewrite solution. Specifically, we train a self-supervised language model to generate high-quality titles in terms of fluency and information priority. Extensive offline test and real-world online test have demonstrated that our solution is effective in reducing the cost and gaining more profit as it lowers our CPC, CPB while improving CTR in the online test by a large amount.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.11.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.11/>Efficient Semi-supervised Consistency Training for Natural Language Understanding</a></strong><br><a href=/people/g/george-leung/>George Leung</a>
|
<a href=/people/j/joshua-tan/>Joshua Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--11><div class="card-body p-3 small">Manually labeled training data is expensive, noisy, and often scarce, such as when developing new features or localizing existing features for a new region. In cases where labeled data is limited but unlabeled data is abundant, semi-supervised learning methods such as consistency training can be used to improve model performance, by training models to output consistent predictions between original and augmented versions of unlabeled data. In this work, we explore different data augmentation methods for consistency training (CT) on Natural Language Understanding (NLU) domain classification (DC) in the limited labeled data regime. We explore three types of augmentation techniques (human paraphrasing, back-translation, and dropout) for unlabeled data and train DC models to jointly minimize both the supervised loss and the consistency loss on unlabeled data. Our results demonstrate that DC models trained with CT methods and dropout based augmentation on only 0.1% (2,998 instances) of labeled data with the remainder as unlabeled can achieve a top-1 relative accuracy reduction of 12.25% compared to fully supervised model trained with 100% of labeled data, outperforming fully supervised models trained on 10x that amount of labeled data. The dropout-based augmentation achieves similar performance compare to back-translation based augmentation with much less computational resources. This paves the way for applications of using large scale unlabeled data for semi-supervised learning in production NLU systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.12/>Distantly Supervised Aspect Clustering And Naming For <span class=acl-fixed-case>E</span>-Commerce Reviews</a></strong><br><a href=/people/p/prateek-sircar/>Prateek Sircar</a>
|
<a href=/people/a/aniket-chakrabarti/>Aniket Chakrabarti</a>
|
<a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/a/anirban-majumdar/>Anirban Majumdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--12><div class="card-body p-3 small">Product aspect extraction from reviews is a critical task for e-commerce services to understand customer preferences and pain points. While aspect phrases extraction and sentiment analysis have received a lot of attention, clustering of aspect phrases and assigning human readable names to clusters in e-commerce reviews is an extremely important and challenging problem due to the scale of the reviews that makes human review infeasible. In this paper, we propose fully automated methods for clustering aspect words and generating human readable names for the clusters without any manually labeled data. We train transformer based sentence embeddings that are aware of unique e-commerce language characteristics (eg. incomplete sentences, spelling and grammar errors, vernacular etc.). We also train transformer based sequence to sequence models to generate human readable aspect names from clusters. Both the models are trained using heuristic based distant supervision. Additionally, the models are used to improve each other. Extensive empirical testing showed that the clustering model improves the Silhouette Score by 64% when compared to the state-of-the-art baseline and the aspect naming model achieves a high ROUGE-L score of 0.79.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.13.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.13/>Local-to-global learning for iterative training of production <span class=acl-fixed-case>SLU</span> models on new features</a></strong><br><a href=/people/y/yulia-grishina/>Yulia Grishina</a>
|
<a href=/people/d/daniil-sorokin/>Daniil Sorokin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--13><div class="card-body p-3 small">In production SLU systems, new training data becomes available with time so that ML models need to be updated on a regular basis. Specifically, releasing new features adds new classes of data while the old data remains constant. However, retraining the full model each time from scratch is computationally expensive. To address this problem, we propose to consider production releases from the curriculum learning perspective and to adapt the local-to-global learning (LGL) schedule (Cheng et. al, 2019) for a statistical model that starts with fewer output classes and adds more classes with each iteration. We report experiments for the tasks of intent classification and slot filling in the context of a production voice-assistant. First, we apply the original LGL schedule on our data and then adapt LGL to the production setting where the full data is not available at initial training iterations. We demonstrate that our method improves model error rates by 7.3% and saves up to 25% training time for individual iterations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.14/><span class=acl-fixed-case>CULG</span>: Commercial Universal Language Generation</a></strong><br><a href=/people/h/haonan-li/>Haonan Li</a>
|
<a href=/people/y/yameng-huang/>Yameng Huang</a>
|
<a href=/people/y/yeyun-gong/>Yeyun Gong</a>
|
<a href=/people/j/jian-jiao/>Jian Jiao</a>
|
<a href=/people/r/ruofei-zhang/>Ruofei Zhang</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--14><div class="card-body p-3 small">Pre-trained language models (PLMs) have dramatically improved performance for many natural language processing (NLP) tasks in domains such as finance and healthcare. However, the application of PLMs in the domain of commerce, especially marketing and advertising, remains less studied. In this work, we adapt pre-training methods to the domain of commerce, by proposing CULG, a large-scale commercial universal language generation model which is pre-trained on a corpus drawn from 10 markets across 7 languages. We propose 4 commercial generation tasks and a two-stage training strategy for pre-training, and demonstrate that the proposed strategy yields performance improvements on three generation tasks as compared to single-stage pre-training. Extensive experiments show that our model outperforms other models by a large margin on commercial generation tasks, and we conclude with a discussion on additional applications over other markets, languages, and tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.15.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-industry.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.15/>Constraining word alignments with posterior regularization for label transfer</a></strong><br><a href=/people/k/kevin-jose/>Kevin Jose</a>
|
<a href=/people/t/thomas-gueudre/>Thomas Gueudre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--15><div class="card-body p-3 small">Unsupervised word alignments offer a lightweight and interpretable method to transfer labels from high- to low-resource languages, as long as semantically related words have the same label across languages. But such an assumption is often not true in industrial NLP pipelines, where multilingual annotation guidelines are complex and deviate from semantic consistency due to various factors (such as annotation difficulty, conflicting ontology, upcoming feature launches etc.);We address this difficulty by constraining the alignments models to remain consistent with both source and target annotation guidelines , leveraging posterior regularization and labeled examples. We illustrate the overall approach using IBM 2 (fast_align) as a base model, and report results on both internal and external annotated datasets. We measure consistent accuracy improvements on the MultiATIS++ dataset over AWESoME, a popular transformer-based alignment model, in the label projection task (<span class=tex-math>+2.7%</span> at word-level and <span class=tex-math>+15%</span> at sentence-level), and show how even a small amount of target language annotations help substantially.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.16.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.16/>Explaining the Effectiveness of Multi-Task Learning for Efficient Knowledge Extraction from Spine <span class=acl-fixed-case>MRI</span> Reports</a></strong><br><a href=/people/a/arijit-sehanobish/>Arijit Sehanobish</a>
|
<a href=/people/m/mccullen-sandora/>McCullen Sandora</a>
|
<a href=/people/n/nabila-abraham/>Nabila Abraham</a>
|
<a href=/people/j/jayashri-pawar/>Jayashri Pawar</a>
|
<a href=/people/d/danielle-torres/>Danielle Torres</a>
|
<a href=/people/a/anasuya-das/>Anasuya Das</a>
|
<a href=/people/m/murray-becker/>Murray Becker</a>
|
<a href=/people/r/richard-herzog/>Richard Herzog</a>
|
<a href=/people/b/benjamin-odry/>Benjamin Odry</a>
|
<a href=/people/r/ron-vianu/>Ron Vianu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--16><div class="card-body p-3 small">Pretrained Transformer based models finetuned on domain specific corpora have changed the landscape of NLP. However, training or fine-tuning these models for individual tasks can be time consuming and resource intensive. Thus, a lot of current research is focused on using transformers for multi-task learning (Raffel et al., 2020) and how to group the tasks to help a multi-task model to learn effective representations that can be shared across tasks (Standley et al., 2020; Fifty et al., 2021) . In this work, we show that a single multi-tasking model can match the performance of task specific model when the task specific models show similar representations across all of their hidden layers and their gradients are aligned, i.e. their gradients follow the same direction. We hypothesize that the above observations explain the effectiveness of multi-task learning. We validate our observations on our internal radiologist-annotated datasets on the cervical and lumbar spine. Our method is simple and intuitive, and can be used in a wide range of NLP problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.17.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.17/><span class=acl-fixed-case>FPI</span>: Failure Point Isolation in Large-scale Conversational Assistants</a></strong><br><a href=/people/r/rinat-khaziev/>Rinat Khaziev</a>
|
<a href=/people/u/usman-shahid/>Usman Shahid</a>
|
<a href=/people/t/tobias-roding/>Tobias Röding</a>
|
<a href=/people/r/rakesh-chada/>Rakesh Chada</a>
|
<a href=/people/e/emir-kapanci/>Emir Kapanci</a>
|
<a href=/people/p/pradeep-natarajan/>Pradeep Natarajan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--17><div class="card-body p-3 small">Large-scale conversational assistants such as Cortana, Alexa, Google Assistant and Siri process requests through a series of modules for wake word detection, speech recognition, language understanding and response generation. An error in one of these modules can cascade through the system. Given the large traffic volumes in these assistants, it is infeasible to manually analyze the data, identify requests with processing errors and isolate the source of error. We present a machine learning system to address this challenge. First, we embed the incoming request and context, such as system response and subsequent turns, using pre-trained transformer models. Then, we combine these embeddings with encodings of additional metadata features (such as confidence scores from different modules in the online system) using a “mixing-encoder” to output the failure point predictions. Our system obtains 92.2% of human performance on this task while scaling to analyze the entire traffic in 8 different languages of a large-scale conversational assistant. We present detailed ablation studies analyzing the impact of different modeling choices.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.18.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.18/>Asynchronous Convergence in Multi-Task Learning via Knowledge Distillation from Converged Tasks</a></strong><br><a href=/people/w/weiyi-liu/>Weiyi Lu</a>
|
<a href=/people/s/sunny-rajagopalan/>Sunny Rajagopalan</a>
|
<a href=/people/p/priyanka-nigam/>Priyanka Nigam</a>
|
<a href=/people/j/jaspreet-singh/>Jaspreet Singh</a>
|
<a href=/people/x/xiaodi-sun/>Xiaodi Sun</a>
|
<a href=/people/y/yi-xu/>Yi Xu</a>
|
<a href=/people/b/belinda-zeng/>Belinda Zeng</a>
|
<a href=/people/t/trishul-chilimbi/>Trishul Chilimbi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--18><div class="card-body p-3 small">Multi-task learning (MTL) aims to solve multiple tasks jointly by sharing a base representation among them. This can lead to more efficient learning and better generalization, as compared to learning each task individually. However, one issue that often arises in MTL is the convergence speed between tasks varies due to differences in task difficulty, so it can be a challenge to simultaneously achieve the best performance on all tasks with a single model checkpoint. Various techniques have been proposed to address discrepancies in task convergence rate, including weighting the per-task losses and modifying task gradients. In this work, we propose a novel approach that avoids the problem of requiring all tasks to converge at the same rate, but rather allows for “asynchronous” convergence among the tasks where each task can converge on its own schedule. As our main contribution, we monitor per-task validation metrics and switch to a knowledge distillation loss once a task has converged instead of continuing to train on the true labels. This prevents the model from overfitting on converged tasks while it learns the remaining tasks. We evaluate the proposed method in two 5-task MTL setups consisting of internal e-commerce datasets. The results show that our method consistently outperforms existing loss weighting and gradient balancing approaches, achieving average improvements of 0.9% and 1.5% over the best performing baseline model in the two setups, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.19.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.19/>Augmenting Training Data for Massive Semantic Matching Models in Low-Traffic <span class=acl-fixed-case>E</span>-commerce Stores</a></strong><br><a href=/people/a/ashutosh-joshi/>Ashutosh Joshi</a>
|
<a href=/people/s/shankar-vishwanath/>Shankar Vishwanath</a>
|
<a href=/people/c/choon-teo/>Choon Teo</a>
|
<a href=/people/v/vaclav-petricek/>Vaclav Petricek</a>
|
<a href=/people/v/vishy-vishwanathan/>Vishy Vishwanathan</a>
|
<a href=/people/r/rahul-bhagat/>Rahul Bhagat</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--19><div class="card-body p-3 small">Extreme multi-label classification (XMC) systems have been successfully applied in e-commerce (Shen et al., 2020; Dahiya et al., 2021) for retrieving products based on customer behavior. Such systems require large amounts of customer behavior data (e.g. queries, clicks, purchases) for training. However, behavioral data is limited in low-traffic e-commerce stores, impacting performance of these systems. In this paper, we present a technique that augments behavioral training data via query reformulation. We use the Aggregated Label eXtreme Multi-label Classification (AL-XMC) system (Shen et al., 2020) as an example semantic matching model and show via crowd-sourced human judgments that, when the training data is augmented through query reformulations, the quality of AL-XMC improves over a baseline that does not use query reformulation. We also show in online A/B tests that our method significantly improves business metrics for the AL-XMC model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.20.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.20/>Retrieval Based Response Letter Generation For a Customer Care Setting</a></strong><br><a href=/people/b/biplob-biswas/>Biplob Biswas</a>
|
<a href=/people/r/renhao-cui/>Renhao Cui</a>
|
<a href=/people/r/rajiv-ramnath/>Rajiv Ramnath</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--20><div class="card-body p-3 small">Letter-like communications (such as email) are a major means of customer relationship management within customer-facing organizations. These communications are initiated on a channel by requests from customers and then responded to by the organization on the same channel. For decades, the job has almost entirely been conducted by human agents who attempt to provide the most appropriate reaction to the request. Rules have been made to standardize the overall customer service process and make sure the customers receive professional responses. Recent progress in natural language processing has made it possible to automate response generation. However, the diversity and open nature of customer queries and the lack of structured knowledge bases make this task even more challenging than typical task-oriented language generation tasks. Keeping those obstacles in mind, we propose a deep-learning based response letter generation framework that attempts to retrieve knowledge from historical responses and utilize it to generate an appropriate reply. Our model uses data augmentation to address the insufficiency of query-response pairs and employs a ranking mechanism to choose the best response from multiple potential options. We show that our technique outperforms the baselines by significant margins while producing consistent and informative responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.21.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.21/><span class=acl-fixed-case>M</span>edical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning</a></strong><br><a href=/people/a/angelo-ziletti/>Angelo Ziletti</a>
|
<a href=/people/a/alan-akbik/>Alan Akbik</a>
|
<a href=/people/c/christoph-berns/>Christoph Berns</a>
|
<a href=/people/t/thomas-herold/>Thomas Herold</a>
|
<a href=/people/m/marion-legler/>Marion Legler</a>
|
<a href=/people/m/martina-viell/>Martina Viell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--21><div class="card-body p-3 small">Medical coding (MC) is an essential pre-requisite for reliable data retrieval and reporting. Given a free-text <i>reported term</i> (RT) such as “pain of right thigh to the knee”, the task is to identify the matching <i>lowest-level term</i> (LLT) –in this case “unilateral leg pain”– from a very large and continuously growing repository of standardized medical terms. However, automating this task is challenging due to a large number of LLT codes (as of writing over <span class=tex-math>80\,000</span>), limited availability of training data for long tail/emerging classes, and the general high accuracy demands of the medical domain.With this paper, we introduce the MC task, discuss its challenges, and present a novel approach called xTARS that combines traditional BERT-based classification with a recent zero/few-shot learning approach (TARS). We present extensive experiments that show that our combined approach outperforms strong baselines, especially in the few-shot regime. The approach is developed and deployed at Bayer, live since November 2021. As we believe our approach potentially promising beyond MC, and to ensure reproducibility, we release the code to the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.22.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.22/>Knowledge extraction from aeronautical messages (<span class=acl-fixed-case>NOTAM</span>s) with self-supervised language models for aircraft pilots</a></strong><br><a href=/people/a/alexandre-arnold/>Alexandre Arnold</a>
|
<a href=/people/f/fares-ernez/>Fares Ernez</a>
|
<a href=/people/c/catherine-kobus/>Catherine Kobus</a>
|
<a href=/people/m/marion-cecile-martin/>Marion-Cécile Martin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--22><div class="card-body p-3 small">During their pre-flight briefings, aircraft pilots must analyse a long list of NoTAMs (NOtice To AirMen) indicating potential hazards along the flight route, sometimes up to pages for long-haul flights. NOTAM free-text fields typically have a very special phrasing, with lots of acronyms and domain-specific vocabulary, which makes it differ significantly from standard English. In this paper, we pretrain language models derived from BERT on circa 1 million unlabeled NOTAMs and reuse the learnt representations on three downstream tasks valuable for pilots: criticality prediction, named entity recognition and translation into a structured language called Airlang. This self-supervised approach, where smaller amounts of labeled data are enough for task-specific fine-tuning, is well suited in the aeronautical context since expert annotations are expensive and time-consuming. We present evaluation scores across the tasks showing a high potential for an operational usability of such models (by pilots, airlines or service providers), which is a first to the best of our knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.23.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.23/>Intent Discovery for Enterprise Virtual Assistants: Applications of Utterance Embedding and Clustering to Intent Mining</a></strong><br><a href=/people/m/minhua-chen/>Minhua Chen</a>
|
<a href=/people/b/badrinath-jayakumar/>Badrinath Jayakumar</a>
|
<a href=/people/m/michael-johnston/>Michael Johnston</a>
|
<a href=/people/s/s-eman-mahmoodi/>S. Eman Mahmoodi</a>
|
<a href=/people/d/daniel-pressel/>Daniel Pressel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--23><div class="card-body p-3 small">A key challenge in the creation and refinement of virtual assistants is the ability to mine unlabeled utterance data to discover common intents. We develop an approach to this problem that combines large-scale pre-training and multi-task learning to derive a semantic embedding that can be leveraged to identify clusters of utterances that correspond to unhandled intents. An utterance encoder is first trained with a language modeling objective and subsequently adapted to predict intent labels from a large collection of cross-domain enterprise virtual assistant data using a multi-task cosine softmax loss. Experimental evaluation shows significant advantages for this multi-step pre-training approach, with large gains in downstream clustering accuracy on new applications compared to standard sentence embedding approaches. The approach has been incorporated into an interactive discovery tool that enables visualization and exploration of intents by system analysts and builders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.24.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-industry.24" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.24/><span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>F</span>in<span class=acl-fixed-case>ED</span>: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking</a></strong><br><a href=/people/t/tom-ayoola/>Tom Ayoola</a>
|
<a href=/people/s/shubhi-tyagi/>Shubhi Tyagi</a>
|
<a href=/people/j/joseph-fisher/>Joseph Fisher</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/a/andrea-pierleoni/>Andrea Pierleoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--24><div class="card-body p-3 small">We introduce ReFinED, an efficient end-to-end entity linking model which uses fine-grained entity types and entity descriptions to perform linking. The model performs mention detection, fine-grained entity typing, and entity disambiguation for all mentions within a document in a single forward pass, making it more than 60 times faster than competitive existing approaches. ReFinED also surpasses state-of-the-art performance on standard entity linking datasets by an average of 3.7 F1. The model is capable of generalising to large-scale knowledge bases such as Wikidata (which has 15 times more entities than Wikipedia) and of zero-shot entity linking. The combination of speed, accuracy and scale makes ReFinED an effective and cost-efficient system for extracting entities from web-scale datasets, for which the model has been successfully deployed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.25.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.25/>Lightweight Transformers for Conversational <span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/d/daniel-pressel/>Daniel Pressel</a>
|
<a href=/people/w/wenshuo-liu/>Wenshuo Liu</a>
|
<a href=/people/m/michael-johnston/>Michael Johnston</a>
|
<a href=/people/m/minhua-chen/>Minhua Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--25><div class="card-body p-3 small">To understand how training on conversational language impacts performance of pre-trained models on downstream dialogue tasks, we build compact Transformer-based Language Models from scratch on several large corpora of conversational data. We compare the performance and characteristics of these models against BERT and other strong baselines on dialogue probing tasks. Commercial dialogue systems typically require a small footprint and fast execution time, but recent trends are in the other direction, with an ever-increasing number of parameters, resulting in difficulties in model deployment. We focus instead on training fast, lightweight models that excel at natural language understanding (NLU) and can replace existing lower-capacity conversational AI models with similar size and speed. In the process, we develop a simple but unique curriculum-based approach that moves from general-purpose to dialogue-targeted both in terms of data and objective. Our resultant models have around 1/3 the number of parameters of BERT-base and produce better representations for a wide array of intent detection datasets using linear and Mutual-Information probing techniques. Additionally, the models can be easily fine-tuned on a single consumer GPU card and deployed in near real-time production environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.26.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.26.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.26/><span class=acl-fixed-case>NER-MQMRC</span>: <span class=acl-fixed-case>F</span>ormulating Named Entity Recognition as Multi Question Machine Reading Comprehension</a></strong><br><a href=/people/a/anubhav-shrimal/>Anubhav Shrimal</a>
|
<a href=/people/a/avi-jain/>Avi Jain</a>
|
<a href=/people/k/kartik-mehta/>Kartik Mehta</a>
|
<a href=/people/p/promod-yenigalla/>Promod Yenigalla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--26><div class="card-body p-3 small">NER has been traditionally formulated as a sequence labeling task. However, there has been recent trend in posing NER as a machine reading comprehension task (Wang et al., 2020; Mengge et al., 2020), where entity name (or other information) is considered as a question, text as the context and entity value in text as answer snippet. These works consider MRC based on a single question (entity) at a time. We propose posing NER as a multi-question MRC task, where multiple questions (one question per entity) are considered at the same time for a single text. We propose a novel BERT-based multi-question MRC (NER-MQMRC) architecture for this formulation. NER-MQMRC architecture considers all entities as input to BERT for learning token embeddings with self-attention and leverages BERT-based entity representation for further improving these token embeddings for NER task. Evaluation on three NER datasets show that our proposed architecture leads to average 2.5 times faster training and 2.3 times faster inference as compared to NER-SQMRC framework based models by considering all entities together in a single pass. Further, we show that our model performance does not degrade compared to single-question based MRC (NER-SQMRC) (Devlin et al., 2019) leading to F1 gain of +0.41%, +0.32% and +0.27% for AE-Pub, Ecommerce5PT and Twitter datasets respectively. We propose this architecture primarily to solve large scale e-commerce attribute (or entity) extraction from unstructured text of a magnitude of 50k+ attributes to be extracted on a scalable production environment with high performance and optimised training and inference runtimes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.27.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.27.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.27/>What Do Users Care About? Detecting Actionable Insights from User Feedback</a></strong><br><a href=/people/k/kasturi-bhattacharjee/>Kasturi Bhattacharjee</a>
|
<a href=/people/r/rashmi-gangadharaiah/>Rashmi Gangadharaiah</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--27><div class="card-body p-3 small">Users often leave feedback on a myriad of aspects of a product which, if leveraged successfully, can help yield useful insights that can lead to further improvements down the line. Detecting actionable insights can be challenging owing to large amounts of data as well as the absence of labels in real-world scenarios. In this work, we present an aggregation and graph-based ranking strategy for unsupervised detection of these insights from real-world, noisy, user-generated feedback. Our proposed approach significantly outperforms strong baselines on two real-world user feedback datasets and one academic dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.28.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.28.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.28/><span class=acl-fixed-case>CTM</span> - A Model for Large-Scale Multi-View Tweet Topic Classification</a></strong><br><a href=/people/v/vivek-kulkarni/>Vivek Kulkarni</a>
|
<a href=/people/k/kenny-leung/>Kenny Leung</a>
|
<a href=/people/a/aria-haghighi/>Aria Haghighi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--28><div class="card-body p-3 small">Automatically associating social media posts with topics is an important prerequisite for effective search and recommendation on many social media platforms. However, topic classification of such posts is quite challenging because of (a) a large topic space (b) short text with weak topical cues, and (c) multiple topic associations per post. In contrast to most prior work which only focuses on post-classification into a small number of topics (<span class=tex-math>10-20</span>), we consider the task of large-scale topic classification in the context of Twitter where the topic space is 10 times larger with potentially multiple topic associations per Tweet. We address the challenges above and propose a novel neural model, that (a) supports a large topic space of 300 topics (b) takes a holistic approach to tweet content modeling – leveraging multi-modal content, author context, and deeper semantic cues in the Tweet. Our method offers an effective way to classify Tweets into topics at scale by yielding superior performance to other approaches (a relative lift of <span class=tex-math><strong>20</strong>%</span> in median average precision score) and has been successfully deployed in production at Twitter.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.29.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.29.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.29/>Developing a Production System for <span class=acl-fixed-case>P</span>urpose of <span class=acl-fixed-case>C</span>all Detection in Business Phone Conversations</a></strong><br><a href=/people/e/elena-khasanova/>Elena Khasanova</a>
|
<a href=/people/p/pooja-hiranandani/>Pooja Hiranandani</a>
|
<a href=/people/s/shayna-gardiner/>Shayna Gardiner</a>
|
<a href=/people/c/cheng-chen/>Cheng Chen</a>
|
<a href=/people/s/simon-corston-oliver/>Simon Corston-Oliver</a>
|
<a href=/people/x/xue-yong-fu/>Xue-Yong Fu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--29><div class="card-body p-3 small">For agents at a contact centre receiving calls, the most important piece of information is the reason for a given call. An agent cannot provide support on a call if they do not know why a customer is calling. In this paper we describe our implementation of a commercial system to detect Purpose of Call statements in English business call transcripts in real time. We present a detailed analysis of types of Purpose of Call statements and language patterns related to them, discuss an approach to collect rich training data by bootstrapping from a set of rules to a neural model, and describe a hybrid model which consists of a transformer-based classifier and a set of rules by leveraging insights from the analysis of call transcripts. The model achieved 88.6 F1 on average in various types of business calls when tested on real life data and has low inference time. We reflect on the challenges and design decisions when developing and deploying the system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.30.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.30.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.30/>Adversarial Text Normalization</a></strong><br><a href=/people/j/joanna-bitton/>Joanna Bitton</a>
|
<a href=/people/m/maya-pavlova/>Maya Pavlova</a>
|
<a href=/people/i/ivan-evtimov/>Ivan Evtimov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--30><div class="card-body p-3 small">Text-based adversarial attacks are becoming more commonplace and accessible to general internet users. As these attacks proliferate, the need to address the gap in model robustness becomes imminent. While retraining on adversarial data may increase performance, there remains an additional class of character-level attacks on which these models falter. Additionally, the process to retrain a model is time and resource intensive, creating a need for a lightweight, reusable defense. In this work, we propose the Adversarial Text Normalizer, a novel method that restores baseline performance on attacked content with low computational overhead. We evaluate the efficacy of the normalizer on two problem areas prone to adversarial attacks, i.e. Hate Speech and Natural Language Inference. We find that text normalization provides a task-agnostic defense against character-level attacks that can be implemented supplementary to adversarial retraining solutions, which are more suited for semantic alterations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.31.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.31.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.31/>Constraint-based Multi-hop Question Answering with Knowledge Graph</a></strong><br><a href=/people/s/sayantan-mitra/>Sayantan Mitra</a>
|
<a href=/people/r/roshni-ramnani/>Roshni Ramnani</a>
|
<a href=/people/s/shubhashis-sengupta/>Shubhashis Sengupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--31><div class="card-body p-3 small">The objective of a Question-Answering system over Knowledge Graph (KGQA) is to respond to natural language queries presented over the KG. A complex question answering system typically addresses one of the two categories of complexity: questions with constraints and questions involving multiple hops of relations. Most of the previous works have addressed these complexities separately. Multi-hop KGQA necessitates reasoning across numerous edges of the KG in order to arrive at the correct answer. Because KGs are frequently sparse, multi-hop KGQA presents extra complications. Recent works have developed KG embedding approaches to reduce KG sparsity by performing missing link prediction. In this paper, we tried to address multi-hop constrained-based queries using KG embeddings to generate more flexible query graphs. Empirical results indicate that the proposed methodology produces state-of-the-art outcomes on three KGQA datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.32.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.32.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.32/>Fast Bilingual Grapheme-To-Phoneme Conversion</a></strong><br><a href=/people/h/hwa-yeon-kim/>Hwa-Yeon Kim</a>
|
<a href=/people/j/jong-hwan-kim/>Jong-Hwan Kim</a>
|
<a href=/people/j/jae-min-kim/>Jae-Min Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--32><div class="card-body p-3 small">Autoregressive transformer (ART)-based grapheme-to-phoneme (G2P) models have been proposed for bi/multilingual text-to-speech systems. Although they have achieved great success, they suffer from high inference latency in real-time industrial applications, especially processing long sentence. In this paper, we propose a fast and high-performance bilingual G2P model. For fast and exact decoding, we used a non-autoregressive structured transformer-based architecture and data augmentation for predicting output length. Our model achieved better performance than that of the previous autoregressive model and about 2700% faster inference speed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.33.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.33.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-industry.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.33/>Knowledge Extraction From Texts Based on <span class=acl-fixed-case>W</span>ikidata</a></strong><br><a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a>
|
<a href=/people/j/johannes-heinecke/>Johannes Heinecke</a>
|
<a href=/people/f/frederic-herledan/>Frédéric Herledan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--33><div class="card-body p-3 small">This paper presents an effort within our company of developing knowledge extraction pipeline for English, which can be further used for constructing an entreprise-specific knowledge base. We present a system consisting of entity detection and linking, coreference resolution, and relation extraction based on the Wikidata schema. We highlight existing challenges of knowledge extraction by evaluating the deployed pipeline on real-world data. We also make available a database, which can serve as a new resource for sentential relation extraction, and we underline the importance of having balanced data for training classification models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.34.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.34.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-industry.34" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.34/><span class=acl-fixed-case>AIT-QA</span>: <span class=acl-fixed-case>Q</span>uestion Answering Dataset over Complex Tables in the Airline Industry</a></strong><br><a href=/people/y/yannis-katsis/>Yannis Katsis</a>
|
<a href=/people/s/saneem-chemmengath/>Saneem Chemmengath</a>
|
<a href=/people/v/vishwajeet-kumar/>Vishwajeet Kumar</a>
|
<a href=/people/s/samarth-bharadwaj/>Samarth Bharadwaj</a>
|
<a href=/people/m/mustafa-canim/>Mustafa Canim</a>
|
<a href=/people/m/michael-glass/>Michael Glass</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/f/feifei-pan/>Feifei Pan</a>
|
<a href=/people/j/jaydeep-sen/>Jaydeep Sen</a>
|
<a href=/people/k/karthik-sankaranarayanan/>Karthik Sankaranarayanan</a>
|
<a href=/people/s/soumen-chakrabarti/>Soumen Chakrabarti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--34><div class="card-body p-3 small">Table Question Answering (Table QA) systems have been shown to be highly accurate when trained and tested on open-domain datasets built on top of Wikipedia tables. However, it is not clear whether their performance remains the same when applied to domain-specific scientific and business documents, encountered in industrial settings, which exhibit some unique characteristics: (a) they contain tables with a much more complex layout than Wikipedia tables (including hierarchical row and column headers), (b) they contain domain-specific terms, and (c) they are typically not accompanied by domain-specific labeled data that can be used to train Table QA models. To understand the performance of Table QA approaches in this setting, we introduce AIT-QA; a domain-specific Table QA test dataset. While focusing on the airline industry, AIT-QA reflects the challenges that domain-specific documents pose to Table QA, outlined above. In this work, we describe the creation of the dataset and report zero-shot experimental results of three SOTA Table QA methods. The results clearly expose the limitations of current methods with a best accuracy of just 51.8%. We also present pragmatic table pre-processing steps to pivot and project complex tables into a layout suitable for the SOTA Table QA models. Finally, we provide data-driven insights on how different aspects of this setting (including hierarchical headers, domain-specific terminology, and paraphrasing) affect Table QA methods, in order to help the community develop improved methods for domain-specific Table QA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.35.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.35.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.35/>Parameter-efficient Continual Learning Framework in Industrial Real-time Text Classification System</a></strong><br><a href=/people/t/tao-zhu/>Tao Zhu</a>
|
<a href=/people/z/zhe-zhao/>Zhe Zhao</a>
|
<a href=/people/w/weijie-liu/>Weijie Liu</a>
|
<a href=/people/j/jiachi-liu/>Jiachi Liu</a>
|
<a href=/people/y/yiren-chen/>Yiren Chen</a>
|
<a href=/people/w/weiquan-mao/>Weiquan Mao</a>
|
<a href=/people/h/haoyan-liu/>Haoyan Liu</a>
|
<a href=/people/k/kunbo-ding/>Kunbo Ding</a>
|
<a href=/people/y/yudong-li/>Yudong Li</a>
|
<a href=/people/x/xuefeng-yang/>Xuefeng Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--35><div class="card-body p-3 small">Catastrophic forgetting is a challenge for model deployment in industrial real-time systems, which requires the model to quickly master a new task without forgetting the old one. Continual learning aims to solve this problem; however, it usually updates all the model parameters, resulting in extensive training times and the inability to deploy quickly. To address this challenge, we propose a parameter-efficient continual learning framework, in which efficient parameters are selected through an offline parameter selection strategy and then trained using an online regularization method. In our framework, only a few parameters need to be updated, which not only alleviates catastrophic forgetting, but also allows the model to be saved with the changed parameters instead of all parameters. Extensive experiments are conducted to examine the effectiveness of our proposal. We believe this paper will provide useful insights and experiences on developing deep learning-based online real-time systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.36.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.36.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.36/>Self-Aware Feedback-Based Self-Learning in Large-Scale Conversational <span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/p/pragaash-ponnusamy/>Pragaash Ponnusamy</a>
|
<a href=/people/c/clint-solomon-mathialagan/>Clint Solomon Mathialagan</a>
|
<a href=/people/g/gustavo-aguilar/>Gustavo Aguilar</a>
|
<a href=/people/c/chengyuan-ma/>Chengyuan Ma</a>
|
<a href=/people/c/chenlei-guo/>Chenlei Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--36><div class="card-body p-3 small">Self-learning paradigms in large-scale conversational AI agents tend to leverage user feedback in bridging between what they say and what they mean. However, such learning, particularly in Markov-based query rewriting systems have far from addressed the impact of these models on future training where successive feedback is inevitably contingent on the rewrite itself, especially in a continually updating environment. In this paper, we explore the consequences of this inherent lack of self-awareness towards impairing the model performance, ultimately resulting in both Type I and II errors over time. To that end, we propose augmenting the Markov Graph construction with a superposition-based adjacency matrix. Here, our method leverages an induced stochasticity to reactively learn a locally-adaptive decision boundary based on the performance of the individual rewrites in a bi-variate beta setting. We also surface a data augmentation strategy that leverages template-based generation in abridging complex conversation hierarchies of dialogs so as to simplify the learning process. All in all, we demonstrate that our self-aware model improves the overall PR-AUC by 27.45%, achieves a relative defect reduction of up to 31.22%, and is able to adapt quicker to changes in global preferences across a large number of customers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.37.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.37.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.naacl-industry.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.37/>Fast and Light-Weight Answer Text Retrieval in Dialogue Systems</a></strong><br><a href=/people/h/hui-wan/>Hui Wan</a>
|
<a href=/people/s/siva-sankalp-patel/>Siva Sankalp Patel</a>
|
<a href=/people/j/j-william-murdock/>J William Murdock</a>
|
<a href=/people/s/saloni-potdar/>Saloni Potdar</a>
|
<a href=/people/s/sachindra-joshi/>Sachindra Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--37><div class="card-body p-3 small">Dialogue systems can benefit from being able to search through a corpus of text to find information relevant to user requests, especially when encountering a request for which no manually curated response is available. The state-of-the-art technology for neural dense retrieval or re-ranking involves deep learning models with hundreds of millions of parameters. However, it is difficult and expensive to get such models to operate at an industrial scale, especially for cloud services that often need to support a big number of individually customized dialogue systems, each with its own text corpus. We report our work on enabling advanced neural dense retrieval systems to operate effectively at scale on relatively inexpensive hardware. We compare with leading alternative industrial solutions and show that we can provide a solution that is effective, fast, and cost-efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.38.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.38.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.38/><span class=acl-fixed-case>BLINK</span> with <span class=acl-fixed-case>E</span>lasticsearch for Efficient Entity Linking in Business Conversations</a></strong><br><a href=/people/m/md-tahmid-rahman-laskar/>Md Tahmid Rahman Laskar</a>
|
<a href=/people/c/cheng-chen/>Cheng Chen</a>
|
<a href=/people/a/aliaksandr-martsinovich/>Aliaksandr Martsinovich</a>
|
<a href=/people/j/jonathan-johnston/>Jonathan Johnston</a>
|
<a href=/people/x/xue-yong-fu/>Xue-Yong Fu</a>
|
<a href=/people/s/shashi-bhushan-tn/>Shashi Bhushan Tn</a>
|
<a href=/people/s/simon-corston-oliver/>Simon Corston-Oliver</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--38><div class="card-body p-3 small">An Entity Linking system aligns the textual mentions of entities in a text to their corresponding entries in a knowledge base. However, deploying a neural entity linking system for efficient real-time inference in production environments is a challenging task. In this work, we present a neural entity linking system that connects the product and organization type entities in business conversations to their corresponding Wikipedia and Wikidata entries. The proposed system leverages Elasticsearch to ensure inference efficiency when deployed in a resource limited cloud machine, and obtains significant improvements in terms of inference speed and memory consumption while retaining high accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.39.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.39.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.39/><span class=acl-fixed-case>Q</span>2<span class=acl-fixed-case>R</span>: A Query-to-Resolution System for Natural-Language Queries</a></strong><br><a href=/people/s/shiau-hong-lim/>Shiau Hong Lim</a>
|
<a href=/people/l/laura-wynter/>Laura Wynter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--39><div class="card-body p-3 small">We present a system for document retrieval that combines direct classification with standard content-based retrieval approaches to significantly improve the relevance of the retrieved documents. Our system exploits the availability of an imperfect but sizable amount of labeled data from past queries. For domains such as technical support, the proposed approach enhances the system’s ability to retrieve documents that are otherwise ranked very low based on content alone. The system is easy to implement and can make use of existing text ranking methods, augmenting them through the novel Q2R orchestration framework. Q2R has been extensively tested and is in use at IBM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.naacl-industry.40.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--naacl-industry--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.naacl-industry.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.naacl-industry.40.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.naacl-industry.40/><span class=acl-fixed-case>I</span>dentifying <span class=acl-fixed-case>C</span>orporate <span class=acl-fixed-case>C</span>redit <span class=acl-fixed-case>R</span>isk <span class=acl-fixed-case>S</span>entiments from <span class=acl-fixed-case>F</span>inancial <span class=acl-fixed-case>N</span>ews</a></strong><br><a href=/people/n/noujoud-ahbali/>Noujoud Ahbali</a>
|
<a href=/people/x/xinyuan-liu/>Xinyuan Liu</a>
|
<a href=/people/a/albert-nanda/>Albert Nanda</a>
|
<a href=/people/j/jamie-stark/>Jamie Stark</a>
|
<a href=/people/a/ashit-talukder/>Ashit Talukder</a>
|
<a href=/people/r/rupinder-paul-khandpur/>Rupinder Paul Khandpur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--naacl-industry--40><div class="card-body p-3 small">Credit risk management is one central practice for financial institutions, and such practice helps them measure and understand the inherent risk within their portfolios. Historically, firms relied on the assessment of default probabilities and used the press as one tool to gather insights on the latest credit event developments of an entity. However, due to the deluge of the current news coverage for companies, analyzing news manually by financial experts is considered a highly laborious task. To this end, we propose a novel deep learning-powered approach to automate news analysis and credit adverse events detection to score the credit sentiment associated with a company. This paper showcases a complete system that leverages news extraction and data enrichment with targeted sentiment entity recognition to detect companies and text classification to identify credit events. We developed a custom scoring mechanism to provide the company’s credit sentiment score (<span class=tex-math>CSS<sup>TM</sup></span>) based on these detected events. Additionally, using case studies, we illustrate how this score helps understand the company’s credit profile and discriminates between defaulters and non-defaulters.</div></div></div><hr><div id=2022findings-naacl><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.findings-naacl.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.findings-naacl/>Findings of the Association for Computational Linguistics: NAACL 2022</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.0/>Findings of the Association for Computational Linguistics: NAACL 2022</a></strong><br><a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Vladimir Meza Ruiz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.1.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.1/><span class=acl-fixed-case>P</span>ub<span class=acl-fixed-case>H</span>ealth<span class=acl-fixed-case>T</span>ab: <span class=acl-fixed-case>A</span> Public Health Table-based Dataset for Evidence-based Fact Checking</a></strong><br><a href=/people/m/mubashara-akhtar/>Mubashara Akhtar</a>
|
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/e/elena-simperl/>Elena Simperl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--1><div class="card-body p-3 small">Inspired by human fact checkers, who use different types of evidence (e.g. tables, images, audio) in addition to text, several datasets with tabular evidence data have been released in recent years. Whilst the datasets encourage research on table fact-checking, they rely on information from restricted data sources, such as Wikipedia for creating claims and extracting evidence data, making the fact-checking process different from the real-world process used by fact checkers. In this paper, we introduce PubHealthTab, a table fact-checking dataset based on real world public health claims and noisy evidence tables from sources similar to those used by real fact checkers. We outline our approach for collecting evidence data from various websites and present an in-depth analysis of our dataset. Finally, we evaluate state-of-the-art table representation and pre-trained models fine-tuned on our dataset, achieving an overall <span class=tex-math>F<sub>1</sub></span> score of 0.73.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.2/>Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context</a></strong><br><a href=/people/d/daniel-spokoyny/>Daniel Spokoyny</a>
|
<a href=/people/i/ivan-lee/>Ivan Lee</a>
|
<a href=/people/z/zhao-jin/>Zhao Jin</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--2><div class="card-body p-3 small">Physical measurements constitute a large portion of numbers in academic papers, engineering reports, and web tables. Current benchmarks fall short of properly evaluating numeracy of pretrained language models on measurements, hindering research on developing new methods and applying them to numerical tasks. To that end, we introduce a novel task, Masked Measurement Prediction (MMP), where a model learns to reconstruct a number together with its associated unit given masked text. MMP is useful for both training new numerically informed models as well as evaluating numeracy of existing systems. To address this task, we introduce a new Generative Masked Measurement (GeMM) model that jointly learns to predict numbers along with their units. We perform fine-grained analyses comparing our model with various ablations and baselines. We use linear probing of traditional pretrained transformer models (RoBERTa) to show that they significantly underperform jointly trained number-unit models, highlighting the difficulty of this new task and the benefits of our proposed pretraining approach. We hope this framework accelerates the progress towards building more robust numerical reasoning systems in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.3.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.3/><span class=acl-fixed-case>P</span>rompt<span class=acl-fixed-case>G</span>en: Automatically Generate Prompts using Generative Models</a></strong><br><a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/h/hongliang-fei/>Hongliang Fei</a>
|
<a href=/people/d/dingcheng-li/>Dingcheng Li</a>
|
<a href=/people/p/ping-li/>Ping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--3><div class="card-body p-3 small">Recently, prompt learning has received significant attention, where the downstream tasks are reformulated to the mask-filling task with the help of a textual prompt. The key point of prompt learning is finding the most appropriate prompt. This paper proposes a novel model PromptGen, which can automatically generate prompts conditional on the input sentence. PromptGen is the first work considering dynamic prompt generation for knowledge probing, based on a pre-trained generative model. To mitigate any label information leaking from the pre-trained generative model, when given a generated prompt, we replace the query input with “None”. We pursue that this perturbed context-free prompt cannot trigger the correct label. We evaluate our model on the knowledge probing LAMA benchmark, and show that PromptGen significantly outperforms other baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.4/>Improving Conversational Recommendation Systems’ Quality with Context-Aware Item Meta-Information</a></strong><br><a href=/people/b/bowen-yang/>Bowen Yang</a>
|
<a href=/people/c/cong-han/>Cong Han</a>
|
<a href=/people/y/yu-li/>Yu Li</a>
|
<a href=/people/l/lei-zuo/>Lei Zuo</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--4><div class="card-body p-3 small">A key challenge of Conversational Recommendation Systems (CRS) is to integrate the recommendation function and the dialog generation function smoothly. Previous works employ graph neural networks with external knowledge graphs (KG) to model individual recommendation items and integrate KGs with language models through attention mechanism for response generation. Although previous approaches prove effective, there is still room for improvement. For example, KG-based approaches only rely on entity relations and bag-of-words to recommend items and neglect the information in the conversational context. We propose to improve the usage of dialog context for both recommendation and response generation using an encoding architecture along with the self-attention mechanism of transformers. In this paper, we propose a simple yet effective architecture comprising a pre-trained language model (PLM) and an item metadata encoder to integrate the recommendation and the dialog generation better. The proposed item encoder learns to map item metadata to embeddings reflecting the rich information of the item, which can be matched with dialog context. The PLM then consumes the context-aware item embeddings and dialog context to generate high-quality recommendations and responses. Experimental results on the benchmark dataset ReDial show that our model obtains state-of-the-art results on both recommendation and response generation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.5/><span class=acl-fixed-case>SEQZERO</span>: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models</a></strong><br><a href=/people/j/jingfeng-yang/>Jingfeng Yang</a>
|
<a href=/people/h/haoming-jiang/>Haoming Jiang</a>
|
<a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/d/danqing-zhang/>Danqing Zhang</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--5><div class="card-body p-3 small">Recent research showed promising results on combining pretrained language models (LMs) with canonical utterance for few-shot semantic parsing. The canonical utterance is often lengthy and complex due to the compositional structure of formal languages. Learning to generate such canonical utterance requires significant amount of data to reach high performance. Fine-tuning with only few-shot samples, the LMs can easily forget pretrained knowledge, overfit spurious biases, and suffer from compositionally out-of-distribution generalization errors. To tackle these issues, we propose a novel few-shot semantic parsing method – SEQZERO. SEQZERO decomposes the problem into a sequence of sub-problems, which corresponds to the sub-clauses of the formal language. Based on the decomposition, the LMs only need to generate short answers using prompts for predicting sub-clauses. Thus, SEQZERO avoids generating a long canonical utterance at once. Moreover, SEQZERO employs not only a few-shot model but also a zero-shot model to alleviate the overfitting.In particular, SEQZERO brings out the merits from both models via ensemble equipped with our proposed constrained rescaling.SEQZERO achieves SOTA performance of BART-based models on GeoQuery and EcommerceQuery, which are two few-shot datasets with compositional data split.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.6/><span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>V</span>er<span class=acl-fixed-case>S</span>: Improving scientific claim verification with weak supervision and full-document context</a></strong><br><a href=/people/d/david-wadden/>David Wadden</a>
|
<a href=/people/k/kyle-lo/>Kyle Lo</a>
|
<a href=/people/l/lucy-lu-wang/>Lucy Lu Wang</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/i/iz-beltagy/>Iz Beltagy</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--6><div class="card-body p-3 small">The scientific claim verification task requires an NLP system to label scientific documents which Support or Refute an input claim, and to select evidentiary sentences (or rationales) justifying each predicted label. In this work, we present MultiVerS, which predicts a fact-checking label and identifies rationales in a multitask fashion based on a shared encoding of the claim and full document context. This approach accomplishes two key modeling goals. First, it ensures that all relevant contextual information is incorporated into each labeling decision. Second, it enables the model to learn from instances annotated with a document-level fact-checking label, but lacking sentence-level rationales. This allows MultiVerS to perform weakly-supervised domain adaptation by training on scientific documents labeled using high-precision heuristics. Our approach outperforms two competitive baselines on three scientific claim verification datasets, with particularly strong performance in zero / few-shot domain adaptation experiments. Our code and data are available at <a href=https://github.com/dwadden/multivers class=acl-markup-url>https://github.com/dwadden/multivers</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.7/>An Item Response Theory Framework for Persuasion</a></strong><br><a href=/people/a/anastassia-kornilova/>Anastassia Kornilova</a>
|
<a href=/people/v/vladimir-eidelman/>Vladimir Eidelman</a>
|
<a href=/people/d/daniel-douglass/>Daniel Douglass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--7><div class="card-body p-3 small">In this paper, we apply Item Response Theory, popular in education and political science research, to the analysis of argument persuasiveness in language. We empirically evaluate the model’s performance on three datasets, including a novel dataset in the area of political advocacy. We show the advantages of separating these components under several style and content representations, including evaluating the ability of the speaker embeddings generated by the model to parallel real-world observations about persuadability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.8/>Self-Supervised Contrastive Learning with Adversarial Perturbations for Defending Word Substitution-based Attacks</a></strong><br><a href=/people/z/zhao-meng/>Zhao Meng</a>
|
<a href=/people/y/yihan-dong/>Yihan Dong</a>
|
<a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a>
|
<a href=/people/r/roger-wattenhofer/>Roger Wattenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--8><div class="card-body p-3 small">In this paper, we present an approach to improve the robustness of BERT language models against word substitution-based adversarial attacks by leveraging adversarial perturbations for self-supervised contrastive learning. We create a word-level adversarial attack generating hard positives on-the-fly as adversarial examples during contrastive learning. In contrast to previous works, our method improves model robustness without using any labeled data. Experimental results show that our method improves robustness of BERT against four different word substitution-based adversarial attacks, and combining our method with adversarial training gives higher robustness than adversarial training alone. As our method improves the robustness of BERT purely with unlabeled data, it opens up the possibility of using large text datasets to train robust language models against word substitution-based adversarial attacks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.9/>Quiz Design Task: Helping Teachers Create Quizzes with Automated Question Generation</a></strong><br><a href=/people/p/philippe-laban/>Philippe Laban</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/l/lidiya-murakhovska/>Lidiya Murakhovs’ka</a>
|
<a href=/people/w/wenhao-liu/>Wenhao Liu</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--9><div class="card-body p-3 small">Question generation (QGen) models are often evaluated with standardized NLG metrics that are based on n-gram overlap. In this paper, we measure whether these metric improvements translate to gains in a practical setting, focusing on the use case of helping teachers automate the generation of reading comprehension quizzes. In our study, teachers building a quiz receive question suggestions, which they can either accept or refuse with a reason. Even though we find that recent progress in QGen leads to a significant increase in question acceptance rates, there is still large room for improvement, with the best model having only 68.4% of its questions accepted by the ten teachers who participated in our study. We then leverage the annotations we collected to analyze standard NLG metrics and find that model performance has reached projected upper-bounds, suggesting new automatic metrics are needed to guide QGen research forward.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.10/>In-<span class=acl-fixed-case>B</span>o<span class=acl-fixed-case>XBART</span>: Get Instructions into Biomedical Multi-Task Learning</a></strong><br><a href=/people/m/mihir-parmar/>Mihir Parmar</a>
|
<a href=/people/s/swaroop-mishra/>Swaroop Mishra</a>
|
<a href=/people/m/mirali-purohit/>Mirali Purohit</a>
|
<a href=/people/m/man-luo/>Man Luo</a>
|
<a href=/people/m/murad-mohammad/>Murad Mohammad</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--10><div class="card-body p-3 small">Single-task models have proven pivotal in solving specific tasks; however, they have limitations in real-world applications where multi-tasking is necessary and domain shifts are exhibited. Recently, instructional prompts have shown significant improvement towards multi-task generalization; however, the effect of instructional prompts and Multi-Task Learning (MTL) has not been systematically studied in the biomedical domain. Motivated by this, this paper explores the impact of instructional prompts for biomedical MTL. We introduce the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X) various categories. Using this meta-dataset, we propose a unified model termed as In-BoXBART, that can jointly learn all tasks of the BoX without any task-specific modules. To the best of our knowledge, this is the first attempt to propose a unified model in the biomedical domain and use instructions to achieve generalization across several biomedical tasks. Experimental results indicate that the proposed model: 1) outperforms single-task baseline by ~3% and multi-task (without instruction) baseline by ~18% on an average, and 2) shows ~23% improvement compared to single-task baseline in few-shot learning (i.e., 32 instances per task) on an average. Our analysis indicates that there is significant room for improvement across tasks in the BoX, implying the scope for future research direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.11/>How to Translate Your Samples and Choose Your Shots? Analyzing Translate-train & Few-shot Cross-lingual Transfer</a></strong><br><a href=/people/i/iman-jundi/>Iman Jundi</a>
|
<a href=/people/g/gabriella-lapesa/>Gabriella Lapesa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--11><div class="card-body p-3 small">Translate-train or few-shot cross-lingual transfer can be used to improve the zero-shot performance of multilingual pretrained language models. Few-shot utilizes high-quality low-quantity samples (often manually translated from the English corpus ). Translate-train employs a machine translation of the English corpus, resulting in samples with lower quality that could be scaled to high quantity. Given the lower cost and higher availability of machine translation compared to manual professional translation, it is important to systematically compare few-shot and translate-train, understand when each has an advantage, and investigate how to choose the shots to translate in order to increase the few-shot gain. This work aims to fill this gap: we compare and quantify the performance gain of few-shot vs. translate-train using three different base models and a varying number of samples for three tasks/datasets (XNLI, PAWS-X, XQuAD) spanning 17 languages. We show that scaling up the training data using machine translation gives a larger gain compared to using the small-scale (higher-quality) few-shot data. When few-shot is beneficial, we show that there are random sets of samples that perform better across languages and that the performance on English and on the machine-translation of the samples can both be used to choose the shots to manually translate for an increased few-shot gain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.12/>Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge</a></strong><br><a href=/people/y/yue-feng/>Yue Feng</a>
|
<a href=/people/z/zhen-han/>Zhen Han</a>
|
<a href=/people/m/mingming-sun/>Mingming Sun</a>
|
<a href=/people/p/ping-li/>Ping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--12><div class="card-body p-3 small">Open-domain question answering systems need to answer question of our interests with structured and unstructured information. However, existing approaches only select one source to generate answer or only conduct reasoning on structured information. In this paper, we pro- pose a Document-Entity Heterogeneous Graph Network, referred to as DEHG, to effectively integrate different sources of information, and conduct reasoning on heterogeneous information. DEHG employs a graph constructor to integrate structured and unstructured information, a context encoder to represent nodes and question, a heterogeneous information reasoning layer to conduct multi-hop reasoning on both information sources, and an answer decoder to generate answers for the question. Experimental results on HybirdQA dataset show that DEHG outperforms the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.13/><span class=acl-fixed-case>F</span>ed<span class=acl-fixed-case>NLP</span>: Benchmarking Federated Learning Methods for Natural Language Processing Tasks</a></strong><br><a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/c/chaoyang-he/>Chaoyang He</a>
|
<a href=/people/z/zihang-ze/>Zihang Ze</a>
|
<a href=/people/h/hulin-wang/>Hulin Wang</a>
|
<a href=/people/y/yufen-hua/>Yufen Hua</a>
|
<a href=/people/c/christophe-dupuy/>Christophe Dupuy</a>
|
<a href=/people/r/rahul-gupta/>Rahul Gupta</a>
|
<a href=/people/m/mahdi-soltanolkotabi/>Mahdi Soltanolkotabi</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/s/salman-avestimehr/>Salman Avestimehr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--13><div class="card-body p-3 small">Increasing concerns and regulations about data privacy and sparsity necessitate the study of privacy-preserving, decentralized learning methods for natural language processing (NLP) tasks. Federated learning (FL) provides promising approaches for a large number of clients (e.g., personal devices or organizations) to collaboratively learn a shared global model to benefit all clients while allowing users to keep their data locally. Despite interest in studying FL methods for NLP tasks, a systematic comparison and analysis is lacking in the literature. Herein, we present the FedNLP, a benchmarking framework for evaluating federated learning methods on four different task formulations: text classification, sequence tagging, question answering, and seq2seq. We propose a universal interface between Transformer-based language models (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under various non-IID partitioning strategies. Our extensive experiments with FedNLP provide empirical comparisons between FL methods and help us better understand the inherent challenges of this direction. The comprehensive analysis points to intriguing and exciting future research aimed at developing FL methods for NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.14.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.14/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>A</span>ttack: Natural Textual Attacks via Different Semantic Spaces</a></strong><br><a href=/people/b/boxin-wang/>Boxin Wang</a>
|
<a href=/people/c/chejian-xu/>Chejian Xu</a>
|
<a href=/people/x/xiangyu-liu/>Xiangyu Liu</a>
|
<a href=/people/y/yu-cheng/>Yu Cheng</a>
|
<a href=/people/b/bo-li-vanderbilt/>Bo Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--14><div class="card-body p-3 small">Recent studies show that pre-trained language models (LMs) are vulnerable to textual adversarial attacks. However, existing attack methods either suffer from low attack success rates or fail to search efficiently in the exponentially large perturbation space. We propose an efficient and effective framework SemAttack to generate natural adversarial text by constructing different semantic perturbation functions. In particular, SemAttack optimizes the generated perturbations constrained on generic semantic spaces, including typo space, knowledge space (e.g., WordNet), contextualized semantic space (e.g., the embedding space of BERT clusterings), or the combination of these spaces. Thus, the generated adversarial texts are more semantically close to the original inputs. Extensive experiments reveal that state-of-the-art (SOTA) large-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are still vulnerable to SemAttack. We further demonstrate that SemAttack is general and able to generate natural adversarial texts for different languages (e.g., English and Chinese) with high attack success rates. Human evaluations also confirm that our generated adversarial texts are natural and barely affect human performance. Our code is publicly available at <a href=https://github.com/AI-secure/SemAttack class=acl-markup-url>https://github.com/AI-secure/SemAttack</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.15.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.15/>Lacuna Reconstruction: Self-Supervised Pre-Training for Low-Resource Historical Document Transcription</a></strong><br><a href=/people/n/nikolai-vogler/>Nikolai Vogler</a>
|
<a href=/people/j/jonathan-allen/>Jonathan Allen</a>
|
<a href=/people/m/matthew-miller/>Matthew Miller</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--15><div class="card-body p-3 small">We present a self-supervised pre-training approach for learning rich visual language representations for both handwritten and printed historical document transcription. After supervised fine-tuning of our pre-trained encoder representations for low-resource document transcription on two languages, (1) a heterogeneous set of handwritten Islamicate manuscript images and (2) early modern English printed documents, we show a meaningful improvement in recognition accuracy over the same supervised model trained from scratch with as few as 30 line image transcriptions for training. Our masked language model-style pre-training strategy, where the model is trained to be able to identify the true masked visual representation from distractors sampled from within the same line, encourages learning robust contextualized language representations invariant to scribal writing style and printing noise present across documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.16.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.16/><span class=acl-fixed-case>F</span>ree<span class=acl-fixed-case>T</span>ransfer-<span class=acl-fixed-case>X</span>: Safe and Label-Free Cross-Lingual Transfer from Off-the-Shelf Models</a></strong><br><a href=/people/y/yinpeng-guo/>Yinpeng Guo</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--16><div class="card-body p-3 small">Cross-lingual transfer (CLT) is of various applications. However, labeled cross-lingual corpus is expensive or even inaccessible, especially in the fields where labels are private, such as diagnostic results of symptoms in medicine and user profiles in business. Nevertheless, there are off-the-shelf models in these sensitive fields. Instead of pursuing the original labels, a workaround for CLT is to transfer knowledge from the off-the-shelf models without labels. To this end, we define a novel CLT problem named FreeTransfer-X that aims to achieve knowledge transfer from the off-the-shelf models in rich-resource languages. To address the problem, we propose a 2-step knowledge distillation (KD, Hinton et al., 2015) framework based on multilingual pre-trained language models (mPLM). The significant improvement over strong neural machine translation (NMT) baselines demonstrates the effectiveness of the proposed method. In addition to reducing annotation cost and protecting private labels, the proposed method is compatible with different networks and easy to be deployed. Finally, a range of analyses indicate the great potential of the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.17/>Opportunities for Human-centered Evaluation of Machine Translation Systems</a></strong><br><a href=/people/d/daniel-liebling/>Daniel Liebling</a>
|
<a href=/people/k/katherine-heller/>Katherine Heller</a>
|
<a href=/people/s/samantha-robertson/>Samantha Robertson</a>
|
<a href=/people/w/wesley-deng/>Wesley Deng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--17><div class="card-body p-3 small">Machine translation models are embedded in larger user-facing systems. Although model evaluation has matured, evaluation at the systems level is still lacking. We review literature from both the translation studies and HCI communities about who uses machine translation and for what purposes. We emphasize an important difference in evaluating machine translation models versus the physical and cultural systems in which they are embedded. We then propose opportunities for improved measurement of user-facing translation systems. We pay particular attention to the need for design and evaluation to aid engendering trust and enhancing user agency in future machine translation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.18.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.18/>Aligning Generative Language Models with Human Values</a></strong><br><a href=/people/r/ruibo-liu/>Ruibo Liu</a>
|
<a href=/people/g/ge-zhang/>Ge Zhang</a>
|
<a href=/people/x/xinyu-feng/>Xinyu Feng</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--18><div class="card-body p-3 small">Although current large-scale generative language models (LMs) can show impressive insights about factual knowledge, they do not exhibit similar success with respect to human values judgements (e.g., whether or not the generations of an LM are moral). Existing methods learn human values either by directly mimicking the behavior of human data, or rigidly constraining the generation space to human-chosen tokens. These methods are inherently limited in that they do not consider the contextual and abstract nature of human values and as a result often fail when dealing with out-of-domain context or sophisticated and abstract human values. This paper proposes SENSEI, a new reinforcement learning based method that can embed human values judgements into each step of language generation. SENSEI deploys an Actor-Critic framework, where the Critic is a reward distributor that simulates the reward assignment procedure of humans, while the Actor guides the generation towards the maximum reward direction. Compared with five existing methods in three human values alignment datasets, SENSEI not only achieves higher alignment performance in terms of both automatic and human evaluations, but also shows improvements on robustness and transfer learning on unseen human values.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.19.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.19/><span class=acl-fixed-case>P</span>er<span class=acl-fixed-case>KGQA</span>: Question Answering over Personalized Knowledge Graphs</a></strong><br><a href=/people/r/ritam-dutt/>Ritam Dutt</a>
|
<a href=/people/k/kasturi-bhattacharjee/>Kasturi Bhattacharjee</a>
|
<a href=/people/r/rashmi-gangadharaiah/>Rashmi Gangadharaiah</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--19><div class="card-body p-3 small">Previous studies on question answering over knowledge graphs have typically operated over a single knowledge graph (KG). This KG is assumed to be known a priori and is lever- aged similarly for all users’ queries during inference. However, such an assumption is not applicable to real-world settings, such as health- care, where one needs to handle queries of new users over unseen KGs during inference. Furthermore, privacy concerns and high computational costs render it infeasible to query the single KG that has information about all users while answering a specific user’s query. The above concerns motivate our question answer- ing setting over personalized knowledge graphs (PERKGQA) where each user has restricted access to their KG. We observe that current state-of-the-art KGQA methods that require learning prior node representations fare poorly. We propose two complementary approaches, PATHCBR and PATHRGCN for PERKGQA. The former is a simple non-parametric technique that employs case-based reasoning, while the latter is a parametric approach using graph neural networks. Our proposed methods circumvent learning prior representations, can generalize to unseen KGs, and outperform strong baselines on an academic and an internal dataset by 6.5% and 10.5%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.20.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.20/>Zero-shot Cross-lingual Conversational Semantic Role Labeling</a></strong><br><a href=/people/h/han-wu/>Han Wu</a>
|
<a href=/people/h/haochen-tan/>Haochen Tan</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/s/shuqi-liu/>Shuqi Liu</a>
|
<a href=/people/l/lianwei-wu/>Lianwei Wu</a>
|
<a href=/people/l/linqi-song/>Linqi Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--20><div class="card-body p-3 small">While conversational semantic role labeling (CSRL) has shown its usefulness on Chinese conversational tasks, it is still under-explored in non-Chinese languages due to the lack of multilingual CSRL annotations for the parser training. To avoid expensive data collection and error-propagation of translation-based methods, we present a simple but effective approach to perform zero-shot cross-lingual CSRL.Our model implicitly learns language-agnostic, conversational structure-aware and semantically rich representations with the hierarchical encoders and elaborately designed pre-training objectives. Experimental results show that our model outperforms all baselines by large margins on two newly collected English CSRL test sets. More importantly, we confirm the usefulness of CSRL to non-Chinese conversational tasks such as the question-in-context rewriting task in English and the multi-turn dialogue response generation tasks in English, German and Japanese by incorporating the CSRL information into the downstream conversation-based models. We believe this finding is significant and will facilitate the research of non-Chinese dialogue tasks which suffer the problems of ellipsis and anaphora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.21.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.21/>A Framework to Generate High-Quality Datapoints for Multiple Novel Intent Detection</a></strong><br><a href=/people/a/ankan-mullick/>Ankan Mullick</a>
|
<a href=/people/s/sukannya-purkayastha/>Sukannya Purkayastha</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a>
|
<a href=/people/n/niloy-ganguly/>Niloy Ganguly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--21><div class="card-body p-3 small">Systems like Voice-command based conversational agents are characterized by a pre-defined set of skills or intents to perform user specified tasks. In the course of time, newer intents may emerge requiring retraining. However, the newer intents may not be explicitly announced and need to be inferred dynamically. Thus, there are two important tasks at hand (a). identifying emerging new intents, (b). annotating data of the new intents so that the underlying classifier can be retrained efficiently. The tasks become specially challenging when a large number of new intents emerge simultaneously and there is a limited budget of manual annotation. In this paper, we propose MNID (Multiple Novel Intent Detection) which is a cluster based framework to detect multiple novel intents with budgeted human annotation cost. Empirical results on various benchmark datasets (of different sizes) demonstrate that MNID, by intelligently using the budget for annotation, outperforms the baseline methods in terms of accuracy and F1-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.22.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.22/>Design Challenges for a Multi-Perspective Search Engine</a></strong><br><a href=/people/s/sihao-chen/>Sihao Chen</a>
|
<a href=/people/s/siyi-liu/>Siyi Liu</a>
|
<a href=/people/x/xander-uyttendaele/>Xander Uyttendaele</a>
|
<a href=/people/y/yi-zhang/>Yi Zhang</a>
|
<a href=/people/w/william-bruno/>William Bruno</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--22><div class="card-body p-3 small">Many users turn to document retrieval systems (e.g. search engines) to seek answers to controversial or open-ended questions. However, classical document retrieval systems fall short at delivering users a set of direct and diverse responses in such cases, which requires identifying responses within web documents in the context of the query, and aggregating the responses based on their different perspectives. The goal of this work is to survey and study the user information needs for building a multi-perspective search engine of such. We examine the challenges of synthesizing such language understanding objectives with document retrieval, and study a new <i>perspective-oriented</i> document retrieval paradigm. We discuss and assess the inherent natural language understanding challenges one needs to address in order to achieve the goal. Following the design challenges and principles, we propose and evaluate a practical prototype pipeline system. We use the prototype system to conduct a user survey in order to assess the utility of our paradigm, as well as understanding the user information needs when issuing controversial and open-ended queries to a search engine.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.23.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.23/>Exploring the Value of Multi-View Learning for Session-Aware Query Representation</a></strong><br><a href=/people/d/diego-ortiz/>Diego Ortiz</a>
|
<a href=/people/j/jose-g-moreno/>Jose Moreno</a>
|
<a href=/people/g/gilles-hubert/>Gilles Hubert</a>
|
<a href=/people/k/karen-pinel-sauvagnat/>Karen Pinel-Sauvagnat</a>
|
<a href=/people/l/lynda-tamine/>Lynda Tamine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--23><div class="card-body p-3 small">Recent years have witnessed a growing interest towards learning distributed query representations that are able to capture search intent semantics. Most existing approaches learn query embeddings using relevance supervision making them suited only to document ranking tasks. Besides, they generally consider either user’s query reformulations or system’s rankings whereas previous findings show that user’s query behavior and knowledge change depending on the system’s results, intertwine and affect each other during the completion of a search task. In this paper, we explore the value of multi-view learning for generic and unsupervised session-aware query representation learning. First, single-view query embeddings are obtained in separate spaces from query reformulations and document ranking representations using transformers. Then, we investigate the use of linear (CCA) and non linear (UMAP) multi-view learning methods, to align those spaces with the aim of revealing similarity traits in the multi-view shared space. Experimental evaluation is carried out in a query classification and session-based retrieval downstream tasks using respectively the KDD and TREC session datasets. The results show that multi-view learning is an effective and controllable approach for unsupervised learning of generic query representations and can reflect search behavior patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.24.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.24/>Hierarchical Relation-Guided Type-Sentence Alignment for Long-Tail Relation Extraction with Distant Supervision</a></strong><br><a href=/people/y/yang-li/>Yang Li</a>
|
<a href=/people/g/guodong-long/>Guodong Long</a>
|
<a href=/people/t/tao-shen/>Tao Shen</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--24><div class="card-body p-3 small">Distant supervision uses triple facts in knowledge graphs to label a corpus for relation extraction, leading to wrong labeling and long-tail problems. Some works use the hierarchy of relations for knowledge transfer to long-tail relations. However, a coarse-grained relation often implies only an attribute (e.g., domain or topic) of the distant fact, making it hard to discriminate relations based solely on sentence semantics. One solution is resorting to entity types, but open questions remain about how to fully leverage the information of entity types and how to align multi-granular entity types with sentences. In this work, we propose a novel model to enrich distantly-supervised sentences with entity types. It consists of (1) a pairwise type-enriched sentence encoding module injecting both context-free and -related backgrounds to alleviate sentence-level wrong labeling, and (2) a hierarchical type-sentence alignment module enriching a sentence with the triple fact’s basic attributes to support long-tail relations. Our model achieves new state-of-the-art results in overall and long-tail performance on benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.25/><span class=acl-fixed-case>PCEE</span>-<span class=acl-fixed-case>BERT</span>: Accelerating <span class=acl-fixed-case>BERT</span> Inference via Patient and Confident Early Exiting</a></strong><br><a href=/people/z/zhen-zhang/>Zhen Zhang</a>
|
<a href=/people/w/wei-zhu/>Wei Zhu</a>
|
<a href=/people/j/jinfan-zhang/>Jinfan Zhang</a>
|
<a href=/people/p/peng-wang/>Peng Wang</a>
|
<a href=/people/r/rize-jin/>Rize Jin</a>
|
<a href=/people/t/tae-sun-chung/>Tae-Sun Chung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--25><div class="card-body p-3 small">BERT and other pretrained language models (PLMs) are ubiquitous in modern NLP. Even though PLMs are the state-of-the-art (SOTA) models for almost every NLP task (CITATION), the significant latency during inference prohibits wider industrial usage. In this work, we propose Patient and Confident Early Exiting BERT (PCEE-BERT), an off-the-shelf sample-dependent early exiting method that can work with different PLMs and can also work along with popular model compression methods. With a multi-exit BERT as the backbone model, PCEE-BERT will make the early exiting decision if enough numbers (patience parameter) of consecutive intermediate layers are confident about their predictions. The entropy value measures the confidence level of an intermediate layer’s prediction. Experiments on the GLUE benchmark demonstrate that our method outperforms previous SOTA early exiting methods. Ablation studies show that: (a) our method performs consistently well on other PLMs, such as ALBERT and TinyBERT; (b) PCEE-BERT can achieve different speed-up ratios by adjusting the patience parameter and the confidence threshold. The code for PCEE-BERT can be found at <a href=https://github.com/michael-wzhu/PCEE-BERT class=acl-markup-url>https://github.com/michael-wzhu/PCEE-BERT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.26.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.26.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.26/>Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback</a></strong><br><a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/a/aman-madaan/>Aman Madaan</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/y/yiming-yang/>Yiming Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--26><div class="card-body p-3 small">Large language models (LMs), while powerful, are not immune to mistakes, but can be difficult to retrain. Our goal is for an LM to continue to improve after deployment, without retraining, using feedback from the user. Our approach pairs an LM with (i) a growing memory of cases where the user identified an output error and provided general feedback on how to correct it (ii) a <i>corrector model</i>, trained to translate this general feedback into specific edits to repair the model output. Given a new, unseen input, our model can then use feedback from similar, past cases to repair output errors that may occur. We instantiate our approach using an existing, fixed model for <i>script generation</i>, that takes a goal (e.g., “bake a cake”) and generates a partially ordered sequence of actions to achieve that goal, sometimes containing errors. Our memory-enhanced system, , learns to apply user feedback to repair such errors (up to 30 points improvement), while making a start at avoiding similar past mistakes on new, unseen examples (up to 7 points improvement in a controlled setting). This is a first step towards strengthening deployed models, potentially broadening their utility. Our code and data is available at <a href=https://github.com/allenai/interscript class=acl-markup-url>https://github.com/allenai/interscript</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.27.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.27/>One Size Does Not Fit All: The Case for Personalised Word Complexity Models</a></strong><br><a href=/people/s/sian-gooding/>Sian Gooding</a>
|
<a href=/people/m/manuel-tragut/>Manuel Tragut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--27><div class="card-body p-3 small">Complex Word Identification (CWI) aims to detect words within a text that a reader may find difficult to understand. It has been shown that CWI systems can improve text simplification, readability prediction and vocabulary acquisition modelling. However, the difficulty of a word is a highly idiosyncratic notion that depends on a reader’s first language, proficiency and reading experience. In this paper, we show that personal models are best when predicting word complexity for individual readers. We use a novel active learning framework that allows models to be tailored to individuals and release a dataset of complexity annotations and models as a benchmark for further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.28.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.28.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.28/><span class=acl-fixed-case>TEAM</span>: A multitask learning based Taxonomy Expansion approach for Attach and Merge</a></strong><br><a href=/people/b/bornali-phukon/>Bornali Phukon</a>
|
<a href=/people/a/anasua-mitra/>Anasua Mitra</a>
|
<a href=/people/r/ranbir-sanasam/>Ranbir Sanasam</a>
|
<a href=/people/p/priyankoo-sarmah/>Priyankoo Sarmah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--28><div class="card-body p-3 small">Taxonomy expansion is a crucial task. Most of Automatic expansion of taxonomy are of two types, attach and merge. In a taxonomy like WordNet, both merge and attach are integral parts of the expansion operations but majority of study consider them separately. This paper proposes a novel mult-task learning-based deep learning method known as Taxonomy Expansion with Attach and Merge (TEAM) that performs both the merge and attach operations. To the best of our knowledge this is the first study which integrates both merge and attach operations in a single model. The proposed models have been evaluated on three separate WordNet taxonomies, viz., Assamese, Bangla, and Hindi. From the various experimental setups, it is shown that TEAM outperforms its state-of-the-art counterparts for attach operation, and also provides highly encouraging performance for the merge operation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.29.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.29.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.29" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.29/>Extracting Temporal Event Relation with Syntax-guided Graph Transformer</a></strong><br><a href=/people/s/shuaicheng-zhang/>Shuaicheng Zhang</a>
|
<a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--29><div class="card-body p-3 small">Extracting temporal relations (e.g., before, after, and simultaneous) among events is crucial to natural language understanding. One of the key challenges of this problem is that when the events of interest are far away in text, the context in-between often becomes complicated, making it challenging to resolve the temporal relationship between them. This paper thus proposes a new Syntax-guided Graph Transformer network (SGT) to mitigate this issue, by (1) explicitly exploiting the connection between two events based on their dependency parsing trees, and (2) automatically locating temporal cues between two events via a novel syntax-guided attention mechanism. Experiments on two benchmark datasets, MATRES and TB-DENSE, show that our approach significantly outperforms previous state-of-the-art methods on both end-to-end temporal relation extraction and temporal relation classification with up to 7.9% absolute F-score gain; This improvement also proves to be robust on the contrast set of MATRES. We will make all the programs publicly available once the paper is accepted.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.30.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.30.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.30/>From Cognitive to Computational Modeling: <span class=acl-fixed-case>T</span>ext-based Risky Decision-Making Guided by Fuzzy Trace Theory</a></strong><br><a href=/people/j/jaron-mar/>Jaron Mar</a>
|
<a href=/people/j/jiamou-liu/>Jiamou Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--30><div class="card-body p-3 small">Understanding, modelling and predicting human risky decision-making is challenging due to intrinsic individual differences and irrationality. Fuzzy trace theory (FTT) is a powerful paradigm that explains human decision-making by incorporating gists, i.e., fuzzy representations of information which capture only its quintessential meaning. Inspired by Broniatowski and Reyna’s FTT cognitive model, we propose a computational framework which combines the effects of the underlying semantics and sentiments on text-based decision-making. In particular, we introduce Category-2-Vector to learn categorical gists and categorical sentiments, and demonstrate how our computational model can be optimised to predict risky decision-making in groups and individuals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.31.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.31.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.31" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.31/>Few-Shot Self-Rationalization with Natural Language Prompts</a></strong><br><a href=/people/a/ana-marasovic/>Ana Marasovic</a>
|
<a href=/people/i/iz-beltagy/>Iz Beltagy</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a>
|
<a href=/people/m/matthew-e-peters/>Matthew Peters</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--31><div class="card-body p-3 small">Self-rationalization models that predict task labels and generate free-text elaborations for their predictions could enable more intuitive interaction with NLP systems. These models are, however, currently trained with a large amount of human-written free-text explanations for each task which hinders their broader usage. We propose to study a more realistic setting of self-rationalization using few training examples. We present FEB—a standardized collection of four existing English-language datasets and associated metrics. We identify the right prompting approach by extensively exploring natural language prompts on FEB. Then, by using this prompt and scaling the model size, we demonstrate that making progress on few-shot self-rationalization is possible. We show there is still ample room for improvement in this task: the average plausibility of generated explanations assessed by human annotators is at most 51% (with GPT-3), while plausibility of human explanations is 76%. We hope that FEB and our proposed approach will spur the community to take on the few-shot self-rationalization challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.32.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.32.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.32/><span class=acl-fixed-case>DOC</span>m<span class=acl-fixed-case>T</span>5: Document-Level Pretraining of Multilingual Language Models</a></strong><br><a href=/people/c/chia-hsuan-lee/>Chia-Hsuan Lee</a>
|
<a href=/people/a/aditya-siddhant/>Aditya Siddhant</a>
|
<a href=/people/v/viresh-ratnakar/>Viresh Ratnakar</a>
|
<a href=/people/m/melvin-johnson/>Melvin Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--32><div class="card-body p-3 small">In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence language model pretrained with large-scale parallel documents. While previous approaches have focused on leveraging sentence-level parallel data, we try to build a general-purpose pretrained model that can understand and generate long documents. We propose a simple and effective pretraining objective - Document reordering Machine Translation (DrMT), in which the input documents that are shuffled and masked need to be translated. DrMT brings consistent improvements over strong baselines on a variety of document-level generation tasks, including over 12 BLEU points for seen-language pair document-level MT, over 7 BLEU points for unseen-language-pair document-level MT and over 3 ROUGE-1 points for seen-language pair cross-lingual summarization. We achieve state-of-the-art (SOTA) on WMT20 De-En and IWSLT15 Zh-En document translation tasks. We also conduct extensive analysis on various factors for document pretraining, including (1) the effects of pretraining data quality and (2) The effects of combining mono-lingual and cross-lingual pretraining. We plan to make our model checkpoints publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.33.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.33.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.33/>Literature-Augmented Clinical Outcome Prediction</a></strong><br><a href=/people/a/aakanksha-naik/>Aakanksha Naik</a>
|
<a href=/people/s/sravanthi-parasa/>Sravanthi Parasa</a>
|
<a href=/people/s/sergey-feldman/>Sergey Feldman</a>
|
<a href=/people/l/lucy-lu-wang/>Lucy Lu Wang</a>
|
<a href=/people/t/tom-hope/>Tom Hope</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--33><div class="card-body p-3 small">We present BEEP (Biomedical Evidence-Enhanced Predictions), a novel approach for clinical outcome prediction that retrieves patient-specific medical literature and incorporates it into predictive models. Based on each individual patient’s clinical notes, we train language models (LMs) to find relevant papers and fuse them with information from notes to predict outcomes such as in-hospital mortality. We develop methods to retrieve literature based on noisy, information-dense patient notes, and to augment existing outcome prediction models with retrieved papers in a manner that maximizes predictive accuracy. Our approach boosts predictive performance on three important clinical tasks in comparison to strong recent LM baselines, increasing F1 by up to 5 points and precision@Top-K by a large margin of over 25%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.34.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.34.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.34/>Improving Few-Shot Relation Classification by Prototypical Representation Learning with Definition Text</a></strong><br><a href=/people/l/li-zhenzhen/>Li Zhenzhen</a>
|
<a href=/people/y/yuyang-zhang/>Yuyang Zhang</a>
|
<a href=/people/j/jian-yun-nie/>Jian-Yun Nie</a>
|
<a href=/people/d/dongsheng-li/>Dongsheng Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--34><div class="card-body p-3 small">Few-shot relation classification is difficult because the few instances available may not represent well the relation patterns. Some existing approaches explored extra information such as relation definition, in addition to the instances, to learn a better relation representation. However, the encoding of the extra information has been performed independently from the labeled instances. In this paper, we propose to learn a prototype encoder from relation definition in a way that is useful for relation instance classification. To this end, we use a joint training approach to train both a prototype encoder from definition and an instance encoder. Extensive experiments on several datasets demonstrate the effectiveness and usefulness of our prototype encoder from definition text, enabling us to outperform state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.35.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.35.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.35" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.35/>Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner</a></strong><br><a href=/people/d/danilo-neves-ribeiro/>Danilo Neves Ribeiro</a>
|
<a href=/people/s/shen-wang/>Shen Wang</a>
|
<a href=/people/x/xiaofei-ma/>Xiaofei Ma</a>
|
<a href=/people/r/rui-dong/>Rui Dong</a>
|
<a href=/people/x/xiaokai-wei/>Xiaokai Wei</a>
|
<a href=/people/h/henghui-zhu/>Henghui Zhu</a>
|
<a href=/people/x/xinchi-chen/>Xinchi Chen</a>
|
<a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/z/zhiheng-huang/>Zhiheng Huang</a>
|
<a href=/people/a/andrew-arnold/>Andrew Arnold</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--35><div class="card-body p-3 small">Large language models have achieved high performance on various question answering (QA) benchmarks, but the explainability of their output remains elusive. Structured explanations, called entailment trees, were recently suggested as a way to explain the reasoning behind a QA system’s answer. In order to better generate such entailment trees, we propose an architecture called Iterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a given hypothesis by systematically generating a step-by-step explanation from textual premises. The IRGR model iteratively searches for suitable premises, constructing a single entailment step at a time. Contrary to previous approaches, our method combines generation steps and retrieval of premises, allowing the model to leverage intermediate conclusions, and mitigating the input size limit of baseline encoder-decoder models. We conduct experiments using the EntailmentBank dataset, where we outperform existing benchmarks on premise retrieval and entailment tree generation, with around 300% gain in overall correctness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.36.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.36.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.36.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.36/>Multimodal Intent Discovery from Livestream Videos</a></strong><br><a href=/people/a/adyasha-maharana/>Adyasha Maharana</a>
|
<a href=/people/q/quan-hung-tran/>Quan Tran</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/s/seunghyun-yoon/>Seunghyun Yoon</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/w/walter-chang/>Walter Chang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--36><div class="card-body p-3 small">Individuals, educational institutions, and businesses are prolific at generating instructional video content such as “how-to” and tutorial guides. While significant progress has been made in basic video understanding tasks, identifying procedural intent within these instructional videos is a challenging and important task that remains unexplored but essential to video summarization, search, and recommendations. This paper introduces the problem of instructional intent identification and extraction from software instructional livestreams. We construct and present a new multimodal dataset consisting of software instructional livestreams and containing manual annotations for both detailed and abstract procedural intent that enable training and evaluation of joint video and text understanding models. We then introduce a multimodal cascaded cross-attention model to efficiently combine the weaker and noisier video signal with the more discriminative text signal. Our experiments show that our proposed model brings significant gains compared to strong baselines, including large-scale pretrained multimodal models. Our analysis further identifies that the task benefits from spatial as well as motion features extracted from videos, and provides insight on how the video signal is preferentially used for intent discovery. We also show that current models struggle to comprehend the nature of abstract intents, revealing important gaps in multimodal understanding and paving the way for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.37.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.37.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.37.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.37/>A Question-Answer Driven Approach to Reveal Affirmative Interpretations from Verbal Negations</a></strong><br><a href=/people/m/md-mosharaf-hossain/>Md Mosharaf Hossain</a>
|
<a href=/people/l/luke-holman/>Luke Holman</a>
|
<a href=/people/a/anusha-kakileti/>Anusha Kakileti</a>
|
<a href=/people/t/tiffany-kao/>Tiffany Kao</a>
|
<a href=/people/n/nathan-brito/>Nathan Brito</a>
|
<a href=/people/a/aaron-mathews/>Aaron Mathews</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--37><div class="card-body p-3 small">This paper explores a question-answer driven approach to reveal affirmative interpretations from verbal negations (i.e., when a negation cue grammatically modifies a verb). We create a new corpus consisting of 4,472 verbal negations and discover that 67.1% of them convey that an event actually occurred. Annotators generate and answer 7,277 questions % converted for 4,000 for the 3,001 negations that convey an affirmative interpretation. We first cast the problem of revealing affirmative interpretations from negations as a natural language inference (NLI) classification task. Experimental results show that state-of-the-art transformers trained with existing NLI corpora are insufficient to reveal affirmative interpretations. We also observe, however, that fine-tuning brings substantial improvements. In addition to NLI classification, we also explore the more realistic task of generating affirmative interpretations directly from negations with the T5 transformer. We conclude that the generation task remains a challenge as T5 substantially underperforms humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.38.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.38.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.38/>Harmless Transfer Learning for Item Embeddings</a></strong><br><a href=/people/c/chengyue-gong/>Chengyue Gong</a>
|
<a href=/people/x/xiaocong-du/>Xiaocong Du</a>
|
<a href=/people/d/dhruv-choudhary/>Dhruv Choudhary</a>
|
<a href=/people/b/bhargav-bhushanam/>Bhargav Bhushanam</a>
|
<a href=/people/q/qiang-liu/>Qiang Liu</a>
|
<a href=/people/a/arun-kejariwal/>Arun Kejariwal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--38><div class="card-body p-3 small">Learning embedding layers (for classes, words, items, etc.) is a key component of lots of applications, ranging from natural language processing, recommendation systems to electronic health records, etc. However, the frequency of real-world items follows a long-tail distribution in these applications, causing naive training methods perform poorly on the rare items. A line of previous works address this problem by transferring the knowledge from the frequent items to rare items by introducing an auxiliary transfer loss. However, when defined improperly, the transfer loss may introduce harmful biases and deteriorate the performance. In this work, we propose a harmless transfer learning framework that limits the impact of the potential biases in both the definition and optimization of the transfer loss. On the definition side, we reduce the bias in transfer loss by focusing on the items to which information from high-frequency items can be efficiently transferred. On the optimization side, we leverage a lexicographic optimization framework to efficiently incorporate the information of the transfer loss without hurting the minimization of the main prediction loss function. Our method serves as a plug-in module and significantly boosts the performance on a variety of NLP and recommendation system tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.39.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.39.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.39" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.39/>Fine-grained Image Captioning with <span class=acl-fixed-case>CLIP</span> Reward</a></strong><br><a href=/people/j/jaemin-cho/>Jaemin Cho</a>
|
<a href=/people/s/seunghyun-yoon/>Seunghyun Yoon</a>
|
<a href=/people/a/ajinkya-kale/>Ajinkya Kale</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--39><div class="card-body p-3 small">Modern image captioning models are usually trained with text similarity objectives. However, since reference captions in public datasets often describe the most salient common objects, models trained with the text similarity objectives tend to ignore specific and detailed aspects of an image that distinguish it from others. Towards more descriptive and distinctive caption generation, we propose to use CLIP, a multimodal encoder trained on huge image-text pairs from the web, to calculate multi-modal similarity and use it as a reward function. We also propose a simple finetuning strategy of CLIP text encoder to improve grammar that does not require extra text annotation. This completely eliminates the need for reference captions during the reward computation. To comprehensively evaluate descriptive captions, we introduce FineCapEval, a new dataset for caption evaluation with fine-grained criteria: overall, background, object, relations. In our experiments on text-to-image retrieval and FineCapEval, the proposed CLIP-guided model generates more distinctive captions than the CIDEroptimized model. We also show that our unsupervised grammar finetuning of the CLIP text encoder alleviates the degeneration problem of the naive CLIP reward. Lastly, we show human analysis where the annotators strongly prefer CLIP reward to CIDEr and MLE objectives on diverse criteria.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.40.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.40/>Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control</a></strong><br><a href=/people/h/haopeng-zhang/>Haopeng Zhang</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/w/wojciech-kryscinski/>Wojciech Kryscinski</a>
|
<a href=/people/k/kazuma-hashimoto/>Kazuma Hashimoto</a>
|
<a href=/people/y/yingbo-zhou/>Yingbo Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--40><div class="card-body p-3 small">Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly guides the model to recognize faithfulness contents in the training phase. We further extend our method via intermediate fine-tuning on large but noisy data extracted from Wikipedia to unlock zero-shot summarization. We show that the proposed method leads to more faithful and salient abstractive summarization in supervised fine-tuning and zero-shot settings according to our experimental results on three benchmark datasets XSum, Pubmed, and SAMSum of very different domains and styles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.41.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.41.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.41.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.41" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.41/>Modeling Ideological Salience and Framing in Polarized Online Groups with Graph Neural Networks and Structured Sparsity</a></strong><br><a href=/people/v/valentin-hofmann/>Valentin Hofmann</a>
|
<a href=/people/x/xiaowen-dong/>Xiaowen Dong</a>
|
<a href=/people/j/janet-pierrehumbert/>Janet Pierrehumbert</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schuetze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--41><div class="card-body p-3 small">The increasing polarization of online political discourse calls for computational tools that automatically detect and monitor ideological divides in social media. We introduce a minimally supervised method that leverages the network structure of online discussion forums, specifically Reddit, to detect polarized concepts. We model polarization along the dimensions of salience and framing, drawing upon insights from moral psychology. Our architecture combines graph neural networks with structured sparsity learning and results in representations for concepts and subreddits that capture temporal ideological dynamics such as right-wing and left-wing radicalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.42.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.42.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.42" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.42/>On Measuring Social Biases in Prompt-Based Multi-Task Learning</a></strong><br><a href=/people/a/afra-feyza-akyurek/>Afra Feyza Akyürek</a>
|
<a href=/people/s/sejin-paik/>Sejin Paik</a>
|
<a href=/people/m/muhammed-kocyigit/>Muhammed Kocyigit</a>
|
<a href=/people/s/seda-akbiyik/>Seda Akbiyik</a>
|
<a href=/people/s/serife-leman-runyun/>Serife Leman Runyun</a>
|
<a href=/people/d/derry-wijaya/>Derry Wijaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--42><div class="card-body p-3 small">Large language models trained on a mixture of NLP tasks that are converted into a text-to-text format using prompts, can generalize into novel forms of language and handle novel tasks. A large body of work within prompt engineering attempts to understand the effects of input forms and prompts in achieving superior performance. We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs. In this paper, we study T0, a large-scale multi-task text-to-text language model trained using prompt-based learning. We consider two different forms of semantically equivalent inputs: question-answer format and premise-hypothesis format. We use an existing bias benchmark for the former BBQ and create the first bias benchmark in natural language inference BBNLI with hand-written hypotheses while also converting each benchmark into the other form. The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training, compared to premise-hypothesis form which is unlike its training examples. Code and data are released under <a href=https://github.com/feyzaakyurek/bbnli class=acl-markup-url>https://github.com/feyzaakyurek/bbnli</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.43.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.43.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.43/>Anti-Overestimation Dialogue Policy Learning for Task-Completion Dialogue System</a></strong><br><a href=/people/c/chang-tian/>Chang Tian</a>
|
<a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--43><div class="card-body p-3 small">A dialogue policy module is an essential part of task-completion dialogue systems. Recently, increasing interest has focused on reinforcement learning (RL)-based dialogue policy. Its favorable performance and wise action decisions rely on an accurate estimation of action values. The overestimation problem is a widely known issue of RL since its estimate of the maximum action value is larger than the ground truth, which results in an unstable learning process and suboptimal policy. This problem is detrimental to RL-based dialogue policy learning. To mitigate this problem, this paper proposes a dynamic partial average estimator (DPAV) of the ground truth maximum action value. DPAV calculates the partial average between the predicted maximum action value and minimum action value, where the weights are dynamically adaptive and problem-dependent. We incorporate DPAV into a deep Q-network as the dialogue policy and show that our method can achieve better or comparable results compared to top baselines on three dialogue datasets of different domains with a lower computational load. In addition, we also theoretically prove the convergence and derive the upper and lower bounds of the bias compared with those of other methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.44.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.44.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.44/><span class=acl-fixed-case>P</span>enn-<span class=acl-fixed-case>H</span>elsinki Parsed Corpus of Early <span class=acl-fixed-case>M</span>odern <span class=acl-fixed-case>E</span>nglish: First Parsing Results and Analysis</a></strong><br><a href=/people/s/seth-kulick/>Seth Kulick</a>
|
<a href=/people/n/neville-ryant/>Neville Ryant</a>
|
<a href=/people/b/beatrice-santorini/>Beatrice Santorini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--44><div class="card-body p-3 small">The Penn-Helsinki Parsed Corpus of Early Modern English (PPCEME), a 1.7-million-word treebank that is an important resource for research in syntactic change, has several properties that present potential challenges for NLP technologies. We describe these key features of PPCEME that make it challenging for parsing, including a larger and more varied set of function tags than in the Penn Treebank, and present results for this corpus using a modified version of the Berkeley Neural Parser and the approach to function tag recovery of Gabbard et al. (2006). While this approach to function tag recovery gives reasonable results, it is in some ways inappropriate for span-based parsers. We also present further evidence of the importance of in-domain pretraining for contextualized word representations. The resulting parser will be used to parse Early English Books Online, a 1.5 billion word corpus whose utility for the study of syntactic change will be greatly increased with the addition of accurate parse trees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.45.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.45.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.45" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.45/>Instilling Type Knowledge in Language Models via Multi-Task <span class=acl-fixed-case>QA</span></a></strong><br><a href=/people/s/shuyang-li/>Shuyang Li</a>
|
<a href=/people/m/mukund-sridhar/>Mukund Sridhar</a>
|
<a href=/people/c/chandana-satya-prakash/>Chandana Satya Prakash</a>
|
<a href=/people/j/jin-cao/>Jin Cao</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--45><div class="card-body p-3 small">Understanding human language often necessitates understanding entities and their place in a taxonomy of knowledge—their <i>types</i>.Previous methods to learn entity types rely on training classifiers on datasets with coarse, noisy, and incomplete labels. We introduce a method to instill fine-grained type knowledge in language models with text-to-text pre-training on type-centric questions leveraging knowledge base documents and knowledge graphs.We create the <b>WikiWiki</b> dataset: entities and passages from 10M Wikipedia articles linked to the Wikidata knowledge graph with 41K types.Models trained on WikiWiki achieve state-of-the-art performance in zero-shot dialog state tracking benchmarks, accurately infer entity types in Wikipedia articles, and can discover new types deemed useful by human judges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.46.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.46.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.46.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.46" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.46/><span class=acl-fixed-case>S</span>t<span class=acl-fixed-case>ATIK</span>: Structure and Text for Inductive Knowledge Graph Completion</a></strong><br><a href=/people/e/elan-markowitz/>Elan Markowitz</a>
|
<a href=/people/k/keshav-balasubramanian/>Keshav Balasubramanian</a>
|
<a href=/people/m/mehrnoosh-mirtaheri/>Mehrnoosh Mirtaheri</a>
|
<a href=/people/m/murali-annavaram/>Murali Annavaram</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a>
|
<a href=/people/g/greg-ver-steeg/>Greg Ver Steeg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--46><div class="card-body p-3 small">Knowledge graphs (KGs) often represent knowledge bases that are incomplete. Machine learning models can alleviate this by helping automate graph completion. Recently, there has been growing interest in completing knowledge bases that are dynamic, where previously unseen entities may be added to the KG with many missing links. In this paper, we present <b>StATIK</b>–<b>St</b>ructure <b>A</b>nd <b>T</b>ext for <b>I</b>nductive <b>K</b>nowledge Completion. StATIK uses Language Models to extract the semantic information from text descriptions, while using Message Passing Neural Networks to capture the structural information. StATIK achieves state of the art results on three challenging inductive baselines. We further analyze our hybrid model through detailed ablation studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.47.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.47.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.47" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.47/><span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>A</span>-<span class=acl-fixed-case>MT</span>: A Dataset and Benchmark for Contrastive Controlled <span class=acl-fixed-case>MT</span> with Application to Formality</a></strong><br><a href=/people/m/maria-nadejde/>Maria Nadejde</a>
|
<a href=/people/a/anna-currey/>Anna Currey</a>
|
<a href=/people/b/benjamin-hsu/>Benjamin Hsu</a>
|
<a href=/people/x/xing-niu/>Xing Niu</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/g/georgiana-dinu/>Georgiana Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--47><div class="card-body p-3 small">The machine translation (MT) task is typically formulated as that of returning a single translation for an input segment. However, in many cases, multiple different translations are valid and the appropriate translation may depend on the intended target audience, characteristics of the speaker, or even the relationship between speakers. Specific problems arise when dealing with honorifics, particularly translating from English into languages with formality markers. For example, the sentence “Are you sure?” can be translated in German as “Sind Sie sich sicher?” (formal register) or “Bist du dir sicher?” (informal). Using wrong or inconsistent tone may be perceived as inappropriate or jarring for users of certain cultures and demographics. This work addresses the problem of learning to control target language attributes, in this case formality, from a small amount of labeled contrastive data. We introduce an annotated dataset (CoCoA-MT) and an associated evaluation metric for training and evaluating formality-controlled MT models for six diverse target languages. We show that we can train formality-controlled models by fine-tuning on labeled contrastive data, achieving high accuracy (82% in-domain and 73% out-of-domain) while maintaining overall quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.48.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.48.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--48 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.48 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.48.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.48.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.48" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.48/><span class=acl-fixed-case>CLEAR</span>: Improving Vision-Language Navigation with Cross-Lingual, Environment-Agnostic Representations</a></strong><br><a href=/people/j/jialu-li/>Jialu Li</a>
|
<a href=/people/h/hao-tan/>Hao Tan</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--48><div class="card-body p-3 small">Vision-and-Language Navigation (VLN) tasks require an agent to navigate through the environment based on language instructions. In this paper, we aim to solve two key challenges in this task: utilizing multilingual instructions for improved instruction-path grounding and navigating through new environments that are unseen during training. To address these challenges, first, our agent learns a shared and visually-aligned cross-lingual language representation for the three languages (English, Hindi and Telugu) in the Room-Across-Room dataset. Our language representation learning is guided by text pairs that are aligned by visual information. Second, our agent learns an environment-agnostic visual representation by maximizing the similarity between semantically-aligned image pairs (with constraints on object-matching) from different environments. Our environment agnostic visual representation can mitigate the environment bias induced by low-level visual information. Empirically, on the Room-Across-Room dataset, we show that our multi-lingual agent gets large improvements in all metrics over the strong baseline model when generalizing to unseen environments with the cross-lingual language representation and the environment-agnostic visual representation. Furthermore, we show that our learned language and visual representations can be successfully transferred to the Room-to-Room and Cooperative Vision-and-Dialogue Navigation task, and present detailed qualitative and quantitative generalization and grounding analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.49.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.49.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.49/>Language Models for Code-switch Detection of te reo <span class=acl-fixed-case>M</span>āori and <span class=acl-fixed-case>E</span>nglish in a Low-resource Setting</a></strong><br><a href=/people/j/jesin-james/>Jesin James</a>
|
<a href=/people/v/vithya-yogarajan/>Vithya Yogarajan</a>
|
<a href=/people/i/isabella-shields/>Isabella Shields</a>
|
<a href=/people/c/catherine-i-watson/>Catherine Watson</a>
|
<a href=/people/p/peter-keegan/>Peter Keegan</a>
|
<a href=/people/k/keoni-mahelona/>Keoni Mahelona</a>
|
<a href=/people/p/peter-lucas-jones/>Peter-Lucas Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--49><div class="card-body p-3 small">Te reo Māori, New Zealand’s only indigenous language, is code-switched with English. Māori speakers are atleast bilingual, and the use of Māori is increasing in New Zealand English. Unfortunately, due to the minimal availability of resources, including digital data, Māori is under-represented in technological advances. Cloud-based multilingual systems such as Google and Microsoft Azure support Māori language detection. However, we provide experimental evidence to show that the accuracy of such systems is low when detecting Māori. Hence, with the support of Māori community, we collect Māori and bilingual data to use natural language processing (NLP) to improve Māori language detection. We train bilingual sub-word embeddings and provide evidence to show that our bilingual embeddings improve overall accuracy compared to the publicly-available monolingual embeddings. This improvement has been verified for various NLP tasks using three bilingual databases containing formal transcripts and informal social media data. We also show that BiLSTM with pre-trained Māori-English sub-word embeddings outperforms large-scale contextual language models such as BERT on down streaming tasks of detecting Māori language. However, this research uses large models ‘as is’ for transfer learning, where no further training was done on Māori-English data. The best accuracy of 87% was obtained using BiLSTM with bilingual embeddings to detect Māori-English code-switching points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.50.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.50.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.50" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.50/>Opponent Modeling in Negotiation Dialogues by Related Data Adaptation</a></strong><br><a href=/people/k/kushal-chawla/>Kushal Chawla</a>
|
<a href=/people/g/gale-lucas/>Gale Lucas</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/j/jonathan-gratch/>Jonathan Gratch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--50><div class="card-body p-3 small">Opponent modeling is the task of inferring another party’s mental state within the context of social interactions. In a multi-issue negotiation, it involves inferring the relative importance that the opponent assigns to each issue under discussion, which is crucial for finding high-value deals. A practical model for this task needs to infer these priorities of the opponent on the fly based on partial dialogues as input, without needing additional annotations for training. In this work, we propose a ranker for identifying these priorities from negotiation dialogues. The model takes in a partial dialogue as input and predicts the priority order of the opponent. We further devise ways to adapt related data sources for this task to provide more explicit supervision for incorporating the opponent’s preferences and offers, as a proxy to relying on granular utterance-level annotations. We show the utility of our proposed approach through extensive experiments based on two dialogue datasets. We find that the proposed data adaptations lead to strong performance in zero-shot and few-shot scenarios. Moreover, they allow the model to perform better than baselines while accessing fewer utterances from the opponent. We release our code to support future work in this direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.51.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.51/><span class=acl-fixed-case>LMT</span>urk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework</a></strong><br><a href=/people/m/mengjie-zhao/>Mengjie Zhao</a>
|
<a href=/people/f/fei-mi/>Fei Mi</a>
|
<a href=/people/y/yasheng-wang/>Yasheng Wang</a>
|
<a href=/people/m/minglei-li/>Minglei Li</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schuetze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--51><div class="card-body p-3 small">Vast efforts have been devoted to creating high-performance few-shot learners, i.e., large-scale pretrained language models (PLMs) that perform well with little downstream task training data. Training PLMs has incurred significant cost, but utilizing the few-shot learners is still challenging due to their enormous size. This work focuses on a crucial question: How to make effective use of these few-shot learners? We propose LMTurk, a novel approach that treats few-shotlearners as crowdsourcing workers. The rationale is that crowdsourcing workers are in fact few-shot learners: They are shown a few illustrative examples to learn about a task and then start annotating. LMTurk employs few-shot learners built upon PLMs as workers. We show that the resulting annotations can be utilized to train models that solve the task well and are small enough to be deployable in practical scenarios. Active learning is integrated into LMTurk to reduce the amount of queries made to PLMs, minimizing the computational cost of running PLM inference passes. Altogether, LMTurk is an important step towards making effective use of current PLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.52.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.52.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.52/>Entity Cloze By Date: What <span class=acl-fixed-case>LM</span>s Know About Unseen Entities</a></strong><br><a href=/people/y/yasumasa-onoe/>Yasumasa Onoe</a>
|
<a href=/people/m/michael-zhang/>Michael Zhang</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--52><div class="card-body p-3 small">Language models (LMs) are typically trained once on a large-scale corpus and used for years without being updated. However, in a dynamic world, new entities constantly arise. We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained. We derive a dataset of entities indexed by their origination date and paired with their English Wikipedia articles, from which we can find sentences about each entity. We evaluate LMs’ perplexity on masked spans within these sentences. We show that models more informed about the entities, such as those with access to a textual definition of them, achieve lower perplexity on this benchmark. Our experimental results demonstrate that making inferences about new entities remains difficult for LMs. Given its wide coverage on entity knowledge and temporal indexing, our dataset can be used to evaluate LMs and techniques designed to modify or extend their knowledge. Our automatic data collection pipeline can be easily used to continually update our benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.53.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.53.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.53/>Data Augmentation for Low-Resource Dialogue Summarization</a></strong><br><a href=/people/y/yongtai-liu/>Yongtai Liu</a>
|
<a href=/people/j/joshua-maynez/>Joshua Maynez</a>
|
<a href=/people/g/goncalo-simoes/>Gonçalo Simões</a>
|
<a href=/people/s/shashi-narayan/>Shashi Narayan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--53><div class="card-body p-3 small">We present DADS, a novel Data Augmentation technique for low-resource Dialogue Summarization. Our method generates synthetic examples by replacing sections of text from both the input dialogue and summary while preserving the augmented summary to correspond to a viable summary for the augmented dialogue. We utilize pretrained language models that produce highly likely dialogue alternatives while still being free to generate diverse alternatives. We applied our data augmentation method to the SAMSum dataset in low resource scenarios, mimicking real world problems such as chat, thread, and meeting summarization where large scale supervised datasets with human-written summaries are scarce. Through both automatic and human evaluations, we show that DADS shows strong improvements for low resource scenarios while generating topically diverse summaries without introducing additional hallucinations to the summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.54.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.54.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.54/>A Versatile Adaptive Curriculum Learning Framework for Task-oriented Dialogue Policy Learning</a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/h/hua-qin/>Hua Qin</a>
|
<a href=/people/w/wang-zhenyu/>Wang Zhenyu</a>
|
<a href=/people/c/changxi-zhu/>Changxi Zhu</a>
|
<a href=/people/s/shihan-wang/>Shihan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--54><div class="card-body p-3 small">Training a deep reinforcement learning-based dialogue policy with brute-force random sampling is costly. A new training paradigm was proposed to improve learning performance and efficiency by combining curriculum learning. However, attempts in the field of dialogue policy are very limited due to the lack of reliable evaluation of difficulty scores of dialogue tasks and the high sensitivity to the mode of progression through dialogue tasks. In this paper, we present a novel versatile adaptive curriculum learning (VACL) framework, which presents a substantial step toward applying automatic curriculum learning on dialogue policy tasks. It supports evaluating the difficulty of dialogue tasks only using the learning experiences of dialogue policy and skip-level selection according to their learning needs to maximize the learning efficiency. Moreover, an attractive feature of VACL is the construction of a generic, elastic global curriculum while training a good dialogue policy that could guide different dialogue policy learning without extra effort on re-training. The superiority and versatility of VACL are validated on three public dialogue datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.55.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.55.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.55" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.55/><span class=acl-fixed-case>L</span>ong<span class=acl-fixed-case>T</span>5: <span class=acl-fixed-case>E</span>fficient Text-To-Text Transformer for Long Sequences</a></strong><br><a href=/people/m/mandy-guo/>Mandy Guo</a>
|
<a href=/people/j/joshua-ainslie/>Joshua Ainslie</a>
|
<a href=/people/d/david-c-uthus/>David Uthus</a>
|
<a href=/people/s/santiago-ontanon/>Santiago Ontanon</a>
|
<a href=/people/j/jianmo-ni/>Jianmo Ni</a>
|
<a href=/people/y/yun-hsuan-sung/>Yun-Hsuan Sung</a>
|
<a href=/people/y/yinfei-yang/>Yinfei Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--55><div class="card-body p-3 small">Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present LongT5, a new model that explores the effects of scaling both the input length and model size at the same time. Specifically, we integrate attention ideas from long-input transformers (ETC), and adopt pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call Transient Global (TGlobal), which mimics ETC’s local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization and question answering tasks, as well as outperform the original T5 models on these tasks. We have open sourced our architecture and training code, as well as our pre-trained model checkpoints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.56.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.56.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.56/>Challenging <span class=acl-fixed-case>A</span>merica: Modeling language in longer time scales</a></strong><br><a href=/people/j/jakub-pokrywka/>Jakub Pokrywka</a>
|
<a href=/people/f/filip-gralinski/>Filip Graliński</a>
|
<a href=/people/k/krzysztof-jassem/>Krzysztof Jassem</a>
|
<a href=/people/k/karol-kaczmarek/>Karol Kaczmarek</a>
|
<a href=/people/k/krzysztof-jurkiewicz/>Krzysztof Jurkiewicz</a>
|
<a href=/people/p/piotr-wierzchon/>Piotr Wierzchon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--56><div class="card-body p-3 small">The aim of the paper is to apply, for historical texts, the methodology used commonly to solve various NLP tasks defined for contemporary data, i.e. pre-train and fine-tune large Transformer models. This paper introduces an ML challenge, named Challenging America (ChallAm), based on OCR-ed excerpts from historical newspapers collected from the Chronicling America portal. ChallAm provides a dataset of clippings, labeled with metadata on their origin, and paired with their textual contents retrieved by an OCR tool. Three, publicly available, ML tasks are defined in the challenge: to determine the article date, to detect the location of the issue, and to deduce a word in a text gap (cloze test). Strong baselines are provided for all three ChallAm tasks. In particular, we pre-trained a RoBERTa model from scratch from the historical texts. We also discuss the issues of discrimination and hate-speech present in the historical American texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.57.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.57.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.57" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.57/><span class=acl-fixed-case>LM</span>-<span class=acl-fixed-case>CORE</span>: Language Models with Contextually Relevant External Knowledge</a></strong><br><a href=/people/j/jivat-kaur/>Jivat Kaur</a>
|
<a href=/people/s/sumit-bhatia/>Sumit Bhatia</a>
|
<a href=/people/m/milan-aggarwal/>Milan Aggarwal</a>
|
<a href=/people/r/rachit-bansal/>Rachit Bansal</a>
|
<a href=/people/b/balaji-krishnamurthy/>Balaji Krishnamurthy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--57><div class="card-body p-3 small">Large transformer-based pre-trained language models have achieved impressive performance on a variety of knowledge-intensive tasks and can capture factual knowledge in their parameters. We argue that storing large amounts of knowledge in the model parameters is sub-optimal given the ever-growing amounts of knowledge and resource requirements. We posit that a more efficient alternative is to provide explicit access to contextually relevant structured knowledge to the model and train it to use that knowledge. We present LM-CORE – a general framework to achieve this– that allows <i>decoupling</i> of the language model training from the external knowledge source and allows the latter to be updated without affecting the already trained model. Experimental results show that LM-CORE, having access to external knowledge, achieves significant and robust outperformance over state-of-the-art knowledge-enhanced language models on knowledge probing tasks; can effectively handle knowledge updates; and performs well on two downstream tasks. We also present a thorough error analysis highlighting the successes and failures of LM-CORE. Our code and model checkpoints are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.58.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.58.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.58" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.58/>A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis</a></strong><br><a href=/people/e/ehsan-hosseini-asl/>Ehsan Hosseini-Asl</a>
|
<a href=/people/w/wenhao-liu/>Wenhao Liu</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--58><div class="card-body p-3 small">Sentiment analysis is an important task in natural language processing. In recent works, pre-trained language models are often used to achieve state-of-the-art results, especially when training data is scarce. It is common to fine-tune on the downstream task, usually by adding task-specific layers on top of the model. In this paper, we focus on aspect-based sentiment analysis, which involves extracting aspect term, category, and predicting their corresponding polarities. In particular, we are interested in few-shot settings. We propose to reformulate the extraction and prediction tasks into the sequence generation task, using a generative language model with unidirectional attention (GPT2 is used unless stated otherwise). This way, the model learns to accomplish the tasks via language generation without the need of training task-specific layers. Our evaluation results on the single-task polarity prediction show that our approach outperforms the previous state-of-the-art (based on BERT) on average performance by a large margins in few-shot and full-shot settings. More importantly, our generative approach significantly reduces the model variance caused by low-resource data. We further demonstrate that the proposed generative language model can handle joint and multi-task settings, unlike previous work. We observe that the proposed sequence generation method achieves further improved performances on polarity prediction when the model is trained via joint and multi-task settings. Further evaluation on similar sentiment analysis datasets, SST-2, SST-5 and OOS intent detection validates the superiority and noise robustness of generative language model in few-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.59.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.59.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.59/>Permutation Invariant Strategy Using Transformer Encoders for Table Understanding</a></strong><br><a href=/people/s/sarthak-dash/>Sarthak Dash</a>
|
<a href=/people/s/sugato-bagchi/>Sugato Bagchi</a>
|
<a href=/people/n/nandana-mihindukulasooriya/>Nandana Mihindukulasooriya</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--59><div class="card-body p-3 small">Representing text in tables is essential for many business intelligence tasks such as semantic retrieval, data exploration and visualization, and question answering. Existing methods that leverage pretrained Transformer encoders range from a simple construction of pseudo-sentences by concatenating text across rows or columns to complex parameter-intensive models that encode table structure and require additional pretraining. In this work, we introduce a novel encoding strategy for Transformer encoders that preserves the critical property of permutation invariance across rows or columns. Unlike existing state-of-the-art methods for Table Understanding, our proposed approach does not require any additional pretraining and still substantially outperforms existing methods in almost all instances. We demonstrate the effectiveness of our proposed approach on three table interpretation tasks: column type annotation, relation extraction, and entity linking through extensive experiments on existing tabular datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.60.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.60.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.60" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.60/><span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>NERD</span>: A Multilingual, Multi-Genre and Fine-Grained Dataset for Named Entity Recognition (and Disambiguation)</a></strong><br><a href=/people/s/simone-tedeschi/>Simone Tedeschi</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--60><div class="card-body p-3 small">Named Entity Recognition (NER) is the task of identifying named entities in texts and classifying them through specific semantic categories, a process which is crucial for a wide range of NLP applications. Current datasets for NER focus mainly on coarse-grained entity types, tend to consider a single textual genre and to cover a narrow set of languages, thus limiting the general applicability of NER systems. In this work, we design a new methodology for automatically producing NER annotations, and address the aforementioned limitations by introducing a novel dataset that covers 10 languages, 15 NER categories and 2 textual genres. We also introduce a manually-annotated test set, and extensively evaluate the quality of our novel dataset on both this new test set and standard benchmarks for NER.In addition, in our dataset, we include: i) disambiguation information to enable the development of multilingual entity linking systems, and ii) image URLs to encourage the creation of multimodal systems. We release our dataset at <a href=https://github.com/Babelscape/multinerd class=acl-markup-url>https://github.com/Babelscape/multinerd</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.61.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.61.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.61/>Learning to Embed Multi-Modal Contexts for Situated Conversational Agents</a></strong><br><a href=/people/h/haeju-lee/>Haeju Lee</a>
|
<a href=/people/o/oh-joon-kwon/>Oh Joon Kwon</a>
|
<a href=/people/y/yunseon-choi/>Yunseon Choi</a>
|
<a href=/people/m/minho-park/>Minho Park</a>
|
<a href=/people/r/ran-han/>Ran Han</a>
|
<a href=/people/y/yoonhyung-kim/>Yoonhyung Kim</a>
|
<a href=/people/j/jinhyeon-kim/>Jinhyeon Kim</a>
|
<a href=/people/y/youngjune-lee/>Youngjune Lee</a>
|
<a href=/people/h/haebin-shin/>Haebin Shin</a>
|
<a href=/people/k/kangwook-lee/>Kangwook Lee</a>
|
<a href=/people/k/kee-eung-kim/>Kee-Eung Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--61><div class="card-body p-3 small">The Situated Interactive Multi-Modal Conversations (SIMMC) 2.0 aims to create virtual shopping assistants that can accept complex multi-modal inputs, i.e. visual appearances of objects and user utterances. It consists of four subtasks, multi-modal disambiguation (MM-Disamb), multi-modal coreference resolution (MM-Coref), multi-modal dialog state tracking (MM-DST), and response retrieval and generation. While many task-oriented dialog systems usually tackle each subtask separately, we propose a jointly learned multi-modal encoder-decoder that incorporates visual inputs and performs all four subtasks at once for efficiency. This approach won the MM-Coref and response retrieval subtasks and nominated runner-up for the remaining subtasks using a single unified model at the 10th Dialog Systems Technology Challenge (DSTC10), setting a high bar for the novel task of multi-modal task-oriented dialog systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.62.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.62.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.62.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.62" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.62/>Measuring and Improving Compositional Generalization in Text-to-<span class=acl-fixed-case>SQL</span> via Component Alignment</a></strong><br><a href=/people/y/yujian-gan/>Yujian Gan</a>
|
<a href=/people/x/xinyun-chen/>Xinyun Chen</a>
|
<a href=/people/q/qiuping-huang/>Qiuping Huang</a>
|
<a href=/people/m/matthew-purver/>Matthew Purver</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--62><div class="card-body p-3 small">In text-to-SQL tasks — as in much of NLP — <i>compositional generalization</i> is a major challenge: neural networks struggle with compositional generalization where training and test distributions differ. However, most recent attempts to improve this are based on word-level synthetic data or specific dataset splits to generate compositional biases. In this work, we propose a clause-level compositional example generation method. We first split the sentences in the Spider text-to-SQL dataset into sub-sentences, annotating each sub-sentence with its corresponding SQL clause, resulting in a new dataset Spider-SS. We then construct a further dataset, Spider-CG, by composing Spider-SS sub-sentences in different combinations, to test the ability of models to generalize compositionally. Experiments show that existing models suffer significant performance degradation when evaluated on Spider-CG, even though every sub-sentence is seen during training. To deal with this problem, we modify a number of state-of-the-art models to train on the segmented data of Spider-SS, and we show that this method improves the generalization performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.63.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.63.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.63.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.63/>Empathetic Persuasion: Reinforcing Empathy and Persuasiveness in Dialogue Systems</a></strong><br><a href=/people/a/azlaan-mustafa-samad/>Azlaan Mustafa Samad</a>
|
<a href=/people/k/kshitij-mishra/>Kshitij Mishra</a>
|
<a href=/people/m/mauajama-firdaus/>Mauajama Firdaus</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--63><div class="card-body p-3 small">Persuasion is an intricate process involving empathetic connection between two individuals. Plain persuasive responses may make a conversation non-engaging. Even the most well-intended and reasoned persuasive conversations can fall through in the absence of empathetic connection between the speaker and listener. In this paper, we propose a novel task of incorporating empathy when generating persuasive responses. We develop an empathetic persuasive dialogue system by fine-tuning a maximum likelihood Estimation (MLE)-based language model in a reinforcement learning (RL) framework. To design feedbacks for our RL-agent, we define an effective and efficient reward function considering consistency, repetitiveness, emotion and persuasion rewards to ensure consistency, non-repetitiveness, empathy and persuasiveness in the generated responses. Due to lack of emotion annotated persuasive data, we first annotate the existing Persuaion For Good dataset with emotions, then build transformer based classifiers to provide emotion based feedbacks to our RL agent. Experimental results confirm that our proposed model increases the rate of generating persuasive responses as compared to the available state-of-the-art dialogue models while making the dialogues empathetically more engaging and retaining the language quality in responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.64.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.64.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.64/>Attention Fusion: a light yet efficient late fusion mechanism for task adaptation in <span class=acl-fixed-case>NLU</span></a></strong><br><a href=/people/j/jin-cao/>Jin Cao</a>
|
<a href=/people/c/chandana-satya-prakash/>Chandana Satya Prakash</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--64><div class="card-body p-3 small">Fine-tuning a pre-trained language model using annotated data has become the de-facto standard for adapting general-purpose pre-trained models like BERT to downstream tasks. However, given the trend of larger pre-trained models, fine-tuning these models for each downstream task is parameter-inefficient and computationally-expensive deeming this approach sub-optimal for adoption by NLU systems. In recent years, various approaches have been proposed for parameter efficient task adaptation such as Adaptor, Bitfit, Prompt tuning, Prefix tuning etc. However, most of these efforts propose to insert task specific parameters in-between or inside intermediate layers of the pre-trained encoder resulting in higher computational cost due to back-propagation of errors to all layers. To mitigate this issue, we propose a light but efficient, attention based fusion module which computes task-attuned token representations by aggregating intermediate layer representations from a pre-trained network. Our proposed fusion module trains only 0.0009% of total parameters and achieves competitive performance to the standard fine-tuning approach on various tasks. It is also decoupled from the pre-trained network making it efficient during computation and scalable during deployment. Last but not the least, we demonstrate that our proposed attention-fusion mechanism can transfer effectively to different languages for further re-use and expansion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.65.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.65.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.65.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.65/>The Limits of Word Level Differential Privacy</a></strong><br><a href=/people/j/justus-mattern/>Justus Mattern</a>
|
<a href=/people/b/benjamin-weggenmann/>Benjamin Weggenmann</a>
|
<a href=/people/f/florian-kerschbaum/>Florian Kerschbaum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--65><div class="card-body p-3 small">As the issues of privacy and trust are receiving increasing attention within the research community, various attempts have been made to anonymize textual data. A significant subset of these approaches incorporate differentially private mechanims to perturb word embeddings, thus replacing individual words in a sentence. While these methods represent very important contributions, have various advantages over other techniques and do show anonymization capabilities,they have several shortcomings. In this paper, we investigate these weaknesses and demonstrate significant mathematical constraints diminishing the theoretical privacy guaranteeas well as major practical shortcomings with regard to the protection against deanonymization attacks, the preservation of content of the original sentences as well as the quality of the language output. Finally, we propose a new method for text anonymization based on transformer based language models fine-tuned for paraphrasing that circumvents most of the identified weaknesses and also offers a formal privacy guarantee. We evaluate the performance of our method via thourough experimentation and demonstrate superior performance over the discussed mechanisms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.66.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.66.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.66/>Efficient Learning of Multiple <span class=acl-fixed-case>NLP</span> Tasks via Collective Weight Factorization on <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/c/christos-papadopoulos/>Christos Papadopoulos</a>
|
<a href=/people/y/yannis-panagakis/>Yannis Panagakis</a>
|
<a href=/people/m/manolis-koubarakis/>Manolis Koubarakis</a>
|
<a href=/people/m/mihalis-nicolaou/>Mihalis Nicolaou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--66><div class="card-body p-3 small">The Transformer architecture continues to show remarkable performance gains in many Natural Language Processing tasks. However, obtaining such state-of-the-art performance in different tasks requires fine-tuning the same model separately for each task. Clearly, such an approach is demanding in terms of both memory requirements and computing power. In this paper, aiming to improve training efficiency across multiple tasks, we propose to collectively factorize the weighs of the multi-head attention module of a pre-trained Transformer. We test our proposed method on finetuning multiple natural language understanding tasks by employing BERT-Large as an instantiation of the Transformer and the GLUE as the evaluation benchmark. Experimental results show that our method requires training and storing only 1% of the initial model parameters for each task and matches or improves the original fine-tuned model’s performance for each task while effectively decreasing the parameter requirements by two orders of magnitude. Furthermore, compared to well-known adapter-based alternatives on the GLUE benchmark, our method consistently reaches the same levels of performance while requiring approximately four times fewer total and trainable parameters per task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.67.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.67.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.67" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.67/>Learning Rich Representation of Keyphrases from Text</a></strong><br><a href=/people/m/mayank-kulkarni/>Mayank Kulkarni</a>
|
<a href=/people/d/debanjan-mahata/>Debanjan Mahata</a>
|
<a href=/people/r/ravneet-arora/>Ravneet Arora</a>
|
<a href=/people/r/rajarshi-bhowmik/>Rajarshi Bhowmik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--67><div class="card-body p-3 small">In this work, we explore how to train task-specific language models aimed towards learning rich representation of keyphrases from text documents. We experiment with different masking strategies for pre-training transformer language models (LMs) in discriminative as well as generative settings. In the discriminative setting, we introduce a new pre-training objective - Keyphrase Boundary Infilling with Replacement (KBIR), showing large gains in performance (upto 8.16 points in F1) over SOTA, when the LM pre-trained using KBIR is fine-tuned for the task of keyphrase extraction. In the generative setting, we introduce a new pre-training setup for BART - KeyBART, that reproduces the keyphrases related to the input text in the CatSeq format, instead of the denoised original input. This also led to gains in performance (upto 4.33 points in F1@M) over SOTA for keyphrase generation. Additionally, we also fine-tune the pre-trained language models on named entity recognition (NER), question answering (QA), relation extraction (RE), abstractive summarization and achieve comparable performance with that of the SOTA, showing that learning rich representation of keyphrases is indeed beneficial for many other fundamental NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.68.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.68.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.68.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.68/>Improving Contextual Representation with Gloss Regularized Pre-training</a></strong><br><a href=/people/y/yu-lin/>Yu Lin</a>
|
<a href=/people/z/zhecheng-an/>Zhecheng An</a>
|
<a href=/people/p/peihao-wu/>Peihao Wu</a>
|
<a href=/people/z/zejun-ma/>Zejun Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--68><div class="card-body p-3 small">Though achieving impressive results on many NLP tasks, the BERT-like masked language models (MLM) encounter the discrepancy between pre-training and inference. In light of this gap, we investigate the contextual representation of pre-training and inference from the perspective of word probability distribution. We discover that BERT risks neglecting the contextual word similarity in pre-training. To tackle this issue, we propose an auxiliary gloss regularizer module to BERT pre-training (GR-BERT), to enhance word semantic similarity. By predicting masked words and aligning contextual embeddings to corresponding glosses simultaneously, the word similarity can be explicitly modeled. We design two architectures for GR-BERT and evaluate our model in downstream tasks. Experimental results show that the gloss regularizer benefits BERT in word-level and sentence-level semantic representation. The GR-BERT achieves new state-of-the-art in lexical substitution task and greatly promotes BERT sentence representation in both unsupervised and supervised STS tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.69.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.69.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.69.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.69" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.69/>An Information-Theoretic Approach and Dataset for Probing Gender Stereotypes in Multilingual Masked Language Models</a></strong><br><a href=/people/v/victor-steinborn/>Victor Steinborn</a>
|
<a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/h/haris-jabbar/>Haris Jabbar</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schuetze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--69><div class="card-body p-3 small">Bias research in NLP is a rapidly growing and developing field. Similar to CrowS-Pairs (Nangia et al., 2020), we assess gender bias in masked-language models (MLMs) by studying pairs of sentences with gender swapped person references. Most bias research focuses on and often is specific to English.Using a novel methodology for creating sentence pairs that is applicable across languages, we create, based on CrowS-Pairs, a multilingual dataset for English, Finnish, German, Indonesian and Thai.Additionally, we propose <span class=tex-math>S<sub>JSD</sub></span>, a new bias measure based on Jensen–Shannon divergence, which we argue retains more information from the model output probabilities than other previously proposed bias measures for MLMs.Using multilingual MLMs, we find that <span class=tex-math>S<sub>JSD</sub></span> diagnoses the same systematic biased behavior for non-English that previous studies have found for monolingual English pre-trained MLMs. <span class=tex-math>S<sub>JSD</sub></span> outperforms the CrowS-Pairs measure, which struggles to find such biases for smaller non-English datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.70.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.70.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.70/>Self-Training with Differentiable Teacher</a></strong><br><a href=/people/s/simiao-zuo/>Simiao Zuo</a>
|
<a href=/people/y/yue-yu/>Yue Yu</a>
|
<a href=/people/c/chen-liang/>Chen Liang</a>
|
<a href=/people/h/haoming-jiang/>Haoming Jiang</a>
|
<a href=/people/s/siawpeng-er/>Siawpeng Er</a>
|
<a href=/people/c/chao-zhang/>Chao Zhang</a>
|
<a href=/people/t/tuo-zhao/>Tuo Zhao</a>
|
<a href=/people/h/hongyuan-zha/>Hongyuan Zha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--70><div class="card-body p-3 small">Self-training achieves enormous success in various semi-supervised and weakly-supervised learning tasks. The method can be interpreted as a teacher-student framework, where the teacher generates pseudo-labels, and the student makes predictions. The two models are updated alternatingly. However, such a straightforward alternating update rule leads to training instability. This is because a small change in the teacher may result in a significant change in the student. To address this issue, we propose DRIFT, short for differentiable self-training, that treats teacher-student as a Stackelberg game. In this game, a leader is always in a more advantageous position than a follower. In self-training, the student contributes to the prediction performance, and the teacher controls the training process by generating pseudo-labels. Therefore, we treat the student as the leader and the teacher as the follower. The leader procures its advantage by acknowledging the follower’s strategy, which involves differentiable pseudo-labels and differentiable sample weights. Consequently, the leader-follower interaction can be effectively captured via Stackelberg gradient, obtained by differentiating the follower’s strategy. Experimental results on semi- and weakly-supervised classification and named entity recognition tasks show that our model outperforms existing approaches by large margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.71.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.71.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.71/><span class=acl-fixed-case>SHARP</span>: Search-Based Adversarial Attack for Structured Prediction</a></strong><br><a href=/people/l/liwen-zhang/>Liwen Zhang</a>
|
<a href=/people/z/zixia-jia/>Zixia Jia</a>
|
<a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/z/zilong-zheng/>Zilong Zheng</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--71><div class="card-body p-3 small">Adversarial attack of structured prediction models faces various challenges such as the difficulty of perturbing discrete words, the sentence quality issue, and the sensitivity of outputs to small perturbations. In this work, we introduce SHARP, a new attack method that formulates the black-box adversarial attack as a search-based optimization problem with a specially designed objective function considering sentence fluency, meaning preservation and attacking effectiveness. Additionally, three different searching strategies are analyzed and compared, i.e., Beam Search, Metropolis-Hastings Sampling, and Hybrid Search. We demonstrate the effectiveness of our attacking strategies on two challenging structured prediction tasks: Pos-tagging and dependency parsing. Through automatic and human evaluations, we show that our method performs a more potent attack compared with pioneer arts. Moreover, the generated adversarial examples can be used to successfully boost the robustness and performance of the victim model via adversarial training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.72.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.72.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--72 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.72 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.72.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.72" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.72/><span class=acl-fixed-case>MM</span>-Claims: A Dataset for Multimodal Claim Detection in Social Media</a></strong><br><a href=/people/g/gullal-singh-cheema/>Gullal Singh Cheema</a>
|
<a href=/people/s/sherzod-hakimov/>Sherzod Hakimov</a>
|
<a href=/people/a/abdul-sittar/>Abdul Sittar</a>
|
<a href=/people/e/eric-muller-budack/>Eric Müller-Budack</a>
|
<a href=/people/c/christian-otto/>Christian Otto</a>
|
<a href=/people/r/ralph-ewerth/>Ralph Ewerth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--72><div class="card-body p-3 small">In recent years, the problem of misinformation on the web has become widespread across languages, countries, and various social media platforms. Although there has been much work on automated fake news detection, the role of images and their variety are not well explored. In this paper, we investigate the roles of image and text at an earlier stage of the fake news detection pipeline, called claim detection. For this purpose, we introduce a novel dataset, MM-Claims, which consists of tweets and corresponding images over three topics: COVID-19, Climate Change and broadly Technology. The dataset contains roughly 86000 tweets, out of which 3400 are labeled manually by multiple annotators for the training and evaluation of multimodal models. We describe the dataset in detail, evaluate strong unimodal and multimodal baselines, and analyze the potential and drawbacks of current models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.73.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.73" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.73/><span class=acl-fixed-case>QLEVR</span>: A Diagnostic Dataset for Quantificational Language and Elementary Visual Reasoning</a></strong><br><a href=/people/z/zechen-li/>Zechen Li</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--73><div class="card-body p-3 small">Synthetic datasets have successfully been used to probe visual question-answering datasets for their reasoning abilities. CLEVR (John- son et al., 2017), for example, tests a range of visual reasoning abilities. The questions in CLEVR focus on comparisons of shapes, colors, and sizes, numerical reasoning, and existence claims. This paper introduces a minimally biased, diagnostic visual question-answering dataset, QLEVR, that goes beyond existential and numerical quantification and focus on more complex quantifiers and their combinations, e.g., asking whether there are more than two red balls that are smaller than at least three blue balls in an image. We describe how the dataset was created and present a first evaluation of state-of-the-art visual question-answering models, showing that QLEVR presents a formidable challenge to our current models. Code and Dataset are available at <a href=https://github.com/zechenli03/QLEVR class=acl-markup-url>https://github.com/zechenli03/QLEVR</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.74.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.74.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.74" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.74/><span class=acl-fixed-case>MWP</span>-<span class=acl-fixed-case>BERT</span>: Numeracy-Augmented Pre-training for Math Word Problem Solving</a></strong><br><a href=/people/z/zhenwen-liang/>Zhenwen Liang</a>
|
<a href=/people/j/jipeng-zhang/>Jipeng Zhang</a>
|
<a href=/people/l/lei-wang/>Lei Wang</a>
|
<a href=/people/w/wei-qin/>Wei Qin</a>
|
<a href=/people/y/yunshi-lan/>Yunshi Lan</a>
|
<a href=/people/j/jie-shao/>Jie Shao</a>
|
<a href=/people/x/xiangliang-zhang/>Xiangliang Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--74><div class="card-body p-3 small">Math word problem (MWP) solving faces a dilemma in number representation learning. In order to avoid the number representation issue and reduce the search space of feasible solutions, existing works striving for MWP solving usually replace real numbers with symbolic placeholders to focus on logic reasoning. However, different from common symbolic reasoning tasks like program synthesis and knowledge graph reasoning, MWP solving has extra requirements in numerical reasoning. In other words, instead of the number value itself, it is the reusable numerical property that matters more in numerical reasoning. Therefore, we argue that injecting numerical properties into symbolic placeholders with contextualized representation learning schema can provide a way out of the dilemma in the number representation issue here. In this work, we introduce this idea to the popular pre-training language model (PLM) techniques and build MWP-BERT, an effective contextual number representation PLM. We demonstrate the effectiveness of our MWP-BERT on MWP solving and several MWP-specific understanding tasks on both English and Chinese benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.75.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.75.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.75.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.75" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.75/>Restoring <span class=acl-fixed-case>H</span>ebrew Diacritics Without a Dictionary</a></strong><br><a href=/people/e/elazar-gershuni/>Elazar Gershuni</a>
|
<a href=/people/y/yuval-pinter/>Yuval Pinter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--75><div class="card-body p-3 small">We demonstrate that it is feasible to accurately diacritize Hebrew script without any human-curated resources other than plain diacritized text. We present Nakdimon, a two-layer character-level LSTM, that performs on par with much more complicated curation-dependent systems, across a diverse array of modern Hebrew sources. The model is accompanied by a training set and a test set, collected from diverse sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.76.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.76.software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.76.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.76" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.76/>Masked Summarization to Generate Factually Inconsistent Summaries for Improved Factual Consistency Checking</a></strong><br><a href=/people/h/hwanhee-lee/>Hwanhee Lee</a>
|
<a href=/people/k/kang-min-yoo/>Kang Min Yoo</a>
|
<a href=/people/j/joonsuk-park/>Joonsuk Park</a>
|
<a href=/people/h/hwaran-lee/>Hwaran Lee</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--76><div class="card-body p-3 small">Despite the recent advances in abstractive summarization systems, it is still difficult to determine whether a generated summary is factual consistent with the source text. To this end, the latest approach is to train a factual consistency classifier on factually consistent and inconsistent summaries. Luckily, the former is readily available as reference summaries in existing summarization datasets. However, generating the latter remains a challenge, as they need to be factually inconsistent, yet closely relevant to the source text to be effective. In this paper, we propose to generate factually inconsistent summaries using source texts and reference summaries with key information masked. Experiments on seven benchmark datasets demonstrate that factual consistency classifiers trained on summaries generated using our method generally outperform existing models and show a competitive correlation with human judgments. We also analyze the characteristics of the summaries generated using our method. We will release the pre-trained model and the code at <a href=https://github.com/hwanheelee1993/MFMA class=acl-markup-url>https://github.com/hwanheelee1993/MFMA</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.77.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.77.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.77/>Probing the Role of Positional Information in Vision-Language Models</a></strong><br><a href=/people/p/philipp-j-rosch/>Philipp J. Rösch</a>
|
<a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--77><div class="card-body p-3 small">In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object’s depth as new feature for a better object localization in the space. Even though we were able to improve the model properties as defined by our probes, it only has a negligible effect on the downstream performance. Our results thus highlight an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.78.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.78.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--78 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.78 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.78.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.78/>”Diversity and Uncertainty in Moderation” are the Key to Data Selection for Multilingual Few-shot Transfer</a></strong><br><a href=/people/s/shanu-kumar/>Shanu Kumar</a>
|
<a href=/people/s/sandipan-dandapat/>Sandipan Dandapat</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--78><div class="card-body p-3 small">Few-shot transfer often shows substantial gain over zero-shot transfer (CITATION), which is a practically useful trade-off between fully supervised and unsupervised learning approaches for multilingual pretained model-based systems. This paper explores various strategies for selecting data for annotation that can result in a better few-shot transfer. The proposed approaches rely on multiple measures such as data entropy using <span class=tex-math>n</span>-gram language model, predictive entropy, and gradient embedding. We propose a loss embedding method for sequence labeling tasks, which induces diversity and uncertainty sampling similar to gradient embedding. The proposed data selection strategies are evaluated and compared for POS tagging, NER, and NLI tasks for up to 20 languages. Our experiments show that the gradient and loss embedding-based strategies consistently outperform random data selection baselines, with gains varying with the initial performance of the zero-shot transfer. Furthermore, the proposed method shows similar trends in improvement even when the model is fine-tuned using a lower proportion of the original task-specific labeled training data for zero-shot transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.79.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.79/>A Self-supervised Joint Training Framework for Document Reranking</a></strong><br><a href=/people/x/xiaozhi-zhu/>Xiaozhi Zhu</a>
|
<a href=/people/t/tianyong-hao/>Tianyong Hao</a>
|
<a href=/people/s/sijie-cheng/>Sijie Cheng</a>
|
<a href=/people/f/fu-lee-wang/>Fu Lee Wang</a>
|
<a href=/people/h/hai-liu/>Hai Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--79><div class="card-body p-3 small">Pretrained language models such as BERT have been successfully applied to a wide range of natural language processing tasks and also achieved impressive performance in document reranking tasks. Recent works indicate that further pretraining the language models on the task-specific datasets before fine-tuning helps improve reranking performance. However, the pre-training tasks like masked language model and next sentence prediction were based on the context of documents instead of encouraging the model to understand the content of queries in document reranking task. In this paper, we propose a new self-supervised joint training framework (SJTF) with a self-supervised method called Masked Query Prediction (MQP) to establish semantic relations between given queries and positive documents. The framework randomly masks a token of query and encodes the masked query paired with positive documents, and uses a linear layer as a decoder to predict the masked token. In addition, the MQP is used to jointly optimize the models with supervised ranking objective during fine-tuning stage without an extra further pre-training stage. Extensive experiments on the MS MARCO passage ranking and TREC Robust datasets show that models trained with our framework obtain significant improvements compared to original models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.80.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.80.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.80/><span class=acl-fixed-case>CODE</span>-<span class=acl-fixed-case>MVP</span>: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training</a></strong><br><a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/y/yasheng-wang/>Yasheng Wang</a>
|
<a href=/people/y/yao-wan/>Yao Wan</a>
|
<a href=/people/j/jiawei-wang/>Jiawei Wang</a>
|
<a href=/people/p/pingyi-zhou/>Pingyi Zhou</a>
|
<a href=/people/l/li-li/>Li Li</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/j/jin-liu/>Jin Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--80><div class="card-body p-3 small">Recent years have witnessed increasing interest in code representation learning, which aims to represent the semantics of source code into distributed vectors. Currently, various works have been proposed to represent the complex semantics of source code from different views, including plain text, Abstract Syntax Tree (AST), and several kinds of code graphs (e.g., Control/Data Flow Graph). However, most of them only consider a single view of source code independently, ignoring the correspondences among different views. In this paper, we propose to integrate different views with the natural-language description of source code into a unified framework with Multi-View contrastive Pre-training, and name our model as CODE-MVP. Specifically, we first extract multiple code views using compiler tools, and learn the complementary information among them under a contrastive learning framework. Inspired by the type checking in compilation, we also design a fine-grained type inference objective in the pre-training. Experiments on three downstream tasks over five datasets demonstrate the superiority of CODE-MVP when compared with several state-of-the-art baselines. For example, we achieve 2.4/2.3/1.1 gain in terms of MRR/MAP/Accuracy metrics on natural language code retrieval, code similarity, and code defect detection tasks, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.81.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.81.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.81/><span class=acl-fixed-case>RGL</span>: A Simple yet Effective Relation Graph Augmented Prompt-based Tuning Approach for Few-Shot Learning</a></strong><br><a href=/people/y/yaqing-wang/>Yaqing Wang</a>
|
<a href=/people/x/xin-tian/>Xin Tian</a>
|
<a href=/people/h/haoyi-xiong/>Haoyi Xiong</a>
|
<a href=/people/y/yueyang-li/>Yueyang Li</a>
|
<a href=/people/z/zeyu-chen/>Zeyu Chen</a>
|
<a href=/people/s/sheng-guo/>Sheng Guo</a>
|
<a href=/people/d/dejing-dou/>Dejing Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--81><div class="card-body p-3 small">Pre-trained language models (PLMs) can provide a good starting point for downstream applications. However, it is difficult to generalize PLMs to new tasks given a few labeled samples. In this work, we show that Relation Graph augmented Learning (RGL) can improve the performance of few-shot natural language understanding tasks. During learning, RGL constructs a relation graph based on the label consistency between samples in the same batch, and learns to solve the resultant node classification and link prediction problems on the relation graph. In this way, RGL fully exploits the limited supervised information, which can boost the tuning effectiveness. Extensive experimental results show that RGL consistently improves the performance of prompt-based tuning strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.82.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.82.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.82" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.82/>Seeing the wood for the trees: a contrastive regularization method for the low-resource Knowledge Base Question Answering</a></strong><br><a href=/people/j/jpliu-wtu-edu-cn-jpliu-wtu-edu-cn/>Jpliu@wtu.edu.cn Jpliu@wtu.edu.cn</a>
|
<a href=/people/s/shijie-mei/>Shijie Mei</a>
|
<a href=/people/x/xinrong-hu/>Xinrong Hu</a>
|
<a href=/people/x/xun-yao/>Xun Yao</a>
|
<a href=/people/j/jack-yang/>Jack Yang</a>
|
<a href=/people/y/yi-guo/>Yi Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--82><div class="card-body p-3 small">Given a context knowledge base (KB) and a corresponding question, the Knowledge Base Question Answering task aims to retrieve correct answer entities from this KB. Despite sophisticated retrieval algorithms, the impact of the low-resource (incomplete) KB is not fully exploited, where contributing components (. key entities and/or relations) may be absent for question answering. To effectively address this problem, we propose a contrastive regularization based method, which is motivated by the learn-by-analogy capability from human readers. Specifically, the proposed work includes two major modules: the knowledge extension and sMoCo module. The former aims at exploiting the latent knowledge from the context KB and generating auxiliary information in the form of question-answer pairs. The later module utilizes those additional pairs and applies the contrastive regularization to learn informative representations, that making hard positive pairs attracted and hard negative pairs separated. Empirically, we achieved the state-of-the-art performance on the WebQuestionsSP dataset and the effectiveness of proposed modules is also evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.83.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.83.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.83" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.83/>Phrase-level Textual Adversarial Attack with Label Preservation</a></strong><br><a href=/people/y/yibin-lei/>Yibin Lei</a>
|
<a href=/people/y/yu-cao/>Yu Cao</a>
|
<a href=/people/d/dianqi-li/>Dianqi Li</a>
|
<a href=/people/t/tianyi-zhou/>Tianyi Zhou</a>
|
<a href=/people/m/meng-fang/>Meng Fang</a>
|
<a href=/people/m/mykola-pechenizkiy/>Mykola Pechenizkiy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--83><div class="card-body p-3 small">Generating high-quality textual adversarial examples is critical for investigating the pitfalls of natural language processing (NLP) models and further promoting their robustness. Existing attacks are usually realized through word-level or sentence-level perturbations, which either limit the perturbation space or sacrifice fluency and textual quality, both affecting the attack effectiveness. In this paper, we propose Phrase-Level Textual Adversarial ATtack (PLAT) that generates adversarial samples through phrase-level perturbations. PLAT first extracts the vulnerable phrases as attack targets by a syntactic parser, and then perturbs them by a pre-trained blank-infilling model. Such flexible perturbation design substantially expands the search space for more effective attacks without introducing too many modifications, and meanwhile maintaining the textual fluency and grammaticality via contextualized generation using surrounding texts. Moreover, we develop a label preservation filter leveraging the likelihoods of language models fine-tuned on each class, rather than textual similarity, to rule out those perturbations that potentially alter the original class label for humans. Extensive experiments and human evaluation demonstrate that PLAT has a superior attack effectiveness as well as a better label consistency than strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.84.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.84.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.84/>Prompt Augmented Generative Replay via Supervised Contrastive Learning for Lifelong Intent Detection</a></strong><br><a href=/people/v/vaibhav-varshney/>Vaibhav Varshney</a>
|
<a href=/people/m/mayur-patidar/>Mayur Patidar</a>
|
<a href=/people/r/rajat-kumar/>Rajat Kumar</a>
|
<a href=/people/l/lovekesh-vig/>Lovekesh Vig</a>
|
<a href=/people/g/gautam-shroff/>Gautam Shroff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--84><div class="card-body p-3 small">Identifying all possible user intents for a dialog system at design time is challenging even for skilled domain experts. For practical applications, novel intents may have to be inferred incrementally on the fly. This typically entails repeated retraining of the intent detector on both the existing and novel intents which can be expensive and would require storage of all past data corresponding to prior intents. In this paper, the objective is to continually train an intent detector on new intents while maintaining performance on prior intents without mandating access to prior intent data. Several data replay-based approaches have been introduced to avoid catastrophic forgetting during continual learning, including exemplar and generative replay. Current generative replay approaches struggle to generate representative samples because the generation is conditioned solely on the class/task label. Motivated by the recent work around prompt-based generation via pre-trained language models (PLMs), we employ generative replay using PLMs for incremental intent detection. Unlike exemplar replay, we only store the relevant contexts per intent in memory and use these stored contexts (with the class label) as prompts for generating intent-specific utterances. We use a common model for both generation and classification to promote optimal sharing of knowledge across both tasks. To further improve generation, we employ supervised contrastive fine-tuning of the PLM. Our proposed approach achieves state-of-the-art (SOTA) for lifelong intent detection on four public datasets and even outperforms exemplar replay-based approaches. The technique also achieves SOTA on a lifelong relation extraction task, suggesting that the approach is extendable to other continual learning tasks beyond intent detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.85.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.85.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.85.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.85" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.85/><span class=acl-fixed-case>OTE</span>xt<span class=acl-fixed-case>S</span>um: <span class=acl-fixed-case>E</span>xtractive <span class=acl-fixed-case>T</span>ext <span class=acl-fixed-case>S</span>ummarisation with <span class=acl-fixed-case>O</span>ptimal <span class=acl-fixed-case>T</span>ransport</a></strong><br><a href=/people/p/peggy-tang/>Peggy Tang</a>
|
<a href=/people/k/kun-hu/>Kun Hu</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a>
|
<a href=/people/l/lei-zhang/>Lei Zhang</a>
|
<a href=/people/j/junbin-gao/>Junbin Gao</a>
|
<a href=/people/z/zhiyong-wang/>Zhiyong Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--85><div class="card-body p-3 small">Extractive text summarisation aims to select salient sentences from a document to form a short yet informative summary. While learning-based methods have achieved promising results, they have several limitations, such as dependence on expensive training and lack of interpretability. Therefore, in this paper, we propose a novel non-learning-based method by for the first time formulating text summarisation as an Optimal Transport (OT) problem, namely Optimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction is conceptualised as obtaining an optimal summary that minimises the transportation cost to a given document regarding their semantic distributions. Such a cost is defined by the Wasserstein distance and used to measure the summary’s semantic coverage of the original document. Comprehensive experiments on four challenging and widely used datasets - MultiNews, PubMed, BillSum, and CNN/DM demonstrate that our proposed method outperforms the state-of-the-art non-learning-based methods and several recent learning-based methods in terms of the ROUGE metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.86.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.86.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.86" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.86/>Speeding Up Entmax</a></strong><br><a href=/people/m/maxat-tezekbayev/>Maxat Tezekbayev</a>
|
<a href=/people/v/vassilina-nikoulina/>Vassilina Nikoulina</a>
|
<a href=/people/m/matthias-galle/>Matthias Gallé</a>
|
<a href=/people/z/zhenisbek-assylbekov/>Zhenisbek Assylbekov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--86><div class="card-body p-3 small">Softmax is the de facto standard for normalizing logits in modern neural networks for language processing. However, by producing a dense probability distribution each token in the vocabulary has a nonzero chance of being selected at each generation step, leading to a variety of reported problems in text generation. <span class=tex-math>𝛼</span>-entmax of Peters et al. (2019) solves this problem, but is unfortunately slower than softmax. In this paper, we propose an alternative to <span class=tex-math>𝛼</span>-entmax, which keeps its virtuous characteristics, but is as fast as optimized softmax and achieves on par or better performance in machine translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.87.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.87" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.87/>Improving Code-Switching Dependency Parsing with Semi-Supervised Auxiliary Tasks</a></strong><br><a href=/people/s/saziye-betul-ozates/>Şaziye Betül Özateş</a>
|
<a href=/people/a/arzucan-ozgur/>Arzucan Özgür</a>
|
<a href=/people/t/tunga-gungor/>Tunga Gungor</a>
|
<a href=/people/o/ozlem-cetinoglu/>Özlem Çetinoğlu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--87><div class="card-body p-3 small">Code-switching dependency parsing stands as a challenging task due to both the scarcity of necessary resources and the structural difficulties embedded in code-switched languages. In this study, we introduce novel sequence labeling models to be used as auxiliary tasks for dependency parsing of code-switched text in a semi-supervised scheme. We show that using auxiliary tasks enhances the performance of an LSTM-based dependency parsing model and leads to better results compared to an XLM-R-based model with significantly less computational and time complexity. As the first study that focuses on multiple code-switching language pairs for dependency parsing, we acquire state-of-the-art scores on all of the studied languages. Our best models outperform the previous work by 7.4 LAS points on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.88.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.88.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.88.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.88/>Dangling-Aware Entity Alignment with Mixed High-Order Proximities</a></strong><br><a href=/people/j/juncheng-liu/>Juncheng Liu</a>
|
<a href=/people/z/zequn-sun/>Zequn Sun</a>
|
<a href=/people/b/bryan-hooi/>Bryan Hooi</a>
|
<a href=/people/y/yiwei-wang/>Yiwei Wang</a>
|
<a href=/people/d/dayiheng-liu/>Dayiheng Liu</a>
|
<a href=/people/b/baosong-yang/>Baosong Yang</a>
|
<a href=/people/x/xiaokui-xiao/>Xiaokui Xiao</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--88><div class="card-body p-3 small">We study dangling-aware entity alignment in knowledge graphs (KGs), which is an underexplored but important problem. As different KGs are naturally constructed by different sets of entities, a KG commonly contains some dangling entities that cannot find counterparts in other KGs. Therefore, dangling-aware entity alignment is more realistic than the conventional entity alignment where prior studies simply ignore dangling entities. We propose a framework using mixed high-order proximities on dangling-aware entity alignment. Our framework utilizes both the local high-order proximity in a nearest neighbor subgraph and the global high-order proximity in an embedding space for both dangling detection and entity alignment. Extensive experiments with two evaluation settings shows that our method more precisely detects dangling entities, and better aligns matchable entities. Further investigations demonstrate that our framework can mitigate the hubness problem on dangling-aware entity alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.89.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.89/><span class=acl-fixed-case>D</span>ec<span class=acl-fixed-case>BERT</span>: Enhancing the Language Understanding of <span class=acl-fixed-case>BERT</span> with Causal Attention Masks</a></strong><br><a href=/people/z/ziyang-luo/>Ziyang Luo</a>
|
<a href=/people/y/yadong-xi/>Yadong Xi</a>
|
<a href=/people/j/jing-ma/>Jing Ma</a>
|
<a href=/people/z/zhiwei-yang/>Zhiwei Yang</a>
|
<a href=/people/x/xiaoxi-mao/>Xiaoxi Mao</a>
|
<a href=/people/c/changjie-fan/>Changjie Fan</a>
|
<a href=/people/r/rongsheng-zhang/>Rongsheng Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--89><div class="card-body p-3 small">Since 2017, the Transformer-based models play critical roles in various downstream Natural Language Processing tasks. However, a common limitation of the attention mechanism utilized in Transformer Encoder is that it cannot automatically capture the information of word order, so explicit position embeddings are generally required to be fed into the target model. In contrast, Transformer Decoder with the causal attention masks is naturally sensitive to the word order. In this work, we focus on improving the position encoding ability of BERT with the causal attention masks. Furthermore, we propose a new pre-trained language model <i>DecBERT</i> and evaluate it on the GLUE benchmark. Experimental results show that (1) the causal attention mask is effective for BERT on the language understanding tasks; (2) our <i>DecBERT</i> model without position embeddings achieve comparable performance on the GLUE benchmark; and (3) our modification accelerates the pre-training process and <i>DecBERT w/ PE</i> achieves better overall performance than the baseline systems when pre-training with the same amount of computational resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.90.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.90.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.90" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.90/>Towards Computationally Feasible Deep Active Learning</a></strong><br><a href=/people/a/akim-tsvigun/>Akim Tsvigun</a>
|
<a href=/people/a/artem-shelmanov/>Artem Shelmanov</a>
|
<a href=/people/g/gleb-kuzmin/>Gleb Kuzmin</a>
|
<a href=/people/l/leonid-sanochkin/>Leonid Sanochkin</a>
|
<a href=/people/d/daniil-larionov/>Daniil Larionov</a>
|
<a href=/people/g/gleb-gusev/>Gleb Gusev</a>
|
<a href=/people/m/manvel-avetisian/>Manvel Avetisian</a>
|
<a href=/people/l/leonid-zhukov/>Leonid Zhukov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--90><div class="card-body p-3 small">Active learning (AL) is a prominent technique for reducing the annotation effort required for training machine learning models. Deep learning offers a solution for several essential obstacles to deploying AL in practice but introduces many others. One of such problems is the excessive computational resources required to train an acquisition model and estimate its uncertainty on instances in the unlabeled pool. We propose two techniques that tackle this issue for text classification and tagging tasks, offering a substantial reduction of AL iteration duration and the computational overhead introduced by deep acquisition models in AL. We also demonstrate that our algorithm that leverages pseudo-labeling and distilled models overcomes one of the essential obstacles revealed previously in the literature. Namely, it was shown that due to differences between an acquisition model used to select instances during AL and a successor model trained on the labeled data, the benefits of AL can diminish. We show that our algorithm, despite using a smaller and faster acquisition model, is capable of training a more expressive successor model with higher performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.91.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.91.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.91/>End-to-end Spoken Conversational Question Answering: Task, Dataset and Model</a></strong><br><a href=/people/c/chenyu-you/>Chenyu You</a>
|
<a href=/people/n/nuo-chen/>Nuo Chen</a>
|
<a href=/people/f/fenglin-liu/>Fenglin Liu</a>
|
<a href=/people/s/shen-ge/>Shen Ge</a>
|
<a href=/people/x/xian-wu/>Xian Wu</a>
|
<a href=/people/y/yuexian-zou/>Yuexian Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--91><div class="card-body p-3 small">In spoken question answering, the systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling the systems to model complex dialogues flow given the speech documents. In this task, our main objective is to build the system to deal with conversational questions based on the audio recordings, and to explore the plausibility of providing more cues from different modalities with systems in information gathering. To this end, instead of directly adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which effectively ingests cross-modal information to achieve fine-grained representations of the speech and language modalities. Moreover, we propose a simple and novel mechanism, termed Dual Attention, by encouraging better alignments between audio and text to ease the process of knowledge transfer. To evaluate the capacity of SCQA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 40k question-answer pairs from 4k conversations. We first show that the performance of the existing state-of-the-art methods significantly degrade on our dataset, hence demonstrating the necessity of incorporating cross-modal information to achieve good performance gains. Our experimental results demonstrate that our proposed method achieves superior performance in spoken conversational question answering. Codes and datasets will be made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.92.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.92.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.92" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.92/>Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training</a></strong><br><a href=/people/y/yifan-gao/>Yifan Gao</a>
|
<a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/z/zheng-li/>Zheng Li</a>
|
<a href=/people/r/rui-meng/>Rui Meng</a>
|
<a href=/people/t/tong-zhao/>Tong Zhao</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a>
|
<a href=/people/i/irwin-king/>Irwin King</a>
|
<a href=/people/m/michael-lyu/>Michael Lyu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--92><div class="card-body p-3 small">Keyphrase generation is the task of automatically predicting keyphrases given a piece of long text. Despite its recent flourishing, keyphrase generation on non-English languages haven’t been vastly investigated. In this paper, we call attention to a new setting named multilingual keyphrase generation and we contribute two new datasets, EcommerceMKP and AcademicMKP, covering six languages. Technically, we propose a retrieval-augmented method for multilingual keyphrase generation to mitigate the data shortage problem in non-English languages. The retrieval-augmented model leverages keyphrase annotations in English datasets to facilitate generating keyphrases in low-resource languages. Given a non-English passage, a cross-lingual dense passage retrieval module finds relevant English passages. Then the associated English keyphrases serve as external knowledge for keyphrase generation in the current language. Moreover, we develop a retriever-generator iterative training algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual passage retriever. Comprehensive experiments and ablations show that the proposed approach outperforms all baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.93.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.93.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.93.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.93" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.93/><span class=acl-fixed-case>FA</span>t<span class=acl-fixed-case>N</span>et: Cost-Effective Approach Towards Mitigating the Linguistic Bias in Speaker Verification Systems</a></strong><br><a href=/people/d/divya-sharma/>Divya Sharma</a>
|
<a href=/people/a/arun-balaji-buduru/>Arun Balaji Buduru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--93><div class="card-body p-3 small">Linguistic bias in Deep Neural Network (DNN) based Natural Language Processing (NLP) systems is a critical problem that needs attention. The problem further intensifies in the case of security systems, such as speaker verification, where fairness is essential. Speaker verification systems are intelligent systems that determine if two speech recordings belong to the same speaker. Such human-oriented security systems should be usable by diverse people speaking varied languages. Thus, a speaker verification system trained on speech in one language should generalize when tested for other languages. However, DNN-based models are often language-dependent. Previous works explore domain adaptation to fine-tune the pre-trained model for out-of-domain languages. Fine-tuning the model individually for each existing language is expensive. Hence, it limits the usability of the system. This paper proposes the cost-effective idea of integrating a lightweight embedding with existing speaker verification systems to mitigate linguistic bias without adaptation. This work is motivated by the theoretical hypothesis that attentive-frames could help generate language-agnostic embeddings. For scientific validation of this hypothesis, we propose two frame-attentive networks and investigate the effect of their integration with baselines for twelve languages. Empirical results suggest that frame-attentive embedding can cost-effectively reduce linguistic bias and enhance the usability of baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.94.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.94.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.94/>A Survey on Stance Detection for Mis- and Disinformation Identification</a></strong><br><a href=/people/m/momchil-hardalov/>Momchil Hardalov</a>
|
<a href=/people/a/arnav-arora/>Arnav Arora</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--94><div class="card-body p-3 small">Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.95.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.95.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.95" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.95/>Syntax Controlled Knowledge Graph-to-Text Generation with Order and Semantic Consistency</a></strong><br><a href=/people/j/jin-liu/>Jin Liu</a>
|
<a href=/people/c/chongfeng-fan/>Chongfeng Fan</a>
|
<a href=/people/z/zhou-fengyu/>Zhou Fengyu</a>
|
<a href=/people/h/huijuan-xu/>Huijuan Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--95><div class="card-body p-3 small">The knowledge graph (KG) stores a large amount of structural knowledge, while it is not easy for direct human understanding. Knowledge graph-to-text (KG-to-text) generation aims to generate easy-to-understand sentences from the KG, and at the same time, maintains semantic consistency between generated sentences and the KG. Existing KG-to-text generation methods phrase this task as a sequence-to-sequence generation task with linearized KG as input and consider the consistency issue of the generated texts and KG through a simple selection between decoded sentence word and KG node word at each time step. However, the linearized KG order is obtained through a heuristic search without data-driven optimization. In this paper, we optimize the knowledge description order prediction under the order supervision extracted from the caption and further enhance the consistency of the generated sentences and KG through syntactic and semantic regularization. We incorporate the Part-of-Speech (POS) syntactic tags to constrain the positions to copy words from the KG and employ a semantic context scoring function to evaluate the semantic fitness for each word in its local context when decoding each word in the generated sentence. Extensive experiments are conducted on two datasets, WebNLG and DART, and achieve state-of-the-art performances. Our code is now public available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.96.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.96.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.96/>To Answer or Not To Answer? Improving Machine Reading Comprehension Model with Span-based Contrastive Learning</a></strong><br><a href=/people/y/yunjie-ji/>Yunjie Ji</a>
|
<a href=/people/l/liang-yu-chen/>Liangyu Chen</a>
|
<a href=/people/c/chenxiao-dou/>Chenxiao Dou</a>
|
<a href=/people/b/baochang-ma/>Baochang Ma</a>
|
<a href=/people/x/xiangang-li/>Xiangang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--96><div class="card-body p-3 small">Machine Reading Comprehension with Unanswerable Questions is a difficult NLP task, challenged by the questions which can not be answered from passages. It is observed that subtle literal changes often make an answerable question unanswerable, however, most MRC models fail to recognize such changes. To address this problem, in this paper, we propose a span-based method of Contrastive Learning (spanCL) which explicitly contrast answerable questions with their answerable and unanswerable counterparts at the answer span level. With spanCL, MRC models are forced to perceive crucial semantic changes from slight literal differences. Experiments on SQuAD 2.0 dataset show that spanCL can improve baselines significantly, yielding 0.86 2.14 absolute EM improvements. Additional experiments also show that spanCL is an effective way to utilize generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.97.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.97.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.97/>Target-Guided Dialogue Response Generation Using Commonsense and Data Augmentation</a></strong><br><a href=/people/p/prakhar-gupta/>Prakhar Gupta</a>
|
<a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a>
|
<a href=/people/j/jeffrey-p-bigham/>Jeffrey Bigham</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--97><div class="card-body p-3 small">Target-guided response generation enables dialogue systems to smoothly transition a conversation from a dialogue context toward a target sentence. Such control is useful for designing dialogue systems that direct a conversation toward specific goals, such as creating non-obtrusive recommendations or introducing new topics in the conversation. In this paper, we introduce a new technique for target-guided response generation, which first finds a bridging path of commonsense knowledge concepts between the source and the target, and then uses the identified bridging path to generate transition responses. Additionally, we propose techniques to re-purpose existing dialogue datasets for target-guided generation. Experiments reveal that the proposed techniques outperform various baselines on this task. Finally, we observe that the existing automated metrics for this task correlate poorly with human judgement ratings. We propose a novel evaluation metric that we demonstrate is more reliable for target-guided response evaluation. Our work generally enables dialogue system designers to exercise more control over the conversations that their systems produce.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.98.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.98.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.98" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.98/><span class=acl-fixed-case>B</span>angla<span class=acl-fixed-case>BERT</span>: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in <span class=acl-fixed-case>B</span>angla</a></strong><br><a href=/people/a/abhik-bhattacharjee/>Abhik Bhattacharjee</a>
|
<a href=/people/t/tahmid-hasan/>Tahmid Hasan</a>
|
<a href=/people/w/wasi-ahmad/>Wasi Ahmad</a>
|
<a href=/people/k/kazi-samin-mubasshir/>Kazi Samin Mubasshir</a>
|
<a href=/people/m/md-saiful-islam/>Md Saiful Islam</a>
|
<a href=/people/a/anindya-iqbal/>Anindya Iqbal</a>
|
<a href=/people/m/m-sohel-rahman/>M. Sohel Rahman</a>
|
<a href=/people/r/rifat-shahriyar/>Rifat Shahriyar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--98><div class="card-body p-3 small">In this work, we introduce BanglaBERT, a BERT-based Natural Language Understanding (NLU) model pretrained in Bangla, a widely spoken yet low-resource language in the NLP literature. To pretrain BanglaBERT, we collect 27.5 GB of Bangla pretraining data (dubbed ‘Bangla2B+’) by crawling 110 popular Bangla sites. We introduce two downstream task datasets on natural language inference and question answering and benchmark on four diverse NLU tasks covering text classification, sequence labeling, and span prediction. In the process, we bring them under the first-ever Bangla Language Understanding Benchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming multilingual and monolingual models. We are making the models, datasets, and a leaderboard publicly available at <a href=https://github.com/csebuetnlp/banglabert class=acl-markup-url>https://github.com/csebuetnlp/banglabert</a> to advance Bangla NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.99.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.99.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.99/><span class=acl-fixed-case>ALLSH</span>: Active Learning Guided by Local Sensitivity and Hardness</a></strong><br><a href=/people/s/shujian-zhang/>Shujian Zhang</a>
|
<a href=/people/c/chengyue-gong/>Chengyue Gong</a>
|
<a href=/people/x/xingchao-liu/>Xingchao Liu</a>
|
<a href=/people/p/pengcheng-he/>Pengcheng He</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a>
|
<a href=/people/m/mingyuan-zhou/>Mingyuan Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--99><div class="card-body p-3 small">Active learning, which effectively collects informative unlabeled data for annotation, reduces the demand for labeled data. In this work, we propose to retrieve unlabeled samples with a local sensitivity and hardness-aware acquisition function. The proposed method generates data copies through local perturbations and selects data points whose predictive likelihoods diverge the most from their copies. We further empower our acquisition function by injecting the select-worst case perturbation. Our method achieves consistent gains over the commonly used active learning strategies in various classification tasks. Furthermore, we observe consistent improvements over the baselines on the study of prompt selection in prompt-based few-shot learning. These experiments demonstrate that our acquisition guided by local sensitivity and hardness can be effective and beneficial for many NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.100.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--100 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.100.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.100" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.100/>Low-resource Entity Set Expansion: A Comprehensive Study on User-generated Text</a></strong><br><a href=/people/y/yutong-shao/>Yutong Shao</a>
|
<a href=/people/n/nikita-bhutani/>Nikita Bhutani</a>
|
<a href=/people/s/sajjadur-rahman/>Sajjadur Rahman</a>
|
<a href=/people/e/estevam-hruschka/>Estevam Hruschka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--100><div class="card-body p-3 small">Entity set expansion (ESE) aims at obtaining a more complete set of entities given a textual corpus and a seed set of entities of a concept. Although it is a critical task in many NLP applications, existing benchmarks are limited to well-formed text (e.g., Wikipedia) and well-defined concepts (e.g., countries and diseases). Furthermore, only a small number of predictions are evaluated compared to the actual size of an entity set. A rigorous assessment of ESE methods warrants more comprehensive benchmarks and evaluation. In this paper, we consider user-generated text to understand the generalizability of ESE methods. We develop new benchmarks and propose more rigorous evaluation metrics for assessing the performance of ESE methods. Additionally, we identify phenomena such as non-named entities, multifaceted entities, vague concepts that are more prevalent in user-generated text than well-formed text, and use them to profile ESE methods. We observe that the strong performance of state-of-the-art ESE methods does not generalize well to user-generated text. We conduct comprehensive empirical analysis and draw insights from the findings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.101.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.101.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.101/><span class=acl-fixed-case>POLITICS</span>: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection</a></strong><br><a href=/people/y/yujian-liu/>Yujian Liu</a>
|
<a href=/people/x/xinliang-frederick-zhang/>Xinliang Frederick Zhang</a>
|
<a href=/people/d/david-wegsman/>David Wegsman</a>
|
<a href=/people/n/nicholas-beauchamp/>Nicholas Beauchamp</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--101><div class="card-body p-3 small">Ideology is at the core of political science research. Yet, there still does not exist general-purpose tools to characterize and predict ideology across different genres of text. To this end, we study Pretrained Language Models using novel ideology-driven pretraining objectives that rely on the comparison of articles on the same story written by media of different ideologies. We further collect a large-scale dataset, consisting of more than 3.6M political news articles, for pretraining. Our model POLITICS outperforms strong baselines and the previous state-of-the-art models on ideology prediction and stance detection tasks. Further analyses show that POLITICS is especially good at understanding long or formally written texts, and is also robust in few-shot learning scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.102.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.102.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.102/>Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention</a></strong><br><a href=/people/y/yifan-chen/>Yifan Chen</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/m/mahdi-namazifar/>Mahdi Namazifar</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--102><div class="card-body p-3 small">The massive amount of trainable parameters in the pre-trained language models (PLMs) makes them hard to be deployed to multiple downstream tasks. To address this issue, parameter-efficient transfer learning methods have been proposed to tune only a few parameters during fine-tuning while freezing the rest. This paper looks at existing methods along this line through the <i>kernel lens</i>. Motivated by the connection between self-attention in transformer-based PLMs and kernel learning, we propose <i>kernel-wise adapters</i>, namely <i>Kernel-mix</i>, that utilize the kernel structure in self-attention to guide the assignment of the tunable parameters. These adapters use guidelines found in classical kernel learning and enable separate parameter tuning for each attention head. Our empirical results, over a diverse set of natural language generation and understanding tasks, show that our proposed adapters can attain or improve the strong performance of existing baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.103.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.103.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.103/><span class=acl-fixed-case>RAIL</span>-<span class=acl-fixed-case>KD</span>: <span class=acl-fixed-case>RA</span>ndom Intermediate Layer Mapping for Knowledge Distillation</a></strong><br><a href=/people/m/md-akmal-haidar/>Md Akmal Haidar</a>
|
<a href=/people/n/nithin-anchuri/>Nithin Anchuri</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a>
|
<a href=/people/a/abbas-ghaddar/>Abbas Ghaddar</a>
|
<a href=/people/p/philippe-langlais/>Philippe Langlais</a>
|
<a href=/people/p/pascal-poupart/>Pascal Poupart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--103><div class="card-body p-3 small">Intermediate layer knowledge distillation (KD) can improve the standard KD technique (which only targets the output of teacher and student models) especially over large pre-trained language models. However, intermediate layer distillation suffers from excessive computational burdens and engineering efforts required for setting up a proper layer mapping. To address these problems, we propose a RAndom Intermediate Layer Knowledge Distillation (RAIL-KD) approach in which, intermediate layers from the teacher model are selected randomly to be distilled into the intermediate layers of the student model. This randomized selection enforces that all teacher layers are taken into account in the training process, while reducing the computational cost of intermediate layer distillation. Also, we show that it acts as a regularizer for improving the generalizability of the student model. We perform extensive experiments on GLUE tasks as well as on out-of-domain test sets. We show that our proposed RAIL-KD approach outperforms other state-of-the-art intermediate layer KD methods considerably in both performance and training-time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.104.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.104.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.104/>Unbiased Math Word Problems Benchmark for Mitigating Solving Bias</a></strong><br><a href=/people/z/zhicheng-yang/>Zhicheng Yang</a>
|
<a href=/people/j/jinghui-qin/>Jinghui Qin</a>
|
<a href=/people/j/jiaqi-chen/>Jiaqi Chen</a>
|
<a href=/people/x/xiaodan-liang/>Xiaodan Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--104><div class="card-body p-3 small">In this paper, we revisit the solving bias when evaluating models on current Math Word Problem (MWP) benchmarks. However, current solvers exist solving bias which consists of data bias and learning bias due to biased dataset and improper training strategy. Our experiments verify MWP solvers are easy to be biased by the biased training datasets which do not cover diverse questions for each problem narrative of all MWPs, thus a solver can only learn shallow heuristics rather than deep semantics for understanding problems. Besides, an MWP can be naturally solved by multiple equivalent equations while current datasets take only one of the equivalent equations as ground truth, forcing the model to match the labeled ground truth and ignoring other equivalent equations. Here, we first introduce a novel MWP dataset named UnbiasedMWP which is constructed by varying the grounded expressions in our collected data and annotating them with corresponding multiple new questions manually. Then, to further mitigate learning bias, we propose a Dynamic Target Selection (DTS) Strategy to dynamically select more suitable target expressions according to the longest prefix match between the current model output and candidate equivalent equations which are obtained by applying commutative law during training. The results show that our UnbiasedMWP has significantly fewer biases than its original data and other datasets, posing a promising benchmark for fairly evaluating the solvers’ reasoning skills rather than matching nearest neighbors. And the solvers trained with our DTS achieve higher accuracies on multiple MWP benchmarks. The source code is available at <a href=https://github.com/yangzhch6/UnbiasedMWP class=acl-markup-url>https://github.com/yangzhch6/UnbiasedMWP</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.105.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.105.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.105/>Learn To Remember: Transformer with Recurrent Memory for Document-Level Machine Translation</a></strong><br><a href=/people/y/yukun-feng/>Yukun Feng</a>
|
<a href=/people/f/feng-li/>Feng Li</a>
|
<a href=/people/z/ziang-song/>Ziang Song</a>
|
<a href=/people/b/boyuan-zheng/>Boyuan Zheng</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--105><div class="card-body p-3 small">The Transformer architecture has led to significant gains in machine translation. However, most studies focus on only sentence-level translation without considering the context dependency within documents, leading to the inadequacy of document-level coherence. Some recent research tried to mitigate this issue by introducing an additional context encoder or translating with multiple sentences or even the entire document. Such methods may lose the information on the target side or have an increasing computational complexity as documents get longer. To address such problems, we introduce a recurrent memory unit to the vanilla Transformer, which supports the information exchange between the sentence and previous context. The memory unit is recurrently updated by acquiring information from sentences, and passing the aggregated knowledge back to subsequent sentence states. We follow a two-stage training strategy, in which the model is first trained at the sentence level and then finetuned for document-level translation. We conduct experiments on three popular datasets for document-level machine translation and our model has an average improvement of 0.91 s-BLEU over the sentence-level baseline. We also achieve state-of-the-art results on TED and News, outperforming the previous work by 0.36 s-BLEU and 1.49 d-BLEU on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.106.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.106.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.106/>Improving Few-Shot Image Classification Using Machine- and User-Generated Natural Language Descriptions</a></strong><br><a href=/people/k/kosuke-nishida/>Kosuke Nishida</a>
|
<a href=/people/k/kyosuke-nishida/>Kyosuke Nishida</a>
|
<a href=/people/s/shuichi-nishioka/>Shuichi Nishioka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--106><div class="card-body p-3 small">Humans can obtain the knowledge of novel visual concepts from language descriptions, and we thus use the few-shot image classification task to investigate whether a machine learning model can have this capability. Our proposed model, LIDE (Learning from Image and DEscription), has a text decoder to generate the descriptions and a text encoder to obtain the text representations of machine- or user-generated descriptions. We confirmed that LIDE with machine-generated descriptions outperformed baseline models. Moreover, the performance was improved further with high-quality user-generated descriptions. The generated descriptions can be viewed as the explanations of the model’s predictions, and we observed that such explanations were consistent with prediction results. We also investigated why the language description improves the few-shot image classification performance by comparing the image representations and the text representations in the feature spaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.107.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--107 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.107.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.107.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.107/>All Information is Valuable: Question Matching over Full Information Transmission Network</a></strong><br><a href=/people/l/le-qi/>Le Qi</a>
|
<a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/g/guidong-zheng/>Guidong Zheng</a>
|
<a href=/people/w/wen-junjie/>Wen Junjie</a>
|
<a href=/people/j/jinlong-li/>Jinlong Li</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--107><div class="card-body p-3 small">Question matching is the task of identifying whether two questions have the same intent. For better reasoning the relationship between questions, existing studies adopt multiple interaction modules and perform multi-round reasoning via deep neural networks. In this process, there are two kinds of critical information that are commonly employed: the representation information of original questions and the interactive information between pairs of questions. However, previous studies tend to transmit only one kind of information, while failing to utilize both kinds of information simultaneously. To address this problem, in this paper, we propose a Full Information Transmission Network (FITN) that can transmit both representation and interactive information together in a simultaneous fashion. More specifically, we employ a novel memory-based attention for keeping and transmitting the interactive information through a global interaction matrix. Besides, we apply an original-average mixed connection method to effectively transmit the representation information between different reasoning rounds, which helps to preserve the original representation features of questions along with the historical hidden features. Experiments on two standard benchmarks demonstrate that our approach outperforms strong baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.108.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.108.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.108/><span class=acl-fixed-case>P</span>athway2<span class=acl-fixed-case>T</span>ext: Dataset and Method for Biomedical Pathway Description Generation</a></strong><br><a href=/people/j/junwei-yang/>Junwei Yang</a>
|
<a href=/people/z/zequn-liu/>Zequn Liu</a>
|
<a href=/people/m/ming-zhang/>Ming Zhang</a>
|
<a href=/people/s/sheng-wang/>Sheng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--108><div class="card-body p-3 small">Biomedical pathways have been extensively used to characterize the mechanism of complex diseases. One essential step in biomedical pathway analysis is to curate the description of a pathway based on its graph structure and node features. Neural text generation could be a plausible technique to circumvent the tedious manual curation. In this paper, we propose a new dataset Pathway2Text, which contains 2,367 pairs of biomedical pathways and textual descriptions. All pathway graphs are experimentally derived or manually curated. All textual descriptions are written by domain experts. We form this problem as a Graph2Text task and propose a novel graph-based text generation approach <span class=tex-math>k</span>NN-Graph2Text, which explicitly exploited descriptions of similar graphs to generate new descriptions. We observed substantial improvement of our method on both Graph2Text and the reverse task of Text2Graph. We further illustrated how our dataset can be used as a novel benchmark for biomedical named entity recognition. Collectively, we envision our method will become an important benchmark for evaluating Graph2Text methods and advance biomedical research for complex diseases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.109.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.109.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.109/>Exploring Neural Models for Query-Focused Summarization</a></strong><br><a href=/people/j/jesse-vig/>Jesse Vig</a>
|
<a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/w/wojciech-kryscinski/>Wojciech Kryscinski</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/w/wenhao-liu/>Wenhao Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--109><div class="card-body p-3 small">Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. While recently released datasets, such as QMSum or AQuaMuSe, facilitate research efforts in QFS, the field lacks a comprehensive study of the broad space of applicable modeling methods. In this paper we conduct a systematic exploration of neural approaches to QFS, considering two general classes of methods: two-stage extractive-abstractive solutions and end-to-end models. Within those categories, we investigate existing models and explore strategies for transfer learning. We also present two modeling extensions that achieve state-of-the-art performance on the QMSum dataset, up to a margin of 3.38 ROUGE-1, 3.72 ROUGE2, and 3.28 ROUGE-L when combined with transfer learning strategies. Results from human evaluation suggest that the best models produce more comprehensive and factually consistent summaries compared to a baseline model. Code and checkpoints are made publicly available: <a href=https://github.com/salesforce/query-focused-sum class=acl-markup-url>https://github.com/salesforce/query-focused-sum</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.110.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.110.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.110/><span class=acl-fixed-case>B</span>itext<span class=acl-fixed-case>E</span>dit: Automatic Bitext Editing for Improved Low-Resource Machine Translation</a></strong><br><a href=/people/e/eleftheria-briakou/>Eleftheria Briakou</a>
|
<a href=/people/s/sida-i-wang/>Sida Wang</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--110><div class="card-body p-3 small">Mined bitexts can contain imperfect translations that yield unreliable training signals for Neural Machine Translation (NMT). While filtering such pairs out is known to improve final model quality, we argue that it is suboptimal in low-resource conditions where even mined data can be limited. In our work, we propose instead, to refine the mined bitexts via automatic editing: given a sentence in a language <span class=tex-math>x<sub>f</sub></span>, and a possibly imperfect translation of it <span class=tex-math><strong>x<sub>e</sub></strong></span>, our model generates a revised version <span class=tex-math>x<sub>f</sub>'</span> or <span class=tex-math>x<sub>e</sub>'</span> that yields a more equivalent translation pair (i.e., &lt;<span class=tex-math>x<sub>f</sub>, x<sub>e</sub>'</span>> or &lt;<span class=tex-math>x<sub>f</sub>', x<sub>e</sub></span>>). We use a simple editing strategy by (1) mining potentially imperfect translations for each sentence in a given bitext, (2) learning a model to reconstruct the original translations and translate, in a multi-task fashion. Experiments demonstrate that our approach successfully improves the quality of CCMatrix mined bitext for 5 low-resource language-pairs and 10 translation directions by up to 8 BLEU points, in most cases improving upon a competitive translation-based baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.111.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.111.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.111/><span class=acl-fixed-case>M</span>ix<span class=acl-fixed-case>QG</span>: Neural Question Generation with Mixed Answer Types</a></strong><br><a href=/people/l/lidiya-murakhovska/>Lidiya Murakhovs’ka</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/p/philippe-laban/>Philippe Laban</a>
|
<a href=/people/t/tong-niu/>Tong Niu</a>
|
<a href=/people/w/wenhao-liu/>Wenhao Liu</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--111><div class="card-body p-3 small">Asking good questions is an essential ability for both human and machine intelligence. However, existing neural question generation approaches mainly focus on short factoid type of answers. In this paper, we introduce a neural question generator, MixQG, to bridge this gap. We combine nine question answering datasets with diverse answer types, including yes/no, multiple-choice, extractive, and abstractive answers, to train a single generative model. We show with empirical results that our model outperforms existing work in both seen and unseen domains, and can generate questions with different cognitive levels when conditioned on different answer types. We run a human evaluation study to assess the quality of generated questions and find that MixQG outperforms the next best model by 10%. Our code and model checkpoints will be released and integrated with the HuggingFace library to facilitate various downstream applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.112.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.112.software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.112.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.112/>Temporal Attention for Language Models</a></strong><br><a href=/people/g/guy-d-rosin/>Guy D. Rosin</a>
|
<a href=/people/k/kira-radinsky/>Kira Radinsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--112><div class="card-body p-3 small">Pretrained language models based on the transformer architecture have shown great success in NLP.Textual training data often comes from the web and is thus tagged with time-specific information, but most language models ignore this information. They are trained on the textual data alone, limiting their ability to generalize temporally. In this work, we extend the key component of the transformer architecture, i.e., the self-attention mechanism, and propose temporal attention - a time-aware self-attention mechanism. Temporal attention can be applied to any transformer model and requires the input texts to be accompanied with their relevant time points. This mechanism allows the transformer to capture this temporal information and create time-specific contextualized word representations. We leverage these representations for the task of semantic change detection; we apply our proposed mechanism to BERT and experiment on three datasets in different languages (English, German, and Latin) that also vary in time, size, and genre. Our proposed model achieves state-of-the-art results on all the datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.113.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.113.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.113" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.113/>Efficient Few-Shot Fine-Tuning for Opinion Summarization</a></strong><br><a href=/people/a/arthur-brazinskas/>Arthur Brazinskas</a>
|
<a href=/people/r/ramesh-nallapati/>Ramesh Nallapati</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/m/markus-dreyer/>Markus Dreyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--113><div class="card-body p-3 small">Abstractive summarization models are typically pre-trained on large amounts of generic texts, then fine-tuned on tens or hundreds of thousands of annotated samples. However, in opinion summarization, large annotated datasets of reviews paired with reference summaries are not available and would be expensive to create. This calls for fine-tuning methods robust to overfitting on small datasets. In addition, generically pre-trained models are often not accustomed to the specifics of customer reviews and, after fine-tuning, yield summaries with disfluencies and semantic mistakes. To address these problems, we utilize an efficient few-shot method based on adapters which, as we show, can easily store in-domain knowledge. Instead of fine-tuning the entire model, we add adapters and pre-train them in a task-specific way on a large corpus of unannotated customer reviews, using held-out reviews as pseudo summaries. Then, fine-tune the adapters on the small available human-annotated dataset. We show that this self-supervised adapter pre-training improves summary quality over standard fine-tuning by 2.0 and 1.3 ROUGE-L points on the Amazon and Yelp datasets, respectively. Finally, for summary personalization, we condition on aspect keyword queries, automatically created from generic datasets. In the same vein, we pre-train the adapters in a query-based manner on customer reviews and then fine-tune them on annotated datasets. This results in better-organized summary content reflected in improved coherence and fewer redundancies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.114.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.114.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.114/>Domain-matched Pre-training Tasks for Dense Retrieval</a></strong><br><a href=/people/b/barlas-oguz/>Barlas Oguz</a>
|
<a href=/people/k/kushal-lakhotia/>Kushal Lakhotia</a>
|
<a href=/people/a/anchit-gupta/>Anchit Gupta</a>
|
<a href=/people/p/patrick-lewis/>Patrick Lewis</a>
|
<a href=/people/v/vladimir-karpukhin/>Vladimir Karpukhin</a>
|
<a href=/people/a/aleksandra-piktus/>Aleksandra Piktus</a>
|
<a href=/people/x/xilun-chen/>Xilun Chen</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/s/scott-yih/>Scott Yih</a>
|
<a href=/people/s/sonal-gupta/>Sonal Gupta</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--114><div class="card-body p-3 small">Pre-training on larger datasets with ever increasing model size isnow a proven recipe for increased performance across almost all NLP tasks.A notable exception is information retrieval, where additional pre-traininghas so far failed to produce convincing results. We show that, with theright pre-training setup, this barrier can be overcome. We demonstrate thisby pre-training large bi-encoder models on 1) a recently released set of 65 millionsynthetically generated questions, and 2) 200 million post-comment pairs from a preexisting dataset of Reddit conversations made available by pushshift.io. We evaluate on a set of information retrieval and dialogue retrieval benchmarks, showing substantial improvements over supervised baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.115.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.115.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.115/><span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>K</span>-<span class=acl-fixed-case>QA</span>: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering</a></strong><br><a href=/people/b/barlas-oguz/>Barlas Oguz</a>
|
<a href=/people/x/xilun-chen/>Xilun Chen</a>
|
<a href=/people/v/vladimir-karpukhin/>Vladimir Karpukhin</a>
|
<a href=/people/s/stan-peshterliev/>Stan Peshterliev</a>
|
<a href=/people/d/dmytro-okhonko/>Dmytro Okhonko</a>
|
<a href=/people/m/michael-schlichtkrull/>Michael Schlichtkrull</a>
|
<a href=/people/s/sonal-gupta/>Sonal Gupta</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a>
|
<a href=/people/s/scott-yih/>Scott Yih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--115><div class="card-body p-3 small">We study open-domain question answering with structured, unstructured and semi-structured knowledge sources, including text, tables, lists and knowledge bases. Departing from prior work, we propose a unifying approach that homogenizes all sources by reducing them to text and applies the retriever-reader model which has so far been limited to text sources only. Our approach greatly improves the results on knowledge-base QA tasks by 11 points, compared to latest graph-based methods. More importantly, we demonstrate that our unified knowledge (UniK-QA) model is a simple and yet effective way to combine heterogeneous sources of knowledge, advancing the state-of-the-art results on two popular question answering benchmarks, NaturalQuestions and WebQuestions, by 3.5 and 2.6 points, respectively. The code of UniK-QA is available at: <a href=https://github.com/facebookresearch/UniK-QA class=acl-markup-url>https://github.com/facebookresearch/UniK-QA</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.116.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.116.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.116/>White-box Testing of <span class=acl-fixed-case>NLP</span> models with Mask Neuron Coverage</a></strong><br><a href=/people/a/arshdeep-sekhon/>Arshdeep Sekhon</a>
|
<a href=/people/y/yangfeng-ji/>Yangfeng Ji</a>
|
<a href=/people/m/matthew-dwyer/>Matthew Dwyer</a>
|
<a href=/people/y/yanjun-qi/>Yanjun Qi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--116><div class="card-body p-3 small">Recent literature has seen growing interest in using black-box strategies like for testing the behavior of NLP models. Research on white-box testing has developed a number of methods for evaluatinghow thoroughly the internal behavior of deep models is tested, but they are not applicableto NLP models. We propose a set of white-box testing methods that are customized for transformer-based NLP models. These include MASK NEURON COVERAGE (MNCOVER) that measures how thoroughlythe attention layers in models are exercised during testing. We show that MNCOVER can refine testing suites generated by CheckList by substantiallyreduce them in size, for more than 60% on average, while retaining failing tests – thereby concentrating the faultdetection power of the test suite. Further we show how can be used to guide CheckList input generation,evaluate alternative NLP testing methods, and drive data augmentation to improve accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.117.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--117 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.117.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.117.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.117" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.117/>Hierarchical Transformers Are More Efficient Language Models</a></strong><br><a href=/people/p/piotr-nawrot/>Piotr Nawrot</a>
|
<a href=/people/s/szymon-tworkowski/>Szymon Tworkowski</a>
|
<a href=/people/m/michal-tyrolski/>Michał Tyrolski</a>
|
<a href=/people/l/lukasz-kaiser/>Lukasz Kaiser</a>
|
<a href=/people/y/yuhuai-wu/>Yuhuai Wu</a>
|
<a href=/people/c/christian-szegedy/>Christian Szegedy</a>
|
<a href=/people/h/henryk-michalewski/>Henryk Michalewski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--117><div class="card-body p-3 small">Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences, which allows them to produce long coherent outputs: entire paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.118.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.118.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.118/><span class=acl-fixed-case>DISARM</span>: Detecting the Victims Targeted by Harmful Memes</a></strong><br><a href=/people/s/shivam-sharma/>Shivam Sharma</a>
|
<a href=/people/m/md-shad-akhtar/>Md Shad Akhtar</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--118><div class="card-body p-3 small">Internet memes have emerged as an increasingly popular means of communication on the web. Although memes are typically intended to elicit humour, they have been increasingly used to spread hatred, trolling, and cyberbullying, as well as to target specific individuals, communities, or society on political, socio-cultural, and psychological grounds. While previous work has focused on detecting harmful, hateful, and offensive memes in general, identifying whom these memes attack (i.e., the ‘victims’) remains a challenging and underexplored area. We attempt to address this problem in this paper. To this end, we create a dataset in which we annotate each meme with its victim(s) such as the name of the targeted person(s), organization(s), and community(ies). We then propose DISARM (Detecting vIctimS targeted by hARmful Memes), a framework that uses named-entity recognition and person identification to detect all entities a meme is referring to, and then, incorporates a novel contextualized multimodal deep neural network to classify whether the meme intends to harm these entities. We perform several systematic experiments on three different test sets, corresponding to entities that are (i) all seen while training, (ii) not seen as a harmful target while training, and (iii) not seen at all while training. The evaluation shows that DISARM significantly outperforms 10 unimodal and multimodal systems. Finally, we demonstrate that DISARM is interpretable and comparatively more generalizable and that it can reduce the relative error rate of harmful target identification by up to 9 % absolute over multimodal baseline systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.119.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.119.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.119/><span class=acl-fixed-case>KD</span>-<span class=acl-fixed-case>VLP</span>: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation</a></strong><br><a href=/people/y/yongfei-liu/>Yongfei Liu</a>
|
<a href=/people/c/chenfei-wu/>Chenfei Wu</a>
|
<a href=/people/s/shao-yen-tseng/>Shao-Yen Tseng</a>
|
<a href=/people/v/vasudev-lal/>Vasudev Lal</a>
|
<a href=/people/x/xuming-he/>Xuming He</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--119><div class="card-body p-3 small">Self-supervised vision-and-language pretraining (VLP) aims to learn transferable multi-modal representations from large-scale image-text data and to achieve strong performances on a broad scope of vision-language tasks after finetuning. Previous mainstream VLP approaches typically adopt a two-step strategy relying on external object detectors to encode images in a multi-modal Transformer framework, which suffer from restrictive object concept space, limited image context and inefficient computation. In this paper, we propose an object-aware end-to-end VLP framework, which directly feeds image grid features from CNNs into the Transformer and learns the multi-modal representations jointly. More importantly, we propose to perform object knowledge distillation to facilitate learning cross-modal alignment at different semantic levels. To achieve that, we design two novel pretext tasks by taking object features and their semantic labels from external detectors as supervision: 1.) Object-guided masked vision modeling task focuses on enforcing object-aware representation learning in the multi-modal Transformer; 2.) Phrase-region alignment task aims to improve cross-modal alignment by utilizing the similarities between noun phrases and object labels in the linguistic space. Extensive experiments on a wide range of vision-language tasks demonstrate the efficacy of our proposed framework, and we achieve competitive or superior performances over the existing pretraining strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.120.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.120.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.120/>Dependency Position Encoding for Relation Extraction</a></strong><br><a href=/people/q/qiushi-guo/>Qiushi Guo</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/d/dehong-gao/>Dehong Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--120><div class="card-body p-3 small">Leveraging the dependency tree of the input sentence is able to improve the model performance for relation extraction. A challenging issue is how to remove confusions from the tree. Efforts have been made to utilize the dependency connections between words to selectively emphasize target-relevant information. However, these approaches are limited in focusing on exploiting dependency types. In this paper, we propose dependency position encoding (DPE), an efficient way of incorporating both dependency connections and dependency types into the self-attention mechanism to distinguish the importance of different word dependencies for the task. In contrast to previous studies that process input sentence and dependency information in separate streams, DPE can be seamlessly incorporated into the Transformer and makes it possible to use an one-stream scheme to extract relations between entity pairs. Extensive experiments show that models with our DPE significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.121.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--121 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.121.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.121.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.121" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.121/>Good Visual Guidance Make A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction</a></strong><br><a href=/people/x/xiang-chen/>Xiang Chen</a>
|
<a href=/people/n/ningyu-zhang/>Ningyu Zhang</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/y/yunzhi-yao/>Yunzhi Yao</a>
|
<a href=/people/s/shumin-deng/>Shumin Deng</a>
|
<a href=/people/c/chuanqi-tan/>Chuanqi Tan</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/l/luo-si/>Luo Si</a>
|
<a href=/people/h/huajun-chen/>Huajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--121><div class="card-body p-3 small">Multimodal named entity recognition and relation extraction (MNER and MRE) is a fundamental and crucial branch in information extraction. However, existing approaches for MNER and MRE usually suffer from error sensitivity when irrelevant object images incorporated in texts. To deal with these issues, we propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for visual-enhanced entity and relation extraction, aiming to achieve more effective and robust performance. Specifically, we regard visual representation as pluggable visual prefix to guide the textual representation for error insensitive forecasting decision. We further propose a dynamic gated aggregation strategy to achieve hierarchical multi-scaled visual features as visual prefix for fusion. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our method, and achieve state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.122.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--122 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.122.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.122" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.122/>The Role of Context in Detecting Previously Fact-Checked Claims</a></strong><br><a href=/people/s/shaden-shaar/>Shaden Shaar</a>
|
<a href=/people/f/firoj-alam/>Firoj Alam</a>
|
<a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--122><div class="card-body p-3 small">Recent years have seen the proliferation of disinformation and fake news online. Traditional approaches to mitigate these issues is to use manual or automatic fact-checking. Recently, another approach has emerged: checking whether the input claim has previously been fact-checked, which can be done automatically, and thus fast, while also offering credibility and explainability, thanks to the human fact-checking and explanations in the associated fact-checking article. Here, we focus on claims made in a political debate and we study the impact of modeling the context of the claim: both on the source side, i.e., in the debate, as well as on the target side, i.e., in the fact-checking explanation document. We do this by modeling the local context, the global context, as well as by means of co-reference resolution, and multi-hop reasoning over the sentences of the document describing the fact-checked claim. The experimental results show that each of these represents a valuable information source, but that modeling the source-side context is most important, and can yield 10+ points of absolute improvement over a state-of-the-art model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.123.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--123 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.123.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.123.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.123/>Pruning Adatperfusion with Lottery Ticket Hypothesis</a></strong><br><a href=/people/j/jiarun-wu/>Jiarun Wu</a>
|
<a href=/people/q/qingliang-chen/>Qingliang Chen</a>
|
<a href=/people/z/zeguan-xiao/>Zeguan Xiao</a>
|
<a href=/people/y/yuliang-gu/>Yuliang Gu</a>
|
<a href=/people/m/mengsi-sun/>Mengsi Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--123><div class="card-body p-3 small">Pre-trained language models have shown great success in multiple downstream tasks. However, they are computationally expensive to fine-tune. Thus, transfer learning with adapter modules has been introduced to alleviate this problem, helping to extract knowledge of the downstream tasks. Adapterfusion models are an example of the transformers-with-adapter-modules, which merge multiple adapters to incorporate knowledge from different tasks. However, merging multiple adapters will inevitably cause redundancies, increasing the training and inference time massively. Therefore, in this paper, we propose an approach to identify the influence of each adapter module and a novel way to prune adapters based on the prestigious Lottery Ticket Hypothesis. Experiments on GLUE datasets show that the pruned Adapterfusion model with our scheme can achieve state-of-the-art results, reducing sizes significantly while keeping performance intact.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.124.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.124.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.124" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.124/><span class=acl-fixed-case>EVI</span>: Multilingual Spoken Dialogue Tasks and Dataset for Knowledge-Based Enrolment, Verification, and Identification</a></strong><br><a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/m/michal-lis/>Michał Lis</a>
|
<a href=/people/i/inigo-casanueva/>Inigo Casanueva</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--124><div class="card-body p-3 small">Knowledge-based authentication is crucial for task-oriented spoken dialogue systems that offer personalised and privacy-focused services. Such systems should be able to enrol (E), verify (V), and identify (I) new and recurring users based on their personal information, e.g. postcode, name, and date of birth. In this work, we formalise the three authentication tasks and their evaluation protocols, and we present EVI, a challenging spoken multilingual dataset with 5,506 dialogues in English, Polish, and French. Our proposed models set the first competitive benchmarks, explore the challenges of multilingual natural language processing of spoken dialogue, and set directions for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.125.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.125.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.125.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.125" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.125/>Post-Training Dialogue Summarization using Pseudo-Paraphrasing</a></strong><br><a href=/people/q/qi-jia/>Qi Jia</a>
|
<a href=/people/y/yizhu-liu/>Yizhu Liu</a>
|
<a href=/people/h/haifeng-tang/>Haifeng Tang</a>
|
<a href=/people/k/kenny-zhu/>Kenny Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--125><div class="card-body p-3 small">Previous dialogue summarization techniques adapt large language models pretrained on the narrative text by injecting dialogue-specific features into the models. These features either require additional knowledge to recognize or make the resulting models harder to tune. To bridge the format gap between dialogues and narrative summaries in dialogue summarization tasks, we propose to post-train pretrained language models (PLMs) to rephrase from dialogue to narratives. After that, the model is fine-tuned for dialogue summarization as usual. Comprehensive experiments show that our approach significantly improves vanilla PLMs on dialogue summarization and outperforms other SOTA models by the summary quality and implementation costs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.126.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--126 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.126.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.126" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.126/>A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict</a></strong><br><a href=/people/y/yiyi-liu/>Yiyi Liu</a>
|
<a href=/people/y/yequan-wang/>Yequan Wang</a>
|
<a href=/people/a/aixin-sun/>Aixin Sun</a>
|
<a href=/people/x/xuying-meng/>Xuying Meng</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/j/jiafeng-guo/>Jiafeng Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--126><div class="card-body p-3 small">Sarcasm employs ambivalence, where one says something positive but actually means negative, and vice versa. The essence of sarcasm, which is also a sufficient and necessary condition, is the conflict between literal and implied sentiments expressed in one sentence. However, it is difficult to recognize such sentiment conflict because the sentiments are mixed or even implicit. As a result, the recognition of sophisticated and obscure sentiment brings in a great challenge to sarcasm detection. In this paper, we propose a Dual-Channel Framework by modeling both literal and implied sentiments separately. Based on this dual-channel framework, we design the Dual-Channel Network (DC-Net) to recognize sentiment conflict. Experiments on political debates (i.e. IAC-V1 and IAC-V2) and Twitter datasets show that our proposed DC-Net achieves state-of-the-art performance on sarcasm recognition. Our code is released to support research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.127.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.127.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.127" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.127/>Zero-shot Entity Linking with Less Data</a></strong><br><a href=/people/g/g-p-shrivatsa-bhargav/>G P Shrivatsa Bhargav</a>
|
<a href=/people/d/dinesh-khandelwal/>Dinesh Khandelwal</a>
|
<a href=/people/s/saswati-dana/>Saswati Dana</a>
|
<a href=/people/d/dinesh-garg/>Dinesh Garg</a>
|
<a href=/people/p/pavan-kapanipathi/>Pavan Kapanipathi</a>
|
<a href=/people/s/salim-roukos/>Salim Roukos</a>
|
<a href=/people/a/alexander-gray/>Alexander Gray</a>
|
<a href=/people/l/l-venkata-subramaniam/>L Venkata Subramaniam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--127><div class="card-body p-3 small">Entity Linking (EL) maps an entity mention in a natural language sentence to an entity in a knowledge base (KB). The Zero-shot Entity Linking (ZEL) extends the scope of EL to unseen entities at the test time without requiring new labeled data. BLINK (BERT-based) is one of the SOTA models for ZEL. Interestingly, we discovered that BLINK exhibits diminishing returns, i.e., it reaches 98% of its performance with just 1% of the training data and the remaining 99% of the data yields only a marginal increase of 2% in the performance. While this extra 2% gain makes a huge difference for downstream tasks, training BLINK on large amounts of data is very resource-intensive and impractical. In this paper, we propose a neuro-symbolic, multi-task learning approach to bridge this gap. Our approach boosts the BLINK’s performance with much less data by exploiting an auxiliary information about entity types. Specifically, we train our model on two tasks simultaneously - entity linking (primary task) and hierarchical entity type prediction (auxiliary task). The auxiliary task exploits the hierarchical structure of entity types. Our approach achieves superior performance on ZEL task with significantly less training data. On four different benchmark datasets, we show that our approach achieves significantly higher performance than SOTA models when they are trained with just 0.01%, 0.1%, or 1% of the original training data. Our code is available at <a href=https://github.com/IBM/NeSLET class=acl-markup-url>https://github.com/IBM/NeSLET</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.128.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.128/><span class=acl-fixed-case>G</span>raph<span class=acl-fixed-case>C</span>ache: Message Passing as Caching for Sentence-Level Relation Extraction</a></strong><br><a href=/people/y/yiwei-wang/>Yiwei Wang</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/w/wenxuan-zhou/>Wenxuan Zhou</a>
|
<a href=/people/y/yujun-cai/>Yujun Cai</a>
|
<a href=/people/y/yuxuan-liang/>Yuxuan Liang</a>
|
<a href=/people/b/bryan-hooi/>Bryan Hooi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--128><div class="card-body p-3 small">Entity types and textual context are essential properties for sentence-level relation extraction (RE). Existing work only encodes these properties within individual instances, which limits the performance of RE given the insufficient features in a single sentence. In contrast, we model these properties from the whole dataset and use the dataset-level information to enrich the semantics of every instance. We propose the GraphCache (Graph Neural Network as Caching) module, that propagates the features across sentences to learn better representations for RE. GraphCache aggregates the features from sentences in the whole dataset to learn global representations of properties, and use them to augment the local features within individual sentences. The global property features act as dataset-level prior knowledge for RE, and a complement to the sentence-level features. Inspired by the classical caching technique in computer systems, we develop GraphCache to update the property representations in an online manner. Overall, GraphCache yields significant effectiveness gains on RE and enables efficient message passing across all sentences in the dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.129.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.129.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.129" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.129/>Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach</a></strong><br><a href=/people/c/chao-zhao/>Chao Zhao</a>
|
<a href=/people/f/faeze-brahman/>Faeze Brahman</a>
|
<a href=/people/t/tenghao-huang/>Tenghao Huang</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--129><div class="card-body p-3 small">Pre-trained models (PTMs) have lead to great improvements in natural language generation (NLG). However, it is still unclear how much commonsense knowledge they possess. With the goal of evaluating commonsense knowledge of NLG models, recent work has proposed the problem of generative commonsense reasoning, e.g., to compose a logical sentence given a set of unordered concepts. Existing approaches to this problem hypothesize that PTMs lack sufficient parametric knowledge for this task, which can be overcome by introducing external knowledge or task-specific pre-training objectives. Different from this trend, we argue that PTM’s inherent ability for generative commonsense reasoning is underestimated due to the order-agnostic property of its input. In particular, we hypothesize that the order of the input concepts can affect the PTM’s ability to utilize its commonsense knowledge. To this end, we propose a pre-ordering approach to elaborately manipulate the order of the given concepts before generation. Experiments show that our approach can outperform the more sophisticated models that have access to a lot of external data and resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.130.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.130.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.130" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.130/>Identifying and Mitigating Spurious Correlations for Improving Robustness in <span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/r/rohit-sridhar/>Rohit Sridhar</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/x/xuezhi-wang/>Xuezhi Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--130><div class="card-body p-3 small">Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting “spurious correlations”, or “shortcuts” between the training data and the task labels. Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify such spurious correlations in NLP models at scale. We first leverage existing interpretability methods to extract tokens that significantly affect model’s decision process from the input text. We then distinguish “genuine” tokens and “spurious” tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations. We show that our proposed method can effectively and efficiently identify a scalable set of “shortcuts”, and mitigating these leads to more robust models in multiple applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.131.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--131 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.131.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.131" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.131/><span class=tex-math>Great~Truths~are ~Always ~Simple:</span> A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models</a></strong><br><a href=/people/j/jinhao-jiang/>Jinhao Jiang</a>
|
<a href=/people/k/kun-zhou/>Kun Zhou</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a>
|
<a href=/people/w/wayne-xin-zhao/>Xin Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--131><div class="card-body p-3 small">Commonsense reasoning in natural language is a desired ability of artificial intelligent systems. For solving complex commonsense reasoning tasks, a typical solution is to enhance pre-trained language models (PTMs) with a knowledge-aware graph neural network (GNN) encoder that models a commonsense knowledge graph (CSKG).Despite the effectiveness, these approaches are built on heavy architectures, and can’t clearly explain how external knowledge resources improve the reasoning capacity of PTMs. Considering this issue, we conduct a deep empirical analysis, and find that it is indeed <i>relation features</i> from CSKGs (but not <i>node features</i>) that mainly contribute to the performance improvement of PTMs. Based on this finding, we design a simple MLP-based knowledge encoder that utilizes statistical relation paths as features. Extensive experiments conducted on five benchmarks demonstrate the effectiveness of our approach, which also largely reduces the parameters for encoding CSKGs.Our codes and data are publicly available at <a href=https://github.com/RUCAIBox/SAFE class=acl-markup-url>https://github.com/RUCAIBox/SAFE</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.132.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.132.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.132" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.132/>Analyzing the Intensity of Complaints on Social Media</a></strong><br><a href=/people/m/ming-fang/>Ming Fang</a>
|
<a href=/people/s/shi-zong/>Shi Zong</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--132><div class="card-body p-3 small">Complaining is a speech act that expresses a negative inconsistency between reality and human’s expectations. While prior studies mostly focus on identifying the existence or the type of complaints, in this work, we present the first study in computational linguistics of measuring the intensity of complaints from text. Analyzing complaints from such perspective is particularly useful, as complaints of certain degrees may cause severe consequences for companies or organizations. We first collect 3,103 posts about complaints in education domain from Weibo, a popular Chinese social media platform. These posts are then annotated with complaints intensity scores using Best-Worst Scaling (BWS) method. We show that complaints intensity can be accurately estimated by computational models with best mean square error achieving 0.11. Furthermore, we conduct a comprehensive linguistic analysis around complaints, including the connections between complaints and sentiment, and a cross-lingual comparison for complaints expressions used by Chinese and English speakers. We finally show that our complaints intensity scores can be incorporated for better estimating the popularity of posts on social media.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.133.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.133.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.133" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.133/>Detecting Narrative Elements in Informational Text</a></strong><br><a href=/people/e/effi-levi/>Effi Levi</a>
|
<a href=/people/g/guy-mor/>Guy Mor</a>
|
<a href=/people/t/tamir-sheafer/>Tamir Sheafer</a>
|
<a href=/people/s/shaul-shenhav/>Shaul Shenhav</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--133><div class="card-body p-3 small">Automatic extraction of narrative elements from text, combining narrative theories with computational models, has been receiving increasing attention over the last few years. Previous works have utilized the oral narrative theory by Labov and Waletzky to identify various narrative elements in personal stories texts. Instead, we direct our focus to informational texts, specifically news stories. We introduce NEAT (Narrative Elements AnnoTation) – a novel NLP task for detecting narrative elements in raw text. For this purpose, we designed a new multi-label narrative annotation scheme, better suited for informational text (e.g. news media), by adapting elements from the narrative theory of Labov and Waletzky (Complication and Resolution) and adding a new narrative element of our own (Success). We then used this scheme to annotate a new dataset of 2,209 sentences, compiled from 46 news articles from various category domains. We trained a number of supervised models in several different setups over the annotated dataset to identify the different narrative elements, achieving an average <span class=tex-math>F<sub>1</sub></span> score of up to 0.77. The results demonstrate the holistic nature of our annotation scheme as well as its robustness to domain category.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.134.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.134.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.134/>When do Contrastive Word Alignments Improve Many-to-many Neural Machine Translation?</a></strong><br><a href=/people/z/zhuoyuan-mao/>Zhuoyuan Mao</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/h/haiyue-song/>Haiyue Song</a>
|
<a href=/people/z/zhen-wan/>Zhen Wan</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--134><div class="card-body p-3 small">Word alignment has proven to benefit many-to-many neural machine translation (NMT). However, high-quality ground-truth bilingual dictionaries were used for pre-editing in previous methods, which are unavailable for most language pairs. Meanwhile, the contrastive objective can implicitly utilize automatically learned word alignment, which has not been explored in many-to-many NMT. This work proposes a word-level contrastive objective to leverage word alignments for many-to-many NMT. Empirical results show that this leads to 0.8 BLEU gains for several language pairs. Analyses reveal that in many-to-many NMT, the encoder’s sentence retrieval performance highly correlates with the translation quality, which explains when the proposed method impacts translation. This motivates future exploration for many-to-many NMT to improve the encoder’s sentence retrieval performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.135.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.135/>Minimally-Supervised Relation Induction from Pre-trained Language Model</a></strong><br><a href=/people/l/lu-sun/>Lu Sun</a>
|
<a href=/people/y/yongliang-shen/>Yongliang Shen</a>
|
<a href=/people/w/weiming-lu/>Weiming Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--135><div class="card-body p-3 small">Relation Induction is a very practical task in Natural Language Processing (NLP) area. In practical application scenarios, people want to induce more entity pairs having the same relation from only a few seed entity pairs. Thus, instead of the laborious supervised setting, in this paper, we focus on the minimally-supervised setting where only a couple of seed entity pairs per relation are provided. Although the conventional relation induction methods have made some success, their performance depends heavily on the quality of word embeddings. The great success of Pre-trained Language Models, such as BERT, changes the NLP area a lot, and they are proven to be able to better capture relation knowledge. In this paper, we propose a novel method to induce relation with BERT under the minimally-supervised setting. Specifically, we firstly extract proper templates from the corpus by using the mask-prediction task in BERT to build pseudo-sentences as the context of entity pairs. Then we use BERT attention weights to better represent the pseudo-sentences. In addition, We also use the IntegratedGradient of entity pairs to iteratively select better templates further. Finally, with the high-quality pseudo-sentences, we can train a better classifier for relation induction. Experiments onGoogle Analogy Test Sets (GATS), Bigger Analogy TestSet (BATS) and DiffVec demonstrate that our proposed method achieves state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.136.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.136.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.136.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.136/>Crake: Causal-Enhanced Table-Filler for Question Answering over Large Scale Knowledge Base</a></strong><br><a href=/people/m/minhao-zhang/>Minhao Zhang</a>
|
<a href=/people/r/ruoyu-zhang/>Ruoyu Zhang</a>
|
<a href=/people/y/yanzeng-li/>Yanzeng Li</a>
|
<a href=/people/l/lei-zou/>Lei Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--136><div class="card-body p-3 small">Semantic parsing solves knowledge base (KB) question answering (KBQA) by composing a KB query, which generally involves node extraction (NE) and graph composition (GC) to detect and connect related nodes in a query. Despite the strong causal effects between NE and GC, previous works fail to directly model such causalities in their pipeline, hindering the learning of subtask correlations. Also, the sequence-generation process for GC in previous works induces ambiguity and exposure bias, which further harms accuracy. In this work, we formalize semantic parsing into two stages. In the first stage (graph structure generation), we propose a causal-enhanced table-filler to overcome the issues in sequence-modelling and to learn the internal causalities. In the second stage (relation extraction), an efficient beam-search algorithm is presented to scale complex queries on large-scale KBs. Experiments on LC-QuAD 1.0 indicate that our method surpasses previous state-of-the-arts by a large margin (17%) while remaining time and space efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.137.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--137 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.137.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.137/>Exploring the Universal Vulnerability of Prompt-based Learning Paradigm</a></strong><br><a href=/people/l/lei-xu/>Lei Xu</a>
|
<a href=/people/y/yangyi-chen/>Yangyi Chen</a>
|
<a href=/people/g/ganqu-cui/>Ganqu Cui</a>
|
<a href=/people/h/hongcheng-gao/>Hongcheng Gao</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--137><div class="card-body p-3 small">Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain text. In both scenarios, we demonstrate that our triggers can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt-based learning paradigm. Further experiments show that adversarial triggers have good transferability among language models. We also find conventional fine-tuning models are not vulnerable to adversarial triggers constructed from pre-trained language models. We conclude by proposing a potential solution to mitigate our attack methods. Code and data are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.138.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.138.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.138.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.138/>Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering</a></strong><br><a href=/people/j/jeonghwan-kim/>Jeonghwan Kim</a>
|
<a href=/people/j/junmo-kang/>Junmo Kang</a>
|
<a href=/people/k/kyung-min-kim/>Kyung-min Kim</a>
|
<a href=/people/g/giwon-hong/>Giwon Hong</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--138><div class="card-body p-3 small">Numerical reasoning over text is a challenging subtask in question answering (QA) that requires both the understanding of texts and numbers. However, existing language models in these numerical reasoning QA models tend to overly rely on the pre-existing parametric knowledge at inference time, which commonly causes hallucination in interpreting numbers. Our work proposes a novel attention masked reasoning model, the NC-BERT, that learns to leverage the number-related contextual knowledge to alleviate the over-reliance on parametric knowledge and enhance the numerical reasoning capabilities of the QA model. The empirical results suggest that understanding of numbers in their context by reducing the parametric knowledge influence, and refining numerical information in the number embeddings lead to improved numerical reasoning accuracy and performance in DROP, a numerical QA dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.139.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--139 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.139.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.139" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.139/>Learn from Relation Information: Towards Prototype Representation Rectification for Few-Shot Relation Extraction</a></strong><br><a href=/people/y/yang-liu-hk/>Yang Liu</a>
|
<a href=/people/j/jinpeng-hu/>Jinpeng Hu</a>
|
<a href=/people/x/xiang-wan/>Xiang Wan</a>
|
<a href=/people/t/tsung-hui-chang/>Tsung-Hui Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--139><div class="card-body p-3 small">Few-shot Relation Extraction refers to fast adaptation to novel relation classes with few samples through training on the known relation classes. Most existing methods focus on implicitly introducing relation information (i.e., relation label or relation description) to constrain the prototype representation learning, such as contrastive learning, graphs, and specifically designed attentions, which may bring useless and even harmful parameters. Besides, these approaches are limited in handing outlier samples far away from the class center due to the weakly implicit constraint. In this paper, we propose an effective and parameter-less Prototype Rectification Method (PRM) to promote few-shot relation extraction, where we utilize a prototype rectification module to rectify original prototypes explicitly by the relation information. Specifically, PRM is composed of two gate mechanisms. One gate decides how much of the original prototype remains, and another one updates the remained prototype with relation information. In doing so, better and stabler global relation information can be captured for guiding prototype representations, and thus PRM can robustly deal with outliers. Moreover, we also extend PRM to both none-of-the-above (NOTA) and domain adaptation scenarios. Experimental results on FewRel 1.0 and 2.0 datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.140.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.140.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.140/><span class=acl-fixed-case>HUE</span>: Pretrained Model and Dataset for Understanding Hanja Documents of <span class=acl-fixed-case>A</span>ncient <span class=acl-fixed-case>K</span>orea</a></strong><br><a href=/people/h/haneul-yoo/>Haneul Yoo</a>
|
<a href=/people/j/jiho-jin/>Jiho Jin</a>
|
<a href=/people/j/juhee-son/>Juhee Son</a>
|
<a href=/people/j/jinyeong-bak/>JinYeong Bak</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--140><div class="card-body p-3 small">Historical records in Korea before the 20th century were primarily written in Hanja, an extinct language based on Chinese characters and not understood by modern Korean or Chinese speakers. Historians with expertise in this time period have been analyzing the documents, but that process is very difficult and time-consuming, and language models would significantly speed up the process. Toward building and evaluating language models for Hanja, we release the Hanja Understanding Evaluation dataset consisting of chronological attribution, topic classification, named entity recognition, and summary retrieval tasks. We also present BERT-based models continued training on the two major corpora from the 14th to the 19th centuries: the Annals of the Joseon Dynasty and Diaries of the Royal Secretariats. We compare the models with several baselines on all tasks and show there are significant improvements gained by training on the two corpora. Additionally, we run zero-shot experiments on the Daily Records of the Royal Court and Important Officials (DRRI). The DRRI dataset has not been studied much by the historians, and not at all by the NLP community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.141.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.141.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.141" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.141/><span class=acl-fixed-case>S</span>ea<span class=acl-fixed-case>D</span>: End-to-end Text-to-<span class=acl-fixed-case>SQL</span> Generation with Schema-aware Denoising</a></strong><br><a href=/people/k/kuan-xu/>Kuan Xu</a>
|
<a href=/people/y/yongbo-wang/>Yongbo Wang</a>
|
<a href=/people/y/yongliang-wang/>Yongliang Wang</a>
|
<a href=/people/z/zihao-wang/>Zihao Wang</a>
|
<a href=/people/z/zujie-wen/>Zujie Wen</a>
|
<a href=/people/y/yang-dong/>Yang Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--141><div class="card-body p-3 small">On the WikiSQL benchmark, most methods tackle the challenge of text-to-SQL with predefined sketch slots and build sophisticated sub-tasks to fill these slots. Though achieving promising results, these methods suffer from over-complex model structure. In this paper, we present a simple yet effective approach that enables auto-regressive sequence-to-sequence model to robust text-to-SQL generation. Instead of formulating the task of text-to-SQL as slot-filling, we propose to train sequence-to-sequence model with Schema-aware Denoising (SeaD), which consists of two denoising objectives that train model to either recover input or predict output from two novel erosion and shuffle noises. These model-agnostic denoising objectives act as the auxiliary tasks for structural data modeling during sequence-to-sequence generation. In addition, we propose a clause-sensitive execution guided (EG) decoding strategy to overcome the limitation of EG decoding for generative model. The experiments show that the proposed method improves the performance of sequence-to-sequence model in both schema linking and grammar correctness and establishes new state-of-the-art on WikiSQL benchmark. Our work indicates that the capacity of sequence-to-sequence model for text-to-SQL may have been under-estimated and could be enhanced by specialized denoising task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.142.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.142.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.142/>Cross-Lingual Cross-Modal Consolidation for Effective Multilingual Video Corpus Moment Retrieval</a></strong><br><a href=/people/j/jiaheng-liu/>Jiaheng Liu</a>
|
<a href=/people/t/tan-yu/>Tan Yu</a>
|
<a href=/people/h/hanyu-peng/>Hanyu Peng</a>
|
<a href=/people/m/mingming-sun/>Mingming Sun</a>
|
<a href=/people/p/ping-li/>Ping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--142><div class="card-body p-3 small">Existing multilingual video corpus moment retrieval (mVCMR) methods are mainly based on a two-stream structure. The visual stream utilizes the visual content in the video to estimate the query-visual similarity, and the subtitle stream exploits the query-subtitle similarity. The final query-video similarity ensembles similarities from two streams. In our work, we pro- pose a simple and effective strategy termed as Cross-lingual Cross-modal Consolidation (C3 ) to improve mVCMR accuracy. We adopt the ensemble similarity as the teacher to guide the training of each stream, leading to a more powerful ensemble similarity. Meanwhile, we use the teacher for a specific language to guide the student for another language to exploit the complementary knowledge across languages. Ex- tensive experiments on mTVR dataset demonstrate the effectiveness of our C3 method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.143.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.143.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.143/>Delving Deep into Regularity: A Simple but Effective Method for <span class=acl-fixed-case>C</span>hinese Named Entity Recognition</a></strong><br><a href=/people/y/yingjie-gu/>Yingjie Gu</a>
|
<a href=/people/x/xiaoye-qu/>Xiaoye Qu</a>
|
<a href=/people/z/zhefeng-wang/>Zhefeng Wang</a>
|
<a href=/people/y/yi-zheng/>Yi Zheng</a>
|
<a href=/people/b/baoxing-huai/>Baoxing Huai</a>
|
<a href=/people/n/nicholas-jing-yuan/>Nicholas Jing Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--143><div class="card-body p-3 small">Recent years have witnessed the improving performance of Chinese Named Entity Recognition (NER) from proposing new frameworks or incorporating word lexicons. However, the inner composition of entity mentions in character-level Chinese NER has been rarely studied. Actually, most mentions of regular types have strong name regularity. For example, entities end with indicator words such as “公司 (company) ” or “银行 (bank)” usually belong to organization. In this paper, we propose a simple but effective method for investigating the regularity of entity spans in Chinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON). Specifically, the proposed model consists of two branches: a regularity-aware module and a regularity-agnostic module. The regularity-aware module captures the internal regularity of each span for better entity type prediction, while the regularity-agnostic module is employed to locate the boundary of entities and relieve the excessive attention to span regularity. An orthogonality space is further constructed to encourage two modules to extract different aspects of regularity features. To verify the effectiveness of our method, we conduct extensive experiments on three benchmark datasets and a practical medical dataset. The experimental results show that our RICON significantly outperforms previous state-of-the-art methods, including various lexicon-based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.144.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.144.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.144.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.144" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.144/><span class=acl-fixed-case>CRUSH</span>: Contextually Regularized and User anchored Self-supervised Hate speech Detection</a></strong><br><a href=/people/s/souvic-chakraborty/>Souvic Chakraborty</a>
|
<a href=/people/p/parag-dutta/>Parag Dutta</a>
|
<a href=/people/s/sumegh-roychowdhury/>Sumegh Roychowdhury</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--144><div class="card-body p-3 small">The last decade has witnessed a surge in the interaction of people through social networking platforms. While there are several positive aspects of these social platforms, their proliferation has led them to become the breeding ground for cyber-bullying and hate speech. Recent advances in NLP have often been used to mitigate the spread of such hateful content. Since the task of hate speech detection is usually applicable in the context of social networks, we introduce CRUSH, a framework for hate speech detection using User Anchored self-supervision and contextual regularization. Our proposed approach secures ~1-12% improvement in test set metrics over best performing previous approaches on two types of tasks and multiple popular English language social networking datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.145.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.145.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.145" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.145/><span class=acl-fixed-case>METGEN</span>: A Module-Based Entailment Tree Generation Framework for Answer Explanation</a></strong><br><a href=/people/r/ruixin-hong/>Ruixin Hong</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/x/xintong-yu/>Xintong Yu</a>
|
<a href=/people/c/changshui-zhang/>Changshui Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--145><div class="card-body p-3 small">Knowing the reasoning chains from knowledge to the predicted answers can help construct an explainable question answering (QA) system. Advances on QA explanation propose to explain the answers with entailment trees composed of multiple entailment steps. While current work proposes to generate entailment trees with end-to-end generative models, the steps in the generated trees are not constrained and could be unreliable. In this paper, we propose METGEN, a Module-based Entailment Tree GENeration framework that has multiple modules and a reasoning controller. Given a question and several supporting knowledge, METGEN can iteratively generate the entailment tree by conducting single-step entailment with separate modules and selecting the reasoning flow with the controller. As each module is guided to perform a specific type of entailment reasoning, the steps generated by METGEN are more reliable and valid. Experiment results on the standard benchmark show that METGEN can outperform previous state-of-the-art models with only 9% of the parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.146.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.146" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.146/>A Timestep aware Sentence Embedding and Acme Coverage for Brief but Informative Title Generation</a></strong><br><a href=/people/q/quanbin-wang/>Quanbin Wang</a>
|
<a href=/people/x/xiexiong-lin/>XieXiong Lin</a>
|
<a href=/people/f/feng-wang/>Feng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--146><div class="card-body p-3 small">The title generation task that summarizes article content in recapitulatory words relies heavily on utilizing the corresponding key context. To generate a title with appropriate information in the content and avoid repetition, we propose a title generation framework with two complementary components in this paper. First, we propose a Timestep aware Sentence Embedding (TSE) mechanism, which updates the sentences’ representations by re-locating the critical words in the corresponding sentence for each decoding step. Then, we present an Acme Coverage (AC) mechanism to solve the repetition problem and preserve the remaining valuable keywords after each decoding step according to the final vocabulary distribution. We conduct comprehensive experiments on various title generation tasks with different backbones, the evaluation scores of ROUGE and METEOR in varying degrees are significantly outperforming most of the existing state-of-the-art approaches. The experimental results demonstrate the effectiveness and generality of our novel generation framework TSE-AC.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.147.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.147.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.147/>Make The Most of Prior Data: A Solution for Interactive Text Summarization with Preference Feedback</a></strong><br><a href=/people/d/duy-hung-nguyen/>Duy-Hung Nguyen</a>
|
<a href=/people/n/nguyen-viet-dung-nghiem/>Nguyen Viet Dung Nghiem</a>
|
<a href=/people/b/bao-sinh-nguyen/>Bao-Sinh Nguyen</a>
|
<a href=/people/d/dung-tien-tien-le/>Dung Tien Tien Le</a>
|
<a href=/people/s/shahab-sabahi/>Shahab Sabahi</a>
|
<a href=/people/m/minh-tien-nguyen/>Minh-Tien Nguyen</a>
|
<a href=/people/h/hung-le/>Hung Le</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--147><div class="card-body p-3 small">For summarization, human preferences is critical to tame outputs of the summarizer in favor of human interests, as ground-truth summaries are scarce and ambiguous. Practical settings require dynamic exchanges between humans and AI agents wherein feedback is provided in an online manner, a few at a time. In this paper, we introduce a new framework to train summarization models with preference feedback interactively. By properly leveraging offline data and a novel reward model, we improve the performance regarding ROUGE scores and sample-efficiency. Our experiments on three various datasets confirm the benefit of the proposed framework in active, few-shot and online settings of preference learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.148.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.148.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.148" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.148/><span class=acl-fixed-case>XLT</span>ime: A Cross-Lingual Knowledge Transfer Framework for Temporal Expression Extraction</a></strong><br><a href=/people/y/yuwei-cao/>Yuwei Cao</a>
|
<a href=/people/w/william-groves/>William Groves</a>
|
<a href=/people/t/tanay-kumar-saha/>Tanay Kumar Saha</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/a/alejandro-jaimes/>Alejandro Jaimes</a>
|
<a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--148><div class="card-body p-3 small">Temporal Expression Extraction (TEE) is essential for understanding time in natural language. It has applications in Natural Language Processing (NLP) tasks such as question answering, information retrieval, and causal inference. To date, work in this area has mostly focused on English as there is a scarcity of labeled data for other languages. We propose XLTime, a novel framework for multilingual TEE. XLTime works on top of pre-trained language models and leverages multi-task learning to prompt cross-language knowledge transfer both from English and within the non-English languages. XLTime alleviates problems caused by a shortage of data in the target language. We apply XLTime with different language models and show that it outperforms the previous automatic SOTA methods on French, Spanish, Portuguese, and Basque, by large margins. XLTime also closes the gap considerably on the handcrafted HeidelTime method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.149.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.149.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.149" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.149/><span class=acl-fixed-case>B</span>ehance<span class=acl-fixed-case>PR</span>: A Punctuation Restoration Dataset for Livestreaming Video Transcript</a></strong><br><a href=/people/v/viet-lai/>Viet Lai</a>
|
<a href=/people/a/amir-pouran-ben-veyseh/>Amir Pouran Ben Veyseh</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/thien-nguyen/>Thien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--149><div class="card-body p-3 small">Given the increasing number of livestreaming videos, automatic speech recognition and post-processing for livestreaming video transcripts are crucial for efficient data management as well as knowledge mining. A key step in this process is punctuation restoration which restores fundamental text structures such as phrase and sentence boundaries from the video transcripts. This work presents a new human-annotated corpus, called BehancePR, for punctuation restoration in livestreaming video transcripts. Our experiments on BehancePR demonstrate the challenges of punctuation restoration for this domain. Furthermore, we show that popular natural language processing toolkits like Stanford Stanza, Spacy, and Trankit underperform on detecting sentence boundary on non-punctuated transcripts of livestreaming videos. The dataset is publicly accessible at <a href=http://github.com/nlp-uoregon/behancepr class=acl-markup-url>http://github.com/nlp-uoregon/behancepr</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.150.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--150 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.150.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.150/>Event Detection for Suicide Understanding</a></strong><br><a href=/people/l/luis-guzman-nateras/>Luis Guzman-Nateras</a>
|
<a href=/people/v/viet-lai/>Viet Lai</a>
|
<a href=/people/a/amir-pouran-ben-veyseh/>Amir Pouran Ben Veyseh</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/thien-nguyen/>Thien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--150><div class="card-body p-3 small">Suicide is a serious problem in every society. Understanding life events of a potential patient is essential for successful suicide-risk assessment and prevention. In this work, we focus on the Event Detection (ED) task to identify event trigger words of suicide-related events in public posts of discussion forums. In particular, we introduce SuicideED: a new dataset for the ED task that features seven suicidal event types to comprehensively capture suicide actions and ideation, and general risk and protective factors. Our experiments with current state-of-the-art ED systems suggest that this domain poses meaningful challenges as there is significant room for improvement of ED models. We will release SuicideED to support future research in this important area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.151.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.151.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.151/>Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models</a></strong><br><a href=/people/j/joseph-mcdonald/>Joseph McDonald</a>
|
<a href=/people/b/baolin-li/>Baolin Li</a>
|
<a href=/people/n/nathan-frey/>Nathan Frey</a>
|
<a href=/people/d/devesh-tiwari/>Devesh Tiwari</a>
|
<a href=/people/v/vijay-gadepally/>Vijay Gadepally</a>
|
<a href=/people/s/siddharth-samsi/>Siddharth Samsi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--151><div class="card-body p-3 small">The energy requirements of current natural language processing models continue to grow at a rapid, unsustainable pace. Recent works highlighting this problem conclude there is an urgent need for methods that reduce the energy needs of NLP and machine learning more broadly. In this article, we investigate techniques that can be used to reduce the energy consumption of common NLP applications. In particular, we focus on techniques to measure energy usage and different hardware and datacenter-oriented settings that can be tuned to reduce energy consumption for training and inference for language models. We characterize the impact of these settings on metrics such as computational performance and energy consumption through experiments conducted on a high performance computing system as well as popular cloud computing platforms. These techniques can lead to significant reduction in energy consumption when training language models or their use for inference. For example, power-capping, which limits the maximum power a GPU can consume, can enable a 15% decrease in energy usage with marginal increase in overall computation time when training a transformer-based language model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.152.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.152.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.152/>What kinds of errors do reference resolution models make and what can we learn from them?</a></strong><br><a href=/people/j/jorge-sanchez/>Jorge Sánchez</a>
|
<a href=/people/m/mauricio-mazuecos/>Mauricio Mazuecos</a>
|
<a href=/people/h/hernan-maina/>Hernán Maina</a>
|
<a href=/people/l/luciana-benotti/>Luciana Benotti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--152><div class="card-body p-3 small">Referring resolution is the task of identifying the referent of a natural language expression, for example “the woman behind the other woman getting a massage”. In this paper we investigate which are the kinds of referring expressions on which current transformer based models fail. Motivated by this analysis we identify the weakening of the spatial natural constraints as one of its causes and propose a model that aims to restore it. We evaluate our proposed model on different datasets for the task showing improved performance on the most challenging kinds of referring expressions. Finally we present a thorough analysis of the kinds errors that are improved by the new model and those that are not and remain future challenges for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.153.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--153 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.153.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.153.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.153" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.153/>Uncertainty-Aware Cross-Lingual Transfer with Pseudo Partial Labels</a></strong><br><a href=/people/s/shuo-lei/>Shuo Lei</a>
|
<a href=/people/x/xuchao-zhang/>Xuchao Zhang</a>
|
<a href=/people/j/jianfeng-he/>Jianfeng He</a>
|
<a href=/people/f/fanglan-chen/>Fanglan Chen</a>
|
<a href=/people/c/chang-tien-lu/>Chang-Tien Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--153><div class="card-body p-3 small">Large-scale multilingual pre-trained language models have achieved remarkable performance in zero-shot cross-lingual tasks. A recent study has demonstrated the effectiveness of self-learning-based approach on cross-lingual transfer, where only unlabeled data of target languages are required, without any efforts to annotate gold labels for target languages. However, it suffers from noisy training due to the incorrectly pseudo-labeled samples. In this work, we propose an uncertainty-aware Cross-Lingual Transfer framework with Pseudo-Partial-Label (CLTP)1 to maximize the utilization of unlabeled data by reducing the noise introduced in the training phase. To estimate pseudo-partial-label for each unlabeled data, we propose a novel estimation method, considering both prediction confidence and the limitation to the number of similar labels. Extensive experiments are conducted on two cross-lingual tasks, including Named Entity Recognition (NER) and Natural Language Inference (NLI) across 40 languages, which shows our method can outperform the baselines on both high-resource and low-resource languages, such as 6.9 on Kazakh (kk) and 5.2 Marathi (mr) for NER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.154.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--154 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.154.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.154" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.154/><span class=acl-fixed-case>NLU</span>++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue</a></strong><br><a href=/people/i/inigo-casanueva/>Inigo Casanueva</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--154><div class="card-body p-3 small">We present NLU++, a novel dataset for natural language understanding (NLU) in task-oriented dialogue (ToD) systems, with the aim to provide a much more challenging evaluation environment for dialogue NLU models, up to date with the current application and industry requirements. NLU++ is divided into two domains (BANKING and HOTELS) and brings several crucial improvements over current commonly used NLU datasets. 1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences combined with finer-grained and thus more challenging slot sets. 2) The ontology is divided into domain-specific and generic (i.e., domain-universal) intents that overlap across domains, promoting cross-domain reusability of annotated examples. 3) The dataset design has been inspired by the problems observed in industrial ToD systems, and 4) it has been collected, filtered and carefully annotated by dialogue NLU experts, yielding high-quality annotated data. Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, and call for further research on ToD NLU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.155.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.155.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.155" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.155/>Challenges in Generalization in Open Domain Question Answering</a></strong><br><a href=/people/l/linqing-liu/>Linqing Liu</a>
|
<a href=/people/p/patrick-lewis/>Patrick Lewis</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--155><div class="card-body p-3 small">Recent work on Open Domain Question Answering has shown that there is a large discrepancy in model performance between novel test questions and those that largely overlap with training questions. However, it is unclear which aspects of novel questions make them challenging. Drawing upon studies on systematic generalization, we introduce and annotate questions according to three categories that measure different levels and kinds of generalization: training set overlap, compositional generalization (comp-gen), and novel-entity generalization (novel-entity). When evaluating six popular parametric and non-parametric models, we find that for the established Natural Questions and TriviaQA datasets, even the strongest model performance for comp-gen/novel-entity is 13.1/5.4% and 9.6/1.5% lower compared to that for the full test set – indicating the challenge posed by these types of questions. Furthermore, we show that whilst non-parametric models can handle questions containing novel entities relatively well, they struggle with those requiring compositional generalization. Lastly, we find that key question difficulty factors are: cascading errors from the retrieval component, frequency of question pattern, and frequency of the entity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.156.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--156 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.156.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.156" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.156/>Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence</a></strong><br><a href=/people/m/myeongjun-jang/>Myeongjun Jang</a>
|
<a href=/people/f/frank-mtumbuka/>Frank Mtumbuka</a>
|
<a href=/people/t/thomas-lukasiewicz/>Thomas Lukasiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--156><div class="card-body p-3 small">The logical negation property (LNP), which implies generating different predictions for semantically opposite inputs (p is true iff ¬p is false), is an important property that a trustworthy language model must satisfy. However, much recent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property. In this paper, we perform experiments using probing tasks to assess PLMs’ LNP understanding. Unlike previous studies that only examined negation expressions, we expand the boundary of the investigation to lexical semantics. Through experiments, we observe that PLMs violate the LNP frequently. To alleviate the issue, we propose a novel intermediate training task, named meaning-matching, designed to directly learn a meaning text correspondence, instead of relying on the distributional hypothesis. Through multiple experiments, we find that the task enables PLMs to learn lexical semantic information. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm that it is a safe intermediate task that guarantees a similar or better performance of downstream tasks. Finally, we observe that our proposed approach outperforms our previous counterparts despite its time and resource efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.157.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.157.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.157" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.157/>Por Qué Não Utiliser Alla Språk? Mixed Training with Gradient Optimization in Few-Shot Cross-Lingual Transfer</a></strong><br><a href=/people/h/haoran-xu/>Haoran Xu</a>
|
<a href=/people/k/kenton-murray/>Kenton Murray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--157><div class="card-body p-3 small">The current state-of-the-art for few-shot cross-lingual transfer learning first trains on abundant labeled data in the source language and then fine-tunes with a few examples on the target language, termed target-adapting. Though this has been demonstrated to work on a variety of tasks, in this paper we show some deficiencies of this approach and propose a one-step mixed training method that trains on both source and target data with stochastic gradient surgery, a novel gradient-level optimization. Unlike the previous studies that focus on one language at a time when target-adapting, we use one model to handle all target languages simultaneously to avoid excessively language-specific models. Moreover, we discuss the unreality of utilizing large target development sets for model selection in previous literature. We further show that our method is both development-free for target languages, and is also able to escape from overfitting issues. We conduct a large-scale experiment on 4 diverse NLP tasks across up to 48 languages. Our proposed method achieves state-of-the-art performance on all tasks and outperforms target-adapting by a large margin, especially for languages that are linguistically distant from the source language, e.g., 7.36% F1 absolute gain on average for the NER task, up to 17.60% on Punjabi.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.158.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.158.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.158" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.158/>Learning to Execute Actions or Ask Clarification Questions</a></strong><br><a href=/people/z/zhengxiang-shi/>Zhengxiang Shi</a>
|
<a href=/people/y/yue-feng/>Yue Feng</a>
|
<a href=/people/a/aldo-lipani/>Aldo Lipani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--158><div class="card-body p-3 small">Collaborative tasks are ubiquitous activities where a form of communication is required in order to reach a joint goal. Collaborative building is one of such tasks. We wish to develop an intelligent builder agent in a simulated building environment (Minecraft) that can build whatever users wish to build by just talking to the agent. In order to achieve this goal, such agents need to be able to take the initiative by asking clarification questions when further information is needed. Existing works on Minecraft Corpus Dataset only learn to execute instructions neglecting the importance of asking for clarifications. In this paper, we extend the Minecraft Corpus Dataset by annotating all builder utterances into eight types, including clarification questions, and propose a new builder agent model capable of determining when to ask or execute instructions. Experimental results show that our model achieves state-of-the-art performance on the collaborative building task with a substantial improvement. We also define two new tasks, the learning to ask task and the joint learning task. The latter consists of solving both collaborating building and learning to ask tasks jointly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.159.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.159.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.159" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.159/>Capturing Conversational Interaction for Question Answering via Global History Reasoning</a></strong><br><a href=/people/j/jin-qian/>Jin Qian</a>
|
<a href=/people/b/bowei-zou/>Bowei Zou</a>
|
<a href=/people/m/mengxing-dong/>Mengxing Dong</a>
|
<a href=/people/x/xiao-li/>Xiao Li</a>
|
<a href=/people/a/aiti-aw/>AiTi Aw</a>
|
<a href=/people/y/yu-hong/>Yu Hong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--159><div class="card-body p-3 small">Conversational Question Answering (ConvQA) is required to answer the current question, conditioned on the observable paragraph-level context and conversation history. Previous works have intensively studied history-dependent reasoning. They perceive and absorb topic-related information of prior utterances in the interactive encoding stage. It yielded significant improvement compared to history-independent reasoning. This paper further strengthens the ConvQA encoder by establishing long-distance dependency among global utterances in multi-turn conversation. We use multi-layer transformers to resolve long-distance relationships, which potentially contribute to the reweighting of attentive information in historical utterances. Experiments on QuAC show that our method obtains a substantial improvement (1%), yielding the F1 score of 73.7%. All source codes are available at <a href=https://github.com/jaytsien/GHR class=acl-markup-url>https://github.com/jaytsien/GHR</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.160.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.160.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.160/>Learning Structural Information for Syntax-Controlled Paraphrase Generation</a></strong><br><a href=/people/e/erguang-yang/>Erguang Yang</a>
|
<a href=/people/c/chenglin-bai/>Chenglin Bai</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/y/yujie-zhang/>Yujie Zhang</a>
|
<a href=/people/y/yao-meng/>Yao Meng</a>
|
<a href=/people/j/jinan-xu/>Jinan Xu</a>
|
<a href=/people/y/yufeng-chen/>Yufeng Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--160><div class="card-body p-3 small">Syntax-controlled paraphrase generation aims to produce paraphrase conform to given syntactic patterns. To address this task, recent works have started to use parse trees (or syntactic templates) to guide generation.A constituency parse tree contains abundant structural information, such as parent-child relation, sibling relation, and the alignment relation between words and nodes. Previous works have only utilized parent-child and alignment relations, which may affect the generation quality. To address this limitation, we propose a Structural Information-augmented Syntax-Controlled Paraphrasing (SI-SCP) model. Particularly, we design a syntax encoder based on tree-transformer to capture parent-child and sibling relations. To model the alignment relation between words and nodes, we propose an attention regularization objective, which makes the decoder accurately select corresponding syntax nodes to guide the generation of words. Experiments show that SI-SCP achieves state-of-the-art performances in terms of semantic and syntactic quality on two popular benchmark datasets. Additionally, we propose a Syntactic Template Retriever (STR) to retrieve compatible syntactic structures. We validate that STR is capable of retrieving compatible syntactic structures. We further demonstrate the effectiveness of SI-SCP to generate diverse paraphrases with retrieved syntactic structures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.161.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--161 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.161.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.161" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.161/>Controllable Sentence Simplification via Operation Classification</a></strong><br><a href=/people/l/liam-cripwell/>Liam Cripwell</a>
|
<a href=/people/j/joel-legrand/>Joël Legrand</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--161><div class="card-body p-3 small">Different types of transformations have been used to model sentence simplification ranging from mainly local operations such as phrasal or lexical rewriting, deletion and re-ordering to the more global affecting the whole input sentence such as sentence rephrasing, copying and splitting. In this paper, we propose a novel approach to sentence simplification which encompasses four global operations: whether to rephrase or copy and whether to split based on syntactic or discourse structure. We create a novel dataset that can be used to train highly accurate classification systems for these four operations. We propose a controllable-simplification model that tailors simplifications to these operations and show that it outperforms both end-to-end, non-controllable approaches and previous controllable approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.162.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.162.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.162.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.162/>Balancing Multi-Domain Corpora Learning for Open-Domain Response Generation</a></strong><br><a href=/people/y/yujie-xing/>Yujie Xing</a>
|
<a href=/people/j/jinglun-cai/>Jinglun Cai</a>
|
<a href=/people/n/nils-barlaug/>Nils Barlaug</a>
|
<a href=/people/p/peng-liu/>Peng Liu</a>
|
<a href=/people/j/jon-atle-gulla/>Jon Atle Gulla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--162><div class="card-body p-3 small">Open-domain conversational systems are assumed to generate equally good responses on multiple domains. Previous work achieved good performance on the single corpus, but training and evaluating on multiple corpora from different domains are less studied. This paper explores methods of generating relevant responses for each of multiple multi-domain corpora. We first examine interleaved learning which intermingles multiple corpora as the baseline. We then investigate two multi-domain learning methods, labeled learning and multi-task labeled learning, which encode each corpus through a unique corpus embedding. Furthermore, we propose Domain-specific Frequency (DF), a novel word-level importance weight that measures the relative importance of a word for a specific corpus compared to other corpora. Based on DF, we propose weighted learning, a method that integrates DF to the loss function. We also adopt DF as a new evaluation metric. Extensive experiments show that our methods gain significant improvements on both automatic and human evaluation. We share our code and data for reproducibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.163.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--163 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.163/>Semantic-Preserving Abstractive Text Summarization with <span class=acl-fixed-case>S</span>iamese Generative Adversarial Net</a></strong><br><a href=/people/x/xin-sheng/>Xin Sheng</a>
|
<a href=/people/l/linli-xu/>Linli Xu</a>
|
<a href=/people/y/yinlong-xu/>Yinlong Xu</a>
|
<a href=/people/d/deqiang-jiang/>Deqiang Jiang</a>
|
<a href=/people/b/bo-ren/>Bo Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--163><div class="card-body p-3 small">We propose a novel siamese generative adversarial net for abstractive text summarization (SSPGAN), which can preserve the main semantics of the source text. Different from previous generative adversarial net based methods, SSPGAN is equipped with a siamese semantic-preserving discriminator, which can not only be trained to discriminate the machine-generated summaries from the human-summarized ones, but also ensure the semantic consistency between the source text and target summary. As a consequence of the min-max game between the generator and the siamese semantic-preserving discriminator, the generator can generate a summary that conveys the key content of the source text more accurately. Extensive experiments on several text summarization benchmarks in different languages demonstrate that the proposed model can achieve significant improvements over the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.164.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.164.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.164" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.164/>Towards Job-Transition-Tag Graph for a Better Job Title Representation Learning</a></strong><br><a href=/people/j/jun-zhu/>Jun Zhu</a>
|
<a href=/people/c/celine-hudelot/>Celine Hudelot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--164><div class="card-body p-3 small">Works on learning job title representation are mainly based on <i>Job-Transition Graph</i>, built from the working history of talents. However, since these records are usually messy, this graph is very sparse, which affects the quality of the learned representation and hinders further analysis. To address this specific issue, we propose to enrich the graph with additional nodes that improve the quality of job title representation. Specifically, we construct <i>Job-Transition-Tag Graph</i>, a heterogeneous graph containing two types of nodes, i.e., job titles and tags (i.e., words related to job responsibilities or functionalities). Along this line, we reformulate job title representation learning as the task of learning node embedding on the <i>Job-Transition-Tag Graph</i>. Experiments on two datasets show the interest of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.165.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--165 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.165.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.165" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.165/><span class=acl-fixed-case>CL</span>-<span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>LKT</span>: Cross-lingual Language Knowledge Transfer for Multilingual Retrieval Question Answering</a></strong><br><a href=/people/p/peerat-limkonchotiwat/>Peerat Limkonchotiwat</a>
|
<a href=/people/w/wuttikorn-ponwitayarat/>Wuttikorn Ponwitayarat</a>
|
<a href=/people/c/can-udomcharoenchaikit/>Can Udomcharoenchaikit</a>
|
<a href=/people/e/ekapol-chuangsuwanich/>Ekapol Chuangsuwanich</a>
|
<a href=/people/s/sarana-nutanong/>Sarana Nutanong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--165><div class="card-body p-3 small">Cross-Lingual Retrieval Question Answering (CL-ReQA) is concerned with retrieving answer documents or passages to a question written in a different language. A common approach to CL-ReQA is to create a multilingual sentence embedding space such that question-answer pairs across different languages are close to each other. In this paper, we propose a novel CL-ReQA method utilizing the concept of language knowledge transfer and a new cross-lingual consistency training technique to create a multilingual embedding space for ReQA. To assess the effectiveness of our work, we conducted comprehensive experiments on CL-ReQA and a downstream task, machine reading QA. We compared our proposed method with the current state-of-the-art solutions across three public CL-ReQA corpora. Our method outperforms competitors in 19 out of 21 settings of CL-ReQA. When used with a downstream machine reading QA task, our method outperforms the best existing language-model-based method by 10% in F1 while being 10 times faster in sentence embedding computation. The code and models are available at <a href=https://github.com/mrpeerat/CL-ReLKT class=acl-markup-url>https://github.com/mrpeerat/CL-ReLKT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.166.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--166 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.166.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.166.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.166" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.166/><span class=acl-fixed-case>BORT</span>: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog</a></strong><br><a href=/people/h/haipeng-sun/>Haipeng Sun</a>
|
<a href=/people/j/junwei-bao/>Junwei Bao</a>
|
<a href=/people/y/youzheng-wu/>Youzheng Wu</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--166><div class="card-body p-3 small">A typical end-to-end task-oriented dialog system transfers context into dialog state, and upon which generates a response, which usually faces the problem of error propagation from both previously generated inaccurate dialog states and responses, especially in low-resource scenarios. To alleviate these issues, we propose BORT, a back and denoising reconstruction approach for end-to-end task-oriented dialog system. Squarely, to improve the accuracy of dialog states, back reconstruction is used to reconstruct the original input context from the generated dialog states since inaccurate dialog states cannot recover the corresponding input context. To enhance the denoising capability of the model to reduce the impact of error propagation, denoising reconstruction is used to reconstruct the corrupted dialog state and response. Extensive experiments conducted on MultiWOZ 2.0 and CamRest676 show the effectiveness of BORT. Furthermore, BORT demonstrates its advanced capabilities in the zero-shot domain and low-resource scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.167.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--167 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.167.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.167" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.167/>Multi-stage Distillation Framework for Cross-Lingual Semantic Similarity Matching</a></strong><br><a href=/people/k/kunbo-ding/>Kunbo Ding</a>
|
<a href=/people/w/weijie-liu/>Weijie Liu</a>
|
<a href=/people/y/yuejian-fang/>Yuejian Fang</a>
|
<a href=/people/z/zhe-zhao/>Zhe Zhao</a>
|
<a href=/people/q/qi-ju/>Qi Ju</a>
|
<a href=/people/x/xuefeng-yang/>Xuefeng Yang</a>
|
<a href=/people/r/rong-tian/>Rong Tian</a>
|
<a href=/people/z/zhu-tao/>Zhu Tao</a>
|
<a href=/people/h/haoyan-liu/>Haoyan Liu</a>
|
<a href=/people/h/han-guo/>Han Guo</a>
|
<a href=/people/x/xingyu-bai/>Xingyu Bai</a>
|
<a href=/people/w/weiquan-mao/>Weiquan Mao</a>
|
<a href=/people/y/yudong-li/>Yudong Li</a>
|
<a href=/people/w/weigang-guo/>Weigang Guo</a>
|
<a href=/people/t/taiqiang-wu/>Taiqiang Wu</a>
|
<a href=/people/n/ningyuan-sun/>Ningyuan Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--167><div class="card-body p-3 small">Previous studies have proved that cross-lingual knowledge distillation can significantly improve the performance of pre-trained models for cross-lingual similarity matching tasks. However, the student model needs to be large in this operation. Otherwise, its performance will drop sharply, thus making it impractical to be deployed to memory-limited devices. To address this issue, we delve into cross-lingual knowledge distillation and propose a multi-stage distillation framework for constructing a small-size but high-performance cross-lingual model. In our framework, contrastive learning, bottleneck, and parameter recurrent strategies are delicately combined to prevent performance from being compromised during the compression process. The experimental results demonstrate that our method can compress the size of XLM-R and MiniLM by more than 50%, while the performance is only reduced by about 1%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.168.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--168 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.168.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.168/>On the Limitations of Dataset Balancing: The Lost Battle Against Spurious Correlations</a></strong><br><a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--168><div class="card-body p-3 small">Recent work has shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to over-fitting and lack of generalization. To mitigate this problem, a common practice is to balance datasets by adding new instances or by filtering out “easy” instances (Sakaguchi et al., 2020), culminating in a recent proposal to eliminate single-word correlations altogether (Gardner et al., 2021). In this opinion paper, we identify that despite these efforts, increasingly-powerful models keep exploiting ever-smaller spurious correlations, and as a result even balancing all single-word features is insufficient for mitigating all of these correlations. In parallel, a truly balanced dataset may be bound to “throw the baby out with the bathwater” and miss important signal encoding common sense and world knowledge. We highlight several alternatives to dataset balancing, focusing on enhancing datasets with richer contexts, allowing models to abstain and interact with users, and turning from large-scale fine-tuning to zero- or few-shot setups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.169.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.169.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.169/>Specializing Pre-trained Language Models for Better Relational Reasoning via Network Pruning</a></strong><br><a href=/people/s/siyu-ren/>Siyu Ren</a>
|
<a href=/people/k/kenny-zhu/>Kenny Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--169><div class="card-body p-3 small">Pretrained masked language models (PLMs) were shown to be inheriting a considerable amount of relational knowledge from the source corpora. In this paper, we present an in-depth and comprehensive study concerning specializing PLMs into relational models from the perspective of network pruning. We show that it is possible to find subnetworks capable of representing grounded commonsense relations at non-trivial sparsity while being more generalizable than original PLMs in scenarios requiring knowledge of single or multiple commonsense relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.170.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.170.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.170/><span class=acl-fixed-case>D</span>2<span class=acl-fixed-case>GCLF</span>: Document-to-Graph Classifier for Legal Document Classification</a></strong><br><a href=/people/q/qiqi-wang/>Qiqi Wang</a>
|
<a href=/people/k/kaiqi-zhao/>Kaiqi Zhao</a>
|
<a href=/people/r/robert-amor/>Robert Amor</a>
|
<a href=/people/b/benjamin-liu/>Benjamin Liu</a>
|
<a href=/people/r/ruofan-wang/>Ruofan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--170><div class="card-body p-3 small">Legal document classification is an essential task in law intelligence to automate the labor-intensive law case filing process. Unlike traditional document classification problems, legal documents should be classified by reasons and facts instead of topics. We propose a Document-to-Graph Classifier (D2GCLF), which extracts facts as relations between key participants in the law case and represents a legal document with four relation graphs. Each graph is responsible for capturing different relations between the litigation participants. We further develop a graph attention network on top of the four relation graphs to classify the legal documents. Experiments on a real-world legal document dataset show that D2GCLF outperforms the state-of-the-art methods in terms of accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.171.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--171 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.171.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.171.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.171/>A Label-Aware Autoregressive Framework for Cross-Domain <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/j/jinpeng-hu/>Jinpeng Hu</a>
|
<a href=/people/h/he-zhao/>He Zhao</a>
|
<a href=/people/d/dan-guo/>Dan Guo</a>
|
<a href=/people/x/xiang-wan/>Xiang Wan</a>
|
<a href=/people/t/tsung-hui-chang/>Tsung-Hui Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--171><div class="card-body p-3 small">Cross-domain named entity recognition (NER) aims to borrow the entity information from the source domain to help the entity recognition in the target domain with limited labeled data. Despite the promising performance of existing approaches, most of them focus on reducing the discrepancy of token representation between source and target domains, while the transfer of the valuable label information is often not explicitly considered or even ignored. Therefore, we propose a novel autoregressive framework to advance cross-domain NER by first enhancing the relationship between labels and tokens and then further improving the transferability of label information. Specifically, we associate each label with an embedding vector, and for each token, we utilize a bidirectional LSTM (Bi-LSTM) to encode the labels of its previous tokens for modeling internal context information and label dependence. Afterward, we propose a Bi-Attention module that merges the token representation from a pre-trained model and the label features from the Bi-LSTM as the label-aware information, which is concatenated to the token representation to facilitate cross-domain NER. In doing so, label information contained in the embedding vectors can be effectively transferred to the target domain, and Bi-LSTM can further model the label relationship among different domains by pre-train and then fine-tune setting. Experimental results on several datasets confirm the effectiveness of our model, where our model achieves significant improvements over the state of the arts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.172.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--172 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.172.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.172.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.172/>A Dog Is Passing Over The Jet? A Text-Generation Dataset for <span class=acl-fixed-case>K</span>orean Commonsense Reasoning and Evaluation</a></strong><br><a href=/people/j/jaehyung-seo/>Jaehyung Seo</a>
|
<a href=/people/s/seounghoon-lee/>Seounghoon Lee</a>
|
<a href=/people/c/chanjun-park/>Chanjun Park</a>
|
<a href=/people/y/yoonna-jang/>Yoonna Jang</a>
|
<a href=/people/h/hyeonseok-moon/>Hyeonseok Moon</a>
|
<a href=/people/s/sugyeong-eo/>Sugyeong Eo</a>
|
<a href=/people/s/seonmin-koo/>Seonmin Koo</a>
|
<a href=/people/h/heui-seok-lim/>Heuiseok Lim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--172><div class="card-body p-3 small">Recent natural language understanding (NLU) research on the Korean language has been vigorously maturing with the advancements of pretrained language models and datasets. However, Korean pretrained language models still struggle to generate a short sentence with a given condition based on compositionality and commonsense reasoning (i.e., generative commonsense reasoning). The two major challenges are inadequate data resources to develop generative commonsense reasoning regarding Korean linguistic features and to evaluate language models which are necessary for natural language generation (NLG). To solve these problems, we propose a text-generation dataset for Korean generative commonsense reasoning and language model evaluation. In this work, a semi-automatic dataset construction approach filters out contents inexplicable to commonsense, ascertains quality, and reduces the cost of building the dataset. We also present an in-depth analysis of the generation results of language models with various evaluation metrics along with human-annotated scores. The whole dataset is publicly available at (<a href=https://aihub.or.kr/opendata/korea-university class=acl-markup-url>https://aihub.or.kr/opendata/korea-university</a>).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.173.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--173 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.173.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.173/>Improve Discourse Dependency Parsing with Contextualized Representations</a></strong><br><a href=/people/y/yifei-zhou/>Yifei Zhou</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--173><div class="card-body p-3 small">Previous works show that discourse analysis benefits from modeling intra- and inter-sentential levels separately, where proper representations for text units of different granularities are desired to capture both the information of the text units and their relation to the context. In this paper, we propose to take advantage of transformers to encode different contextualized representations of units of different levels to dynamically capture the information required for discourse dependency analysis on intra- and inter-sentential levels. Motivated by the observation of writing patterns shared across articles to improve discourse analysis, we propose to design sequence labeling methods to take advantage of such structural information from the context that substantially outperforms traditional direct classification methods. Experiments show that our model achieves state-of-the-art results on both English and Chinese datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.174.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.174.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.174" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.174/><span class=acl-fixed-case>L</span>i<span class=acl-fixed-case>ST</span>: Lite Prompted Self-training Makes Parameter-efficient Few-shot Learners</a></strong><br><a href=/people/y/yaqing-wang/>Yaqing Wang</a>
|
<a href=/people/s/subhabrata-mukherjee/>Subhabrata Mukherjee</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/j/jing-gao/>Jing Gao</a>
|
<a href=/people/a/ahmed-awadallah/>Ahmed Awadallah</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--174><div class="card-body p-3 small">We present a new method LiST for efficient fine-tuning of large pre-trained language models (PLMs) in few-shot learning settings. LiST improves over recent methods that adopt prompt-based fine-tuning (FN) using two key techniques. The first is the use of self-training to leverage large amounts of unlabeled data for prompt-based FN in few-shot settings. We use self-training in conjunction with meta-learning for re-weighting noisy pseudo-prompt labels. Traditionally, self-training is expensive as it requires updating all the model parameters repetitively. Therefore, we use a second technique for light-weight fine-tuning where we introduce a small number of task-specific parameters that are fine-tuned during self-training while keeping the PLM encoder frozen. Our experiments show that LiST can effectively leverage unlabeled data to improve the model performance for few-shot learning. Additionally, the finetuning process is efficient as it only updates a small percentage of the parameters and the overall model footprint is reduced since several tasks can share a common PLM encoder as backbone. We present a comprehensive study on six NLU tasks to validate the effectiveness of LiST. The results show that LiST improves by 35% over classic fine-tuning methods and 6% over prompt-based FN with 96% reduction in number of trainable parameters when fine-tuned with no more than 30 labeled examples from each task. With only 14M tunable parameters, LiST outperforms GPT-3 in-context learning by 33% on few-shot NLU tasks</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.175.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--175 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.175.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.175" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.175/><span class=acl-fixed-case>CLMLF</span>:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection</a></strong><br><a href=/people/z/zhen-li/>Zhen Li</a>
|
<a href=/people/b/bing-xu/>Bing Xu</a>
|
<a href=/people/c/conghui-zhu/>Conghui Zhu</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--175><div class="card-body p-3 small">Compared with unimodal data, multimodal data can provide more features to help the model analyze the sentiment of data. Previous research works rarely consider token-level feature fusion, and few works explore learning the common features related to sentiment in multimodal data to help the model fuse multimodal features. In this paper, we propose a Contrastive Learning and Multi-Layer Fusion (CLMLF) method for multimodal sentiment detection. Specifically, we first encode text and image to obtain hidden representations, and then use a multi-layer fusion module to align and fuse the token-level features of text and image. In addition to the sentiment analysis task, we also designed two contrastive learning tasks, label based contrastive learning and data based contrastive learning tasks, which will help the model learn common features related to sentiment in multimodal data. Extensive experiments conducted on three publicly available multimodal datasets demonstrate the effectiveness of our approach for multimodal sentiment detection compared with existing methods. The codes are available for use at https: //github.com/Link-Li/CLMLF</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.176.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.176.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.176" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.176/>Weakly Supervised Text Classification using Supervision Signals from a Language Model</a></strong><br><a href=/people/z/ziqian-zeng/>Ziqian Zeng</a>
|
<a href=/people/w/weimin-ni/>Weimin Ni</a>
|
<a href=/people/t/tianqing-fang/>Tianqing Fang</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/x/xinran-zhao/>Xinran Zhao</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--176><div class="card-body p-3 small">Solving text classification in a weakly supervised manner is important for real-world applications where human annotations are scarce. In this paper, we propose to query a masked language model with cloze style prompts to obtain supervision signals. We design a prompt which combines the document itself and “this article is talking about [MASK].” A masked language model can generate words for the [MASK] token. The generated words which summarize the content of a document can be utilized as supervision signals. We propose a latent variable model to learn a word distribution learner which associates generated words to pre-defined categories and a document classifier simultaneously without using any annotated data. Evaluation on three datasets, AGNews, 20Newsgroups, and UCINews, shows that our method can outperform baselines by 2%, 4%, and 3%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.177.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--177 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.177.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.177" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.177/>Analytical Reasoning of Text</a></strong><br><a href=/people/w/wanjun-zhong/>Wanjun Zhong</a>
|
<a href=/people/s/siyuan-wang/>Siyuan Wang</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/z/zenan-xu/>Zenan Xu</a>
|
<a href=/people/d/daya-guo/>Daya Guo</a>
|
<a href=/people/y/yining-chen/>Yining Chen</a>
|
<a href=/people/j/jiahai-wang/>Jiahai Wang</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--177><div class="card-body p-3 small">Analytical reasoning is an essential and challenging task that requires a system to analyze a scenario involving a set of particular circumstances and perform reasoning over it to make conclusions. However, current neural models with implicit reasoning ability struggle to solve this task. In this paper, we study the challenge of analytical reasoning of text and collect a new dataset consisting of questions from the Law School Admission Test from 1991 to 2016. We analyze what knowledge understanding and reasoning abilities are required to do well on this task, and present an approach dubbed ARM. It extracts knowledge such as participants and facts from the context. Such knowledge are applied to an inference engine to deduce legitimate solutions for drawing conclusions. In our experiments, we find that ubiquitous pre-trained models struggle to deal with this task as their performance is close to random guess. Results show that ARM outperforms pre-trained models significantly. Moreover, we demonstrate that ARM has better explicit interpretable reasoning ability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.178.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.178.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.178.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.178/>Denoising Neural Network for News Recommendation with Positive and Negative Implicit Feedback</a></strong><br><a href=/people/y/yunfan-hu/>Yunfan Hu</a>
|
<a href=/people/z/zhaopeng-qiu/>Zhaopeng Qiu</a>
|
<a href=/people/x/xian-wu/>Xian Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--178><div class="card-body p-3 small">News recommendation is different from movie or e-commercial recommendation as people usually do not grade the news. Therefore, user feedback for news is always implicit (click behavior, reading time, etc). Inevitably, there are noises in implicit feedback. On one hand, the user may exit immediately after clicking the news as he dislikes the news content, leaving the noise in his positive implicit feedback; on the other hand, the user may be recommended multiple interesting news at the same time and only click one of them, producing the noise in his negative implicit feedback. Opposite implicit feedback could construct more integrated user preferences and help each other to minimize the noise influence. Previous works on news recommendation only used positive implicit feedback and suffered from the noise impact. In this paper, we propose a denoising neural network for news recommendation with positive and negative implicit feedback, named DRPN. DRPN utilizes both feedback for recommendation with a module to denoise both positive and negative implicit feedback to further enhance the performance. Experiments on the real-world large-scale dataset demonstrate the state-of-the-art performance of DRPN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.179.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--179 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.179.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.179/>Continual Machine Reading Comprehension via Uncertainty-aware Fixed Memory and Adversarial Domain Adaptation</a></strong><br><a href=/people/z/zhijing-wu/>Zhijing Wu</a>
|
<a href=/people/h/hua-xu/>Hua Xu</a>
|
<a href=/people/j/jingliang-fang/>Jingliang Fang</a>
|
<a href=/people/k/kai-gao/>Kai Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--179><div class="card-body p-3 small">Continual Machine Reading Comprehension aims to incrementally learn from a continuous data stream across time without access the previous seen data, which is crucial for the development of real-world MRC systems. However, it is a great challenge to learn a new domain incrementally without catastrophically forgetting previous knowledge. In this paper, MA-MRC, a continual MRC model with uncertainty-aware fixed Memory and Adversarial domain adaptation, is proposed. In MA-MRC, a fixed size memory stores a small number of samples in previous domain data along with an uncertainty-aware updating strategy when new domain data arrives. For incremental learning, MA-MRC not only keeps a stable understanding by learning both memory and new domain data, but also makes full use of the domain adaptation relationship between them by adversarial learning strategy. The experimental results show that MA-MRC is superior to strong baselines and has a substantial incremental learning ability without catastrophically forgetting under two different continual MRC settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.180.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.180/>Jointly Learning Guidance Induction and Faithful Summary Generation via Conditional Variational Autoencoders</a></strong><br><a href=/people/w/wang-xu/>Wang Xu</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--180><div class="card-body p-3 small">Abstractive summarization can generate high quality results with the development of the neural network. However, generating factual consistency summaries is a challenging task for abstractive summarization. Recent studies extract the additional information with off-the-shelf tools from the source document as a clue to guide the summary generation, which shows effectiveness to improve the faithfulness. Unlike these work, we present a novel framework based on conditional variational autoencoders, which induces the guidance information and generates the summary equipment with the guidance synchronously. Experiments on XSUM and CNNDM dataset show that our approach can generate relevant and fluent summaries which is more faithful than the existing state-of-the-art approaches, according to multiple factual consistency metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.181.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.181.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.181/>Context-Aware Language Modeling for Goal-Oriented Dialogue Systems</a></strong><br><a href=/people/c/charlie-snell/>Charlie Snell</a>
|
<a href=/people/s/sherry-yang/>Sherry Yang</a>
|
<a href=/people/j/justin-fu/>Justin Fu</a>
|
<a href=/people/y/yi-su/>Yi Su</a>
|
<a href=/people/s/sergey-levine/>Sergey Levine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--181><div class="card-body p-3 small">Goal-oriented dialogue systems face a trade-off between fluent language generation and task-specific control. While supervised learning with large language models is capable of producing realistic text, how to steer such responses towards completing a specific task without sacrificing language quality remains an open question. In this work, we formulate goal-oriented dialogue as a partially observed Markov decision process, interpreting the language model as a representation of both the dynamics and the policy. This view allows us to extend techniques from learning-based control, such as task relabeling, to derive a simple and effective method to finetune language models in a goal-aware way, leading to significantly improved task performance. We additionally introduce a number of training strategies that serve to better focus the model on the task at hand. We evaluate our method, Context-Aware Language Models (CALM), on a practical flight-booking task using AirDialogue. Empirically, CALM outperforms the state-of-the-art method by 7% in terms of task success, matching human-level task performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.182.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.182.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.182/>Am <span class=acl-fixed-case>I</span> Me or You? State-of-the-Art Dialogue Models Cannot Maintain an Identity</a></strong><br><a href=/people/k/kurt-shuster/>Kurt Shuster</a>
|
<a href=/people/j/jack-urbanek/>Jack Urbanek</a>
|
<a href=/people/a/arthur-szlam/>Arthur Szlam</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--182><div class="card-body p-3 small">State-of-the-art dialogue models still often stumble with regards to factual accuracy and self-contradiction. Anecdotally, they have been observed to fail to maintain character identity throughout discourse; and more specifically, may take on the role of their interlocutor. In this work we formalize and quantify this deficiency, and show experimentally through human evaluations that this is indeed a problem. In contrast, we show that discriminative models trained specifically to recognize who is speaking can perform well; and further, these can be used as automated metrics. Finally, we evaluate a wide variety of mitigation methods, including changes to model architecture, training protocol, and decoding strategy. Our best models reduce mistaken identity issues by nearly 65% according to human annotators, while simultaneously improving engagingness. Despite these results, we find that maintaining character identity still remains a challenging problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.183.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--183 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.183/>Unsupervised Domain Adaptation for Question Generation with <span class=acl-fixed-case>D</span>omain<span class=acl-fixed-case>D</span>ata Selection and Self-training</a></strong><br><a href=/people/p/peide-zhu/>Peide Zhu</a>
|
<a href=/people/c/claudia-hauff/>Claudia Hauff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--183><div class="card-body p-3 small">Question generation (QG) approaches based on large neural models require (i) large-scale and (ii) high-quality training data. These two requirements pose difficulties for specific application domains where training data is expensive and difficult to obtain. The trained QG models’ effectiveness can degrade significantly when they are applied on a different domain due to domain shift. In this paper, we explore an <i>unsupervised domain adaptation</i> approach to combat the lack of training data and domain shift issue with domain data selection and self-training. We first present a novel answer-aware strategy for domain data selection to select data with the most similarity to a new domain. The selected data are then used as pseudo-in-domain data to retrain the QG model. We then present generation confidence guided self-training with two generation confidence modeling methods (i) generated questions’ perplexity and (ii) the fluency score. We test our approaches on three large public datasets with different domain similarities, using a transformer-based pre-trained QG model. The results show that our proposed approaches outperform the baselines, and show the viability of unsupervised domain adaptation with answer-aware data selection and self-training on the QG task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.184.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.184.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.184" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.184/><span class=acl-fixed-case>CCQA</span>: A New Web-Scale Question Answering Dataset for Model Pre-Training</a></strong><br><a href=/people/p/patrick-huber/>Patrick Huber</a>
|
<a href=/people/a/armen-aghajanyan/>Armen Aghajanyan</a>
|
<a href=/people/b/barlas-oguz/>Barlas Oguz</a>
|
<a href=/people/d/dmytro-okhonko/>Dmytro Okhonko</a>
|
<a href=/people/s/scott-yih/>Scott Yih</a>
|
<a href=/people/s/sonal-gupta/>Sonal Gupta</a>
|
<a href=/people/x/xilun-chen/>Xilun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--184><div class="card-body p-3 small">We propose a novel open-domain question-answering dataset based on the Common Crawl project. With a previously unseen number of around 130 million multilingual question-answer pairs (including about 60 million English data-points), we use our large-scale, natural, diverse and high-quality corpus to in-domain pre-train popular language models for the task of question-answering. In our experiments, we find that our Common Crawl Question Answering dataset (CCQA) achieves promising results in zero-shot, low resource and fine-tuned settings across multiple tasks, models and benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.185.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.185.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.185/>The Case for a Single Model that can Both Generate Continuations and Fill-in-the-Blank</a></strong><br><a href=/people/d/daphne-ippolito/>Daphne Ippolito</a>
|
<a href=/people/l/liam-dugan/>Liam Dugan</a>
|
<a href=/people/e/emily-reif/>Emily Reif</a>
|
<a href=/people/a/ann-yuan/>Ann Yuan</a>
|
<a href=/people/a/andy-coenen/>Andy Coenen</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--185><div class="card-body p-3 small">The task of inserting text into a specified position in a passage, known as fill in the blank (FitB), is useful for a variety of applications where writers interact with a natural language generation (NLG) system to craft text. While previous work has tackled this problem with models trained specifically to do fill in the blank, a more useful model is one that can effectively perform _both_ FitB and continuation tasks. In this work, we evaluate the feasibility of using a single model to do both tasks. We show that models pre-trained with a FitB-style objective are capable of both tasks, while models pre-trained for continuation are not. Finally, we show how these models can be easily finetuned to allow for fine-grained control over the length and word choice of the generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.186.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--186 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.186.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.186.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.186" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.186/>Learning Discriminative Representations for Open Relation Extraction with Instance Ranking and Label Calibration</a></strong><br><a href=/people/s/shusen-wang/>Shusen Wang</a>
|
<a href=/people/b/bin-duan/>Bin Duan</a>
|
<a href=/people/y/yanan-wu/>Yanan Wu</a>
|
<a href=/people/y/yajing-xu/>Yajing Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--186><div class="card-body p-3 small">Open relation extraction is the task to extract relational facts without pre-defined relation types from open-domain corpora. However, since there are some hard or semi-hard instances sharing similar context and entity information but belonging to different underlying relation, current OpenRE methods always cluster them into the same relation type. In this paper, we propose a novel method based on Instance Ranking and Label Calibration strategies (IRLC) to learn discriminative representations for open relation extraction. Due to lacking the original instance label, we provide three surrogate strategies to generate the positive, hard negative, and semi-hard negative instances for the original instance. Instance ranking aims to refine the relational feature space by pushing the hard and semi-hard negative instances apart from the original instance with different margins and pulling the original instance and its positive instance together. To refine the cluster probability distributions of these instances, we introduce a label calibration strategy to model the constraint relationship between instances. Experimental results on two public datasets demonstrate that our proposed method can significantly outperform the previous state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.187.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--187 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.187.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.187" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.187/>Textual Entailment for Event Argument Extraction: Zero- and Few-Shot with Multi-Source Learning</a></strong><br><a href=/people/o/oscar-sainz/>Oscar Sainz</a>
|
<a href=/people/i/itziar-gonzalez-dios/>Itziar Gonzalez-Dios</a>
|
<a href=/people/o/oier-lopez-de-lacalle/>Oier Lopez de Lacalle</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/e/eneko-agirre/>Eneko Agirre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--187><div class="card-body p-3 small">Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as a Textual Entailment tasks using verbalizations, with strong performance in zero-shot and few-shot settings thanks to pre-trained entailment models. The fact that relations in current RE datasets are easily verbalized casts doubts on whether entailment would be effective in more complex tasks. In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE and WikiEvents, respectively, while achieving the same performance as with full training. More importantly, we show that recasting EAE as entailment alleviates the dependency on schemas, which has been a roadblock for transferring annotations between domains. Thanks to entailment, the multi-source transfer between ACE and WikiEvents further reduces annotation down to 10% and 5% (respectively) of the full training without transfer. Our analysis shows that key to good results is the use of several entailment datasets to pre-train the entailment model. Similar to previous approaches, our method requires a small amount of effort for manual verbalization: only less than 15 minutes per event argument types is needed; comparable results can be achieved from users of different level of expertise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.188.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--188 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.188.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.188" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.188/><span class=acl-fixed-case>RCL</span>: Relation Contrastive Learning for Zero-Shot Relation Extraction</a></strong><br><a href=/people/s/shusen-wang/>Shusen Wang</a>
|
<a href=/people/b/bosen-zhang/>Bosen Zhang</a>
|
<a href=/people/y/yajing-xu/>Yajing Xu</a>
|
<a href=/people/y/yanan-wu/>Yanan Wu</a>
|
<a href=/people/b/bo-xiao/>Bo Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--188><div class="card-body p-3 small">Zero-shot relation extraction aims to identify novel relations which cannot be observed at the training stage. However, it still faces some challenges since the unseen relations of instances are similar or the input sentences have similar entities, the unseen relation representations from different categories tend to overlap and lead to errors. In this paper, we propose a novel Relation Contrastive Learning framework (RCL) to mitigate above two types of similar problems: Similar Relations and Similar Entities. By jointly optimizing a contrastive instance loss with a relation classification loss on seen relations, RCL can learn subtle difference between instances and achieve better separation between different relation categories in the representation space simultaneously. Especially in contrastive instance learning, the dropout noise as data augmentation is adopted to amplify the semantic difference between similar instances without breaking relation representation, so as to promote model to learn more effective representations. Experiments conducted on two well-known datasets show that RCL can significantly outperform previous state-of-the-art methods. Moreover, if the seen relations are insufficient, RCL can also obtain comparable results with the model trained on the full training set, showing the robustness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.189.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.189.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.189/>Latent Group Dropout for Multilingual and Multidomain Machine Translation</a></strong><br><a href=/people/m/minh-quang-pham/>Minh-Quang Pham</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--189><div class="card-body p-3 small">Multidomain and multilingual machine translation often rely on parameter sharing strategies, where large portions of the network are meant to capture the commonalities of the tasks at hand, while smaller parts are reserved to model the peculiarities of a language or a domain. In adapter-based approaches, these strategies are hardcoded in the network architecture, independent of the similarities between tasks. In this work, we propose a new method to better take advantage of these similarities, using a latent-variable model. We also develop new techniques to train this model end-to-end and report experimental results showing that the learned patterns are both meaningful and yield improved translation performance without any increase of the model size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.190.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--190 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.190" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.190/><span class=acl-fixed-case>ATP</span>: <span class=acl-fixed-case>AMR</span>ize Then Parse! Enhancing <span class=acl-fixed-case>AMR</span> Parsing with <span class=acl-fixed-case>P</span>seudo<span class=acl-fixed-case>AMR</span>s</a></strong><br><a href=/people/l/liang-chen/>Liang Chen</a>
|
<a href=/people/p/peiyi-wang/>Peiyi Wang</a>
|
<a href=/people/r/runxin-xu/>Runxin Xu</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--190><div class="card-body p-3 small">As Abstract Meaning Representation (AMR) implicitly involves compound semantic annotations, we hypothesize auxiliary tasks which are semantically or formally related can better enhance AMR parsing. We find that 1) Semantic role labeling (SRL) and dependency parsing (DP), would bring more performance gain than other tasks e.g. MT and summarization in the text-to-AMR transition even with much less data. 2) To make a better fit for AMR, data from auxiliary tasks should be properly “AMRized” to PseudoAMR before training. Knowledge from shallow level parsing tasks can be better transferred to AMR Parsing with structure transform. 3) Intermediate-task learning is a better paradigm to introduce auxiliary tasks to AMR parsing, compared to multitask learning. From an empirical perspective, we propose a principled method to involve auxiliary tasks to boost AMR parsing. Extensive experiments show that our method achieves new state-of-the-art performance on different benchmarks especially in topology-related scores. Code and models are released at <a href=https://github.com/PKUnlp-icler/ATP class=acl-markup-url>https://github.com/PKUnlp-icler/ATP</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.191.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.191.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.191/><span class=acl-fixed-case>T</span>a<span class=acl-fixed-case>CL</span>: Improving <span class=acl-fixed-case>BERT</span> Pre-training with Token-aware Contrastive Learning</a></strong><br><a href=/people/y/yixuan-su/>Yixuan Su</a>
|
<a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/z/zaiqiao-meng/>Zaiqiao Meng</a>
|
<a href=/people/t/tian-lan/>Tian Lan</a>
|
<a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/e/ehsan-shareghi/>Ehsan Shareghi</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--191><div class="card-body p-3 small">Masked language models (MLMs) such as BERT have revolutionized the field of Natural Language Understanding in the past few years. However, existing pre-trained MLMs often output an anisotropic distribution of token representations that occupies a narrow subset of the entire representation space. Such token representations are not ideal, especially for tasks that demand discriminative semantic meanings of distinct tokens. In this work, we propose TaCL (Token-aware Contrastive Learning), a novel continual pre-training approach that encourages BERT to learn an isotropic and discriminative distribution of token representations. TaCL is fully unsupervised and requires no additional data. We extensively test our approach on a wide range of English and Chinese benchmarks. The results show that TaCL brings consistent and notable improvements over the original BERT model. Furthermore, we conduct detailed analysis to reveal the merits and inner-workings of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.192.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--192 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.192.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.192" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.192/><span class=acl-fixed-case>MTG</span>: A Benchmark Suite for Multilingual Text Generation</a></strong><br><a href=/people/y/yiran-chen/>Yiran Chen</a>
|
<a href=/people/z/zhenqiao-song/>Zhenqiao Song</a>
|
<a href=/people/x/xianze-wu/>Xianze Wu</a>
|
<a href=/people/d/danqing-wang/>Danqing Wang</a>
|
<a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/j/jiaze-chen/>Jiaze Chen</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--192><div class="card-body p-3 small">We introduce MTG, a new benchmark suite for training and evaluating multilingual text generation. It is the first-proposed multilingual multiway text generation dataset with the largest human-annotated data (400k). It includes four generation tasks (story generation, question generation, title generation and text summarization) across five languages (English, German, French, Spanish and Chinese). The multiway setup enables testing knowledge transfer capabilities for a model across languages and tasks. Using MTG, we train and analyze several popular multilingual generation models from different aspects. Our benchmark suite fosters model performance enhancement with more human-annotated parallel data. It provides comprehensive evaluations with diverse generation scenarios. Code and data are available at <a href=https://github.com/zide05/MTG class=acl-markup-url>https://github.com/zide05/MTG</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.193.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--193 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.193.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.193" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.193/>Weakly Supervised Text-to-<span class=acl-fixed-case>SQL</span> Parsing through Question Decomposition</a></strong><br><a href=/people/t/tomer-wolfson/>Tomer Wolfson</a>
|
<a href=/people/d/daniel-deutch/>Daniel Deutch</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--193><div class="card-body p-3 small">Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query relational data. Training such parsers, by contrast, generally requires expertise in annotating natural language (NL) utterances with corresponding SQL queries. In this work, we propose a weak supervision approach for training text-to-SQL parsers. We take advantage of the recently proposed question meaning representation called QDMR, an intermediate between NL and formal query languages. Given questions, their QDMR structures (annotated by non-experts or automatically predicted), and the answers, we are able to automatically synthesize SQL queries that are used to train text-to-SQL models. We test our approach by experimenting on five benchmark datasets. Our results show that the weakly supervised models perform competitively with those trained on annotated NL-SQL data. Overall, we effectively train text-to-SQL parsers, while using zero SQL annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.194.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--194 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.194.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.194" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.194/>Detect Rumors in Microblog Posts for Low-Resource Domains via Adversarial Contrastive Learning</a></strong><br><a href=/people/h/hongzhan-lin/>Hongzhan Lin</a>
|
<a href=/people/j/jing-ma/>Jing Ma</a>
|
<a href=/people/l/liangliang-chen/>Liangliang Chen</a>
|
<a href=/people/z/zhiwei-yang/>Zhiwei Yang</a>
|
<a href=/people/m/mingfei-cheng/>Mingfei Cheng</a>
|
<a href=/people/c/chen-guang/>Chen Guang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--194><div class="card-body p-3 small">Massive false rumors emerging along with breaking news or trending topics severely hinder the truth. Existing rumor detection approaches achieve promising performance on the yesterday’s news, since there is enough corpus collected from the same domain for model training. However, they are poor at detecting rumors about unforeseen events especially those propagated in minority languages due to the lack of training data and prior knowledge (i.e., low-resource regimes). In this paper, we propose an adversarial contrastive learning framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. Our model explicitly overcomes the restriction of domain and/or language usage via language alignment and a novel supervised contrastive training paradigm. Moreover, we develop an adversarial augmentation mechanism to further enhance the robustness of low-resource rumor representation. Extensive experiments conducted on two low-resource datasets collected from real-world microblog platforms demonstrate that our framework achieves much better performance than state-of-the-art methods and exhibits a superior capacity for detecting rumors at early stages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.195.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.195.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.195" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.195/><span class=acl-fixed-case>D</span>ialo<span class=acl-fixed-case>KG</span>: Knowledge-Structure Aware Task-Oriented Dialogue Generation</a></strong><br><a href=/people/m/md-rashad-al-hasan-rony/>Md Rashad Al Hasan Rony</a>
|
<a href=/people/r/ricardo-usbeck/>Ricardo Usbeck</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--195><div class="card-body p-3 small">Task-oriented dialogue generation is challenging since the underlying knowledge is often dynamic and effectively incorporating knowledge into the learning process is hard. It is particularly challenging to generate both human-like and informative responses in this setting. Recent research primarily focused on various knowledge distillation methods where the underlying relationship between the facts in a knowledge base is not effectively captured. In this paper, we go one step further and demonstrate how the structural information of a knowledge graph can improve the system’s inference capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue system that effectively incorporates knowledge into a language model. Our proposed system views relational knowledge as a knowledge graph and introduces (1) a structure-aware knowledge embedding technique, and (2) a knowledge graph-weighted attention masking strategy to facilitate the system selecting relevant information during the dialogue generation. An empirical evaluation demonstrates the effectiveness of DialoKG over state-of-the-art methods on several standard benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.196.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--196 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.196.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.196" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.196/>Zero-Shot Event Detection Based on Ordered Contrastive Learning and Prompt-Based Prediction</a></strong><br><a href=/people/s/senhui-zhang/>Senhui Zhang</a>
|
<a href=/people/t/tao-ji/>Tao Ji</a>
|
<a href=/people/w/wendi-ji/>Wendi Ji</a>
|
<a href=/people/x/xiaoling-wang/>Xiaoling Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--196><div class="card-body p-3 small">Event detection is a classic natural language processing task. However, the constantly emerging new events make supervised methods not applicable to unseen types. Previous zero-shot event detection methods either require predefined event types as heuristic rules or resort to external semantic analyzing tools. To overcome this weakness, we propose an end-to-end framework named Zero-Shot Event Detection Based on Ordered Contrastive Learning and Prompt-Based Prediction (ZEOP). By creatively introducing multiple contrastive samples with ordered similarities, the encoder can learn event representations from both instance-level and class-level, which makes the distinctions between different unseen types more significant. Meanwhile, we utilize the prompt-based prediction to identify trigger words without relying on external resources. Experiments demonstrate that our model detects events more effectively and accurately than state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.197.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.197.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.197/><span class=acl-fixed-case>KETOD</span>: Knowledge-Enriched Task-Oriented Dialogue</a></strong><br><a href=/people/z/zhiyu-chen/>Zhiyu Chen</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/c/chinnadhurai-sankar/>Chinnadhurai Sankar</a>
|
<a href=/people/p/paul-a-crook/>Paul Crook</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--197><div class="card-body p-3 small">Existing studies in dialogue system research mostly treat task-oriented dialogue and chit-chat as separate domains. Towards building a human-like assistant that can converse naturally and seamlessly with users, it is important to build a dialogue system that conducts both types of conversations effectively. In this work, we investigate how task-oriented dialogue and knowledge-grounded chit-chat can be effectively integrated into a single model. To this end, we create a new dataset, KETOD (Knowledge-Enriched Task-Oriented Dialogue), where we naturally enrich task-oriented dialogues with chit-chat based on relevant entity knowledge. We also propose two new models, SimpleToDPlus and Combiner, for the proposed task. Experimental results on both automatic and human evaluations show that the proposed methods can significantly improve the performance in knowledge-enriched response generation while maintaining a competitive task-oriented dialog performance. We believe our new dataset will be a valuable resource for future studies. Our dataset and code are publicly available at <a href=https://github.com/facebookresearch/ketod class=acl-markup-url>https://github.com/facebookresearch/ketod</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.198.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.198.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.198/><span class=acl-fixed-case>TAN</span>et: <span class=acl-fixed-case>T</span>hread-<span class=acl-fixed-case>A</span>ware <span class=acl-fixed-case>P</span>retraining for <span class=acl-fixed-case>A</span>bstractive <span class=acl-fixed-case>C</span>onversational <span class=acl-fixed-case>S</span>ummarization</a></strong><br><a href=/people/z/ze-yang/>Ze Yang</a>
|
<a href=/people/c/christian-wang/>Christian Wang</a>
|
<a href=/people/z/zhoujin-tian/>Zhoujin Tian</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--198><div class="card-body p-3 small">Although pre-trained language models (PLMs) have achieved great success and become a milestone in NLP, abstractive conversational summarization remains a challenging but less studied task. The difficulty lies in two aspects. One is the lack of large-scale conversational summary data. Another is that applying the existing pre-trained models to this task is tricky because of the structural dependence within the conversation and its informal expression, etc. In this work, we first build a large-scale (11M) pretraining dataset called RCSum, based on the multi-person discussions in the Reddit community. We then present TANet, a thread-aware Transformer-based network. Unlike the existing pre-trained models that treat a conversation as a sequence of sentences, we argue that the inherent contextual dependency among the utterances plays an essential role in understanding the entire conversation and thus propose two new techniques to incorporate the structural information into our model. The first is thread-aware attention which is computed by taking into account the contextual dependency within utterances. Second, we apply thread prediction loss to predict the relations between utterances. We evaluate our model on four datasets of real conversations, covering types of meeting transcripts, customer-service records, and forum threads. Experimental results demonstrate that TANet achieves a new state-of-the-art in terms of both automatic evaluation and human judgment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.199.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.199.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.199" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.199/><span class=acl-fixed-case>A</span>dapter<span class=acl-fixed-case>B</span>ias: Parameter-efficient Token-dependent Representation Shift for Adapters in <span class=acl-fixed-case>NLP</span> Tasks</a></strong><br><a href=/people/c/chin-lun-fu/>Chin-Lun Fu</a>
|
<a href=/people/z/zih-ching-chen/>Zih-Ching Chen</a>
|
<a href=/people/y/yun-ru-lee/>Yun-Ru Lee</a>
|
<a href=/people/h/hung-yi-lee/>Hung-yi Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--199><div class="card-body p-3 small">Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias. The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pre-trained models. We further find that AdapterBias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.200.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--200 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.200.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.200" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.200/>Bridging the Gap between Training and Inference: Multi-Candidate Optimization for Diverse Neural Machine Translation</a></strong><br><a href=/people/h/huan-lin/>Huan Lin</a>
|
<a href=/people/b/baosong-yang/>Baosong Yang</a>
|
<a href=/people/l/liang-yao/>Liang Yao</a>
|
<a href=/people/d/dayiheng-liu/>Dayiheng Liu</a>
|
<a href=/people/h/haibo-zhang/>Haibo Zhang</a>
|
<a href=/people/j/jun-xie/>Jun Xie</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--200><div class="card-body p-3 small">Diverse NMT aims at generating multiple diverse yet faithful translations given a source sentence. In this paper, we investigate a common shortcoming in existing diverse NMT studies: the model is usually trained with single reference, while expected to generate multiple candidate translations in inference. The discrepancy between training and inference enlarges the confidence variance and quality gap among candidate translations and thus hinders model performance. To deal with this defect, we propose a multi-candidate optimization framework for diverse NMT. Specifically, we define assessments to score the diversity and the quality of candidate translations during training, and optimize the diverse NMT model with two strategies based on reinforcement learning, namely hard constrained training and soft constrained training. We conduct experiments on NIST Chinese-English and WMT14 English-German translation tasks. The results illustrate that our framework is transparent to basic diverse NMT models, and universally makes better trade-off between diversity and quality. Our source codeis available at <a href=https://github.com/DeepLearnXMU/MultiCanOptim class=acl-markup-url>https://github.com/DeepLearnXMU/MultiCanOptim</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.201.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--201 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.201.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.201" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.201/>Learning from Bootstrapping and Stepwise Reinforcement Reward: A Semi-Supervised Framework for Text Style Transfer</a></strong><br><a href=/people/z/zhengyuan-liu/>Zhengyuan Liu</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--201><div class="card-body p-3 small">Text style transfer is an important task in controllable language generation. Supervised approaches have pushed performance improvement on style-oriented rewriting such as formality conversion. However, challenges remain due to the scarcity of large-scale parallel data in many domains. While unsupervised approaches do not rely on annotated sentence pairs for each style, they are often plagued with instability issues such as mode collapse or quality degradation. To take advantage of both supervised and unsupervised paradigms and tackle the challenges, in this work, we propose a semi-supervised framework for text style transfer. First, the learning process is bootstrapped with supervision guided by automatically constructed pseudo-parallel pairs using lexical and semantic-based methods. Then the model learns from unlabeled data via reinforcement rewards. Specifically, we propose to improve the sequence-to-sequence policy gradient via stepwise reward optimization, providing fine-grained learning signals and stabilizing the reinforced learning process. Experimental results show that the proposed approach achieves state-of-the-art performance on multiple datasets, and produces effective generation with as minimal as 10% of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.202.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--202 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.202.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.202.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.202" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.202/><span class=acl-fixed-case>EA</span><span class=tex-math><sup>2</sup></span><span class=acl-fixed-case>E</span>: Improving Consistency with Event Awareness for Document-Level Argument Extraction</a></strong><br><a href=/people/q/qi-zeng/>Qi Zeng</a>
|
<a href=/people/q/qiusi-zhan/>Qiusi Zhan</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--202><div class="card-body p-3 small">Events are inter-related in documents. Motivated by the one-sense-per-discourse theory, we hypothesize that a participant tends to play consistent roles across multiple events in the same document. However recent work on document-level event argument extraction models each individual event in isolation and therefore causes inconsistency among extracted arguments across events, which will further cause discrepancy for downstream applications such as event knowledge base population, question answering, and hypothesis generation. In this work, we formulate event argument consistency as the constraints from event-event relations under the document-level setting. To improve consistency we introduce the Event-Aware Argument Extraction (EA<span class=tex-math><sup>2</sup></span>E) model with augmented context for training and inference. Experiment results on WIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA<span class=tex-math><sup>2</sup></span>E compared to baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.203.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.203.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.203.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.203/>Label Refinement via Contrastive Learning for Distantly-Supervised Named Entity Recognition</a></strong><br><a href=/people/h/huaiyuan-ying/>Huaiyuan Ying</a>
|
<a href=/people/s/shengxuan-luo/>Shengxuan Luo</a>
|
<a href=/people/t/tiantian-dang/>Tiantian Dang</a>
|
<a href=/people/s/sheng-yu/>Sheng Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--203><div class="card-body p-3 small">Distantly-supervised named entity recognition (NER) locates and classifies entities using only knowledge bases and unlabeled corpus to mitigate the reliance on human-annotated labels. The distantly annotated data suffer from the noise in labels, and previous works on DSNER have proved the importance of pre-refining distant labels with hand-crafted rules and extra existing semantic information. In this work, we explore the way to directly learn the distant label refinement knowledge by imitating annotations of different qualities and comparing these annotations in contrastive learning frameworks. the proposed distant label refinement model can give modified suggestions on distant data without additional supervised labels, and thus reduces the requirement on the quality of the knowledge bases. We perform extensive experiments and observe that recent and state-of-the-art DSNER methods gain evident benefits with our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.204.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.204.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.204" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.204/>Negative Sample is Negative in Its Own Way: Tailoring Negative Sentences for Image-Text Retrieval</a></strong><br><a href=/people/z/zhihao-fan/>Zhihao Fan</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/z/zejun-li/>Zejun Li</a>
|
<a href=/people/s/siyuan-wang/>Siyuan Wang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/j/jianqing-fan/>Jianqing Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--204><div class="card-body p-3 small">Matching model is essential for Image-Text Retrieval framework. Existing research usually train the model with a triplet loss and explore various strategy to retrieve hard negative sentences in the dataset. We argue that current retrieval-based negative sample construction approach is limited in the scale of the dataset thus fail to identify negative sample of high difficulty for every image. We propose our TAiloring neGative Sentences with Discrimination and Correction (TAGS-DC) to generate synthetic sentences automatically as negative samples. TAGS-DC is composed of masking and refilling to generate synthetic negative sentences with higher difficulty. To keep the difficulty during training, we mutually improve the retrieval and generation through parameter sharing. To further utilize fine-grained semantic of mismatch in the negative sentence, we propose two auxiliary tasks, namely word discrimination and word correction to improve the training. In experiments, we verify the effectiveness of our model on MS-COCO and Flickr30K compared with current state-of-the-art models and demonstrates its robustness and faithfulness in the further analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.205.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.205.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.205" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.205/>Explore More Guidance: A Task-aware Instruction Network for Sign Language Translation Enhanced with Data Augmentation</a></strong><br><a href=/people/y/yong-cao/>Yong Cao</a>
|
<a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/x/xianzhi-li/>Xianzhi Li</a>
|
<a href=/people/m/min-chen/>Min Chen</a>
|
<a href=/people/g/guangyong-chen/>Guangyong Chen</a>
|
<a href=/people/l/long-hu/>Long Hu</a>
|
<a href=/people/z/zhengdao-li/>Zhengdao Li</a>
|
<a href=/people/k/kai-hwang/>Kai Hwang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--205><div class="card-body p-3 small">Sign language recognition and translation first uses a recognition module to generate glosses from sign language videos and then employs a translation module to translate glosses into spoken sentences. Most existing works focus on the recognition step, while paying less attention to sign language translation. In this work, we propose a task-aware instruction network, namely TIN-SLT, for sign language translation, by introducing the isntruction module and the learning-based feature fuse strategy into a Transformer network. In this way, the pre-trained model’s language ability can be well explored and utilized to further boost the translation performance. Moreover, by exploring the representation space of sign language glosses and target spoken language, we propose a multi-level data augmentation scheme to adjust the data distribution of the training set. We conduct extensive experiments on two challenging benchmark datasets, PHOENIX-2014-T and ASLG-PC12, on which our method outperforms former best solutions by 1.65 and 1.42 in terms of BLEU-4. Our code and trained networks will be available upon the publication of this work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.206.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--206 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.206.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.206" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.206/><span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>V</span>i<span class=acl-fixed-case>ST</span>: Learning Robust Metrics for Visual Storytelling</a></strong><br><a href=/people/e/eileen-wang/>Eileen Wang</a>
|
<a href=/people/c/caren-han/>Caren Han</a>
|
<a href=/people/j/josiah-poon/>Josiah Poon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--206><div class="card-body p-3 small">Visual storytelling (VST) is the task of generating a story paragraph that describes a given image sequence. Most existing storytelling approaches have evaluated their models using traditional natural language generation metrics like BLEU or CIDEr. However, such metrics based on <span class=tex-math>n</span>-gram matching tend to have poor correlation with human evaluation scores and do not explicitly consider other criteria necessary for storytelling such as sentence structure or topic coherence. Moreover, a single score is not enough to assess a story as it does not inform us about what specific errors were made by the model. In this paper, we propose 3 evaluation metrics sets that analyses which aspects we would look for in a good story: 1) visual grounding, 2) coherence, and 3) non-redundancy. We measure the reliability of our metric sets by analysing its correlation with human judgement scores on a sample of machine stories obtained from 4 state-of-the-arts models trained on the Visual Storytelling Dataset (VIST). Our metric sets outperforms other metrics on human correlation, and could be served as a learning based evaluation metric set that is complementary to existing rule-based metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.207.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.207.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.207" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.207/><span class=acl-fixed-case>Q</span>uery2<span class=acl-fixed-case>P</span>articles: Knowledge Graph Reasoning with Particle Embeddings</a></strong><br><a href=/people/j/jiaxin-bai/>Jiaxin Bai</a>
|
<a href=/people/z/zihao-wang/>Zihao Wang</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--207><div class="card-body p-3 small">Answering complex logical queries on incomplete knowledge graphs (KGs) with missing edges is a fundamental and important task for knowledge graph reasoning. The query embedding method is proposed to answer these queries by jointly encoding queries and entities to the same embedding space. Then the answer entities are selected according to the similarities between the entity embeddings and the query embedding. As the answers to a complex query are obtained from a combination of logical operations over sub-queries, the embeddings of the answer entities may not always follow a uni-modal distribution in the embedding space. Thus, it is challenging to simultaneously retrieve a set of diverse answers from the embedding space using a single and concentrated query representation such as a vector or a hyper-rectangle. To better cope with queries with diversified answers, we propose Query2Particles (Q2P), a complex KG query answering method. Q2P encodes each query into multiple vectors, named particle embeddings. By doing so, the candidate answers can be retrieved from different areas over the embedding space using the maximal similarities between the entity embeddings and any of the particle embeddings. Meanwhile, the corresponding neural logic operations are defined to support its reasoning over arbitrary first-order logic queries. The experiments show that Query2Particles achieves state-of-the-art performance on the complex query answering tasks on FB15k, FB15K-237, and NELL knowledge graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.208.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.208.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.208/><span class=acl-fixed-case>ID</span>10<span class=acl-fixed-case>M</span>: Idiom Identification in 10 Languages</a></strong><br><a href=/people/s/simone-tedeschi/>Simone Tedeschi</a>
|
<a href=/people/f/federico-martelli/>Federico Martelli</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--208><div class="card-body p-3 small">Idioms are phrases which present a figurative meaning that cannot be (completely) derived by looking at the meaning of their individual components. Identifying and understanding idioms in context is a crucial goal and a key challenge in a wide range of Natural Language Understanding tasks. Although efforts have been undertaken in this direction, the automatic identification and understanding of idioms is still a largely under-investigated area, especially when operating in a multilingual scenario. In this paper, we address such limitations and put forward several new contributions: we propose a novel multilingual Transformer-based system for the identification of idioms; we produce a high-quality automatically-created training dataset in 10 languages, along with a novel manually-curated evaluation benchmark; finally, we carry out a thorough performance analysis and release our evaluation suite at <a href=https://github.com/Babelscape/ID10M class=acl-markup-url>https://github.com/Babelscape/ID10M</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.findings-naacl.209.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--findings-naacl--209 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.findings-naacl.209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.findings-naacl.209.software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.findings-naacl.209.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.findings-naacl.209" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.findings-naacl.209/>Cross-Domain Classification of Moral Values</a></strong><br><a href=/people/e/enrico-liscio/>Enrico Liscio</a>
|
<a href=/people/a/alin-dondera/>Alin Dondera</a>
|
<a href=/people/a/andrei-geadau/>Andrei Geadau</a>
|
<a href=/people/c/catholijn-jonker/>Catholijn Jonker</a>
|
<a href=/people/p/pradeep-murukannaiah/>Pradeep Murukannaiah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--findings-naacl--209><div class="card-body p-3 small">Moral values influence how we interpret and act upon the information we receive. Identifying human moral values is essential for artificially intelligent agents to co-exist with humans. Recent progress in natural language processing allows the identification of moral values in textual discourse. However, domain-specific moral rhetoric poses challenges for transferring knowledge from one domain to another. We provide the first extensive investigation on the effects of cross-domain classification of moral values from text. We compare a state-of-the-art deep learning model (BERT) in seven domains and four cross-domain settings. We show that a value classifier can generalize and transfer knowledge to novel domains, but it can introduce catastrophic forgetting. We also highlight the typical classification errors in cross-domain value classification and compare the model predictions to the annotators agreement. Our results provide insights to computer and social scientists that seek to identify moral rhetoric specific to a domain of discourse.</div></div></div><hr><div id=2022autosimtrans-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.autosimtrans-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.autosimtrans-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.autosimtrans-1/>Proceedings of the Third Workshop on Automatic Simultaneous Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.autosimtrans-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.autosimtrans-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.autosimtrans-1.0/>Proceedings of the Third Workshop on Automatic Simultaneous Translation</a></strong><br><a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/r/ruiqing-zhang/>Ruiqing Zhang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.autosimtrans-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.autosimtrans-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--autosimtrans-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.autosimtrans-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.autosimtrans-1.1/>Findings of the Third Workshop on Automatic Simultaneous Translation</a></strong><br><a href=/people/r/ruiqing-zhang/>Ruiqing Zhang</a>
|
<a href=/people/c/chuanqiang-zhang/>Chuanqiang Zhang</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/w/wolfgang-macherey/>Wolfgang Macherey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--autosimtrans-1--1><div class="card-body p-3 small">This paper reports the results of the shared task we hosted on the Third Workshop of Automatic Simultaneous Translation (AutoSimTrans). The shared task aims to promote the development of text-to-text and speech-to-text simultaneous translation, and includes Chinese-English and English-Spanish tracks. The number of systems submitted this year has increased fourfold compared with last year. Additionally, the top 1 ranked system in the speech-to-text track is the first end-to-end submission we have received in the past three years, which has shown great potential. This paper reports the results and descriptions of the 14 participating teams, compares different evaluation metrics, and revisits the ranking method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.autosimtrans-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.autosimtrans-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--autosimtrans-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.autosimtrans-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.autosimtrans-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.autosimtrans-1.2/>Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for Simultaneous Speech Translation</a></strong><br><a href=/people/s/sara-papi/>Sara Papi</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--autosimtrans-1--2><div class="card-body p-3 small">Simultaneous speech translation (SimulST) systems aim at generating their output with the lowest possible latency, which is normally computed in terms of Average Lagging (AL). In this paper we highlight that, despite its widespread adoption, AL provides underestimated scores for systems that generate longer predictions compared to the corresponding references. We also show that this problem has practical relevance, as recent SimulST systems have indeed a tendency to over-generate. As a solution, we propose LAAL (Length-Adaptive Average Lagging), a modified version of the metric that takes into account the over-generation phenomenon and allows for unbiased evaluation of both under-/over-generating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.autosimtrans-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.autosimtrans-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--autosimtrans-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.autosimtrans-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.autosimtrans-1.3/>System Description on Automatic Simultaneous Translation Workshop</a></strong><br><a href=/people/z/zecheng-li/>Zecheng Li</a>
|
<a href=/people/y/yue-sun/>Yue Sun</a>
|
<a href=/people/h/haoze-li/>Haoze Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--autosimtrans-1--3><div class="card-body p-3 small">This paper describes our system submitted on the third automatic simultaneous translation workshop at NAACL2022. We participate in the Chinese audio->English text direction of Chinese-to-English translation. Our speech-to-text system is a pipeline system, in which we resort to rhymological features for audio split, ASRT model for speech recoginition, STACL model for streaming text translation. To translate streaming text, we use wait-k policy trained to generate the target sentence concurrently with the source sentence, but always k words behind. We propose a competitive simultaneous translation system and rank 3rd in the audio input track. The code will release soon.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.autosimtrans-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.autosimtrans-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--autosimtrans-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.autosimtrans-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.autosimtrans-1.4/>System Description on Third Automatic Simultaneous Translation Workshop</a></strong><br><a href=/people/z/zhang-yiqiao/>Zhang Yiqiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--autosimtrans-1--4><div class="card-body p-3 small">This paper shows my submission to the Third Automatic Simultaneous Translation Workshop at NAACL2022.The submission includes Chinese audio to English text task, Chinese text to English text tast, and English text to Spanish text task. For the two text-to-text tasks, I use the STACL model of PaddleNLP. As for the audio-to-text task, I first use DeepSpeech2 to translate the audio into text, then apply the STACL model to handle the text-to-text task. The submission results show that the used method can get low delay with a few training samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.autosimtrans-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.autosimtrans-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--autosimtrans-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.autosimtrans-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.autosimtrans-1.5/>End-to-End Simultaneous Speech Translation with Pretraining and Distillation: Huawei <span class=acl-fixed-case>N</span>oah’s System for <span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>S</span>im<span class=acl-fixed-case>T</span>ran<span class=acl-fixed-case>S</span> 2022</a></strong><br><a href=/people/x/xingshan-zeng/>Xingshan Zeng</a>
|
<a href=/people/p/pengfei-li/>Pengfei Li</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--autosimtrans-1--5><div class="card-body p-3 small">This paper describes the system submitted to AutoSimTrans 2022 from Huawei Noah’s Ark Lab, which won the first place in the audio input track of the Chinese-English translation task. Our system is based on RealTranS, an end-to-end simultaneous speech translation model. We enhance the model with pretraining, by initializing the acoustic encoder with ASR encoder, and the semantic encoder and decoder with NMT encoder and decoder, respectively. To relieve the data scarcity, we further construct pseudo training corpus as a kind of knowledge distillation with ASR data and the pretrained NMT model. Meanwhile, we also apply several techniques to improve the robustness and domain generalizability, including punctuation removal, token-level knowledge distillation and multi-domain finetuning. Experiments show that our system significantly outperforms the baselines at all latency and also verify the effectiveness of our proposed methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.autosimtrans-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.autosimtrans-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--autosimtrans-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.autosimtrans-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.autosimtrans-1.6/><span class=acl-fixed-case>BIT</span>-Xiaomi’s System for <span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>S</span>im<span class=acl-fixed-case>T</span>rans 2022</a></strong><br><a href=/people/m/mengge-liu/>Mengge Liu</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/b/bao-chen/>Bao Chen</a>
|
<a href=/people/y/yanzhi-tian/>Yanzhi Tian</a>
|
<a href=/people/t/tianwei-lan/>Tianwei Lan</a>
|
<a href=/people/s/silin-li/>Silin Li</a>
|
<a href=/people/y/yuhang-guo/>Yuhang Guo</a>
|
<a href=/people/j/jian-luan/>Jian Luan</a>
|
<a href=/people/b/bin-wang/>Bin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--autosimtrans-1--6><div class="card-body p-3 small">This system paper describes the BIT-Xiaomi simultaneous translation system for Autosimtrans 2022 simultaneous translation challenge. We participated in three tracks: the Zh-En text-to-text track, the Zh-En audio-to-text track and the En-Es test-to-text track. In our system, wait-k is employed to train prefix-to-prefix translation models. We integrate streaming chunking to detect boundaries as the source streaming read in. We further improve our system with data selection, data-augmentation and R-drop training methods. Results show that our wait-k implementation outperforms organizer’s baseline by 8 BLEU score at most, and our proposed streaming chunking method further improves about 2 BLEU in low latency regime.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.autosimtrans-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.autosimtrans-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--autosimtrans-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.autosimtrans-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.autosimtrans-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.autosimtrans-1.7/><span class=acl-fixed-case>USST</span>’s System for <span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>S</span>im<span class=acl-fixed-case>T</span>rans 2022</a></strong><br><a href=/people/z/zhu-hui/>Zhu Hui</a>
|
<a href=/people/y/yu-jun/>Yu Jun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--autosimtrans-1--7><div class="card-body p-3 small">This paper describes our submitted text-to-text Simultaneous translation (ST) system, which won the second place in the Chinese→English streaming translation task of AutoSimTrans 2022. Our baseline system is a BPE-based Transformer model trained with the PaddlePaddle framework. In our experiments, we employ data synthesis and ensemble approaches to enhance the base model. In order to bridge the gap between general domain and spoken domain, we select in-domain data from general corpus and mixed then with spoken corpus for mixed fine tuning. Finally, we adopt fixed wait-k policy to transfer our full-sentence translation model to simultaneous translation model. Experiments on the development data show that our system outperforms than the baseline system.</div></div></div><hr><div id=2022bea-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.bea-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.bea-1/>Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.0/>Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022)</a></strong><br><a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/j/jill-burstein/>Jill Burstein</a>
|
<a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/r/ronja-laarmann-quante/>Ronja Laarmann-Quante</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/a/anais-tack/>Anaïs Tack</a>
|
<a href=/people/v/victoria-yaneva/>Victoria Yaneva</a>
|
<a href=/people/z/zheng-yuan/>Zheng Yuan</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.1/>Using Item Response Theory to Measure Gender and Racial Bias of a <span class=acl-fixed-case>BERT</span>-based Automated <span class=acl-fixed-case>E</span>nglish Speech Assessment System</a></strong><br><a href=/people/a/alexander-kwako/>Alexander Kwako</a>
|
<a href=/people/y/yixin-wan/>Yixin Wan</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/l/li-cai/>Li Cai</a>
|
<a href=/people/m/mark-hansen/>Mark Hansen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--1><div class="card-body p-3 small">Recent advances in natural language processing and transformer-based models have made it easier to implement accurate, automated English speech assessments. Yet, without careful examination, applications of these models may exacerbate social prejudices based on gender and race. This study addresses the need to examine potential biases of transformer-based models in the context of automated English speech assessment. For this purpose, we developed a BERT-based automated speech assessment system and investigated gender and racial bias of examinees’ automated scores. Gender and racial bias was measured by examining differential item functioning (DIF) using an item response theory framework. Preliminary results, which focused on a single verbal-response item, showed no statistically significant DIF based on gender or race for automated scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.2/>Automatic scoring of short answers using justification cues estimated by <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/s/shunya-takano/>Shunya Takano</a>
|
<a href=/people/o/osamu-ichikawa/>Osamu Ichikawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--2><div class="card-body p-3 small">Automated scoring technology for short-answer questions has been attracting attention to improve the fairness of scoring and reduce the burden on the scorer. In general, a large amount of data is required to train an automated scoring model. The training data consists of the answer texts and the scoring data assigned to them. It may also include annotations indicating key word sequences. These data must be prepared manually, which is costly. Many previous studies have created models with large amounts of training data specific to each question. This paper aims to achieve equivalent performance with less training data by utilizing a BERT model that has been pre-trained on a large amount of general text data not necessarily related to short answer questions. On the RIKEN dataset, the proposed method reduces the training data from the 800 data required in the past to about 400 data, and still achieves scoring accuracy comparable to that of humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.3/>Mitigating Learnerese Effects for <span class=acl-fixed-case>CEFR</span> Classification</a></strong><br><a href=/people/r/rricha-jalota/>Rricha Jalota</a>
|
<a href=/people/p/peter-bourgonje/>Peter Bourgonje</a>
|
<a href=/people/j/jan-van-sas/>Jan Van Sas</a>
|
<a href=/people/h/huiyan-huang/>Huiyan Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--3><div class="card-body p-3 small">The role of an author’s L1 in SLA can be challenging for automated CEFR classification, in that texts from different L1 groups may be too heterogeneous to combine them as training data. We experiment with recent debiasing approaches by attempting to devoid textual representations of L1 features. This results in a more homogeneous group when aggregating CEFR-annotated texts from different L1 groups, leading to better classification performance. Using iterative null-space projection, we marginally improve classification performance for a linear classifier by 1 point. An MLP (e.g. non-linear) classifier remains unaffected by this procedure. We discuss possible directions of future work to attempt to increase this performance gain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.4/>Automatically Detecting Reduced-formed <span class=acl-fixed-case>E</span>nglish Pronunciations by Using Deep Learning</a></strong><br><a href=/people/l/lei-chen/>Lei Chen</a>
|
<a href=/people/c/chenglin-jiang/>Chenglin Jiang</a>
|
<a href=/people/y/yiwei-gu/>Yiwei Gu</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/j/jiahong-yuan/>Jiahong Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--4><div class="card-body p-3 small">Reduced form pronunciations are widely used by native English speakers, especially in casual conversations. Second language (L2) learners have difficulty in processing reduced form pronunciations in listening comprehension and face challenges in production too. Meanwhile, training applications dedicated to reduced forms are still few. To solve this issue, we report on our first effort of using deep learning to evaluate L2 learners’ reduced form pronunciations. Compared with a baseline solution that uses an ASR to determine regular or reduced-formed pronunciations, a classifier that learns representative features via a convolution neural network (CNN) on low-level acoustic features, yields higher detection performance. F-1 metric has been increased from 0.690 to 0.757 on the reduction task. Furthermore, adding word entities to compute attention weights to better adjust the features learned by the CNN model helps increasing F-1 to 0.763.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.bea-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.5/>A Baseline Readability Model for <span class=acl-fixed-case>C</span>ebuano</a></strong><br><a href=/people/j/joseph-marvin-imperial/>Joseph Marvin Imperial</a>
|
<a href=/people/l/lloyd-lois-antonie-reyes/>Lloyd Lois Antonie Reyes</a>
|
<a href=/people/m/michael-antonio-ibanez/>Michael Antonio Ibanez</a>
|
<a href=/people/r/ranz-sapinit/>Ranz Sapinit</a>
|
<a href=/people/m/mohammed-hussien/>Mohammed Hussien</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--5><div class="card-body p-3 small">In this study, we developed the first baseline readability model for the Cebuano language. Cebuano is the second most-used native language in the Philippines with about 27.5 million speakers. As the baseline, we extracted traditional or surface-based features, syllable patterns based from Cebuano’s documented orthography, and neural embeddings from the multilingual BERT model. Results show that the use of the first two handcrafted linguistic features obtained the best performance trained on an optimized Random Forest model with approximately 87% across all metrics. The feature sets and algorithm used also is similar to previous results in readability assessment for the Filipino language—showing potential of crosslingual application. To encourage more work for readability assessment in Philippine languages such as Cebuano, we open-sourced both code and data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.6/>Generation of Synthetic Error Data of Verb Order Errors for <span class=acl-fixed-case>S</span>wedish</a></strong><br><a href=/people/j/judit-casademont-moner/>Judit Casademont Moner</a>
|
<a href=/people/e/elena-volodina/>Elena Volodina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--6><div class="card-body p-3 small">We report on our work-in-progress to generate a synthetic error dataset for Swedish by replicating errors observed in the authentic error annotated dataset. We analyze a small subset of authentic errors, capture regular patterns based on parts of speech, and design a set of rules to corrupt new data. We explore the approach and identify its capabilities, advantages and limitations as a way to enrich the existing collection of error-annotated data. This work focuses on word order errors, specifically those involving the placement of finite verbs in a sentence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.7/>A Dependency Treebank of Spoken Second Language <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/k/kristopher-kyle/>Kristopher Kyle</a>
|
<a href=/people/m/masaki-eguchi/>Masaki Eguchi</a>
|
<a href=/people/a/aaron-miller/>Aaron Miller</a>
|
<a href=/people/t/theodore-sither/>Theodore Sither</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--7><div class="card-body p-3 small">In this paper, we introduce a dependency treebank of spoken second language (L2) English that is annotated with part of speech (Penn POS) tags and syntactic dependencies (Universal Dependencies). We then evaluate the degree to which the use of this treebank as training data affects POS and UD annotation accuracy for L1 web texts, L2 written texts, and L2 spoken texts as compared to models trained on L1 texts only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.8/>Starting from “Zero”: An Incremental Zero-shot Learning Approach for Assessing Peer Feedback Comments</a></strong><br><a href=/people/q/qinjin-jia/>Qinjin Jia</a>
|
<a href=/people/y/yupeng-cao/>Yupeng Cao</a>
|
<a href=/people/e/edward-gehringer/>Edward Gehringer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--8><div class="card-body p-3 small">Peer assessment is an effective and efficient pedagogical strategy for delivering feedback to learners. Asking students to provide quality feedback, which contains suggestions and mentions problems, can promote metacognition by reviewers and better assist reviewees in revising their work. Thus, various supervised machine learning algorithms have been proposed to detect quality feedback. However, all these powerful algorithms have the same Achilles’ heel: the reliance on sufficient historical data. In other words, collecting adequate peer feedback for training a supervised algorithm can take several semesters before the model can be deployed to a new class. In this paper, we present a new paradigm, called incremental zero-shot learning (IZSL), to tackle the problem of lacking sufficient historical data. Our results show that the method can achieve acceptable “cold-start” performance without needing any domain data, and it outperforms BERT when trained on the same data collected incrementally.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.9/>On Assessing and Developing Spoken ’Grammatical Error Correction’ Systems</a></strong><br><a href=/people/y/yiting-lu/>Yiting Lu</a>
|
<a href=/people/s/stefano-banno/>Stefano Bannò</a>
|
<a href=/people/m/mark-gales/>Mark Gales</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--9><div class="card-body p-3 small">Spoken ‘grammatical error correction’ (SGEC) is an important process to provide feedback for second language learning. Due to a lack of end-to-end training data, SGEC is often implemented as a cascaded, modular system, consisting of speech recognition, disfluency removal, and grammatical error correction (GEC). This cascaded structure enables efficient use of training data for each module. It is, however, difficult to compare and evaluate the performance of individual modules as preceeding modules may introduce errors. For example the GEC module input depends on the output of non-native speech recognition and disfluency detection, both challenging tasks for learner data. This paper focuses on the assessment and development of SGEC systems. We first discuss metrics for evaluating SGEC, both individual modules and the overall system. The system-level metrics enable tuning for optimal system performance. A known issue in cascaded systems is error propagation between modules. To mitigate this problem semi-supervised approaches and self-distillation are investigated. Lastly, when SGEC system gets deployed it is important to give accurate feedback to users. Thus, we apply filtering to remove edits with low-confidence, aiming to improve overall feedback precision. The performance metrics are examined on a Linguaskill multi-level data set, which includes the original non-native speech, manual transcriptions and reference grammatical error corrections, to enable system analysis and development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.10/>Automatic True/False Question Generation for Educational Purpose</a></strong><br><a href=/people/b/bowei-zou/>Bowei Zou</a>
|
<a href=/people/p/pengfei-li/>Pengfei Li</a>
|
<a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/a/aiti-aw/>Ai Ti Aw</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--10><div class="card-body p-3 small">In field of teaching, true/false questioning is an important educational method for assessing students’ general understanding of learning materials. Manually creating such questions requires extensive human effort and expert knowledge. Question Generation (QG) technique offers the possibility to automatically generate a large number of questions. However, there is limited work on automatic true/false question generation due to the lack of training data and difficulty finding question-worthy content. In this paper, we propose an unsupervised True/False Question Generation approach (TF-QG) that automatically generates true/false questions from a given passage for reading comprehension test. TF-QG consists of a template-based framework that aims to test the specific knowledge in the passage by leveraging various NLP techniques, and a generative framework to generate more flexible and complicated questions by using a novel masking-and-infilling strategy. Human evaluation shows that our approach can generate high-quality and valuable true/false questions. In addition, simulated testing on the generated questions challenges the state-of-the-art inference models from NLI, QA, and fact verification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.11/>Fine-tuning Transformers with Additional Context to Classify Discursive Moves in Mathematics Classrooms</a></strong><br><a href=/people/a/abhijit-suresh/>Abhijit Suresh</a>
|
<a href=/people/j/jennifer-jacobs/>Jennifer Jacobs</a>
|
<a href=/people/m/margaret-perkoff/>Margaret Perkoff</a>
|
<a href=/people/j/james-h-martin/>James H. Martin</a>
|
<a href=/people/t/tamara-sumner/>Tamara Sumner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--11><div class="card-body p-3 small">“Talk moves” are specific discursive strategies used by teachers and students to facilitate conversations in which students share their thinking, and actively consider the ideas of others, and engage in rich discussions. Experts in instructional practices often rely on cues to identify and document these strategies, for example by annotating classroom transcripts. Prior efforts to develop automated systems to classify teacher talk moves using transformers achieved a performance of 76.32% F1. In this paper, we investigate the feasibility of using enriched contextual cues to improve model performance. We applied state-of-the-art deep learning approaches for Natural Language Processing (NLP), including Robustly optimized bidirectional encoder representations from transformers (Roberta) with a special input representation that supports previous and subsequent utterances as context for talk moves classification. We worked with the publically available TalkMoves dataset, which contains utterances sourced from real-world classroom sessions (human- transcribed and annotated). Through a series of experimentations, we found that a combination of previous and subsequent utterances improved the transformers’ ability to differentiate talk moves (by 2.6% F1). These results constitute a new state of the art over previously published results and provide actionable insights to those in the broader NLP community who are working to develop similar transformer-based classification models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.12/>Cross-corpora experiments of automatic proficiency assessment and error detection for spoken <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/s/stefano-banno/>Stefano Bannò</a>
|
<a href=/people/m/marco-matassoni/>Marco Matassoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--12><div class="card-body p-3 small">The growing demand for learning English as a second language has led to an increasing interest in automatic approaches for assessing spoken language proficiency. One of the most significant challenges in this field is the lack of publicly available annotated spoken data. Another common issue is the lack of consistency and coherence in human assessment. To tackle both problems, in this paper we address the task of automatically predicting the scores of spoken test responses of English-as-a-second-language learners by training neural models on written data and using the presence of grammatical errors as a feature, as they can be considered consistent indicators of proficiency through their distribution and frequency. Specifically, we train a feature extractor on EFCAMDAT, a large written corpus containing error annotations and proficiency levels assigned by human experts, in order to extract information related to grammatical errors and, in turn, we use the resulting model for inference on the CLC-FCE corpus, on the ICNALE corpus, and on the spoken section of the TLT-school corpus, a collection of proficiency tests taken by Italian students. The work investigates the impact of the feature extractor on spoken proficiency assessment as well as the written-to-spoken approach. We find that our error-based approach can be beneficial for assessing spoken proficiency. The results obtained on the considered datasets are discussed and evaluated with appropriate metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.13/>Activity focused Speech Recognition of Preschool Children in Early Childhood Classrooms</a></strong><br><a href=/people/s/satwik-dutta/>Satwik Dutta</a>
|
<a href=/people/d/dwight-irvin/>Dwight Irvin</a>
|
<a href=/people/j/jay-buzhardt/>Jay Buzhardt</a>
|
<a href=/people/j/john-h-l-hansen/>John H.L. Hansen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--13><div class="card-body p-3 small">A supportive environment is vital for overall cognitive development in children. Challenges with direct observation and limitations of access to data driven approaches often hinder teachers or practitioners in early childhood research to modify or enhance classroom structures. Deploying sensor based tools in naturalistic preschool classrooms will thereby help teachers/practitioners to make informed decisions and better support student learning needs. In this study, two elements of eco-behavioral assessment: conversational speech and real-time location are fused together. While various challenges remain in developing Automatic Speech Recognition systems for spontaneous preschool children speech, efforts are made to develop a hybrid ASR engine reporting an effective Word-Error-Rate of 40%. The ASR engine further supports recognition of spoken words, WH-words, and verbs in various activity learning zones in a naturalistic preschool classroom scenario. Activity areas represent various locations within the physical ecology of an early childhood setting, each of which is suited for knowledge and skill enhancement in young children. Capturing children’s communication engagement in such areas could help teachers/practitioners fine-tune their daily activities, without the need for direct observation. This investigation provides evidence of the use of speech technology in educational settings to better support such early childhood intervention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.14.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.14/>Structural information in mathematical formulas for exercise difficulty prediction: a comparison of <span class=acl-fixed-case>NLP</span> representations</a></strong><br><a href=/people/e/ekaterina-loginova/>Ekaterina Loginova</a>
|
<a href=/people/d/dries-benoit/>Dries Benoit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--14><div class="card-body p-3 small">To tailor a learning system to the student’s level and needs, we must consider the characteristics of the learning content, such as its difficulty. While natural language processing allows us to represent text efficiently, the meaningful representation of mathematical formulas in an educational context is still understudied. This paper adopts structural embeddings as a possible way to bridge this gap. Our experiments validate the approach using publicly available datasets to show that incorporating syntactic information can improve performance in predicting the exercise difficulty.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.15.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.15/>The Specificity and Helpfulness of Peer-to-Peer Feedback in Higher Education</a></strong><br><a href=/people/r/roman-rietsche/>Roman Rietsche</a>
|
<a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/c/cornelius-schramm/>Cornelius Schramm</a>
|
<a href=/people/d/dominik-pfutze/>Dominik Pfütze</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--15><div class="card-body p-3 small">With the growth of online learning through MOOCs and other educational applications, it has become increasingly difficult for course providers to offer personalized feedback to students. Therefore asking students to provide feedback to each other has become one way to support learning. This peer-to-peer feedback has become increasingly important whether in MOOCs to provide feedback to thousands of students or in large-scale classes at universities. One of the challenges when allowing peer-to-peer feedback is that the feedback should be perceived as helpful, and an import factor determining helpfulness is how specific the feedback is. However, in classes including thousands of students, instructors do not have the resources to check the specificity of every piece of feedback between students. Therefore, we present an automatic classification model to measure sentence specificity in written feedback. The model was trained and tested on student feedback texts written in German where sentences have been labelled as general or specific. We find that we can automatically classify the sentences with an accuracy of 76.7% using a conventional feature-based approach, whereas transfer learning with BERT for German gives a classification accuracy of 81.1%. However, the feature-based approach comes with lower computational costs and preserves human interpretability of the coefficients. In addition we show that specificity of sentences in feedback texts has a weak positive correlation with perceptions of helpfulness. This indicates that specificity is one of the ingredients of good feedback, and invites further investigation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.16.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.bea-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.16/>Similarity-Based Content Scoring - How to Make <span class=acl-fixed-case>S</span>-<span class=acl-fixed-case>BERT</span> Keep Up With <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/m/marie-bexte/>Marie Bexte</a>
|
<a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--16><div class="card-body p-3 small">The dominating paradigm for content scoring is to learn an instance-based model, i.e. to use lexical features derived from the learner answers themselves. An alternative approach that receives much less attention is however to learn a similarity-based model. We introduce an architecture that efficiently learns a similarity model and find that results on the standard ASAP dataset are on par with a BERT-based classification approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.17.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.bea-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.17/>Don’t Drop the Topic - The Role of the Prompt in Argument Identification in Student Writing</a></strong><br><a href=/people/y/yuning-ding/>Yuning Ding</a>
|
<a href=/people/m/marie-bexte/>Marie Bexte</a>
|
<a href=/people/a/andrea-horbach/>Andrea Horbach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--17><div class="card-body p-3 small">In this paper, we explore the role of topic information in student essays from an argument mining perspective. We cluster a recently released corpus through topic modeling into prompts and train argument identification models on different data settings. Results show that, given the same amount of training data, prompt-specific training performs better than cross-prompt training. However, the advantage can be overcome by introducing large amounts of cross-prompt training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.18/><span class=acl-fixed-case>ALEN</span> App: Argumentative Writing Support To Foster <span class=acl-fixed-case>E</span>nglish Language Learning</a></strong><br><a href=/people/t/thiemo-wambsganss/>Thiemo Wambsganss</a>
|
<a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--18><div class="card-body p-3 small">This paper introduces a novel tool to support and engage English language learners with feedback on the quality of their argument structures. We present an approach which automatically detects claim-premise structures and provides visual feedback to the learner to prompt them to repair any broken argumentation structures. To investigate, if our persuasive feedback on language learners’ essay writing tasks engages and supports them in learning better English language, we designed the ALEN app (Argumentation for Learning English). We leverage an argumentation mining model trained on texts written by students and embed it in a writing support tool which provides students with feedback in their essay writing process. We evaluated our tool in two field-studies with a total of 28 students from a German high school to investigate the effects of adaptive argumentation feedback on their learning of English. The quantitative results suggest that using the ALEN app leads to a high self-efficacy, ease-of-use, intention to use and perceived usefulness for students in their English language learning process. Moreover, the qualitative answers indicate the potential benefits of combining grammar feedback with discourse level argumentation mining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.19/>Assessing sentence readability for <span class=acl-fixed-case>G</span>erman language learners with broad linguistic modeling or readability formulas: When do linguistic insights make a difference?</a></strong><br><a href=/people/z/zarah-weiss/>Zarah Weiss</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--19><div class="card-body p-3 small">We present a new state-of-the-art sentence-wise readability assessment model for German L2 readers. We build a linguistically broadly informed machine learning model and compare its performance against four commonly used readability formulas. To understand when the linguistic insights used to inform our model make a difference for readability assessment and when simple readability formulas suffice, we compare their performance based on two common automatic readability assessment tasks: predictive regression and sentence pair ranking. We find that leveraging linguistic insights yields top performances across tasks, but that for the identification of simplified sentences also readability formulas – which are easier to compute and more accessible – can be sufficiently precise. Linguistically informed modeling, however, is the only viable option for high quality outcomes in fine-grained prediction tasks. We then explore the sentence-wise readability profile of leveled texts written for language learners at a beginning, intermediate, and advanced level of German to showcase the valuable insights that sentence-wise readability assessment can have for the adaptation of learning materials and better understand how sentences’ individual readability contributes to larger texts’ overall readability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.20/>Parametrizable exercise generation from authentic texts: Effectively targeting the language means on the curriculum</a></strong><br><a href=/people/t/tanja-heck/>Tanja Heck</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--20><div class="card-body p-3 small">We present a parametrizable approach to exercise generation from authentic texts that addresses the need for digital materials designed to practice the language means on the curriculum in a real-life school setting. The tool builds on a language-aware searchengine that helps identify attractive texts rich in the language means to be practiced. Making use of state-of-the-art NLP, the relevant learning targets are identified and transformed intoexercise items embedded in the original context. While the language-aware search engine ensures that these contexts match the learner‘s interests based on the search term used, and the linguistic parametrization of the system then reranks the results to prioritize texts that richly represent the learning targets, for theexercise generation to proceed on this basis, an interactive configuration panel allows users to adjust exercise complexity through a range of parameters specifying both properties of thesource sentences and of the exercises. An evaluation of exercises generated from web documents for a representative sample of language means selected from the English curriculum of 7th grade in German secondary school showed that the ombination of language-aware search and exercise generationsuccessfully facilitates the process of generating exercises from authentic texts that support practice of the pedagogical targets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.21/>Selecting Context Clozes for Lightweight Reading Compliance</a></strong><br><a href=/people/g/greg-keim/>Greg Keim</a>
|
<a href=/people/m/michael-littman/>Michael Littman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--21><div class="card-body p-3 small">We explore a novel approach to reading compliance, leveraging large language models to select inline challenges that discourage skipping during reading. This lightweight ‘testing’ is accomplished through automatically identified context clozes where the reader must supply a missing word that would be hard to guess if earlier material was skipped. Clozes are selected by scoring each word by the contrast between its likelihood with and without prior sentences as context, preferring to leave gaps where this contrast is high. We report results of an initial human-participant test that indicates this method can find clozes that have this property.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.22.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.22/>‘Meet me at the ribary’ – Acceptability of spelling variants in free-text answers to listening comprehension prompts</a></strong><br><a href=/people/r/ronja-laarmann-quante/>Ronja Laarmann-Quante</a>
|
<a href=/people/l/leska-schwarz/>Leska Schwarz</a>
|
<a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--22><div class="card-body p-3 small">When listening comprehension is tested as a free-text production task, a challenge for scoring the answers is the resulting wide range of spelling variants. When judging whether a variant is acceptable or not, human raters perform a complex holistic decision. In this paper, we present a corpus study in which we analyze human acceptability decisions in a high stakes test for German. We show that for human experts, spelling variants are harder to score consistently than other answer variants. Furthermore, we examine how the decision can be operationalized using features that could be applied by an automatic scoring system. We show that simple measures like edit distance and phonetic similarity between a given answer and the target answer can model the human acceptability decisions with the same inter-annotator agreement as humans, and discuss implications of the remaining inconsistencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.23/>Educational Tools for Mapuzugun</a></strong><br><a href=/people/c/cristian-ahumada/>Cristian Ahumada</a>
|
<a href=/people/c/claudio-gutierrez/>Claudio Gutierrez</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--23><div class="card-body p-3 small">Mapuzugun is the language of the Mapuche people. Due to political and historical reasons, its number of speakers has decreased and the language has been excluded from the educational system in Chile and Argentina. For this reason, it is very important to support the revitalization of the Mapuzugun in all spaces and media of society. In this work we present a tool towards supporting educational activities of Mapuzugun, tailored to the characteristics of the language. The tool consists of three parts: design and development of an orthography detector and converter; a morphological analyzer; and an informal translator. We also present a case study with Mapuzugun students showing promising results. Short abstract in Mapuzugun: Tüfachi küzaw pegelfi kiñe zugun küzawpeyüm kelluaetew pu mapuzugun chillkatufe kimal kizu tañi zugun.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.24/>An Evaluation of Binary Comparative Lexical Complexity Models</a></strong><br><a href=/people/k/kai-north/>Kai North</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/m/matthew-shardlow/>Matthew Shardlow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--24><div class="card-body p-3 small">Identifying complex words in texts is an important first step in text simplification (TS) systems. In this paper, we investigate the performance of binary comparative Lexical Complexity Prediction (LCP) models applied to a popular benchmark dataset — the CompLex 2.0 dataset used in SemEval-2021 Task 1. With the data from CompLex 2.0, we create a new dataset contain 1,940 sentences referred to as CompLex-BC. Using CompLex-BC, we train multiple models to differentiate which of two target words is more or less complex in the same sentence. A linear SVM model achieved the best performance in our experiments with an F1-score of 0.86.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.25/>Toward Automatic Discourse Parsing of Student Writing Motivated by Neural Interpretation</a></strong><br><a href=/people/j/james-fiacco/>James Fiacco</a>
|
<a href=/people/s/shiyan-jiang/>Shiyan Jiang</a>
|
<a href=/people/d/david-adamson/>David Adamson</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rosé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--25><div class="card-body p-3 small">Providing effective automatic essay feedback is necessary for offering writing instruction at a massive scale. In particular, feedback for promoting coherent flow of ideas in essays is critical. In this paper we propose a state-of-the-art method for automated analysis of structure and flow of writing, referred to as Rhetorical Structure Theory (RST) parsing. In so doing, we lay a foundation for a generalizable approach to automated writing feedback related to structure and flow. We address challenges in automated rhetorical analysis when applied to student writing and evaluate our novel RST parser model on both a recent student writing dataset and a standard benchmark RST parsing dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.26.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.26.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.bea-1.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.26/>Educational Multi-Question Generation for Reading Comprehension</a></strong><br><a href=/people/m/manav-rathod/>Manav Rathod</a>
|
<a href=/people/t/tony-tu/>Tony Tu</a>
|
<a href=/people/k/katherine-stasaski/>Katherine Stasaski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--26><div class="card-body p-3 small">Automated question generation has made great advances with the help of large NLP generation models. However, typically only one question is generated for each intended answer. We propose a new task, Multi-Question Generation, aimed at generating multiple semantically similar but lexically diverse questions assessing the same concept. We develop an evaluation framework based on desirable qualities of the resulting questions. Results comparing multiple question generation approaches in the two-question generation condition show a trade-off between question answerability and lexical diversity between the two questions. We also report preliminary results from sampling multiple questions from our model, to explore generating more than two questions. Our task can be used to further explore the educational impact of showing multiple distinct question wordings to students.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.27.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.bea-1.27.attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.bea-1.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.27/>Computationally Identifying Funneling and Focusing Questions in Classroom Discourse</a></strong><br><a href=/people/s/sterling-alic/>Sterling Alic</a>
|
<a href=/people/d/dorottya-demszky/>Dorottya Demszky</a>
|
<a href=/people/z/zid-mancenido/>Zid Mancenido</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/h/heather-hill/>Heather Hill</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--27><div class="card-body p-3 small">Responsive teaching is a highly effective strategy that promotes student learning. In math classrooms, teachers might <i>funnel</i> students towards a normative answer or <i>focus</i> students to reflect on their own thinking depending their understanding of math concepts. When teachers focus, they treat students’ contributions as resources for collective sensemaking, and thereby significantly improve students’ achievement and confidence in mathematics. We propose the task of computationally detecting funneling and focusing questions in classroom discourse. We do so by creating and releasing an annotated dataset of 2,348 teacher utterances labeled for funneling and focusing questions, or neither. We introduce supervised and unsupervised approaches to differentiating these questions. Our best model, a supervised RoBERTa model fine-tuned on our dataset, has a strong linear correlation of .76 with human expert labels and with positive educational outcomes, including math instruction quality and student achievement, showing the model’s potential for use in automated teacher feedback tools. Our unsupervised measures show significant but weaker correlations with human labels and outcomes, and they highlight interesting linguistic patterns of funneling and focusing questions. The high performance of the supervised measure indicates its promise for supporting teachers in their instruction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.28.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.bea-1.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.28/>Towards an open-domain chatbot for language practice</a></strong><br><a href=/people/g/gladys-tyen/>Gladys Tyen</a>
|
<a href=/people/m/mark-brenchley/>Mark Brenchley</a>
|
<a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--28><div class="card-body p-3 small">State-of-the-art chatbots for English are now able to hold conversations on virtually any topic (e.g. Adiwardana et al., 2020; Roller et al., 2021). However, existing dialogue systems in the language learning domain still use hand-crafted rules and pattern matching, and are much more limited in scope. In this paper, we make an initial foray into adapting open-domain dialogue generation for second language learning. We propose and implement decoding strategies that can adjust the difficulty level of the chatbot according to the learner’s needs, without requiring further training of the chatbot. These strategies are then evaluated using judgements from human examiners trained in language education. Our results show that re-ranking candidate outputs is a particularly effective strategy, and performance can be further improved by adding sub-token penalties and filtering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.29.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.29/>Response Construct Tagging: <span class=acl-fixed-case>NLP</span>-Aided Assessment for Engineering Education</a></strong><br><a href=/people/a/ananya-ganesh/>Ananya Ganesh</a>
|
<a href=/people/h/hugh-scribner/>Hugh Scribner</a>
|
<a href=/people/j/jasdeep-singh/>Jasdeep Singh</a>
|
<a href=/people/k/katherine-goodman/>Katherine Goodman</a>
|
<a href=/people/j/jean-hertzberg/>Jean Hertzberg</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--29><div class="card-body p-3 small">Recent advances in natural language processing (NLP) have greatly helped educational applications, for both teachers and students. In higher education, there is great potential to use NLP tools for advancing pedagogical research. In this paper, we focus on how NLP can help understand student experiences in engineering, thus facilitating engineering educators to carry out large scale analysis that is helpful for re-designing the curriculum. Here, we introduce a new task we call response construct tagging (RCT), in which student responses to tailored survey questions are automatically tagged for six constructs measuring transformative experiences and engineering identity of students. We experiment with state-of-the-art classification models for this task and investigate the effects of different sources of additional information. Our best model achieves an F1 score of 48. We further investigate multi-task training on the related task of sentiment classification, which improves our model’s performance to 55 F1. Finally, we provide a detailed qualitative analysis of model performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.30.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.bea-1.30.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.bea-1.30/>Towards Automatic Short Answer Assessment for <span class=acl-fixed-case>F</span>innish as a Paraphrase Retrieval Task</a></strong><br><a href=/people/l/li-hsin-chang/>Li-Hsin Chang</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--30><div class="card-body p-3 small">Automatic grouping of textual answers has the potential of allowing batch grading, but is challenging because the answers, especially longer essays, have many claims. To explore the feasibility of grouping together answers based on their semantic meaning, this paper investigates the grouping of short textual answers, proxies of single claims. This is approached as a paraphrase identification task, where neural and non-neural sentence embeddings and a paraphrase identification model are tested. These methods are evaluated on a dataset consisting of over 4000 short textual answers from various disciplines. The results map out the suitable question types for the paraphrase identification model and those for the neural and non-neural methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.bea-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.bea-1.31.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--bea-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.bea-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.bea-1.31/>Incremental Disfluency Detection for Spoken Learner <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/l/lucy-skidmore/>Lucy Skidmore</a>
|
<a href=/people/r/roger-k-moore/>Roger Moore</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--bea-1--31><div class="card-body p-3 small">Incremental disfluency detection provides a framework for computing communicative meaning from hesitations, repetitions and false starts commonly found in speech. One application of this area of research is in dialogue-based computer-assisted language learning (CALL), where detecting learners’ production issues word-by-word can facilitate timely and pedagogically driven responses from an automated system. Existing research on disfluency detection in learner speech focuses on disfluency removal for subsequent downstream tasks, processing whole utterances non-incrementally. This paper instead explores the application of laughter as a feature for incremental disfluency detection and shows that when combined with silence, these features reduce the impact of learner errors on model precision as well as lead to an overall improvement of model performance. This work adds to the growing body of research incorporating laughter as a feature for dialogue processing tasks and provides further support for the application of multimodality in dialogue-based CALL systems.</div></div></div><hr><div id=2022clinicalnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.clinicalnlp-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.clinicalnlp-1/>Proceedings of the 4th Clinical Natural Language Processing Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.0/>Proceedings of the 4th Clinical Natural Language Processing Workshop</a></strong><br><a href=/people/t/tristan-naumann/>Tristan Naumann</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/k/kirk-roberts/>Kirk Roberts</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.1/><span class=acl-fixed-case>CLPT</span>: A Universal Annotation Scheme and Toolkit for Clinical Language Processing</a></strong><br><a href=/people/s/saranya-krishnamoorthy/>Saranya Krishnamoorthy</a>
|
<a href=/people/y/yanyi-jiang/>Yanyi Jiang</a>
|
<a href=/people/w/william-buchanan/>William Buchanan</a>
|
<a href=/people/a/ayush-singh/>Ayush Singh</a>
|
<a href=/people/j/john-ortega/>John Ortega</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--1><div class="card-body p-3 small">With the abundance of natural language processing (NLP) frameworks and toolkits being used in the clinical arena, a new challenge has arisen - how do technologists collaborate across several projects in an easy way? Private sector companies are usually not willing to share their work due to intellectual property rights and profit-bearing decisions. Therefore, the annotation schemes and toolkits that they use are rarely shared with the wider community. We present the clinical language pipeline toolkit (CLPT) and its corresponding annotation scheme called the CLAO (Clinical Language Annotation Object) with the aim of creating a way to share research results and other efforts through a software solution. The CLAO is a unified annotation scheme for clinical technology processing (CTP) projects that forms part of the CLPT and is more reliable than previous standards such as UIMA, BioC, and cTakes for annotation searches, insertions, and deletions. Additionally, it offers a standardized object that can be exchanged through an API that the authors release publicly for CTP project inclusion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.clinicalnlp-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.2/><span class=acl-fixed-case>PLM</span>-<span class=acl-fixed-case>ICD</span>: Automatic <span class=acl-fixed-case>ICD</span> Coding with Pretrained Language Models</a></strong><br><a href=/people/c/chao-wei-huang/>Chao-Wei Huang</a>
|
<a href=/people/s/shang-chi-tsai/>Shang-Chi Tsai</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--2><div class="card-body p-3 small">Automatically classifying electronic health records (EHRs) into diagnostic codes has been challenging to the NLP community. State-of-the-art methods treated this problem as a multi-label classification problem and proposed various architectures to model this problem. However, these systems did not leverage the superb performance of pretrained language models, which achieved superb performance on natural language understanding tasks. Prior work has shown that pretrained language models underperformed on this task with the regular fine-tuning scheme. Therefore, this paper aims at analyzing the causes of the underperformance and developing a framework for automatic ICD coding with pretrained language models. We spotted three main issues through the experiments: 1) large label space, 2) long input sequences, and 3) domain mismatch between pretraining and fine-tuning. We propose PLM-ICD, a framework that tackles the challenges with various strategies. The experimental results show that our proposed framework can overcome the challenges and achieves state-of-the-art performance in terms of multiple metrics on the benchmark MIMIC data. Our source code is available at <a href=https://github.com/MiuLab/PLM-ICD class=acl-markup-url>https://github.com/MiuLab/PLM-ICD</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.3/><span class=tex-math>m</span>-Networks: Adapting the Triplet Networks for Acronym Disambiguation</a></strong><br><a href=/people/s/sandaru-seneviratne/>Sandaru Seneviratne</a>
|
<a href=/people/e/elena-daskalaki/>Elena Daskalaki</a>
|
<a href=/people/a/artem-lenskiy/>Artem Lenskiy</a>
|
<a href=/people/h/hanna-suominen/>Hanna Suominen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--3><div class="card-body p-3 small">Acronym disambiguation (AD) is the process of identifying the correct expansion of the acronyms in text. AD is crucial in natural language understanding of scientific and medical documents due to the high prevalence of technical acronyms and the possible expansions. Given that natural language is often ambiguous with more than one meaning for words, identifying the correct expansion for acronyms requires learning of effective representations for words, phrases, acronyms, and abbreviations based on their context. In this paper, we proposed an approach to leverage the triplet networks and triplet loss which learns better representations of text through distance comparisons of embeddings. We tested both the triplet network-based method and the modified triplet network-based method with <span class=tex-math>m</span> networks on the AD dataset from the SDU@AAAI-21 AD task, CASI dataset, and MeDAL dataset. F scores of 87.31%, 70.67%, and 75.75% were achieved by the <span class=tex-math>m</span> network-based approach for SDU, CASI, and MeDAL datasets respectively indicating that triplet network-based methods have comparable performance but with only 12% of the number of parameters in the baseline method. This effective implementation is available at <a href=https://github.com/sandaruSen/m_networks class=acl-markup-url>https://github.com/sandaruSen/m_networks</a> under the MIT license.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.4/>Fine-tuning <span class=acl-fixed-case>BERT</span> Models for Summarizing <span class=acl-fixed-case>G</span>erman Radiology Findings</a></strong><br><a href=/people/s/siting-liang/>Siting Liang</a>
|
<a href=/people/k/klaus-kades/>Klaus Kades</a>
|
<a href=/people/m/matthias-fink/>Matthias Fink</a>
|
<a href=/people/p/peter-full/>Peter Full</a>
|
<a href=/people/t/tim-weber/>Tim Weber</a>
|
<a href=/people/j/jens-kleesiek/>Jens Kleesiek</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a>
|
<a href=/people/k/klaus-maier-hein/>Klaus Maier-Hein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--4><div class="card-body p-3 small">Writing the conclusion section of radiology reports is essential for communicating the radiology findings and its assessment to physician in a condensed form. In this work, we employ a transformer-based Seq2Seq model for generating the conclusion section of German radiology reports. The model is initialized with the pretrained parameters of a German BERT model and fine-tuned in our downstream task on our domain data. We proposed two strategies to improve the factual correctness of the model. In the first method, next to the abstractive learning objective, we introduce an extraction learning objective to train the decoder in the model to both generate one summary sequence and extract the key findings from the source input. The second approach is to integrate the pointer mechanism into the transformer-based Seq2Seq model. The pointer network helps the Seq2Seq model to choose between generating tokens from the vocabulary or copying parts from the source input during generation. The results of the automatic and human evaluations show that the enhanced Seq2Seq model is capable of generating human-like radiology conclusions and that the improved models effectively reduce the factual errors in the generations despite the small amount of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.5/><span class=acl-fixed-case>RRED</span> : A Radiology Report Error Detector based on Deep Learning Framework</a></strong><br><a href=/people/d/dabin-min/>Dabin Min</a>
|
<a href=/people/k/kaeun-kim/>Kaeun Kim</a>
|
<a href=/people/j/jong-hyuk-lee/>Jong Hyuk Lee</a>
|
<a href=/people/y/yisak-kim/>Yisak Kim</a>
|
<a href=/people/c/chang-min-park/>Chang Min Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--5><div class="card-body p-3 small">Radiology report is an official record of radiologists’ interpretation of patients’ radiographs and it’s a crucial component in the overall medical diagnostic process. However, it can contain various types of errors that can lead to inadequate treatment or delay in diagnosis. To address this problem, we propose a deep learning framework to detect errors in radiology reports. Specifically, our method detects errors between findings and conclusion of chest X-ray reports based on a supervised learning framework. To compensate for the lack of data availability of radiology reports with errors, we develop an error generator to systematically create artificial errors in existing reports. In addition, we introduce a Medical Knowledge-enhancing Pre-training to further utilize the knowledge of abbreviations and key phrases frequently used in the medical domain. We believe that this is the first work to propose a deep learning framework for detecting errors in radiology reports based on a rich contextual and medical understanding. Validation on our radiologist-synthesized dataset, based on MIMIC-CXR, shows 0.80 and 0.95 of the area under precision-recall curve (AUPRC) and the area under the ROC curve (AUROC) respectively, indicating that our framework can effectively detect errors in the real-world radiology reports.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.clinicalnlp-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.6/>Cross-Language Transfer of High-Quality Annotations: Combining Neural Machine Translation with Cross-Linguistic Span Alignment to Apply <span class=acl-fixed-case>NER</span> to Clinical Texts in a Low-Resource Language</a></strong><br><a href=/people/h/henning-schafer/>Henning Schäfer</a>
|
<a href=/people/a/ahmad-idrissi-yaghir/>Ahmad Idrissi-Yaghir</a>
|
<a href=/people/p/peter-horn/>Peter Horn</a>
|
<a href=/people/c/christoph-friedrich/>Christoph Friedrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--6><div class="card-body p-3 small">In this work, cross-linguistic span prediction based on contextualized word embedding models is used together with neural machine translation (NMT) to transfer and apply the state-of-the-art models in natural language processing (NLP) to a low-resource language clinical corpus. Two directions are evaluated: (a) English models can be applied to translated texts to subsequently transfer the predicted annotations to the source language and (b) existing high-quality annotations can be transferred beyond translation and then used to train NLP models in the target language. Effectiveness and loss of transmission is evaluated using the German Berlin-Tübingen-Oncology Corpus (BRONCO) dataset with transferred external data from NCBI disease, SemEval-2013 drug-drug interaction (DDI) and i2b2/VA 2010 data. The use of English models for translated clinical texts has always involved attempts to take full advantage of the benefits associated with them (large pre-trained biomedical word embeddings). To improve advances in this area, we provide a general-purpose pipeline to transfer any annotated BRAT or CoNLL format to various target languages. For the entity class medication, good results were obtained with 0.806 <span class=tex-math>F1</span>-score after re-alignment. Limited success occurred in the diagnosis and treatment class with results just below 0.5 <span class=tex-math>F1</span>-score due to differences in annotation guidelines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.clinicalnlp-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.7/>What Do You See in this Patient? Behavioral Testing of Clinical <span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/b/betty-van-aken/>Betty Van Aken</a>
|
<a href=/people/s/sebastian-herrmann/>Sebastian Herrmann</a>
|
<a href=/people/a/alexander-loser/>Alexander Löser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--7><div class="card-body p-3 small">Decision support systems based on clinical notes have the potential to improve patient care by pointing doctors towards overseen risks. Predicting a patient’s outcome is an essential part of such systems, for which the use of deep neural networks has shown promising results. However, the patterns learned by these networks are mostly opaque and previous work revealed both reproduction of systemic biases and unexpected behavior for out-of-distribution patients. For application in clinical practice it is crucial to be aware of such behavior. We thus introduce a testing framework that evaluates clinical models regarding certain changes in the input. The framework helps to understand learned patterns and their influence on model decisions. In this work, we apply it to analyse the change in behavior with regard to the patient characteristics gender, age and ethnicity. Our evaluation of three current clinical NLP models demonstrates the concrete effects of these characteristics on the models’ decisions. They show that model behavior varies drastically even when fine-tuned on the same data with similar AUROC score. These results exemplify the need for a broader communication of model behavior in the clinical domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.clinicalnlp-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.8/>Learning to Ask Like a Physician</a></strong><br><a href=/people/e/eric-lehman/>Eric Lehman</a>
|
<a href=/people/v/vladislav-lialin/>Vladislav Lialin</a>
|
<a href=/people/k/katelyn-edelwina-legaspi/>Katelyn Edelwina Legaspi</a>
|
<a href=/people/a/anne-janelle-sy/>Anne Janelle Sy</a>
|
<a href=/people/p/patricia-therese-pile/>Patricia Therese Pile</a>
|
<a href=/people/n/nicole-rose-alberto/>Nicole Rose Alberto</a>
|
<a href=/people/r/richard-raymund-ragasa/>Richard Raymund Ragasa</a>
|
<a href=/people/c/corinna-victoria-puyat/>Corinna Victoria Puyat</a>
|
<a href=/people/m/marianne-katharina-talino/>Marianne Katharina Taliño</a>
|
<a href=/people/i/isabelle-rose-alberto/>Isabelle Rose Alberto</a>
|
<a href=/people/p/pia-gabrielle-alfonso/>Pia Gabrielle Alfonso</a>
|
<a href=/people/d/dana-moukheiber/>Dana Moukheiber</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/j/jennifer-liang/>Jennifer Liang</a>
|
<a href=/people/p/preethi-raghavan/>Preethi Raghavan</a>
|
<a href=/people/l/leo-anthony-celi/>Leo Anthony Celi</a>
|
<a href=/people/p/peter-szolovits/>Peter Szolovits</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--8><div class="card-body p-3 small">Existing question answering (QA) datasets derived from electronic health records (EHR) are artificially generated and consequently fail to capture realistic physician information needs. We present Discharge Summary Clinical Questions (DiSCQ), a newly curated question dataset composed of 2,000+ questions paired with the snippets of text (triggers) that prompted each question. The questions are generated by medical experts from 100+ MIMIC-III discharge summaries. We analyze this dataset to characterize the types of information sought by medical experts. We also train baseline models for trigger detection and question generation (QG), paired with unsupervised answer retrieval over EHRs. Our baseline model is able to generate high quality questions in over 62% of cases when prompted with human selected triggers. We release this dataset (and all code to reproduce baseline model results) to facilitate further research into realistic clinical QA and QG: <a href=https://github.com/elehman16/discq class=acl-markup-url>https://github.com/elehman16/discq</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.clinicalnlp-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.9/>Clinical Flair: A Pre-Trained Language Model for <span class=acl-fixed-case>S</span>panish Clinical Natural Language Processing</a></strong><br><a href=/people/m/matias-rojas/>Matías Rojas</a>
|
<a href=/people/j/jocelyn-dunstan/>Jocelyn Dunstan</a>
|
<a href=/people/f/fabian-villena/>Fabián Villena</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--9><div class="card-body p-3 small">Word embeddings have been widely used in Natural Language Processing (NLP) tasks. Although these representations can capture the semantic information of words, they cannot learn the sequence-level semantics. This problem can be handled using contextual word embeddings derived from pre-trained language models, which have contributed to significant improvements in several NLP tasks. Further improvements are achieved when pre-training these models on domain-specific corpora. In this paper, we introduce Clinical Flair, a domain-specific language model trained on Spanish clinical narratives. To validate the quality of the contextual representations retrieved from our model, we tested them on four named entity recognition datasets belonging to the clinical and biomedical domains. Our experiments confirm that incorporating domain-specific embeddings into classical sequence labeling architectures improves model performance dramatically compared to general-domain embeddings, demonstrating the importance of having these resources available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.10/>An exploratory data analysis: the performance differences of a medical code prediction system on different demographic groups</a></strong><br><a href=/people/h/heereen-shim/>Heereen Shim</a>
|
<a href=/people/d/dietwig-lowet/>Dietwig Lowet</a>
|
<a href=/people/s/stijn-luca/>Stijn Luca</a>
|
<a href=/people/b/bart-vanrumste/>Bart Vanrumste</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--10><div class="card-body p-3 small">Recent studies show that neural natural processing models for medical code prediction suffer from a label imbalance issue. This study aims to investigate further imbalance in a medical code prediction dataset in terms of demographic variables and analyse performance differences in demographic groups. We use sample-based metrics to correctly evaluate the performance in terms of the data subject. Also, a simple label distance metric is proposed to quantify the difference in the label distribution between a group and the entire data. Our analysis results reveal that the model performs differently towards different demographic groups: significant differences between age groups and between insurance types are observed. Interestingly, we found a weak positive correlation between the number of training data of the group and the performance of the group. However, a strong negative correlation between the label distance of the group and the performance of the group is observed. This result suggests that the model tends to perform poorly in the group whose label distribution is different from the global label distribution of the training data set. Further analysis of the model performance is required to identify the cause of these differences and to improve the model building.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.11/>Ensemble-based Fine-Tuning Strategy for Temporal Relation Extraction from the Clinical Narrative</a></strong><br><a href=/people/l/lijing-wang/>Lijing Wang</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--11><div class="card-body p-3 small">In this paper, we investigate ensemble methods for fine-tuning transformer-based pretrained models for clinical natural language processing tasks, specifically temporal relation extraction from the clinical narrative. Our experimental results on the THYME data show that ensembling as a fine-tuning strategy can further boost model performance over single learners optimized for hyperparameters. Dynamic snapshot ensembling is particularly beneficial as it fine-tunes a wide array of parameters and results in a 2.8% absolute improvement in F1 over the base single learner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clinicalnlp-1.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clinicalnlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clinicalnlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clinicalnlp-1.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clinicalnlp-1.12/>Exploring Text Representations for Generative Temporal Relation Extraction</a></strong><br><a href=/people/d/dmitriy-dligach/>Dmitriy Dligach</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clinicalnlp-1--12><div class="card-body p-3 small">Sequence-to-sequence models are appealing because they allow both encoder and decoder to be shared across many tasks by formulating those tasks as text-to-text problems. Despite recently reported successes of such models, we find that engineering input/output representations for such text-to-text models is challenging. On the Clinical TempEval 2016 relation extraction task, the most natural choice of output representations, where relations are spelled out in simple predicate logic statements, did not lead to good performance. We explore a variety of input/output representations, with the most successful prompting one event at a time, and achieving results competitive with standard pairwise temporal relation extraction systems.</div></div></div><hr><div id=2022clpsych-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.clpsych-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.clpsych-1/>Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.0/>Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology</a></strong><br><a href=/people/a/ayah-zirikly/>Ayah Zirikly</a>
|
<a href=/people/d/dana-atzil-slonim/>Dana Atzil-Slonim</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a>
|
<a href=/people/b/bart-desmet/>Bart Desmet</a>
|
<a href=/people/m/molly-ireland/>Molly Ireland</a>
|
<a href=/people/a/andrew-lee/>Andrew Lee</a>
|
<a href=/people/s/sean-macavaney/>Sean MacAvaney</a>
|
<a href=/people/m/matthew-purver/>Matthew Purver</a>
|
<a href=/people/r/rebecca-resnik/>Rebecca Resnik</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.clpsych-1.1.appendix.pdf data-toggle=tooltip data-placement=top title=Appendix><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.1/><span class=acl-fixed-case>DEPAC</span>: a Corpus for Depression and Anxiety Detection from Speech</a></strong><br><a href=/people/m/mashrura-tasnim/>Mashrura Tasnim</a>
|
<a href=/people/m/malikeh-ehghaghi/>Malikeh Ehghaghi</a>
|
<a href=/people/b/brian-diep/>Brian Diep</a>
|
<a href=/people/j/jekaterina-novikova/>Jekaterina Novikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--1><div class="card-body p-3 small">Mental distress like depression and anxiety contribute to the largest proportion of the global burden of diseases. Automated diagnosis system of such disorders, empowered by recent innovations in Artificial Intelligence, can pave the way to reduce the sufferings of the affected individuals. Development of such systems requires information-rich and balanced corpora. In this work, we introduce a novel mental distress analysis audio dataset DEPAC, labelled based on established thresholds on depression and anxiety standard screening tools. This large dataset comprises multiple speech tasks per individual, as well as relevant demographic information. Alongside, we present a feature set consisting of hand-curated acoustic and linguistic features, which were found effective in identifying signs of mental illnesses in human speech. Finally, we justify the quality and effectiveness of our proposed audio corpus and feature set in predicting depression severity by comparing the performance of baseline machine learning models built on this dataset with baseline models trained on other well-known depression corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.2/>The ethical role of computational linguistics in digital psychological formulation and suicide prevention.</a></strong><br><a href=/people/m/martin-orr/>Martin Orr</a>
|
<a href=/people/k/kirsten-van-kessel/>Kirsten Van Kessel</a>
|
<a href=/people/d/dave-parry/>Dave Parry</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--2><div class="card-body p-3 small">Formulation is central to clinical practice. Formulation has a factor weighing, pattern recognition and explanatory hypothesis modelling focus. Formulation attempts to make sense of why a person presents in a certain state at a certain time and context, and how that state may be best managed to enhance mental health, safety and optimal change. Inherent to the clinical need for formulation is an appreciation of the complexities, uncertainty and limits of applying theoretical concepts and symptom, diagnostic and risk categories to human experience; or attaching meaning or weight to any particular factor in an individual?s history or mental state without considering the broader biopsychosocial and cultural context. With specific reference to suicide prevention, this paper considers the need and potential for the computer linguistic community to be both cognisant of and ethically contribute to the clinical formulation process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.3/>Explaining Models of Mental Health via Clinically Grounded Auxiliary Tasks</a></strong><br><a href=/people/a/ayah-zirikly/>Ayah Zirikly</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--3><div class="card-body p-3 small">Models of mental health based on natural language processing can uncover latent signals of mental health from language. Models that indicate whether an individual is depressed, or has other mental health conditions, can aid in diagnosis and treatment. A critical aspect of integration of these models into the clinical setting relies on explaining their behavior to domain experts. In the case of mental health diagnosis, clinicians already rely on an assessment framework to make these decisions; that framework can help a model generate meaningful explanations. In this work we propose to use PHQ-9 categories as an auxiliary task to explaining a social media based model of depression. We develop a multi-task learning framework that predicts both depression and PHQ-9 categories as auxiliary tasks. We compare the quality of explanations generated based on the depression task only, versus those that use the predicted PHQ-9 categories. We find that by relying on clinically meaningful auxiliary tasks, we produce more meaningful explanations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.clpsych-1.4.appendix.pdf data-toggle=tooltip data-placement=top title=Appendix><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.4/>Identifying stable speech-language markers of autism in children: Preliminary evidence from a longitudinal telephony-based study</a></strong><br><a href=/people/s/sunghye-cho/>Sunghye Cho</a>
|
<a href=/people/r/riccardo-fusaroli/>Riccardo Fusaroli</a>
|
<a href=/people/m/maggie-rose-pelella/>Maggie Rose Pelella</a>
|
<a href=/people/k/kimberly-tena/>Kimberly Tena</a>
|
<a href=/people/a/azia-knox/>Azia Knox</a>
|
<a href=/people/a/aili-hauptmann/>Aili Hauptmann</a>
|
<a href=/people/m/maxine-covello/>Maxine Covello</a>
|
<a href=/people/a/alison-russell/>Alison Russell</a>
|
<a href=/people/j/judith-miller/>Judith Miller</a>
|
<a href=/people/a/alison-hulink/>Alison Hulink</a>
|
<a href=/people/j/jennifer-uzokwe/>Jennifer Uzokwe</a>
|
<a href=/people/k/kevin-walker/>Kevin Walker</a>
|
<a href=/people/j/james-fiumara/>James Fiumara</a>
|
<a href=/people/j/juhi-pandey/>Juhi Pandey</a>
|
<a href=/people/c/christopher-chatham/>Christopher Chatham</a>
|
<a href=/people/c/christopher-cieri/>Christopher Cieri</a>
|
<a href=/people/r/robert-t-schultz/>Robert Schultz</a>
|
<a href=/people/m/mark-liberman/>Mark Liberman</a>
|
<a href=/people/j/julia-parish-morris/>Julia Parish-morris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--4><div class="card-body p-3 small">This study examined differences in linguistic features produced by autistic and neurotypical (NT) children during brief picture descriptions, and assessed feature stability over time. Weekly speech samples from well-characterized participants were collected using a telephony system designed to improve access for geographically isolated and historically marginalized communities. Results showed stable group differences in certain acoustic features, some of which may potentially serve as key outcome measures in future treatment studies. These results highlight the importance of eliciting semi-structured speech samples in a variety of contexts over time, and adds to a growing body of research showing that fine-grained naturalistic communication features hold promise for intervention research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.clpsych-1.5.appendix.pdf data-toggle=tooltip data-placement=top title=Appendix><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.5/>Psychotherapy is Not One Thing: Simultaneous Modeling of Different Therapeutic Approaches</a></strong><br><a href=/people/m/maitrey-mehta/>Maitrey Mehta</a>
|
<a href=/people/d/derek-caperton/>Derek Caperton</a>
|
<a href=/people/k/katherine-axford/>Katherine Axford</a>
|
<a href=/people/l/lauren-weitzman/>Lauren Weitzman</a>
|
<a href=/people/d/david-atkins/>David Atkins</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>
|
<a href=/people/z/zac-imel/>Zac Imel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--5><div class="card-body p-3 small">There are many different forms of psychotherapy. Itemized inventories of psychotherapeutic interventions provide a mechanism for evaluating the quality of care received by clients and for conducting research on how psychotherapy helps. However, evaluations such as these are slow, expensive, and are rarely used outside of well-funded research studies. Natural language processing research has progressed to allow automating such tasks. Yet, NLP work in this area has been restricted to evaluating a single approach to treatment, when prior research indicates therapists used a wide variety of interventions with their clients, often in the same session. In this paper, we frame this scenario as a multi-label classification task, and develop a group of models aimed at predicting a wide variety of therapist talk-turn level orientations. Our models achieve F1 macro scores of 0.5, with the class F1 ranging from 0.36 to 0.67. We present analyses which offer insights into the capability of such models to capture psychotherapy approaches, and which may complement human judgment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.6/>Then and Now: Quantifying the Longitudinal Validity of Self-Disclosed Depression Diagnoses</a></strong><br><a href=/people/k/keith-harrigian/>Keith Harrigian</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--6><div class="card-body p-3 small">Self-disclosed mental health diagnoses, which serve as ground truth annotations of mental health status in the absence of clinical measures, underpin the conclusions behind most computational studies of mental health language from the last decade. However, psychiatric conditions are dynamic; a prior depression diagnosis may no longer be indicative of an individual’s mental health, either due to treatment or other mitigating factors. We ask: to what extent are self-disclosures of mental health diagnoses actually relevant over time? We analyze recent activity from individuals who disclosed a depression diagnosis on social media over five years ago and, in turn, acquire a new understanding of how presentations of mental health status on social media manifest longitudinally. We also provide expanded evidence for the presence of personality-related biases in datasets curated using self-disclosed diagnoses. Our findings motivate three practical recommendations for improving mental health datasets curated using self-disclosed diagnoses:1) Annotate diagnosis dates and psychiatric comorbidities2) Sample control groups using propensity score matching3) Identify and remove spurious correlations introduced by selection bias</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.7/>Tracking Mental Health Risks and Coping Strategies in Healthcare Workers’ Online Conversations Across the <span class=acl-fixed-case>COVID</span>-19 Pandemic</a></strong><br><a href=/people/m/molly-ireland/>Molly Ireland</a>
|
<a href=/people/k/kaitlin-adams/>Kaitlin Adams</a>
|
<a href=/people/s/sean-farrell/>Sean Farrell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--7><div class="card-body p-3 small">The mental health risks of the COVID-19 pandemic are magnified for medical professionals, such as doctors and nurses. To track conversational markers of psychological distress and coping strategies, we analyzed 67.25 million words written by self-identified healthcare workers (N = 5,409; 60.5% nurses, 40.5% physicians) on Reddit beginning in June 2019. Dictionary-based measures revealed increasing emotionality (including more positive and negative emotion and more swearing), social withdrawal (less affiliation and empathy, more “they” pronouns), and self-distancing (fewer “I” pronouns) over time. Several effects were strongest for conversations that were least health-focused and self-relevant, suggesting that long-term changes in social and emotional behavior are general and not limited to personal or work-related experiences. Understanding protective and risky coping strategies used by healthcare workers during the pandemic is fundamental for maintaining mental health among front-line workers during periods of chronic stress, such as the COVID-19 pandemic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.8/>Are You Really Okay? A Transfer Learning-based Approach for Identification of Underlying Mental Illnesses</a></strong><br><a href=/people/a/ankit-aich/>Ankit Aich</a>
|
<a href=/people/n/natalie-parde/>Natalie Parde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--8><div class="card-body p-3 small">Evidence has demonstrated the presence of similarities in language use across people with various mental health conditions. In this work, we investigate these correlations both in terms of literature and as a data analysis problem. We also introduce a novel state-of-the-art transfer learning-based approach that learns from linguistic feature spaces of previous conditions and predicts unknown ones. Our model achieves strong performance, with F1 scores of 0.75, 0.80, and 0.76 at detecting depression, stress, and suicidal ideation in a first-of-its-kind transfer task and offering promising evidence that language models can harness learned patterns from known mental health conditions to aid in their prediction of others that may lie latent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.9/>Comparing emotion feature extraction approaches for predicting depression and anxiety</a></strong><br><a href=/people/h/hannah-burkhardt/>Hannah Burkhardt</a>
|
<a href=/people/m/michael-pullmann/>Michael Pullmann</a>
|
<a href=/people/t/thomas-hull/>Thomas Hull</a>
|
<a href=/people/p/patricia-arean/>Patricia Areán</a>
|
<a href=/people/t/trevor-cohen/>Trevor Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--9><div class="card-body p-3 small">The increasing adoption of message-based behavioral therapy enables new approaches to assessing mental health using linguistic analysis of patient-generated text. Word counting approaches have demonstrated utility for linguistic feature extraction, but deep learning methods hold additional promise given recent advances in this area. We evaluated the utility of emotion features extracted using a BERT-based model in comparison to emotions extracted using word counts as predictors of symptom severity in a large set of messages from text-based therapy sessions involving over 6,500 unique patients, accompanied by data from repeatedly administered symptom scale measurements. BERT-based emotion features explained more variance in regression models of symptom severity, and improved predictive modeling of scale-derived diagnostic categories. However, LIWC categories that are not directly related to emotions provided valuable and complementary information for modeling of symptom severity, indicating a role for both approaches in inferring the mental states underlying patient-generated language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.clpsych-1.10.appendix.pdf data-toggle=tooltip data-placement=top title=Appendix><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.10/>Detecting Suicidality with a Contextual Graph Neural Network</a></strong><br><a href=/people/d/daeun-lee/>Daeun Lee</a>
|
<a href=/people/m/migyeong-kang/>Migyeong Kang</a>
|
<a href=/people/m/minji-kim/>Minji Kim</a>
|
<a href=/people/j/jinyoung-han/>Jinyoung Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--10><div class="card-body p-3 small">Discovering individuals’ suicidality on social media has become increasingly important. Many researchers have studied to detect suicidality by using a suicide dictionary. However, while prior work focused on matching a word in a post with a suicide dictionary without considering contexts, little attention has been paid to how the word can be associated with the suicide-related context. To address this problem, we propose a suicidality detection model based on a graph neural network to grasp the dynamic semantic information of the suicide vocabulary by learning the relations between a given post and words. The extensive evaluation demonstrates that the proposed model achieves higher performance than the state-of-the-art methods. We believe the proposed model has great utility in identifying the suicidality of individuals and hence preventing individuals from potential suicide risks at an early stage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.11.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.11/>Identifying Distorted Thinking in Patient-Therapist Text Message Exchanges by Leveraging Dynamic Multi-Turn Context</a></strong><br><a href=/people/k/kevin-lybarger/>Kevin Lybarger</a>
|
<a href=/people/j/justin-tauscher/>Justin Tauscher</a>
|
<a href=/people/x/xiruo-ding/>Xiruo Ding</a>
|
<a href=/people/d/dror-ben-zeev/>Dror Ben-zeev</a>
|
<a href=/people/t/trevor-cohen/>Trevor Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--11><div class="card-body p-3 small">There is growing evidence that mobile text message exchanges between patients and therapists can augment traditional cognitive behavioral therapy. The automatic characterization of patient thinking patterns in this asynchronous text communication may guide treatment and assist in therapist training. In this work, we automatically identify distorted thinking in text-based patient-therapist exchanges, investigating the role of conversation history (context) in distortion prediction. We identify six unique types of cognitive distortions and utilize BERT-based architectures to represent text messages within the context of the conversation. We propose two approaches for leveraging dynamic conversation context in model training. By representing the text messages within the context of the broader patient-therapist conversation, the models better emulate the therapist’s task of recognizing distorted thoughts. This multi-turn classification approach also leverages the clustering of distorted thinking in the conversation timeline. We demonstrate that including conversation context, including the proposed dynamic context methods, improves distortion prediction performance. The proposed architectures and conversation encoding approaches achieve performance comparable to inter-rater agreement. The presence of any distorted thinking is identified with relatively high performance at 0.73 F1, significantly outperforming the best context-agnostic models (0.68 F1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.clpsych-1.12.appendix.pdf data-toggle=tooltip data-placement=top title=Appendix><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.clpsych-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.12/>Learning to Automate Follow-up Question Generation using Process Knowledge for Depression Triage on <span class=acl-fixed-case>R</span>eddit Posts</a></strong><br><a href=/people/s/shrey-gupta/>Shrey Gupta</a>
|
<a href=/people/a/anmol-agarwal/>Anmol Agarwal</a>
|
<a href=/people/m/manas-gaur/>Manas Gaur</a>
|
<a href=/people/k/kaushik-roy/>Kaushik Roy</a>
|
<a href=/people/v/vignesh-narayanan/>Vignesh Narayanan</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a>
|
<a href=/people/a/amit-sheth/>Amit Sheth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--12><div class="card-body p-3 small">Conversational Agents (CAs) powered with deep language models (DLMs) have shown tremendous promise in the domain of mental health. Prominently, the CAs have been used to provide informational or therapeutic services (e.g., cognitive behavioral therapy) to patients. However, the utility of CAs to assist in mental health triaging has not been explored in the existing work as it requires a controlled generation of follow-up questions (FQs), which are often initiated and guided by the mental health professionals (MHPs) in clinical settings. In the context of ‘depression’, our experiments show that DLMs coupled with process knowledge in a mental health questionnaire generate 12.54% and 9.37% better FQs based on similarity and longest common subsequence matches to questions in the PHQ-9 dataset respectively, when compared with DLMs without process knowledge support. Despite coupling with process knowledge, we find that DLMs are still prone to hallucination, i.e., generating redundant, irrelevant, and unsafe FQs. We demonstrate the challenge of using existing datasets to train a DLM for generating FQs that adhere to clinical process knowledge. To address this limitation, we prepared an extended PHQ-9 based dataset, PRIMATE, in collaboration with MHPs. PRIMATE contains annotations regarding whether a particular question in the PHQ-9 dataset has already been answered in the user’s initial description of the mental health condition. We used PRIMATE to train a DLM in a supervised setting to identify which of the PHQ-9 questions can be answered directly from the user’s post and which ones would require more information from the user. Using performance analysis based on MCC scores, we show that PRIMATE is appropriate for identifying questions in PHQ-9 that could guide generative DLMs towards controlled FQ generation (with minimal hallucination) suitable for aiding triaging. The dataset created as a part of this research can be obtained from <a href=https://github.com/primate-mh/Primate2022 class=acl-markup-url>https://github.com/primate-mh/Primate2022</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.13/>Masking Morphosyntactic Categories to Evaluate Salience for Schizophrenia Diagnosis</a></strong><br><a href=/people/y/yaara-shriki/>Yaara Shriki</a>
|
<a href=/people/i/ido-ziv/>Ido Ziv</a>
|
<a href=/people/n/nachum-dershowitz/>Nachum Dershowitz</a>
|
<a href=/people/e/eiran-harel/>Eiran Harel</a>
|
<a href=/people/k/kfir-bar/>Kfir Bar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--13><div class="card-body p-3 small">Natural language processing tools have been shown to be effective for detecting symptoms of schizophrenia in transcribed speech. We analyze and assess the contribution of the various syntactic and morphological categories towards successful machine classification of texts produced by subjects with schizophrenia and by others. Specifically, we fine-tune a language model for the classification task, and mask all words that are attributed with each category of interest. The speech samples were generated in a controlled way by interviewing inpatients who were officially diagnosed with schizophrenia, and a corresponding group of healthy controls. All participants are native Hebrew speakers. Our results show that nouns are the most significant category for classification performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.clpsych-1.14.appendix.pdf data-toggle=tooltip data-placement=top title=Appendix><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.14/>Measuring Linguistic Synchrony in Psychotherapy</a></strong><br><a href=/people/n/natalie-shapira/>Natalie Shapira</a>
|
<a href=/people/d/dana-atzil-slonim/>Dana Atzil-Slonim</a>
|
<a href=/people/r/rivka-tuval-mashiach/>Rivka Tuval Mashiach</a>
|
<a href=/people/o/ori-shapira/>Ori Shapira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--14><div class="card-body p-3 small">We study the phenomenon of linguistic synchrony between clients and therapists in a psychotherapy process. Linguistic Synchrony (LS) can be viewed as any observed interdependence or association between more than one person?s linguistic behavior. Accordingly, we establish LS as a methodological task. We suggest a LS function that applies a linguistic similarity measure based on the Jensen-Shannon distance across the observed part-of-speech tag distributions (JSDuPos) of the speakers in different time frames. We perform a study over a unique corpus of 872 transcribed sessions, covering 68 clients and 59 therapists. After establishing the presence of client-therapist LS, we verify its association with therapeutic alliance and treatment outcome (measured using WAI and ORS), and additionally analyse the behavior of JSDuPos throughout treatment. Results indicate that (1) higher linguistic similarity at the session level associates with higher therapeutic alliance as reported by the client and therapist at the end of the session, (2) higher linguistic similarity at the session level associates with higher level of treatment outcome as reported by the client at the beginnings of the next sessions, (3) there is a significant linear increase in linguistic similarity throughout treatment, (4) surprisingly, higher LS associates with lower treatment outcome. Finally, we demonstrate how the LS function can be used to interpret and explore the mechanism for synchrony.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.clpsych-1.15.appendix.pdf data-toggle=tooltip data-placement=top title=Appendix><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.15/>Nonsuicidal Self-Injury and Substance Use Disorders: A Shared Language of Addiction</a></strong><br><a href=/people/s/salvatore-giorgi/>Salvatore Giorgi</a>
|
<a href=/people/m/mckenzie-himelein-wachowiak/>Mckenzie Himelein-wachowiak</a>
|
<a href=/people/d/daniel-habib/>Daniel Habib</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a>
|
<a href=/people/b/brenda-curtis/>Brenda Curtis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--15><div class="card-body p-3 small">Nonsuicidal self-injury (NSSI), or the deliberate injuring of one?s body without intending to die, has been shown to exhibit many similarities to substance use disorders (SUDs), including population-level characteristics, impulsivity traits, and comorbidity with other mental disorders. Research has further shown that people who self-injure adopt language common in SUD recovery communities (e.g., “clean”, “relapse”, “addiction,” and celebratory language about sobriety milestones). In this study, we investigate the shared language of NSSI and SUD by comparing discussions on public Reddit forums related to self-injury and drug addiction. To this end, we build a set of LDA topics across both NSSI and SUD Reddit users and show that shared language across the two domains includes SUD recovery language in addition to other themes common to support forums (e.g., requests for help and gratitude). Next, we examine Reddit-wide posting activity and note that users posting in <i>r/selfharm</i> also post in many mental health-related subreddits, while users of drug addiction related subreddits do not, despite high comorbidity between NSSI and SUDs. These results show that while people who self-injure may contextualize their disorder as an addiction, their posting habits demonstrate comorbidities with other mental disorders more so than their counterparts in recovery from SUDs. These observations have clinical implications for people who self-injure and seek support by sharing their experiences online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.16/>Overview of the <span class=acl-fixed-case>CLP</span>sych 2022 Shared Task: Capturing Moments of Change in Longitudinal User Posts</a></strong><br><a href=/people/a/adam-tsakalidis/>Adam Tsakalidis</a>
|
<a href=/people/j/jenny-chim/>Jenny Chim</a>
|
<a href=/people/i/iman-munire-bilal/>Iman Munire Bilal</a>
|
<a href=/people/a/ayah-zirikly/>Ayah Zirikly</a>
|
<a href=/people/d/dana-atzil-slonim/>Dana Atzil-Slonim</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a>
|
<a href=/people/m/manas-gaur/>Manas Gaur</a>
|
<a href=/people/k/kaushik-roy/>Kaushik Roy</a>
|
<a href=/people/b/becky-inkster/>Becky Inkster</a>
|
<a href=/people/j/jeff-leintz/>Jeff Leintz</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--16><div class="card-body p-3 small">We provide an overview of the CLPsych 2022 Shared Task, which focusses on the automatic identification of ‘Moments of Change’ in lon- gitudinal posts by individuals on social media and its connection with information regarding mental health . This year’s task introduced the notion of longitudinal modelling of the text generated by an individual online over time, along with appropriate temporally sen- sitive evaluation metrics. The Shared Task con- sisted of two subtasks: (a) the main task of cap- turing changes in an individual’s mood (dras- tic changes-‘Switches’- and gradual changes -‘Escalations’- on the basis of textual content shared online; and subsequently (b) the sub- task of identifying the suicide risk level of an individual – a continuation of the CLPsych 2019 Shared Task– where participants were encouraged to explore how the identification of changes in mood in task (a) can help with assessing suicidality risk in task (b).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.17/>Approximate Nearest Neighbour Extraction Techniques and Neural Networks for Suicide Risk Prediction in the <span class=acl-fixed-case>CLP</span>sych 2022 Shared Task</a></strong><br><a href=/people/h/hermenegildo-fabregat-marcos/>Hermenegildo Fabregat Marcos</a>
|
<a href=/people/a/ander-cejudo/>Ander Cejudo</a>
|
<a href=/people/j/juan-martinez-romo/>Juan Martinez-romo</a>
|
<a href=/people/a/alicia-perez/>Alicia Perez</a>
|
<a href=/people/l/lourdes-araujo/>Lourdes Araujo</a>
|
<a href=/people/n/nuria-lebea/>Nuria Lebea</a>
|
<a href=/people/m/maite-oronoz/>Maite Oronoz</a>
|
<a href=/people/a/arantza-casillas/>Arantza Casillas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--17><div class="card-body p-3 small">This paper describes the participation of our group on the CLPsych 2022 shared task. For task A, which tries to capture changes in mood over time, we have applied an Approximate Nearest Neighbour (ANN) extraction technique with the aim of relabelling the user messages according to their proximity, based on the representation of these messages in a vector space. Regarding the subtask B, we have used the output of the subtask A to train a Recurrent Neural Network (RNN) to predict the risk of suicide at the user level. The results obtained are very competitive considering that our team was one of the few that made use of the organisers’ proposed virtual environment and also made use of the Task A output to predict the Task B results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.clpsych-1.18.appendix.pdf data-toggle=tooltip data-placement=top title=Appendix><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.18.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.18/>Capturing Changes in Mood Over Time in Longitudinal Data Using Ensemble Methodologies</a></strong><br><a href=/people/a/ana-maria-bucur/>Ana-Maria Bucur</a>
|
<a href=/people/h/hyewon-jang/>Hyewon Jang</a>
|
<a href=/people/f/farhana-ferdousi-liza/>Farhana Ferdousi Liza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--18><div class="card-body p-3 small">This paper presents the system description of team BLUE for Task A of the CLPsych 2022 Shared Task on identifying changes in mood and behaviour in longitudinal textual data. These moments of change are signals that can be used to screen and prevent suicide attempts. To detect these changes, we experimented with several text representation methods, such as TF-IDF, sentence embeddings, emotion-informed embeddings and several classical machine learning classifiers. We chose to submit three runs of ensemble systems based on maximum voting on the predictions from the best performing models. Of the nine participating teams in Task A, our team ranked second in the Precision-oriented Coverage-based Evaluation, with a score of 0.499. Our best system was an ensemble of Support Vector Machine, Logistic Regression, and Adaptive Boosting classifiers using emotion-informed embeddings as input representation that can model both the linguistic and emotional information found in users? posts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.clpsych-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.19/>Detecting Moments of Change and Suicidal Risks in Longitudinal User Texts Using Multi-task Learning</a></strong><br><a href=/people/t/tayyaba-azim/>Tayyaba Azim</a>
|
<a href=/people/l/loitongbam-gyanendro-singh/>Loitongbam Gyanendro Singh</a>
|
<a href=/people/s/stuart-e-middleton/>Stuart E. Middleton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--19><div class="card-body p-3 small">This work describes the classification system proposed for the Computational Linguistics and Clinical Psychology (CLPsych) Shared Task 2022. We propose the use of multitask learning approach with bidirectional long-short term memory (Bi-LSTM) model for predicting changes in user’s mood and their suicidal risk level. The two classification tasks have been solved independently or in an augmented way previously, where the output of one task is leveraged for learning another task, however this work proposes an ‘all-in-one’ framework that jointly learns the related mental health tasks. The experimental results suggest that the proposed multi-task framework outperforms the remaining single-task frameworks submitted to the challenge and evaluated via timeline based and coverage based performance metrics shared by the organisers. We also assess the potential of using various types of feature embedding schemes that could prove useful in initialising the Bi-LSTM model for better multitask learning in the mental health domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.20.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.20/>Emotionally-Informed Models for Detecting Moments of Change and Suicide Risk Levels in Longitudinal Social Media Data</a></strong><br><a href=/people/u/ulya-bayram/>Ulya Bayram</a>
|
<a href=/people/l/lamia-benhiba/>Lamia Benhiba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--20><div class="card-body p-3 small">In this shared task, we focus on detecting mental health signals in Reddit users’ posts through two main challenges: A) capturing mood changes (anomalies) from the longitudinal set of posts (called timelines), and B) assessing the users’ suicide risk-levels. Our approaches leverage emotion recognition on linguistic content by computing emotion/sentiment scores using pre-trained BERTs on users’ posts and feeding them to machine learning models, including XGBoost, Bi-LSTM, and logistic regression. For Task-A, we detect longitudinal anomalies using a sequence-to-sequence (seq2seq) autoencoder and capture regions of mood deviations. For Task-B, our two models utilize the BERT emotion/sentiment scores. The first computes emotion bandwidths and merges them with n-gram features, and employs logistic regression to detect users’ suicide risk levels. The second model predicts suicide risk on the timeline level using a Bi-LSTM on Task-A results and sentiment scores. Our results outperformed most participating teams and ranked in the top three in Task-A. In Task-B, our methods surpass all others and return the best macro and micro F1 scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.21.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.21/>Exploring transformers and time lag features for predicting changes in mood over time</a></strong><br><a href=/people/j/john-culnan/>John Culnan</a>
|
<a href=/people/d/damian-romero-diaz/>Damian Romero Diaz</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--21><div class="card-body p-3 small">This paper presents transformer-based models created for the CLPsych 2022 shared task. Using posts from Reddit users over a period of time, we aim to predict changes in mood from post to post. We test models that preserve timeline information through explicit ordering of posts as well as those that do not order posts but preserve features on the length of time between a user’s posts. We find that a model with temporal information may provide slight benefits over the same model without such information, although a RoBERTa transformer model provides enough information to make similar predictions without custom-encoded time information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.22.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.22/>Multi-Task Learning to Capture Changes in Mood Over Time</a></strong><br><a href=/people/p/prasadith-kirinde-gamaarachchige/>Prasadith Kirinde Gamaarachchige</a>
|
<a href=/people/a/ahmed-husseini-orabi/>Ahmed Husseini Orabi</a>
|
<a href=/people/m/mahmoud-husseini-orabi/>Mahmoud Husseini Orabi</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--22><div class="card-body p-3 small">This paper investigates the impact of using Multi-Task Learning (MTL) to predict mood changes over time for each individual (social media user). The presented models were developed as a part of the Computational Linguistics and Clinical Psychology (CLPsych) 2022 shared task. Given the limited number of Reddit social media users, as well as their posts, we decided to experiment with different multi-task learning architectures to identify to what extent knowledge can be shared among similar tasks. Due to class imbalance at both post and user levels and to accommodate task alignment, we randomly sampled an equal number of instances from the respective classes and performed ensemble learning to reduce prediction variance. Faced with several constraints, we managed to produce competitive results that could provide insights into the use of multi-task learning to identify mood changes over time and suicide ideation risk.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.23/>Predicting Moments of Mood Changes Overtime from Imbalanced Social Media Data</a></strong><br><a href=/people/f/falwah-alhamed/>Falwah Alhamed</a>
|
<a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--23><div class="card-body p-3 small">Social media data have been used in research for many years to understand users’ mental health. In this paper, using user-generated content we aim to achieve two goals: the first is detecting moments of mood change over time using timelines of users from Reddit. The second is predicting the degree of suicide risk as a user-level classification task. We used different approaches to address longitudinal modelling as well as the problem of the severely imbalanced dataset. Using BERT with undersampling techniques performed the best among other LSTM and basic random forests models for the first task. For the second task, extracting some features related to suicide from posts’ text contributed to the overall performance improvement. Specifically, a number of suicide-related words in a post as a feature improved the accuracy by 17%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.24/>Towards Capturing Changes in Mood and Identifying Suicidality Risk</a></strong><br><a href=/people/s/sravani-boinepelli/>Sravani Boinepelli</a>
|
<a href=/people/s/shivansh-subramanian/>Shivansh Subramanian</a>
|
<a href=/people/a/abhijeeth-singam/>Abhijeeth Singam</a>
|
<a href=/people/t/tathagata-raha/>Tathagata Raha</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--24><div class="card-body p-3 small">This paper describes our systems for CLPsych?s 2022 Shared Task. Subtask A involves capturing moments of change in an individual?s mood over time, while Subtask B asked us to identify the suicidality risk of a user. We explore multiple machine learning and deep learning methods for the same, taking real-life applicability into account while considering the design of the architecture. Our team achieved top results in different categories for both subtasks. Task A was evaluated on a post-level (using macro averaged F1) and on a window-based timeline level (using macro-averaged precision and recall). We scored a post-level F1 of 0.520 and ranked second with a timeline-level recall of 0.646. Task B was a user-level task where we also came in second with a micro F1 of 0.520 and scored third place on the leaderboard with a macro F1 of 0.380.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.clpsych-1.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--clpsych-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.clpsych-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.clpsych-1.25.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.clpsych-1.25/><span class=acl-fixed-case>WWBP</span>-<span class=acl-fixed-case>SQT</span>-lite: Multi-level Models and Difference Embeddings for Moments of Change Identification in Mental Health Forums</a></strong><br><a href=/people/a/adithya-v-ganesan/>Adithya V Ganesan</a>
|
<a href=/people/v/vasudha-varadarajan/>Vasudha Varadarajan</a>
|
<a href=/people/j/juhi-mittal/>Juhi Mittal</a>
|
<a href=/people/s/shashanka-subrahmanya/>Shashanka Subrahmanya</a>
|
<a href=/people/m/matthew-matero/>Matthew Matero</a>
|
<a href=/people/n/nikita-soni/>Nikita Soni</a>
|
<a href=/people/s/sharath-chandra-guntuku/>Sharath Chandra Guntuku</a>
|
<a href=/people/j/johannes-eichstaedt/>Johannes Eichstaedt</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--clpsych-1--25><div class="card-body p-3 small">Psychological states unfold dynamically; to understand and measure mental health at scale we need to detect and measure these changes from sequences of online posts. We evaluate two approaches to capturing psychological changes in text: the first relies on computing the difference between the embedding of a message with the one that precedes it, the second relies on a “human-aware” multi-level recurrent transformer (HaRT). The mood changes of timeline posts of users were annotated into three classes, ‘ordinary,’ ‘switching’ (positive to negative or vice versa) and ‘escalations’ (increasing in intensity). For classifying these mood changes, the difference-between-embeddings technique – applied to RoBERTa embeddings – showed the highest overall F1 score (0.61) across the three different classes on the test set. The technique particularly outperformed the HaRT transformer (and other baselines) in the detection of switches (F1 = .33) and escalations (F1 = .61).Consistent with the literature, the language use patterns associated with mental-health related constructs in prior work (including depression, stress, anger and anxiety) predicted both mood switches and escalations.</div></div></div><hr><div id=2022dadc-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.dadc-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.dadc-1/>Proceedings of the First Workshop on Dynamic Adversarial Data Collection</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dadc-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dadc-1.0/>Proceedings of the First Workshop on Dynamic Adversarial Data Collection</a></strong><br><a href=/people/m/max-bartolo/>Max Bartolo</a>
|
<a href=/people/h/hannah-kirk/>Hannah Kirk</a>
|
<a href=/people/p/pedro-rodriguez/>Pedro Rodriguez</a>
|
<a href=/people/k/katerina-margatina/>Katerina Margatina</a>
|
<a href=/people/t/tristan-thrush/>Tristan Thrush</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dadc-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dadc-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dadc-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dadc-1.1/>Resilience of Named Entity Recognition Models under Adversarial Attack</a></strong><br><a href=/people/s/sudeshna-das/>Sudeshna Das</a>
|
<a href=/people/j/jiaul-paik/>Jiaul Paik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dadc-1--1><div class="card-body p-3 small">Named entity recognition (NER) is a popular language processing task with wide applications. Progress in NER has been noteworthy, as evidenced by the F1 scores obtained on standard datasets. In practice, however, the end-user uses an NER model on their dataset out-of-the-box, on text that may not be pristine. In this paper we present four model-agnostic adversarial attacks to gauge the resilience of NER models in such scenarios. Our experiments on four state-of-the-art NER methods with five English datasets suggest that the NER models are over-reliant on case information and do not utilise contextual information well. As such, they are highly susceptible to adversarial attacks based on these features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dadc-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dadc-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dadc-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.dadc-1.2/><span class=acl-fixed-case>G</span>rease<span class=acl-fixed-case>V</span>ision: Rewriting the Rules of the Interface</a></strong><br><a href=/people/s/siddhartha-datta/>Siddhartha Datta</a>
|
<a href=/people/k/konrad-kollnig/>Konrad Kollnig</a>
|
<a href=/people/n/nigel-shadbolt/>Nigel Shadbolt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dadc-1--2><div class="card-body p-3 small">Digital harms can manifest across any interface. Key problems in addressing these harms include the high individuality of harms and the fast-changing nature of digital systems. We put forth GreaseVision, a collaborative human-in-the-loop learning framework that enables end-users to analyze their screenomes to annotate harms as well as render overlay interventions. We evaluate HITL intervention development with a set of completed tasks in a cognitive walkthrough, and test scalability with one-shot element removal and fine-tuning hate speech classification models. The contribution of the framework and tool allow individual end-users to study their usage history and create personalized interventions. Our contribution also enables researchers to study the distribution of multi-modal harms and interventions at scale.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dadc-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dadc-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dadc-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.dadc-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.dadc-1.3/>Posthoc Verification and the Fallibility of the Ground Truth</a></strong><br><a href=/people/y/yifan-ding/>Yifan Ding</a>
|
<a href=/people/n/nicholas-botzer/>Nicholas Botzer</a>
|
<a href=/people/t/tim-weninger/>Tim Weninger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dadc-1--3><div class="card-body p-3 small">Classifiers commonly make use of pre-annotated datasets, wherein a model is evaluated by pre-defined metrics on a held-out test set typically made of human-annotated labels. Metrics used in these evaluations are tied to the availability of well-defined ground truth labels, and these metrics typically do not allow for inexact matches. These noisy ground truth labels and strict evaluation metrics may compromise the validity and realism of evaluation results. In the present work, we conduct a systematic label verification experiment on the entity linking (EL) task. Specifically, we ask annotators to verify the correctness of annotations after the fact (, posthoc). Compared to pre-annotation evaluation, state-of-the-art EL models performed extremely well according to the posthoc evaluation methodology. Surprisingly, we find predictions from EL models had a similar or higher verification rate than the ground truth. We conclude with a discussion on these findings and recommendations for future evaluations. The source code, raw results, and evaluation scripts are publicly available via the MIT license at <a href=https://github.com/yifding/e2e_EL_evaluate class=acl-markup-url>https://github.com/yifding/e2e_EL_evaluate</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dadc-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dadc-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dadc-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.dadc-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.dadc-1.4/>Overconfidence in the Face of Ambiguity with Adversarial Data</a></strong><br><a href=/people/m/margaret-li/>Margaret Li</a>
|
<a href=/people/j/julian-michael/>Julian Michael</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dadc-1--4><div class="card-body p-3 small">Adversarial data collection has shown promise as a method for building models which are more robust to the spurious correlations that generally appear in naturalistic data. However, adversarially-collected data may itself be subject to biases, particularly with regard to ambiguous or arguable labeling judgments. Searching for examples where an annotator disagrees with a model might over-sample ambiguous inputs, and filtering the results for high inter-annotator agreement may under-sample them. In either case, training a model on such data may produce predictable and unwanted biases. In this work, we investigate whether models trained on adversarially-collected data are miscalibrated with respect to the ambiguity of their inputs. Using Natural Language Inference models as a testbed, we find no clear difference in accuracy between naturalistically and adversarially trained models, but our model trained only on adversarially-sourced data is considerably more overconfident of its predictions and demonstrates worse calibration, especially on ambiguous inputs. This effect is mitigated, however, when naturalistic and adversarial training data are combined.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dadc-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dadc-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dadc-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dadc-1.5/>longhorns at <span class=acl-fixed-case>DADC</span> 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks.</a></strong><br><a href=/people/v/venelin-kovatchev/>Venelin Kovatchev</a>
|
<a href=/people/t/trina-chatterjee/>Trina Chatterjee</a>
|
<a href=/people/v/venkata-s-govindarajan/>Venkata S Govindarajan</a>
|
<a href=/people/j/jifan-chen/>Jifan Chen</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/g/gabriella-chronis/>Gabriella Chronis</a>
|
<a href=/people/a/anubrata-das/>Anubrata Das</a>
|
<a href=/people/k/katrin-erk/>Katrin Erk</a>
|
<a href=/people/m/matthew-lease/>Matthew Lease</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a>
|
<a href=/people/y/yating-wu/>Yating Wu</a>
|
<a href=/people/k/kyle-mahowald/>Kyle Mahowald</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dadc-1--5><div class="card-body p-3 small">Developing methods to adversarially challenge NLP systems is a promising avenue for improving both model performance and interpretability. Here, we describe the approach of the team “longhorns” on Task 1 of the The First Workshop on Dynamic Adversarial Data Collection (DADC), which asked teams to manually fool a model on an Extractive Question Answering task. Our team finished first (pending validation), with a model error rate of 62%. We advocate for a systematic, linguistically informed approach to formulating adversarial questions, and we describe the results of our pilot experiments, as well as our official submission.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dadc-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dadc-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dadc-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dadc-1.6/>Collecting high-quality adversarial data for machine reading comprehension tasks with humans and models in the loop</a></strong><br><a href=/people/d/damian-y-romero-diaz/>Damian Y. Romero Diaz</a>
|
<a href=/people/m/magdalena-aniol/>Magdalena Anioł</a>
|
<a href=/people/j/john-culnan/>John Culnan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dadc-1--6><div class="card-body p-3 small">We present our experience as annotators in the creation of high-quality, adversarial machine-reading-comprehension data for extractive QA for Task 1 of the First Workshop on Dynamic Adversarial Data Collection (DADC). DADC is an emergent data collection paradigm with both models and humans in the loop. We set up a quasi-experimental annotation design and perform quantitative analyses across groups with different numbers of annotators focusing on successful adversarial attacks, cost analysis, and annotator confidence correlation. We further perform a qualitative analysis of our perceived difficulty of the task given the different topics of the passages in our dataset and conclude with recommendations and suggestions that might be of value to people working on future DADC tasks and related annotation interfaces.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dadc-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dadc-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dadc-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.dadc-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.dadc-1.7/>Generalized Quantifiers as a Source of Error in Multilingual <span class=acl-fixed-case>NLU</span> Benchmarks</a></strong><br><a href=/people/r/ruixiang-cui/>Ruixiang Cui</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dadc-1--7><div class="card-body p-3 small">Logical approaches to representing language have developed and evaluated computational models of quantifier words since the 19th century, but today’s NLU models still struggle to capture their semantics. We rely on Generalized Quantifier Theory for language-independent representations of the semantics of quantifier words, to quantify their contribution to the errors of NLU models. We find that quantifiers are pervasive in NLU benchmarks, and their occurrence at test time is associated with performance drops. Multilingual models also exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse for non-English languages. To facilitate directly-targeted probing, we present an adversarial generalized quantifier NLI task (GQNLI) and show that pre-trained language models have a clear lack of robustness in generalized quantifier reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dadc-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dadc-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dadc-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dadc-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dadc-1.8/>Adversarially Constructed Evaluation Sets Are More Challenging, but May Not Be Fair</a></strong><br><a href=/people/j/jason-phang/>Jason Phang</a>
|
<a href=/people/a/angelica-chen/>Angelica Chen</a>
|
<a href=/people/w/william-huang/>William Huang</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dadc-1--8><div class="card-body p-3 small">Large language models increasingly saturate existing task benchmarks, in some cases outperforming humans, leaving little headroom with which to measure further progress. Adversarial dataset creation, which builds datasets using examples that a target system outputs incorrect predictions for, has been proposed as a strategy to construct more challenging datasets, avoiding the more serious challenge of building more precise benchmarks by conventional means. In this work, we study the impact of applying three common approaches for adversarial dataset creation: (1) filtering out easy examples (AFLite), (2) perturbing examples (TextFooler), and (3) model-in-the-loop data collection (ANLI and AdversarialQA), across 18 different adversary models. We find that all three methods can produce more challenging datasets, with stronger adversary models lowering the performance of evaluated models more. However, the resulting ranking of the evaluated models can also be unstable and highly sensitive to the choice of adversary model. Moreover, we find that AFLite oversamples examples with low annotator agreement, meaning that model comparisons hinge on the examples that are most contentious for humans. We recommend that researchers tread carefully when using adversarial methods for building evaluation datasets.</div></div></div><hr><div id=2022deeplo-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.deeplo-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.deeplo-1/>Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.0/>Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing</a></strong><br><a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/g/gholamreza-reza-haffari/>Gholamreza (Reza) Haffari</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/n/nanyun-violet-peng/>Nanyun (Violet) Peng</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/e/ehsan-shareghi/>Ehsan Shareghi</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.1/>Introducing <span class=acl-fixed-case>Q</span>u<span class=acl-fixed-case>BERT</span>: A Large Monolingual Corpus and <span class=acl-fixed-case>BERT</span> Model for <span class=acl-fixed-case>S</span>outhern <span class=acl-fixed-case>Q</span>uechua</a></strong><br><a href=/people/r/rodolfo-zevallos/>Rodolfo Zevallos</a>
|
<a href=/people/j/john-ortega/>John Ortega</a>
|
<a href=/people/w/william-chen/>William Chen</a>
|
<a href=/people/r/richard-castro/>Richard Castro</a>
|
<a href=/people/n/nuria-bel/>Núria Bel</a>
|
<a href=/people/c/cesar-toshio/>Cesar Toshio</a>
|
<a href=/people/r/renzo-venturas/>Renzo Venturas</a>
|
<a href=/people/h/hilario-aradiel/>Hilario Aradiel</a>
|
<a href=/people/n/nelsi-melgarejo/>Nelsi Melgarejo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--1><div class="card-body p-3 small">The lack of resources for languages in the Americas has proven to be a problem for the creation of digital systems such as machine translation, search engines, chat bots, and more. The scarceness of digital resources for a language causes a higher impact on populations where the language is spoken by millions of people. We introduce the first official large combined corpus for deep learning of an indigenous South American low-resource language spoken by millions called Quechua. Specifically, our curated corpus is created from text gathered from the southern region of Peru where a dialect of Quechua is spoken that has not traditionally been used for digital systems as a target dialect in the past. In order to make our work repeatable by others, we also offer a public, pre-trained, BERT model called QuBERT which is the largest linguistic model ever trained for any Quechua type, not just the southern region dialect. We furthermore test our corpus and its corresponding BERT model on two major tasks: (1) named-entity recognition (NER) and (2) part-of-speech (POS) tagging by using state-of-the-art techniques where we achieve results comparable to other work on higher-resource languages. In this article, we describe the methodology, challenges, and results from the creation of QuBERT which is on par with other state-of-the-art multilingual models for natural language processing achieving between 71 and 74% F1 score on NER and 84–87% on POS tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.2/>Improving Distantly Supervised Document-Level Relation Extraction Through Natural Language Inference</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/g/grace-lee/>Grace Lee</a>
|
<a href=/people/a/andrea-pierleoni/>Andrea Pierleoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--2><div class="card-body p-3 small">The distant supervision (DS) paradigm has been widely used for relation extraction (RE) to alleviate the need for expensive annotations. However, it suffers from noisy labels, which leads to worse performance than models trained on human-annotated data, even when trained using hundreds of times more data. We present a systematic study on the use of natural language inference (NLI) to improve distantly supervised document-level RE. We apply NLI in three scenarios: (i) as a filter for denoising DS labels, (ii) as a filter for model prediction, and (iii) as a standalone RE model. Our results show that NLI filtering consistently improves performance, reducing the performance gap with a model trained on human-annotated data by 2.3 F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.deeplo-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.3/><span class=acl-fixed-case>IDANI</span>: Inference-time Domain Adaptation via Neuron-level Interventions</a></strong><br><a href=/people/o/omer-antverg/>Omer Antverg</a>
|
<a href=/people/e/eyal-ben-david/>Eyal Ben-David</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--3><div class="card-body p-3 small">Large pre-trained models are usually fine-tuned on downstream task data, and tested on unseen data. When the train and test data come from different domains, the model is likely to struggle, as it is not adapted to the test domain. We propose a new approach for domain adaptation (DA), using neuron-level interventions: We modify the representation of each test example in specific neurons, resulting in a counterfactual example from the source domain, which the model is more familiar with. The modified example is then fed back into the model. While most other DA methods are applied during training time, ours is applied during inference only, making it more efficient and applicable. Our experiments show that our method improves performance on unseen domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.deeplo-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.4/>Generating unlabelled data for a tri-training approach in a low resourced <span class=acl-fixed-case>NER</span> task</a></strong><br><a href=/people/h/hugo-boulanger/>Hugo Boulanger</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/s/sophie-rosset/>Sophie Rosset</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--4><div class="card-body p-3 small">Training a tagger for Named Entity Recognition (NER) requires a substantial amount of labeled data in the task domain. Manual labeling is a tedious and complicated task. Semisupervised learning methods can reduce the quantity of labeled data necessary to train a model. However, these methods require large quantities of unlabeled data, which remains an issue in many cases.We address this problem by generating unlabeled data. Large language models have proven to be powerful tools for text generation. We use their generative capacity to produce new sentences and variations of the sentences of our available data. This generation method, combined with a semi-supervised method, is evaluated on CoNLL and I2B2. We prepare both of these corpora to simulate a low resource setting. We obtain significant improvements for semisupervised learning with synthetic data against supervised learning on natural data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.5/><span class=acl-fixed-case>ANTS</span>: A Framework for Retrieval of Text Segments in Unstructured Documents</a></strong><br><a href=/people/b/brian-chivers/>Brian Chivers</a>
|
<a href=/people/m/mason-p-jiang/>Mason P. Jiang</a>
|
<a href=/people/w/wonhee-lee/>Wonhee Lee</a>
|
<a href=/people/a/amy-ng/>Amy Ng</a>
|
<a href=/people/n/natalya-i-rapstine/>Natalya I. Rapstine</a>
|
<a href=/people/a/alex-storer/>Alex Storer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--5><div class="card-body p-3 small">Text segmentation and extraction from unstructured documents can provide business researchers with a wealth of new information on firms and their behaviors. However, the most valuable text is often difficult to extract consistently due to substantial variations in how content can appear from document to document. Thus, the most successful way to extract this content has been through costly crowdsourcing and training of manual workers. We propose the Assisted Neural Text Segmentation (ANTS) framework to identify pertinent text in unstructured documents from a small set of labeled examples. ANTS leverages deep learning and transfer learning architectures to empower researchers to identify relevant text with minimal manual coding. Using a real world sample of accounting documents, we identify targeted sections 96% of the time using only 5 training examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.6/>Cross-<span class=acl-fixed-case>TOP</span>: Zero-Shot Cross-Schema Task-Oriented Parsing</a></strong><br><a href=/people/m/melanie-rubino/>Melanie Rubino</a>
|
<a href=/people/n/nicolas-guenon-des-mesnards/>Nicolas Guenon des Mesnards</a>
|
<a href=/people/u/uday-shah/>Uday Shah</a>
|
<a href=/people/n/nanjiang-jiang/>Nanjiang Jiang</a>
|
<a href=/people/w/weiqi-sun/>Weiqi Sun</a>
|
<a href=/people/k/konstantine-arkoudas/>Konstantine Arkoudas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--6><div class="card-body p-3 small">Deep learning methods have enabled taskoriented semantic parsing of increasingly complex utterances. However, a single model is still typically trained and deployed for each task separately, requiring labeled training data for each, which makes it challenging to support new tasks, even within a single business vertical (e.g., food-ordering or travel booking). In this paper we describe Cross-TOP (Cross-Schema Task-Oriented Parsing), a zero-shot method for complex semantic parsing in a given vertical. By leveraging the fact that user requests from the same vertical share lexical and semantic similarities, a single cross-schema parser is trained to service an arbitrary number of tasks, seen or unseen, within a vertical. We show that Cross-TOP can achieve high accuracy on a previously unseen task without requiring any additional training data, thereby providing a scalable way to bootstrap semantic parsers for new tasks. As part of this work we release the FoodOrdering dataset, a task-oriented parsing dataset in the food-ordering vertical, with utterances and annotations derived from five schemas, each from a different restaurant menu.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.7/>Help from the Neighbors: <span class=acl-fixed-case>E</span>stonian Dialect Normalization Using a <span class=acl-fixed-case>F</span>innish Dialect Generator</a></strong><br><a href=/people/m/mika-hamalainen/>Mika Hämäläinen</a>
|
<a href=/people/k/khalid-alnajjar/>Khalid Alnajjar</a>
|
<a href=/people/t/tuuli-tuisk/>Tuuli Tuisk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--7><div class="card-body p-3 small">While standard Estonian is not a low-resourced language, the different dialects of the language are under-resourced from the point of view of NLP, given that there are no vast hand normalized resources available for training a machine learning model to normalize dialectal Estonian to standard Estonian. In this paper, we crawl a small corpus of parallel dialectal Estonian - standard Estonian sentences. In addition, we take a savvy approach of generating more synthetic training data for the normalization task by using an existing dialect generator model built for Finnish to “dialectalize” standard Estonian sentences from the Universal Dependencies tree banks. Our BERT based normalization model achieves a word error rate that is 26.49 points lower when using both the synthetic data and Estonian data in comparison to training the model with only the available Estonian data. Our results suggest that synthetic data generated by a model trained on a more resourced related language can indeed boost the results for a less resourced language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.deeplo-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.8/>Exploring diversity in back translation for low-resource machine translation</a></strong><br><a href=/people/l/laurie-burchell/>Laurie Burchell</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--8><div class="card-body p-3 small">Back translation is one of the most widely used methods for improving the performance of neural machine translation systems. Recent research has sought to enhance the effectiveness of this method by increasing the ‘diversity’ of the generated translations. We argue that the definitions and metrics used to quantify ‘diversity’ in previous work have been insufficient. This work puts forward a more nuanced framework for understanding diversity in training data, splitting it into lexical diversity and syntactic diversity. We present novel metrics for measuring these different aspects of diversity and carry out empirical analysis into the effect of these types of diversity on final neural machine translation model performance for low-resource English↔Turkish and mid-resource English↔Icelandic. Our findings show that generating back translation using nucleus sampling results in higher final model performance, and that this method of generation has high levels of both lexical and syntactic diversity. We also find evidence that lexical diversity is more important than syntactic for back translation performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.9/>Punctuation Restoration in <span class=acl-fixed-case>S</span>panish Customer Support Transcripts using Transfer Learning</a></strong><br><a href=/people/x/xiliang-zhu/>Xiliang Zhu</a>
|
<a href=/people/s/shayna-gardiner/>Shayna Gardiner</a>
|
<a href=/people/d/david-rossouw/>David Rossouw</a>
|
<a href=/people/t/tere-roldan/>Tere Roldán</a>
|
<a href=/people/s/simon-corston-oliver/>Simon Corston-Oliver</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--9><div class="card-body p-3 small">Automatic Speech Recognition (ASR) systems typically produce unpunctuated transcripts that have poor readability. In addition, building a punctuation restoration system is challenging for low-resource languages, especially for domain-specific applications. In this paper, we propose a Spanish punctuation restoration system designed for a real-time customer support transcription service. To address the data sparsity of Spanish transcripts in the customer support domain, we introduce two transferlearning-based strategies: 1) domain adaptation using out-of-domain Spanish text data; 2) crosslingual transfer learning leveraging in-domain English transcript data. Our experiment results show that these strategies improve the accuracy of the Spanish punctuation restoration system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.deeplo-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.10/>Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and <span class=acl-fixed-case>BERT</span> Models for <span class=acl-fixed-case>M</span>altese</a></strong><br><a href=/people/k/kurt-micallef/>Kurt Micallef</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/m/marc-tanti/>Marc Tanti</a>
|
<a href=/people/l/lonneke-van-der-plas/>Lonneke van der Plas</a>
|
<a href=/people/c/claudia-borg/>Claudia Borg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--10><div class="card-body p-3 small">Multilingual language models such as mBERT have seen impressive cross-lingual transfer to a variety of languages, but many languages remain excluded from these models. In this paper, we analyse the effect of pre-training with monolingual data for a low-resource language that is not included in mBERT – Maltese – with a range of pre-training set ups. We conduct evaluations with the newly pre-trained models on three morphosyntactic tasks – dependency parsing, part-of-speech tagging, and named-entity recognition – and one semantic classification task – sentiment analysis. We also present a newly created corpus for Maltese, and determine the effect that the pre-training data size and domain have on the downstream performance. Our results show that using a mixture of pre-training domains is often superior to using Wikipedia text only. We also find that a fraction of this corpus is enough to make significant leaps in performance over Wikipedia-trained models. We pre-train and compare two models on the new corpus: a monolingual BERT model trained from scratch (BERTu), and a further pretrained multilingual BERT (mBERTu). The models achieve state-of-the-art performance on these tasks, despite the new corpus being considerably smaller than typically used corpora for high-resourced languages. On average, BERTu outperforms or performs competitively with mBERTu, and the largest gains are observed for higher-level tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.11.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.11/>Building an Event Extractor with Only a Few Examples</a></strong><br><a href=/people/p/pengfei-yu/>Pengfei Yu</a>
|
<a href=/people/z/zixuan-zhang/>Zixuan Zhang</a>
|
<a href=/people/c/clare-voss/>Clare Voss</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--11><div class="card-body p-3 small">Supervised event extraction models require a substantial amount of training data to perform well. However, event annotation requires a lot of human effort and costs much time, which limits the application of existing supervised approaches to new event types. In order to reduce manual labor and shorten the time to build an event extraction system for an arbitrary event ontology, we present a new framework to train such systems much more efficiently without large annotations. Our event trigger labeling model uses a weak supervision approach, which only requires a set of keywords, a small number of examples and an unlabeled corpus, on which our approach automatically collects weakly supervised annotations. Our argument role labeling component performs zero-shot learning, which only requires the names of the argument roles of new event types. The source codes of our event trigger detection1 and event argument extraction2 models are publicly available for research purposes. We also release a dockerized system connecting the two models into an unified event extraction pipeline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.deeplo-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.12/>Task Transfer and Domain Adaptation for Zero-Shot Question Answering</a></strong><br><a href=/people/x/xiang-pan/>Xiang Pan</a>
|
<a href=/people/a/alex-sheng/>Alex Sheng</a>
|
<a href=/people/d/david-shimshoni/>David Shimshoni</a>
|
<a href=/people/a/aditya-singhal/>Aditya Singhal</a>
|
<a href=/people/s/sara-rosenthal/>Sara Rosenthal</a>
|
<a href=/people/a/avirup-sil/>Avirup Sil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--12><div class="card-body p-3 small">Pretrained language models have shown success in various areas of natural language processing, including reading comprehension tasks. However, when applying machine learning methods to new domains, labeled data may not always be available. To address this, we use supervised pretraining on source-domain data to reduce sample complexity on domainspecific downstream tasks. We evaluate zeroshot performance on domain-specific reading comprehension tasks by combining task transfer with domain adaptation to fine-tune a pretrained model with no labelled data from the target task. Our approach outperforms DomainAdaptive Pretraining on downstream domainspecific reading comprehension tasks in 3 out of 4 domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.13.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.13/>Let the Model Decide its Curriculum for Multitask Learning</a></strong><br><a href=/people/n/neeraj-varshney/>Neeraj Varshney</a>
|
<a href=/people/s/swaroop-mishra/>Swaroop Mishra</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--13><div class="card-body p-3 small">Curriculum learning strategies in prior multitask learning approaches arrange datasets in a difficulty hierarchy either based on human perception or by exhaustively searching the optimal arrangement. However, human perception of difficulty may not always correlate well with machine interpretation leading to poor performance and exhaustive search is computationally expensive. Addressing these concerns, we propose two classes of techniques to arrange training instances into a learning curriculum based on difficulty scores computed via model-based approaches. The two classes i.e Dataset-level and Instance-level differ in granularity of arrangement. Through comprehensive experiments with 12 datasets, we show that instance-level and dataset-level techniques result in strong representations as they lead to an average performance improvement of 4.17% and 3.15% over their respective baselines. Furthermore, we find that most of this improvement comes from correctly answering the difficult instances, implying a greater efficacy of our techniques on difficult tasks</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.14/><span class=acl-fixed-case>A</span>fri<span class=acl-fixed-case>T</span>e<span class=acl-fixed-case>VA</span>: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models</a></strong><br><a href=/people/o/odunayo-jude-ogundepo/>Odunayo Jude Ogundepo</a>
|
<a href=/people/a/akintunde-oladipo/>Akintunde Oladipo</a>
|
<a href=/people/m/mofetoluwa-adeyemi/>Mofetoluwa Adeyemi</a>
|
<a href=/people/k/kelechi-ogueji/>Kelechi Ogueji</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--14><div class="card-body p-3 small">Pretrained language models represent the state of the art in NLP, but the successful construction of such models often requires large amounts of data and computational resources. Thus, the paucity of data for low-resource languages impedes the development of robust NLP capabilities for these languages. There has been some recent success in pretraining encoderonly models solely on a combination of lowresource African languages, exemplified by AfriBERTa. In this work, we extend the approach of “small data” pretraining to encoder– decoder models. We introduce AfriTeVa, a family of sequence-to-sequence models derived from T5 that are pretrained on 10 African languages from scratch. With a pretraining corpus of only around 1GB, we show that it is possible to achieve competitive downstream effectiveness for machine translation and text classification, compared to larger models trained on much more data. All the code and model checkpoints described in this work are publicly available at <a href=https://github.com/castorini/afriteva class=acl-markup-url>https://github.com/castorini/afriteva</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.15.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.15/>Few-shot Learning for <span class=acl-fixed-case>S</span>umerian Named Entity Recognition</a></strong><br><a href=/people/g/guanghai-wang/>Guanghai Wang</a>
|
<a href=/people/y/yudong-liu/>Yudong Liu</a>
|
<a href=/people/j/james-hearne/>James Hearne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--15><div class="card-body p-3 small">This paper presents our study in exploring the task of named entity recognition (NER) in a low resource setting, focusing on few-shot learning on the Sumerian NER task. The Sumerian language is deemed as an extremely low-resource language due to that (1) it is a long dead language, (2) highly skilled language experts are extremely scarce. NER on Sumerian text is important in that it helps identify the actors and entities active in a given period of time from the collections of tens of thousands of texts in building socio-economic networks of the archives of interest. As a text classification task, NER tends to become challenging when the amount of annotated data is limited or the model is required to handle new classes. The Sumerian NER is no exception. In this work, we propose to use two few-shot learning systems, ProtoBERT and NNShot, to the Sumerian NER task. Our experiments show that the ProtoBERT NER generally outperforms both the NNShot NER and the fully supervised BERT NER in low resource settings on the predictions of rare classes. In particular, F1-score of ProtoBERT on unseen entity types on our test set has achieved 89.6% that is significantly better than the F1-score of 84.3% of the BERT NER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.16/>Deep Learning-Based Morphological Segmentation for Indigenous Languages: A Study Case on Innu-Aimun</a></strong><br><a href=/people/n/ngoc-tan-le/>Ngoc Tan Le</a>
|
<a href=/people/a/antoine-cadotte/>Antoine Cadotte</a>
|
<a href=/people/m/mathieu-boivin/>Mathieu Boivin</a>
|
<a href=/people/f/fatiha-sadat/>Fatiha Sadat</a>
|
<a href=/people/j/jimena-terraza/>Jimena Terraza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--16><div class="card-body p-3 small">Recent advances in the field of deep learning have led to a growing interest in the development of NLP approaches for low-resource and endangered languages. Nevertheless, relatively little research, related to NLP, has been conducted on indigenous languages. These languages are considered to be filled with complexities and challenges that make their study incredibly difficult in the NLP and AI fields. This paper focuses on the morphological segmentation of indigenous languages, an extremely challenging task because of polysynthesis, dialectal variations with rich morpho-phonemics, misspellings and resource-limited scenario issues. The proposed approach, towards a morphological segmentation of Innu-Aimun, an extremely low-resource indigenous language of Canada, is based on deep learning. Experiments and evaluations have shown promising results, compared to state-of-the-art rule-based and unsupervised approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.17.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.17/>Clean or Annotate: How to Spend a Limited Data Collection Budget</a></strong><br><a href=/people/d/derek-chen/>Derek Chen</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--17><div class="card-body p-3 small">Crowdsourcing platforms are often used to collect datasets for training machine learning models, despite higher levels of inaccurate labeling compared to expert labeling. There are two common strategies to manage the impact of such noise: The first involves aggregating redundant annotations, but comes at the expense of labeling substantially fewer examples. Secondly, prior works have also considered using the entire annotation budget to label as many examples as possible and subsequently apply denoising algorithms to implicitly clean the dataset. We find a middle ground and propose an approach which reserves a fraction of annotations to explicitly clean up highly probable error samples to optimize the annotation process. In particular, we allocate a large portion of the labeling budget to form an initial dataset used to train a model. This model is then used to identify specific examples that appear most likely to be incorrect, which we spend the remaining budget to relabel. Experiments across three model variations and four natural language processing tasks show our approach outperforms or matches both label aggregation and advanced denoising methods designed to handle noisy labels when allocated the same finite annotation budget.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.deeplo-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.18/>Unsupervised Knowledge Graph Generation Using Semantic Similarity Matching</a></strong><br><a href=/people/l/lixian-liu/>Lixian Liu</a>
|
<a href=/people/a/amin-omidvar/>Amin Omidvar</a>
|
<a href=/people/z/zongyang-ma/>Zongyang Ma</a>
|
<a href=/people/a/ameeta-agrawal/>Ameeta Agrawal</a>
|
<a href=/people/a/aijun-an/>Aijun An</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--18><div class="card-body p-3 small">Knowledge Graphs (KGs) are directed labeled graphs representing entities and the relationships between them. Most prior work focuses on supervised or semi-supervised approaches which require large amounts of annotated data. While unsupervised approaches do not need labeled training data, most existing methods either generate too many redundant relations or require manual mapping of the extracted relations to a known schema. To address these limitations, we propose an unsupervised method for KG generation that requires neither labeled data nor manual mapping to the predefined relation schema. Instead, our method leverages sentence-level semantic similarity for automatically generating relations between pairs of entities. Our proposed method outperforms two baseline systems when evaluated over four datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.19.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.19/><span class=acl-fixed-case>F</span>ar<span class=acl-fixed-case>F</span>etched: Entity-centric Reasoning and Claim Validation for the <span class=acl-fixed-case>G</span>reek Language based on Textually Represented Environments</a></strong><br><a href=/people/d/dimitris-papadopoulos/>Dimitris Papadopoulos</a>
|
<a href=/people/k/katerina-metropoulou/>Katerina Metropoulou</a>
|
<a href=/people/n/nikolaos-papadakis/>Nikolaos Papadakis</a>
|
<a href=/people/n/nikolaos-matsatsinis/>Nikolaos Matsatsinis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--19><div class="card-body p-3 small">Our collective attention span is shortened by the flood of online information. With FarFetched, we address the need for automated claim validation based on the aggregated evidence derived from multiple online news sources. We introduce an entity-centric reasoning framework in which latent connections between events, actions, or statements are revealed via entity mentions and represented in a graph database. Using entity linking and semantic similarity, we offer a way for collecting and combining information from diverse sources in order to generate evidence relevant to the user’s claim. Then, we leverage textual entailment recognition to quantitatively determine whether this assertion is credible, based on the created evidence. Our approach tries to fill the gap in automated claim validation for less-resourced languages and is showcased on the Greek language, complemented by the training of relevant semantic textual similarity (STS) and natural language inference (NLI) models that are evaluated on translated versions of common benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.20.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.20/>Alternative non-<span class=acl-fixed-case>BERT</span> model choices for the textual classification in low-resource languages and environments</a></strong><br><a href=/people/s/syed-mustavi-maheen/>Syed Mustavi Maheen</a>
|
<a href=/people/m/moshiur-rahman-faisal/>Moshiur Rahman Faisal</a>
|
<a href=/people/m/md-rafakat-rahman/>Md. Rafakat Rahman</a>
|
<a href=/people/m/md-shahriar-karim/>Md. Shahriar Karim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--20><div class="card-body p-3 small">Natural Language Processing (NLP) tasks in non-dominant and low-resource languages have not experienced significant progress. Although pre-trained BERT models are available, GPU-dependency, large memory requirement, and data scarcity often limit their applicability. As a solution, this paper proposes a fusion chain architecture comprised of one or more layers of CNN, LSTM, and BiLSTM and identifies precise configuration and chain length. The study shows that a simpler, CPU-trainable non-BERT fusion CNN + BiLSTM + CNN is sufficient to surpass the textual classification performance of the BERT-related models in resource-limited languages and environments. The fusion architecture competitively approaches the state-of-the-art accuracy in several Bengali NLP tasks and a six-class emotion detection task for a newly developed Bengali dataset. Interestingly, the performance of the identified fusion model, for instance, CNN + BiLSTM + CNN, also holds for other lowresource languages and environments. Efficacy study shows that the CNN + BiLSTM + CNN model outperforms BERT implementation for Vietnamese languages and performs almost equally in English NLP tasks experiencing artificial data scarcity. For the GLUE benchmark and other datasets such as Emotion, IMDB, and Intent classification, the CNN + BiLSTM + CNN model often surpasses or competes with BERT-base, TinyBERT, DistilBERT, and mBERT. Besides, a position-sensitive selfattention layer role further improves the fusion models’ performance in the Bengali emotion classification. The models are also compressible to as low as ≈ 5× smaller through pruning and retraining, making them more viable for resource-constrained environments. Together, this study may help NLP practitioners and serve as a blueprint for NLP model choices in textual classification for low-resource languages and environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.21.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.21/>Generating Complement Data for Aspect Term Extraction with <span class=acl-fixed-case>GPT</span>-2</a></strong><br><a href=/people/a/amir-pouran-ben-veyseh/>Amir Pouran Ben Veyseh</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--21><div class="card-body p-3 small">Aspect Term Extraction (ATE) is the task of identifying the word(s) in a review text toward which the author express an opinion. A major challenges for ATE involve data scarcity that hinder the training of deep sequence taggers to identify rare targets. To overcome these issues, we propose a novel method to better exploit the available labeled data for ATE by computing effective complement sentences to augment the input data and facilitate the aspect term prediction. In particular, we introduce a multistep training procedure that first obtains optimal complement representations and sentences for training data with respect to a deep ATE model. Afterward, we fine-tune the generative language model GPT-2 to allow complement sentence generation at test data. The REINFORCE algorithm is employed to incorporate different expected properties into the reward function to perform the fine-tuning. We perform extensive experiments on the benchmark datasets to demonstrate the benefits of the proposed method that achieve the state-of-the-art performance on different datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.22/>How to Translate Your Samples and Choose Your Shots? Analyzing Translate-train & Few-shot Cross-lingual Transfer</a></strong><br><a href=/people/i/iman-jundi/>Iman Jundi</a>
|
<a href=/people/g/gabriella-lapesa/>Gabriella Lapesa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--22><div class="card-body p-3 small">The lack of resources for languages in the Americas has proven to be a problem for the creation of digital systems such as machine translation, search engines, chat bots, and more. The scarceness of digital resources for a language causes a higher impact on populations where the language is spoken by millions of people. We introduce the first official large combined corpus for deep learning of an indigenous South American low-resource language spoken by millions called Quechua. Specifically, our curated corpus is created from text gathered from the southern region of Peru where a dialect of Quechua is spoken that has not traditionally been used for digital systems as a target dialect in the past. In order to make our work repeatable by others, we also offer a public, pre-trained, BERT model called QuBERT which is the largest linguistic model ever trained for any Quechua type, not just the southern region dialect. We furthermore test our corpus and its corresponding BERT model on two major tasks: (1) named-entity recognition (NER) and (2) part-of-speech (POS) tagging by using state-of-the-art techniques where we achieve results comparable to other work on higher-resource languages. In this article, we describe the methodology, challenges, and results from the creation of QuBERT which is on on par with other state-of-the-art multilingual models for natural language processing achieving between 71 and 74% F1 score on NER and 84–87% on POS tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.deeplo-1.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--deeplo-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.deeplo-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.deeplo-1.23.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.deeplo-1.23/>Unified <span class=acl-fixed-case>NMT</span> models for the <span class=acl-fixed-case>I</span>ndian subcontinent, transcending script-barriers</a></strong><br><a href=/people/g/gokul-n-c/>Gokul N.c.</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--deeplo-1--23><div class="card-body p-3 small">Highly accurate machine translation systems are very important in societies and countries where multilinguality is very common, and where English often does not suffice. The Indian subcontinent (or South Asia) is such a region, with all the Indic languages currently being under-represented in the NLP ecosystem. It is essential to thoroughly explore various techniques to improve the performance of such lowresource languages at least using the data available in open-source, which itself is something not very explored in the Indic ecosystem. In our work, we perform a study with a focus on improving the performance of very-low-resource South Asian languages, especially of countries in addition to India. Specifically, we propose how unified models can be built that can exploit the data from comparatively resource-rich languages of the same region. We propose strategies to unify different types of unexplored scripts, especially Perso–Arabic scripts and Indic scripts to build multilingual models for all the South Asian languages despite the script barrier. We also study how augmentation techniques like back-translation can be made useof to build unified models just using openly available raw data, to understand what levels of improvements can be expected for these Indic languages.</div></div></div><hr><div id=2022distcurate-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.distcurate-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.distcurate-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.distcurate-1/>Proceedings of the Workshop on Dimensions of Meaning: Distributional and Curated Semantics (DistCurate 2022)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.distcurate-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.distcurate-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.distcurate-1.0/>Proceedings of the Workshop on Dimensions of Meaning: Distributional and Curated Semantics (DistCurate 2022)</a></strong><br><a href=/people/c/collin-f-baker/>Collin F. Baker</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.distcurate-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.distcurate-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--distcurate-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.distcurate-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.distcurate-1.1/>A Descriptive Study of Metaphors and Frames in the Multilingual Shared Annotation Task</a></strong><br><a href=/people/m/maucha-gamonal/>Maucha Gamonal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--distcurate-1--1><div class="card-body p-3 small">This work assumes that languages are structured by semantic frames, which are schematic representations of concepts. Metaphors, on the other hand, are cognitive projections between domains, which are the result of our interaction in the world, through experiences, expectations and human biology itself. In this work, we use both semantic frames and metaphors in multilingual contrast (Brazilian Portuguese, English and German). The aim is to present a descriptive study of metaphors and frames in the multilingual shared annotation task of Multilingual FrameNet, a task which consisted of using frames from Berkeley FrameNet to annotate a parallel corpora. The result shows parameters for cross-linguistic annotation considering frames and metaphors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.distcurate-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.distcurate-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--distcurate-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.distcurate-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.distcurate-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.distcurate-1.2/>Multi-sense Language Modelling</a></strong><br><a href=/people/a/andrea-lekkas/>Andrea Lekkas</a>
|
<a href=/people/p/peter-schneider-kamp/>Peter Schneider-Kamp</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--distcurate-1--2><div class="card-body p-3 small">The effectiveness of a language model is influenced by its token representations, which must encode contextual information and handle the same word form having a plurality of meanings (polysemy). Currently, none of the common language modelling architectures explicitly model polysemy. We propose a language model which not only predicts the next word, but also its sense in context. We argue that this higher prediction granularity may be useful for end tasks such as assistive writing, and allow for more a precise linking of language models with knowledge bases. We find that multi-sense language modelling requires architectures that go beyond standard language models, and here propose a localized prediction framework that decomposes the task into a word followed by a sense prediction task. To aid sense prediction, we utilise a Graph Attention Network, which encodes definitions and example uses of word senses. Overall, we find that multi-sense language modelling is a highly challenging task, and suggest that future work focus on the creation of more annotated training datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.distcurate-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.distcurate-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--distcurate-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.distcurate-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.distcurate-1.3/>Logical Story Representations via <span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et + Semantic Parsing</a></strong><br><a href=/people/l/lane-lawley/>Lane Lawley</a>
|
<a href=/people/l/lenhart-schubert/>Lenhart Schubert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--distcurate-1--3><div class="card-body p-3 small">We propose a means of augmenting FrameNet parsers with a formal logic parser to obtain rich semantic representations of events. These schematic representations of the frame events, which we call Episodic Logic (EL) schemas, abstract constants to variables, preserving their types and relationships to other individuals in the same text. Due to the temporal semantics of the chosen logical formalism, all identified schemas in a text are also assigned temporally bound “episodes” and related to one another in time. The semantic role information from the FrameNet frames is also incorporated into the schema’s type constraints. We describe an implementation of this method using a neural FrameNet parser, and discuss the approach’s possible applications to question answering and open-domain event schema learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.distcurate-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.distcurate-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--distcurate-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.distcurate-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.distcurate-1.4/>Comparing Distributional and Curated Approaches for Cross-lingual Frame Alignment</a></strong><br><a href=/people/c/collin-f-baker/>Collin F. Baker</a>
|
<a href=/people/m/michael-ellsworth/>Michael Ellsworth</a>
|
<a href=/people/m/miriam-r-l-petruck/>Miriam R. L. Petruck</a>
|
<a href=/people/a/arthur-lorenzi/>Arthur Lorenzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--distcurate-1--4><div class="card-body p-3 small">Despite advances in statistical approaches to the modeling of meaning, many ques- tions about the ideal way of exploiting both knowledge-based (e.g., FrameNet, WordNet) and data-based methods (e.g., BERT) remain unresolved. This workshop focuses on these questions with three session papers that run the gamut from highly distributional methods (Lekkas et al., 2022), to highly curated methods (Gamonal, 2022), and techniques with statistical methods producing structured semantics (Lawley and Schubert, 2022). In addition, we begin the workshop with a small comparison of cross-lingual techniques for frame semantic alignment for one language pair (Spanish and English). None of the distributional techniques consistently aligns the 1-best frame match from English to Spanish, all failing in at least one case. Predicting which techniques will align which frames cross-linguistically is not possible from any known characteristic of the alignment technique or the frames. Although distributional techniques are a rich source of semantic information for many tasks, at present curated, knowledge-based semantics remains the only technique that can consistently align frames across languages.</div></div></div><hr><div id=2022dlg4nlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dlg4nlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.dlg4nlp-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.dlg4nlp-1/>Proceedings of the 2nd Workshop on Deep Learning on Graphs for Natural Language Processing (DLG4NLP 2022)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dlg4nlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dlg4nlp-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dlg4nlp-1.0/>Proceedings of the 2nd Workshop on Deep Learning on Graphs for Natural Language Processing (DLG4NLP 2022)</a></strong><br><a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/b/bang-liu/>Bang Liu</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/j/jian-pei/>Jian Pei</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dlg4nlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dlg4nlp-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dlg4nlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dlg4nlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.dlg4nlp-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.dlg4nlp-1.1/>Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts</a></strong><br><a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/l/lianhui-qin/>Lianhui Qin</a>
|
<a href=/people/z/zhihan-zhang/>Zhihan Zhang</a>
|
<a href=/people/t/tong-zhao/>Tong Zhao</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dlg4nlp-1--1><div class="card-body p-3 small">Generative commonsense reasoning (GCR) in natural language is to reason about the commonsense while generating coherent text. Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks. Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative explanations for a real-world situation or predict all possible outcomes. Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge. In this paper, we propose MoKGE, a novel method that diversifies the generative reasoning by a mixture of expert (MoE) strategy on commonsense knowledge graphs (KG). A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs. Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks, based on both automatic and human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dlg4nlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dlg4nlp-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dlg4nlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dlg4nlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.dlg4nlp-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.dlg4nlp-1.2/>Improving Neural Machine Translation with the <span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation by Combining Graph and Sequence Transformers</a></strong><br><a href=/people/c/changmao-li/>Changmao Li</a>
|
<a href=/people/j/jeffrey-flanigan/>Jeffrey Flanigan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dlg4nlp-1--2><div class="card-body p-3 small">Previous studies have shown that the Abstract Meaning Representation (AMR) can improve Neural Machine Translation (NMT). However, there has been little work investigating incorporating AMR graphs into Transformer models. In this work, we propose a novel encoder-decoder architecture which augments the Transformer model with a Heterogeneous Graph Transformer (Yao et al., 2020) which encodes source sentence AMR graphs. Experimental results demonstrate the proposed model outperforms the Transformer model and previous non-Transformer based models on two different language pairs in both the high resource setting and low resource setting. Our source code, training corpus and released models are available at <a href=https://github.com/jlab-nlp/amr-nmt class=acl-markup-url>https://github.com/jlab-nlp/amr-nmt</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dlg4nlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dlg4nlp-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dlg4nlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dlg4nlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dlg4nlp-1.3/>Continuous Temporal Graph Networks for Event-Based Graph Data</a></strong><br><a href=/people/j/jin-guo/>Jin Guo</a>
|
<a href=/people/z/zhen-han/>Zhen Han</a>
|
<a href=/people/s/su-zhou/>Su Zhou</a>
|
<a href=/people/j/jiliang-li/>Jiliang Li</a>
|
<a href=/people/v/volker-tresp/>Volker Tresp</a>
|
<a href=/people/y/yuyi-wang/>Yuyi Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dlg4nlp-1--3><div class="card-body p-3 small">There has been an increasing interest in modeling continuous-time dynamics of temporal graph data. Previous methods encode time-evolving relational information into a low-dimensional representation by specifying discrete layers of neural networks, while real-world dynamic graphs often vary continuously over time. Hence, we propose Continuous Temporal Graph Networks (CTGNs) to capture continuous dynamics of temporal graph data. We use both the link starting timestamps and link duration as evolving information to model continuous dynamics of nodes. The key idea is to use neural ordinary differential equations (ODE) to characterize the continuous dynamics of node representations over dynamic graphs. We parameterize ordinary differential equations using a novel graph neural network. The existing dynamic graph networks can be considered as a specific discretization of CTGNs. Experiment results on both transductive and inductive tasks demonstrate the effectiveness of our proposed approach over competitive baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dlg4nlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dlg4nlp-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dlg4nlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dlg4nlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dlg4nlp-1.4/>Scene Graph Parsing via <span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation in Pre-trained Language Models</a></strong><br><a href=/people/w/woo-suk-choi/>Woo Suk Choi</a>
|
<a href=/people/y/yu-jung-heo/>Yu-Jung Heo</a>
|
<a href=/people/d/dharani-punithan/>Dharani Punithan</a>
|
<a href=/people/b/byoung-tak-zhang/>Byoung-Tak Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dlg4nlp-1--4><div class="card-body p-3 small">In this work, we propose the application of abstract meaning representation (AMR) based semantic parsing models to parse textual descriptions of a visual scene into scene graphs, which is the first work to the best of our knowledge. Previous works examined scene graph parsing from textual descriptions using dependency parsing and left the AMR parsing approach as future work since sophisticated methods are required to apply AMR. Hence, we use pre-trained AMR parsing models to parse the region descriptions of visual scenes (i.e. images) into AMR graphs and pre-trained language models (PLM), BART and T5, to parse AMR graphs into scene graphs. The experimental results show that our approach explicitly captures high-level semantics from textual descriptions of visual scenes, such as objects, attributes of objects, and relationships between objects. Our textual scene graph parsing approach outperforms the previous state-of-the-art results by 9.3% in the SPICE metric score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dlg4nlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dlg4nlp-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dlg4nlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dlg4nlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dlg4nlp-1.5/>Graph Neural Networks for Adapting Off-the-shelf General Domain Language Models to Low-Resource Specialised Domains</a></strong><br><a href=/people/m/merieme-bouhandi/>Merieme Bouhandi</a>
|
<a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/t/thierry-hamon/>Thierry Hamon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dlg4nlp-1--5><div class="card-body p-3 small">Language models encode linguistic proprieties and are used as input for more specific models. Using their word representations as-is for specialised and low-resource domains might be less efficient. Methods of adapting them exist, but these models often overlook global information about how words, terms, and concepts relate to each other in a corpus due to their strong reliance on attention. We consider that global information can influence the results of the downstream tasks, and combination with contextual information is performed using graph convolution networks or GCN built on vocabulary graphs. By outperforming baselines, we show that this architecture is profitable for domain-specific tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dlg4nlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dlg4nlp-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dlg4nlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dlg4nlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dlg4nlp-1.6/><span class=acl-fixed-case>G</span>ra<span class=acl-fixed-case>DA</span>: Graph Generative Data Augmentation for Commonsense Reasoning</a></strong><br><a href=/people/a/adyasha-maharana/>Adyasha Maharana</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dlg4nlp-1--6><div class="card-body p-3 small">Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dlg4nlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dlg4nlp-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dlg4nlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dlg4nlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.dlg4nlp-1.7/><span class=acl-fixed-case>L</span>i<span class=acl-fixed-case>GCN</span>: Label-interpretable Graph Convolutional Networks for Multi-label Text Classification</a></strong><br><a href=/people/i/irene-li/>Irene Li</a>
|
<a href=/people/a/aosong-feng/>Aosong Feng</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a>
|
<a href=/people/t/tianxiao-li/>Tianxiao Li</a>
|
<a href=/people/t/toyotaro-suzumura/>Toyotaro Suzumura</a>
|
<a href=/people/r/ruihai-dong/>Ruihai Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dlg4nlp-1--7><div class="card-body p-3 small">Multi-label text classification (MLTC) is an attractive and challenging task in natural language processing (NLP). Compared with single-label text classification, MLTC has a wider range of applications in practice. In this paper, we propose a label-interpretable graph convolutional network model to solve the MLTC problem by modeling tokens and labels as nodes in a heterogeneous graph. In this way, we are able to take into account multiple relationships including token-level relationships. Besides, the model allows better interpretability for predicted labels as the token-label edges are exposed. We evaluate our method on four real-world datasets and it achieves competitive scores against selected baseline methods. Specifically, this model achieves a gain of 0.14 on the F1 score in the small label set MLTC, and 0.07 in the large label set scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.dlg4nlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.dlg4nlp-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--dlg4nlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.dlg4nlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.dlg4nlp-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.dlg4nlp-1.8/>Explicit Graph Reasoning Fusing Knowledge and Contextual Information for Multi-hop Question Answering</a></strong><br><a href=/people/z/zhenyun-deng/>Zhenyun Deng</a>
|
<a href=/people/y/yonghua-zhu/>Yonghua Zhu</a>
|
<a href=/people/q/qianqian-qi/>Qianqian Qi</a>
|
<a href=/people/m/michael-j-witbrock/>Michael Witbrock</a>
|
<a href=/people/p/patricia-riddle/>Patricia Riddle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--dlg4nlp-1--8><div class="card-body p-3 small">Current graph-neural-network-based (GNN-based) approaches to multi-hop questions integrate clues from scattered paragraphs in an entity graph, achieving implicit reasoning by synchronous update of graph node representations using information from neighbours; this is poorly suited for explaining how clues are passed through the graph in hops. In this paper, we describe a structured Knowledge and contextual Information Fusion GNN (KIFGraph) whose explicit multi-hop graph reasoning mimics human step by step reasoning. Specifically, we first integrate clues at multiple levels of granularity (question, paragraph, sentence, entity) as nodes in the graph, connected by edges derived using structured semantic knowledge, then use a contextual encoder to obtain the initial node representations, followed by step-by-step two-stage graph reasoning that asynchronously updates node representations. Each node can be related to its neighbour nodes through fused structured knowledge and contextual information, reliably integrating their answer clues. Moreover, a masked attention mechanism (MAM) filters out noisy or redundant nodes and edges, to avoid ineffective clue propagation in graph reasoning. Experimental results show performance competitive with published models on the HotpotQA dataset.</div></div></div><hr><div id=2022emoji-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.emoji-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.emoji-1/>Proceedings of the Fifth International Workshop on Emoji Understanding and Applications in Social Media</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.emoji-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.emoji-1.0/>Proceedings of the Fifth International Workshop on Emoji Understanding and Applications in Social Media</a></strong><br><a href=/people/s/sanjaya-wijeratne/>Sanjaya Wijeratne</a>
|
<a href=/people/j/jennifer-lee/>Jennifer Lee</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a>
|
<a href=/people/a/amit-sheth/>Amit Sheth</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.emoji-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--emoji-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.emoji-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.emoji-1.1/>Interpreting Emoji with Emoji</a></strong><br><a href=/people/j/jens-reelfs/>Jens Reelfs</a>
|
<a href=/people/t/timon-mohaupt/>Timon Mohaupt</a>
|
<a href=/people/s/sandipan-sikdar/>Sandipan Sikdar</a>
|
<a href=/people/m/markus-strohmaier/>Markus Strohmaier</a>
|
<a href=/people/o/oliver-hohlfeld/>Oliver Hohlfeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--emoji-1--1><div class="card-body p-3 small">We study the extent to which emoji can be used to add interpretability to embeddings of text and emoji. To do so, we extend the POLAR-framework that transforms word embeddings to interpretable counterparts and apply it to word-emoji embeddings trained on four years of messaging data from the Jodel social network. We devise a crowdsourced human judgement experiment to study six usecases, evaluating against words only, what role emoji can play in adding interpretability to word embeddings. That is, we use a revised POLAR approach interpreting words and emoji with words, emoji or both according to human judgement. We find statistically significant trends demonstrating that emoji can be used to interpret other emoji very well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.emoji-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--emoji-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.emoji-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.emoji-1.2/>Beyond emojis: an insight into the <span class=acl-fixed-case>IKON</span> language</a></strong><br><a href=/people/l/laura-meloni/>Laura Meloni</a>
|
<a href=/people/p/phimolporn-hitmeangsong/>Phimolporn Hitmeangsong</a>
|
<a href=/people/b/bernhard-appelhaus/>Bernhard Appelhaus</a>
|
<a href=/people/e/edgar-walthert/>Edgar Walthert</a>
|
<a href=/people/c/cesco-reale/>Cesco Reale</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--emoji-1--2><div class="card-body p-3 small">This paper presents a new iconic language, the IKON language, and its philosophical, linguistic, and graphical principles. We examine some case studies to highlight the semantic complexity of the visual representation of meanings. We also introduce the Iconometer test to validate our icons and their application to the medical domain, through the creation of iconic sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.emoji-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--emoji-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.emoji-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.emoji-1.3/>Emoji semantics/pragmatics: investigating commitment and lying</a></strong><br><a href=/people/b/benjamin-weissman/>Benjamin Weissman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--emoji-1--3><div class="card-body p-3 small">This paper presents the results of two experiments investigating the directness of emoji in constituting speaker meaning. This relationship is examined in two ways, with Experiment 1 testing whether speakers are committed to meanings they communicate via a single emoji and Experiment 2 testing whether that speaker is taken to have lied if that meaning is false and intended to deceive. Results indicate that emoji with high meaning agreement in general (i.e., pictorial representations of concrete objects or foods) reliably commit the speaker to that meaning and can constitute lying. Expressive emoji representing facial expressions and emotional states demonstrate a range of commitment and lie ratings: those with high meaning agreement constitute more commitment and more of a lie than those with less meaning agreement in the first place. Emoji can constitute speaker commitment and they can be lies, but this result does not apply uniformly to all emoji and is instead tied to agreement, conventionality, and lexicalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.emoji-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--emoji-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.emoji-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.emoji-1.4/>Understanding the Sarcastic Nature of Emojis with <span class=acl-fixed-case>S</span>arc<span class=acl-fixed-case>O</span>ji</a></strong><br><a href=/people/v/vandita-grover/>Vandita Grover</a>
|
<a href=/people/h/hema-banati/>Hema Banati</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--emoji-1--4><div class="card-body p-3 small">Identifying sarcasm is a challenging research problem owing to its highly contextual nature. Several researchers have attempted numerous mechanisms to incorporate context, linguistic aspects, and supervised and semi-supervised techniques to determine sarcasm. It has also been noted that emojis in a text may also hold key indicators of sarcasm. However, the availability of sarcasm datasets with emojis is scarce. This makes it challenging to effectively study the sarcastic nature of emojis. In this work, we present SarcOji which has been compiled from five publicly available sarcasm datasets. SarcOji contains labeled English texts which all have emojis. We also analyze SarcOji to determine if there is an incongruence in the polarity of text and emojis used therein. Further, emojis’ usage, occurrences, and positions in the context of sarcasm are also studied in this compiled dataset. With SarcOji we have been able to demonstrate that frequency of occurrence of an emoji and its position are strong indicators of sarcasm. SarcOji dataset is now publicly available with several derived features like sentiment scores of text and emojis, most frequent emoji, and its position in the text. Compilation of the SarcOji dataset is an initial step to enable the study of the role of emojis in communicating sarcasm. SarcOji dataset can also serve as a go-to dataset for various emoji-based sarcasm detection techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.emoji-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--emoji-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.emoji-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.emoji-1.5/>Conducting Cross-Cultural Research on <span class=acl-fixed-case>COVID</span>-19 Memes</a></strong><br><a href=/people/j/jing-ge-stadnyk/>Jing Ge-Stadnyk</a>
|
<a href=/people/l/lusha-sa/>Lusha Sa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--emoji-1--5><div class="card-body p-3 small">A cross-linguistic study of COVID-19 memes should allow scholars and professionals to gain insight into how people engage in socially and politically important issues and how culture has influenced societal responses to the global pandemic. This preliminary study employs framing analysis to examine and compare issues, actors and stances conveyed by both English and Chinese memes. The overall findings point to divergence in the way individuals communicate pandemic-related issues in English-speaking countries versus China, although a few similarities were also identified. ‘Regulation’ is the most common issue addressed by both English and Chinese memes, though the latter does so at a comparatively higher rate. The ‘ordinary people’ image within these memes accounts for the largest percentage in both data sets. Although both Chinese and English memes primarily express negative emotions, the former often occurs on an interpersonal level, whereas the latter aims at criticizing society and certain group of people in general. Lastly, this study proposes explanations for these findings in terms of culture and political environment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.emoji-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--emoji-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.emoji-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.emoji-1.6/>Investigating the Influence of Users Personality on the Ambiguous Emoji Perception</a></strong><br><a href=/people/o/olga-iarygina/>Olga Iarygina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--emoji-1--6><div class="card-body p-3 small">Emojis are an integral part of Internet communication nowadays. Even though, they are supposed to make the text clearer and less dubious, some emojis are ambiguous and can be interpreted in different ways. One of the factors that determine the perception of emojis is the user’s personality. In this work, I conducted an experimental study and investigated how personality traits, measured with a Big Five Inventory (BFI) questionnaire, affect reaction time when interpreting emoji. For a set of emoji, for which there are several possible interpretations, participants had to determine whether the emoji fits the presented context or not. Using regression analysis, I found that conscientiousness and neuroticism significantly predict the reaction time the person needs to decide about the emoji. More conscientious people take longer to resolve ambiguity, while more neurotic people make decisions about ambiguous emoji faster. The knowledge of the relationship between personality and emoji interpretation can lead to effective use of knowledge of people’s characters in personalizing interactive computer systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.emoji-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--emoji-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.emoji-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.emoji-1.7/>Semantic Congruency Facilitates Memory for Emojis</a></strong><br><a href=/people/a/andriana-l-christofalos/>Andriana L. Christofalos</a>
|
<a href=/people/l/laurie-feldman/>Laurie Beth Feldman</a>
|
<a href=/people/h/heather-sheridan/>Heather Sheridan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--emoji-1--7><div class="card-body p-3 small">Emojis can assume different relations with the sentence context in which they occur. While affective elaboration and emoji-word redundancy are frequently investigated in laboratory experiments, the role of emojis in inferential processes has received much less attention. Here, we used an online ratings task and a recognition memory task to investigate whether differences in emoji function within a sentence affect judgments of emoji-text coherence and subsequent recognition accuracy. Emojis that function as synonyms of a target word from the passages were rated as better fitting with the passage (more coherent) than emojis consistent with an inference from the passage, and both types of emojis were rated as more coherent than incongruent (unrelated) emojis. In a recognition test, emojis consistent with the semantic content of passages (synonym and inference emojis) were better recognized than incongruent emojis. Findings of the present study provide corroborating evidence that readers extract semantic information from emojis and then integrate it with surrounding passage content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.emoji-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--emoji-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.emoji-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.emoji-1.8/><span class=acl-fixed-case>E</span>moji<span class=acl-fixed-case>C</span>loud: a Tool for Emoji Cloud Visualization</a></strong><br><a href=/people/y/yunhe-feng/>Yunhe Feng</a>
|
<a href=/people/c/cheng-guo/>Cheng Guo</a>
|
<a href=/people/b/bingbing-wen/>Bingbing Wen</a>
|
<a href=/people/p/peng-sun/>Peng Sun</a>
|
<a href=/people/y/yufei-yue/>Yufei Yue</a>
|
<a href=/people/d/dingwen-tao/>Dingwen Tao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--emoji-1--8><div class="card-body p-3 small">This paper proposes EmojiCloud, an open-source Python-based emoji cloud visualization tool, to generate a quick and straightforward understanding of emojis from the perspective of frequency and importance. EmojiCloud is flexible enough to support diverse drawing shapes, such as rectangles, ellipses, and image masked canvases. We also follow inclusive and personalized design principles to cover the unique emoji designs from seven emoji vendors (e.g., Twitter, Apple, and Windows) and allow users to customize plotted emojis and background colors. We hope EmojiCloud can benefit the whole emoji community due to its flexibility, inclusiveness, and customizability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.emoji-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--emoji-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.emoji-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.emoji-1.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.emoji-1.9/>Graphicon Evolution on the <span class=acl-fixed-case>C</span>hinese Social Media Platform <span class=acl-fixed-case>B</span>ili<span class=acl-fixed-case>B</span>ili</a></strong><br><a href=/people/y/yiqiong-zhang/>Yiqiong Zhang</a>
|
<a href=/people/s/susan-herring/>Susan Herring</a>
|
<a href=/people/s/suifu-gan/>Suifu Gan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--emoji-1--9><div class="card-body p-3 small">This study examines the evolutionary trajectory of graphicons in a 13-year corpus of comments from BiliBili, a popular Chinese video-sharing platform. Findings show that emoticons (kaomoji) rose and fell in frequency, while emojis and stickers are both presently on the rise. Graphicon distributions differ in comments and replies to comments. There is also a strong correlation between the types of graphicons used in comments and their corresponding replies, suggesting a priming effect. Finally, qualitative analysis of the 10 most-frequent kaomojis, emojis, and stickers reveals a trend for each successive graphicon type to become less about emotion expression and more integrated with platform-specific culture and the Chinese language. These findings lend partial support to claims in the literature about graphicon evolution.</div></div></div><hr><div id=2022gebnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.gebnlp-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.gebnlp-1/>Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.0/>Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)</a></strong><br><a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/c/christine-basta/>Christine Basta</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/h/hila-gonen/>Hila Gonen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.1/>Analyzing Hate Speech Data along Racial, Gender and Intersectional Axes</a></strong><br><a href=/people/a/antonis-maronikolakis/>Antonis Maronikolakis</a>
|
<a href=/people/p/philip-baader/>Philip Baader</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--1><div class="card-body p-3 small">To tackle the rising phenomenon of hate speech, efforts have been made towards data curation and analysis. When it comes to analysis of bias, previous work has focused predominantly on race. In our work, we further investigate bias in hate speech datasets along racial, gender and intersectional axes. We identify strong bias against African American English (AAE), masculine and AAE+Masculine tweets, which are annotated as disproportionately more hateful and offensive than from other demographics. We provide evidence that BERT-based models propagate this bias and show that balancing the training data for these protected attributes can lead to fairer models with regards to gender, but not race.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.2/>Analysis of Gender Bias in Social Perception and Judgement Using <span class=acl-fixed-case>C</span>hinese Word Embeddings</a></strong><br><a href=/people/j/jiali-li/>Jiali Li</a>
|
<a href=/people/s/shucheng-zhu/>Shucheng Zhu</a>
|
<a href=/people/y/ying-liu/>Ying Liu</a>
|
<a href=/people/p/pengyuan-liu/>Pengyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--2><div class="card-body p-3 small">Gender is a construction in line with social perception and judgment. An important means of this construction is through languages. When natural language processing tools, such as word embeddings, associate gender with the relevant categories of social perception and judgment, it is likely to cause bias and harm to those groups that do not conform to the mainstream social perception and judgment. Using 12,251 Chinese word embeddings as intermedium, this paper studies the relationship between social perception and judgment categories and gender. The results reveal that these grammatical gender-neutral Chinese word embeddings show a certain gender bias, which is consistent with the mainstream society’s perception and judgment of gender. Men are judged by their actions and perceived as bad, easily-disgusted, bad-tempered and rational roles while women are judged by their appearances and perceived as perfect, either happy or sad, and emotional roles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.3/>Don’t Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information</a></strong><br><a href=/people/t/tomasz-limisiewicz/>Tomasz Limisiewicz</a>
|
<a href=/people/d/david-marecek/>David Mareček</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--3><div class="card-body p-3 small">The representations in large language models contain multiple types of gender information. We focus on two types of such signals in English texts: factual gender information, which is a grammatical or semantic property, and gender bias, which is the correlation between a word and specific gender. We can disentangle the model’s embeddings and identify components encoding both types of information with probing. We aim to diminish the stereotypical bias in the representations while preserving the factual gender signal. Our filtering method shows that it is possible to decrease the bias of gender-neutral profession names without significant deterioration of language modeling capabilities. The findings can be applied to language generation to mitigate reliance on stereotypes while preserving gender agreement in coreferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.4/>Uncertainty and Inclusivity in Gender Bias Annotation: An Annotation Taxonomy and Annotated Datasets of <span class=acl-fixed-case>B</span>ritish <span class=acl-fixed-case>E</span>nglish Text</a></strong><br><a href=/people/l/lucy-havens/>Lucy Havens</a>
|
<a href=/people/m/melissa-terras/>Melissa Terras</a>
|
<a href=/people/b/benjamin-bach/>Benjamin Bach</a>
|
<a href=/people/b/beatrice-alex/>Beatrice Alex</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--4><div class="card-body p-3 small">Mitigating harms from gender biased language in Natural Language Processing (NLP) systems remains a challenge, and the situated nature of language means bias is inescapable in NLP data. Though efforts to mitigate gender bias in NLP are numerous, they often vaguely define gender and bias, only consider two genders, and do not incorporate uncertainty into models. To address these limitations, in this paper we present a taxonomy of gender biased language and apply it to create annotated datasets. We created the taxonomy and annotated data with the aim of making gender bias in language transparent. If biases are communicated clearly, varieties of biased language can be better identified and measured. Our taxonomy contains eleven types of gender biases inclusive of people whose gender expressions do not fit into the binary conceptions of woman and man, and whose gender differs from that they were assigned at birth, while also allowing annotators to document unknown gender information. The taxonomy and annotated data will, in future work, underpin analysis and more equitable language model development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.5/>Debiasing Neural Retrieval via In-batch Balancing Regularization</a></strong><br><a href=/people/y/yuantong-li/>Yuantong Li</a>
|
<a href=/people/x/xiaokai-wei/>Xiaokai Wei</a>
|
<a href=/people/z/zijian-wang/>Zijian Wang</a>
|
<a href=/people/s/shen-wang/>Shen Wang</a>
|
<a href=/people/p/parminder-bhatia/>Parminder Bhatia</a>
|
<a href=/people/x/xiaofei-ma/>Xiaofei Ma</a>
|
<a href=/people/a/andrew-arnold/>Andrew Arnold</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--5><div class="card-body p-3 small">People frequently interact with information retrieval (IR) systems, however, IR models exhibit biases and discrimination towards various demographics. The in-processing fair ranking methods provides a trade-offs between accuracy and fairness through adding a fairness-related regularization term in the loss function. However, there haven’t been intuitive objective functions that depend on the click probability and user engagement to directly optimize towards this. In this work, we propose the <b>I</b>n-<b>B</b>atch <b>B</b>alancing <b>R</b>egularization (IBBR) to mitigate the ranking disparity among subgroups. In particular, we develop a differentiable <b>normed Pairwise Ranking Fairness</b> (nPRF) and leverage the T-statistics on top of nPRF over subgroups as a regularization to improve fairness. Empirical results with the BERT-based neural rankers on the MS MARCO Passage Retrieval dataset with the human-annotated non-gendered queries benchmark (CITATION) show that our IBBR method with nPRF achieves significantly less bias with minimal degradation in ranking performance compared with the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.gebnlp-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.6/>Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning</a></strong><br><a href=/people/p/przemyslaw-joniak/>Przemyslaw Joniak</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--6><div class="card-body p-3 small">Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue. We demonstrate a novel framework for inspecting bias in pre-trained transformer-based language models via movement pruning. Given a model and a debiasing objective, our framework finds a subset of the model containing less bias than the original model. We implement our framework by pruning the model while fine-tuning it on the debasing objective. Optimized are only the pruning scores – parameters coupled with the model’s weights that act as gates. We experiment with pruning attention heads, an important building block of transformers: we prune square blocks, as well as establish a new way of pruning the entire heads. Lastly, we demonstrate the usage of our framework using gender bias, and based on our findings, we propose an improvement to an existing debiasing method. Additionally, we re-discover a bias-performance trade-off: the better the model performs, the more bias it contains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.7/>Gendered Language in Resumes and its Implications for Algorithmic Bias in Hiring</a></strong><br><a href=/people/p/prasanna-parasurama/>Prasanna Parasurama</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--7><div class="card-body p-3 small">Despite growing concerns around gender bias in NLP models used in algorithmic hiring, there is little empirical work studying the extent and nature of gendered language in resumes. Using a corpus of 709k resumes from IT firms, we train a series of models to classify the gender of the applicant, thereby measuring the extent of gendered information encoded in resumes. We also investigate whether it is possible to obfuscate gender from resumes by removing gender identifiers, hobbies, gender sub-space in embedding models, etc. We find that there is a significant amount of gendered information in resumes even after obfuscation.A simple Tf-Idf model can learn to classify gender with AUROC=0.75, and more sophisticated transformer-based models achieve AUROC=0.8.We further find that gender predictive values have low correlation with gender direction of embeddings – meaning that, what is predictive of gender is much more than what is “gendered” in the masculine/feminine sense. We discuss the algorithmic bias and fairness implications of these findings in the hiring context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.gebnlp-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.8/>The Birth of Bias: A case study on the evolution of gender bias in an <span class=acl-fixed-case>E</span>nglish language model</a></strong><br><a href=/people/o/oskar-van-der-wal/>Oskar Van Der Wal</a>
|
<a href=/people/j/jaap-jumelet/>Jaap Jumelet</a>
|
<a href=/people/k/katrin-schulz/>Katrin Schulz</a>
|
<a href=/people/w/willem-zuidema/>Willem Zuidema</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--8><div class="card-body p-3 small">Detecting and mitigating harmful biases in modern language models are widely recognized as crucial, open problems. In this paper, we take a step back and investigate how language models come to be biased in the first place. We use a relatively small language model, using the LSTM architecture trained on an English Wikipedia corpus. With full access to the data and to the model parameters as they change during every step while training, we can map in detail how the representation of gender develops, what patterns in the dataset drive this, and how the model’s internal state relates to the bias in a downstream task (semantic textual similarity).We find that the representation of gender is dynamic and identify different phases during training. Furthermore, we show that gender information is represented increasingly locally in the input embeddings of the model and that, as a consequence, debiasing these can be effective in reducing the downstream bias. Monitoring the training dynamics, allows us to detect an asymmetry in how the female and male gender are represented in the input embeddings. This is important, as it may cause naive mitigation strategies to introduce new undesirable biases. We discuss the relevance of the findings for mitigation strategies more generally and the prospects of generalizing our methods to larger language models, the Transformer architecture, other languages and other undesirable biases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.gebnlp-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.9/>Challenges in Measuring Bias via Open-Ended Language Generation</a></strong><br><a href=/people/a/afra-feyza-akyurek/>Afra Feyza Akyürek</a>
|
<a href=/people/m/muhammed-yusuf-kocyigit/>Muhammed Yusuf Kocyigit</a>
|
<a href=/people/s/sejin-paik/>Sejin Paik</a>
|
<a href=/people/d/derry-tanti-wijaya/>Derry Tanti Wijaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--9><div class="card-body p-3 small">Researchers have devised numerous ways to quantify social biases vested in pretrained language models. As some language models are capable of generating coherent completions given a set of textual prompts, several prompting datasets have been proposed to measure biases between social groups—posing language generation as a way of identifying biases. In this opinion paper, we analyze how specific choices of prompt sets, metrics, automatic tools and sampling strategies affect bias results. We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings. We additionally provide recommendations for reporting biases in open-ended language generation for a more complete outlook of biases exhibited by a given language model. Code to reproduce the results is released under <a href=https://github.com/feyzaakyurek/bias-textgen class=acl-markup-url>https://github.com/feyzaakyurek/bias-textgen</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.10/>Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models</a></strong><br><a href=/people/t/tejas-srinivasan/>Tejas Srinivasan</a>
|
<a href=/people/y/yonatan-bisk/>Yonatan Bisk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--10><div class="card-body p-3 small">Numerous works have analyzed biases in vision and pre-trained language models individually - however, less attention has been paid to how these biases interact in multimodal settings. This work extends text-based bias analysis methods to investigate multimodal language models, and analyzes intra- and inter-modality associations and biases learned by these models. Specifically, we demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often preferring to reinforce a stereotype over faithfully describing the visual scene. We demonstrate these findings on a controlled case-study and extend them for a larger set of stereotypically gendered entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.11/>Assessing Group-level Gender Bias in Professional Evaluations: The Case of Medical Student End-of-Shift Feedback</a></strong><br><a href=/people/e/emmy-liu/>Emmy Liu</a>
|
<a href=/people/m/michael-henry-tessler/>Michael Henry Tessler</a>
|
<a href=/people/n/nicole-dubosh/>Nicole Dubosh</a>
|
<a href=/people/k/katherine-hiller/>Katherine Hiller</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--11><div class="card-body p-3 small">Though approximately 50% of medical school graduates today are women, female physicians tend to be underrepresented in senior positions, make less money than their male counterparts and receive fewer promotions. There is a growing body of literature demonstrating gender bias in various forms of evaluation in medicine, but this work was mainly conducted by looking for specific words using fixed dictionaries such as LIWC and focused on global assessments of performance such as recommendation letters. We use a dataset of written and quantitative assessments of medical student performance on individual shifts of work, collected across multiple institutions, to investigate the extent to which gender bias exists in a day-to-day context for medical students. We investigate differences in the narrative comments given to male and female students by both male or female faculty assessors, using a fine-tuned BERT model. This allows us to examine whether groups are written about in systematically different ways, without relying on hand-crafted wordlists or topic models. We compare these results to results from the traditional LIWC method and find that, although we find no evidence of group-level gender bias in this dataset, terms related to family and children are used more in feedback given to women.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.12/>On the Dynamics of Gender Learning in Speech Translation</a></strong><br><a href=/people/b/beatrice-savoldi/>Beatrice Savoldi</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/l/luisa-bentivogli/>Luisa Bentivogli</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--12><div class="card-body p-3 small">Due to the complexity of bias and the opaque nature of current neural approaches, there is a rising interest in auditing language technologies. In this work, we contribute to such a line of inquiry by exploring the emergence of gender bias in Speech Translation (ST). As a new perspective, rather than focusing on the final systems only, we examine their evolution over the course of training. In this way, we are able to account for different variables related to the learning dynamics of gender translation, and investigate when and how gender divides emerge in ST. Accordingly, for three language pairs (en ? es, fr, it) we compare how ST systems behave for masculine and feminine translation at several levels of granularity. We find that masculine and feminine curves are dissimilar, with the feminine one being characterized by more erratic behaviour and late improvements over the course of training. Also, depending on the considered phenomena, their learning trends can be either antiphase or parallel. Overall, we show how such a progressive analysis can inform on the reliability and time-wise acquisition of gender, which is concealed by static evaluations and standard metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.13.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.gebnlp-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.13/>Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias</a></strong><br><a href=/people/y/yarden-tal/>Yarden Tal</a>
|
<a href=/people/i/inbal-magar/>Inbal Magar</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--13><div class="card-body p-3 small">The size of pretrained models is increasing, and so is their performance on a variety of NLP tasks. However, as their memorization capacity grows, they might pick up more social biases. In this work, we examine the connection between model size and its gender bias (specifically, occupational gender bias). We measure bias in three masked language model families (RoBERTa, DeBERTa, and T5) in two setups: directly using prompt based method, and using a downstream task (Winogender). We find on the one hand that larger models receive higher bias scores on the former task, but when evaluated on the latter, they make fewer gender errors. To examine these potentially conflicting results, we carefully investigate the behavior of the different models on Winogender. We find that while larger models outperform smaller ones, the probability that their mistakes are caused by gender bias is higher. Moreover, we find that the proportion of stereotypical errors compared to anti-stereotypical ones grows with the model size. Our findings highlight the potential risks that can arise from increasing model size.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.14/>Unsupervised Mitigating Gender Bias by Character Components: A Case Study of <span class=acl-fixed-case>C</span>hinese Word Embedding</a></strong><br><a href=/people/x/xiuying-chen/>Xiuying Chen</a>
|
<a href=/people/m/mingzhe-li/>Mingzhe Li</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a>
|
<a href=/people/x/xin-gao/>Xin Gao</a>
|
<a href=/people/x/xiangliang-zhang/>Xiangliang Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--14><div class="card-body p-3 small">Word embeddings learned from massive text collections have demonstrated significant levels of discriminative biases. However, debias on the Chinese language, one of the most spoken languages, has been less explored. Meanwhile, existing literature relies on manually created supplementary data, which is time- and energy-consuming. In this work, we propose the first Chinese Gender-neutral word Embedding model (CGE) based on Word2vec, which learns gender-neutral word embeddings without any labeled data. Concretely, CGE utilizes and emphasizes the rich feminine and masculine information contained in radicals, i.e., a kind of component in Chinese characters, during the training procedure. This consequently alleviates discriminative gender biases. Experimental results on public benchmark datasets show that our unsupervised method outperforms the state-of-the-art supervised debiased word embedding models without sacrificing the functionality of the embedding model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.15.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.15/>An Empirical Study on the Fairness of Pre-trained Word Embeddings</a></strong><br><a href=/people/e/emeralda-sesari/>Emeralda Sesari</a>
|
<a href=/people/m/max-hort/>Max Hort</a>
|
<a href=/people/f/federica-sarro/>Federica Sarro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--15><div class="card-body p-3 small">Pre-trained word embedding models are easily distributed and applied, as they alleviate users from the effort to train models themselves. With widely distributed models, it is important to ensure that they do not exhibit undesired behaviour, such as biases against population groups. For this purpose, we carry out an empirical study on evaluating the bias of 15 publicly available, pre-trained word embeddings model based on three training algorithms (GloVe, word2vec, and fastText) with regard to four bias metrics (WEAT, SEMBIAS,DIRECT BIAS, and ECT). The choice of word embedding models and bias metrics is motivated by a literature survey over 37 publications which quantified bias on pre-trained word embeddings. Our results indicate that fastText is the least biased model (in 8 out of 12 cases) and small vector lengths lead to a higher bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.gebnlp-1.16.dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.16/>Mitigating Gender Stereotypes in <span class=acl-fixed-case>H</span>indi and <span class=acl-fixed-case>M</span>arathi</a></strong><br><a href=/people/n/neeraja-kirtane/>Neeraja Kirtane</a>
|
<a href=/people/t/tanvi-anand/>Tanvi Anand</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--16><div class="card-body p-3 small">As the use of natural language processing increases in our day-to-day life, the need to address gender bias inherent in these systems also amplifies. This is because the inherent bias interferes with the semantic structure of the output of these systems while performing tasks in natural language processing. While research is being done in English to quantify and mitigate bias, debiasing methods in Indic Languages are either relatively nascent or absent for some Indic languages altogether. Most Indic languages are gendered, i.e., each noun is assigned a gender according to each language’s rules of grammar. As a consequence, evaluation differs from what is done in English. This paper evaluates the gender stereotypes in Hindi and Marathi languages. The methodologies will differ from the ones in the English language because there are masculine and feminine counterparts in the case of some words. We create a dataset of neutral and gendered occupation words, emotion words and measure bias with the help of Embedding Coherence Test (ECT) and Relative Norm Distance (RND). We also attempt to mitigate this bias from the embeddings. Experiments show that our proposed debiasing techniques reduce gender bias in these languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.17.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.17/>Choose Your Lenses: Flaws in Gender Bias Evaluation</a></strong><br><a href=/people/h/hadas-orgad/>Hadas Orgad</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--17><div class="card-body p-3 small">Considerable efforts to measure and mitigate gender bias in recent years have led to the introduction of an abundance of tasks, datasets, and metrics used in this vein. In this position paper, we assess the current paradigm of gender bias evaluation and identify several flaws in it. First, we highlight the importance of extrinsic bias metrics that measure how a model’s performance on some task is affected by gender, as opposed to intrinsic evaluations of model representations, which are less strongly connected to specific harms to people interacting with systems. We find that only a few extrinsic metrics are measured in most studies, although more can be measured. Second, we find that datasets and metrics are often coupled, and discuss how their coupling hinders the ability to obtain reliable conclusions, and how one may decouple them. We then investigate how the choice of the dataset and its composition, as well as the choice of the metric, affect bias measurement, finding significant variations across each of them. Finally, we propose several guidelines for more reliable gender bias evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.18/>A Taxonomy of Bias-Causing Ambiguities in Machine Translation</a></strong><br><a href=/people/m/michal-mechura/>Michal Měchura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--18><div class="card-body p-3 small">This paper introduces a taxonomy of phenomena which cause bias in machine translation, covering gender bias (people being male and/or female), number bias (singular you versus plural you) and formality bias (informal you versus formal you). Our taxonomy is a formalism for describing situations in machine translation when the source text leaves some of these properties unspecified (eg. does not say whether doctor is male or female) but the target language requires the property to be specified (eg. because it does not have a gender-neutral word for doctor). The formalism described here is used internally by a web-based tool we have built for detecting and correcting bias in the output of any machine translator.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.19/>On Gender Biases in Offensive Language Classification Models</a></strong><br><a href=/people/s/sanjana-marce/>Sanjana Marcé</a>
|
<a href=/people/a/adam-poliak/>Adam Poliak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--19><div class="card-body p-3 small">We explore whether neural Natural Language Processing models trained to identify offensive language in tweets contain gender biases. We add historically gendered and gender ambiguous American names to an existing offensive language evaluation set to determine whether models? predictions are sensitive or robust to gendered names. While we see some evidence that these models might be prone to biased stereotypes that men use more offensive language than women, our results indicate that these models? binary predictions might not greatly change based upon gendered names.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.gebnlp-1.20.note.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.20/>Gender Bias in <span class=acl-fixed-case>BERT</span> - Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task</a></strong><br><a href=/people/s/sophie-jentzsch/>Sophie Jentzsch</a>
|
<a href=/people/c/cigdem-turan/>Cigdem Turan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--20><div class="card-body p-3 small">Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT?s biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.gebnlp-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.21/>Occupational Biases in <span class=acl-fixed-case>N</span>orwegian and Multilingual Language Models</a></strong><br><a href=/people/s/samia-touileb/>Samia Touileb</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--21><div class="card-body p-3 small">In this paper we explore how a demographic distribution of occupations, along gender dimensions, is reflected in pre-trained language models. We give a descriptive assessment of the distribution of occupations, and investigate to what extent these are reflected in four Norwegian and two multilingual models. To this end, we introduce a set of simple bias probes, and perform five different tasks combining gendered pronouns, first names, and a set of occupations from the Norwegian statistics bureau. We show that language specific models obtain more accurate results, and are much closer to the real-world distribution of clearly gendered occupations. However, we see that none of the models have correct representations of the occupations that are demographically balanced between genders. We also discuss the importance of the training data on which the models were trained on, and argue that template-based bias probes can sometimes be fragile, and a simple alteration in a template can change a model’s behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.22.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.gebnlp-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.22/>Looking for a Handsome Carpenter! Debiasing <span class=acl-fixed-case>GPT</span>-3 Job Advertisements</a></strong><br><a href=/people/c/conrad-borchers/>Conrad Borchers</a>
|
<a href=/people/d/dalia-gala/>Dalia Gala</a>
|
<a href=/people/b/benjamin-gilburt/>Benjamin Gilburt</a>
|
<a href=/people/e/eduard-oravkin/>Eduard Oravkin</a>
|
<a href=/people/w/wilfried-bounsi/>Wilfried Bounsi</a>
|
<a href=/people/y/yuki-m-asano/>Yuki M Asano</a>
|
<a href=/people/h/hannah-kirk/>Hannah Kirk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--22><div class="card-body p-3 small">The growing capability and availability of generative language models has enabled a wide range of new downstream tasks. Academic research has identified, quantified and mitigated biases present in language models but is rarely tailored to downstream tasks where wider impact on individuals and society can be felt. In this work, we leverage one popular generative language model, GPT-3, with the goal of writing unbiased and realistic job advertisements. We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2022.gebnlp-1.23.dataset.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.23.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.gebnlp-1.23" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.23/><span class=acl-fixed-case>H</span>etero<span class=acl-fixed-case>C</span>orpus: A Corpus for Heteronormative Language Detection</a></strong><br><a href=/people/j/juan-vasquez/>Juan Vásquez</a>
|
<a href=/people/g/gemma-bel-enguix/>Gemma Bel-Enguix</a>
|
<a href=/people/s/scott-thomas-andersen/>Scott Thomas Andersen</a>
|
<a href=/people/s/sergio-luis-ojeda-trueba/>Sergio-Luis Ojeda-Trueba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--23><div class="card-body p-3 small">In recent years, plenty of work has been done by the NLP community regarding gender bias detection and mitigation in language systems. Yet, to our knowledge, no one has focused on the difficult task of heteronormative language detection and mitigation. We consider this an urgent issue, since language technologies are growing increasingly present in the world and, as it has been proven by various studies, NLP systems with biases can create real-life adverse consequences for women, gender minorities and racial minorities and queer people. For these reasons, we propose and evaluate HeteroCorpus; a corpus created specifically for studying heterononormative language in English. Additionally, we propose a baseline set of classification experiments on our corpus, in order to show the performance of our corpus in classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.24.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.24/>Evaluating Gender Bias Transfer from Film Data</a></strong><br><a href=/people/a/amanda-bertsch/>Amanda Bertsch</a>
|
<a href=/people/a/ashley-oh/>Ashley Oh</a>
|
<a href=/people/s/sanika-natu/>Sanika Natu</a>
|
<a href=/people/s/swetha-gangu/>Swetha Gangu</a>
|
<a href=/people/a/alan-w-black/>Alan W. Black</a>
|
<a href=/people/e/emma-strubell/>Emma Strubell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--24><div class="card-body p-3 small">Films are a rich source of data for natural language processing. OpenSubtitles (Lison and Tiedemann, 2016) is a popular movie script dataset, used for training models for tasks such as machine translation and dialogue generation. However, movies often contain biases that reflect society at the time, and these biases may be introduced during pre-training and influence downstream models. We perform sentiment analysis on template infilling (Kurita et al., 2019) and the Sentence Embedding Association Test (May et al., 2019) to measure how BERT-based language models change after continued pre-training on OpenSubtitles. We consider gender bias as a primary motivating case for this analysis, while also measuring other social biases such as disability. We show that sentiment analysis on template infilling is not an effective measure of bias due to the rarity of disability and gender identifying tokens in the movie dialogue. We extend our analysis to a longitudinal study of bias in film dialogue over the last 110 years and find that continued pre-training on OpenSubtitles encodes additional bias into BERT. We show that BERT learns associations that reflect the biases and representation of each film era, suggesting that additional care must be taken when using historical data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.25/>Indigenous Language Revitalization and the Dilemma of Gender Bias</a></strong><br><a href=/people/o/oussama-hansal/>Oussama Hansal</a>
|
<a href=/people/n/ngoc-tan-le/>Ngoc Tan Le</a>
|
<a href=/people/f/fatiha-sadat/>Fatiha Sadat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--25><div class="card-body p-3 small">Natural Language Processing (NLP), through its several applications, has been considered as one of the most valuable field in interdisciplinary researches, as well as in computer science. However, it is not without its flaws. One of the most common flaws is bias. This paper examines the main linguistic challenges of Inuktitut, an indigenous language of Canada, and focuses on gender bias identification and mitigation. We explore the unique characteristics of this language to help us understand the right techniques that can be used to identify and mitigate implicit biases. We use some methods to quantify the gender bias existing in Inuktitut word embeddings; then we proceed to mitigate the bias and evaluate the performance of the debiased embeddings. Next, we explain how approaches for detecting and reducing bias in English embeddings may be transferred to Inuktitut embeddings by properly taking into account the language’s particular characteristics. Next, we compare the effect of the debiasing techniques on Inuktitut and English. Finally, we highlight some future research directions which will further help to push the boundaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.26.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.26/>What changed? Investigating Debiasing Methods using Causal Mediation Analysis</a></strong><br><a href=/people/s/sullam-jeoung/>Sullam Jeoung</a>
|
<a href=/people/j/jana-diesner/>Jana Diesner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--26><div class="card-body p-3 small">Previous work has examined how debiasing language models affect downstream tasks, specifically, how debiasing techniques influence task performance and whether debiased models also make impartial predictions in downstream tasks or not. However, what we don’t understand well yet is why debiasing methods have varying impacts on downstream tasks and how debiasing techniques affect internal components of language models, i.e., neurons, layers, and attentions. In this paper, we decompose the internal mechanisms of debiasing language models with respect to gender by applying causal mediation analysis to understand the influence of debiasing methods on toxicity detection as a downstream task. Our findings suggest a need to test the effectiveness of debiasing methods with different bias metrics, and to focus on changes in the behavior of certain components of the models, e.g.,first two layers of language models, and attention heads.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.27.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.27/>Why Knowledge Distillation Amplifies Gender Bias and How to Mitigate from the Perspective of <span class=acl-fixed-case>D</span>istil<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/j/jaimeen-ahn/>Jaimeen Ahn</a>
|
<a href=/people/h/hwaran-lee/>Hwaran Lee</a>
|
<a href=/people/j/jinhwa-kim/>Jinhwa Kim</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--27><div class="card-body p-3 small">Knowledge distillation is widely used to transfer the language understanding of a large model to a smaller model. However, after knowledge distillation, it was found that the smaller model is more biased by gender compared to the source large model. This paper studies what causes gender bias to increase after the knowledge distillation process. Moreover, we suggest applying a variant of the mixup on knowledge distillation, which is used to increase generalizability during the distillation process, not for augmentation. By doing so, we can significantly reduce the gender bias amplification after knowledge distillation. We also conduct an experiment on the GLUE benchmark to demonstrate that even if the mixup is applied, it does not have a significant adverse effect on the model’s performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.gebnlp-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.gebnlp-1.28.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--gebnlp-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.gebnlp-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.gebnlp-1.28/>Incorporating Subjectivity into Gendered Ambiguous Pronoun (<span class=acl-fixed-case>GAP</span>) Resolution using Style Transfer</a></strong><br><a href=/people/k/kartikey-pant/>Kartikey Pant</a>
|
<a href=/people/t/tanvi-dadu/>Tanvi Dadu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--gebnlp-1--28><div class="card-body p-3 small">The GAP dataset is a Wikipedia-based evaluation dataset for gender bias detection in coreference resolution, containing mostly objective sentences. Since subjectivity is ubiquitous in our daily texts, it becomes necessary to evaluate models for both subjective and objective instances. In this work, we present a new evaluation dataset for gender bias in coreference resolution, GAP-Subjective, which increases the coverage of the original GAP dataset by including subjective sentences. We outline the methodology used to create this dataset. Firstly, we detect objective sentences and transfer them into their subjective variants using a sequence-to-sequence model. Secondly, we outline the thresholding techniques based on fluency and content preservation to maintain the quality of the sentences. Thirdly, we perform automated and human-based analysis of the style transfer and infer that the transferred sentences are of high quality. Finally, we benchmark both GAP and GAP-Subjective datasets using a BERT-based model and analyze its predictive performance and gender bias.</div></div></div><hr><div id=2022hcinlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.hcinlp-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.hcinlp-1/>Proceedings of the Second Workshop on Bridging Human--Computer Interaction and Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.hcinlp-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.hcinlp-1.0/>Proceedings of the Second Workshop on Bridging Human--Computer Interaction and Natural Language Processing</a></strong><br><a href=/people/s/su-lin-blodgett/>Su Lin Blodgett</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a>
|
<a href=/people/m/michael-madaio/>Michael Madaio</a>
|
<a href=/people/a/ani-nenkova/>Ani Nenkova</a>
|
<a href=/people/b/brendan-o-connor/>Brendan O'Connor</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a>
|
<a href=/people/q/qian-yang/>Qian Yang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.hcinlp-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--hcinlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.hcinlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.hcinlp-1.1/>Taxonomy Builder: a Data-driven and User-centric Tool for Streamlining Taxonomy Construction</a></strong><br><a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/j/john-hungerford/>John Hungerford</a>
|
<a href=/people/y/yee-seng-chan/>Yee Seng Chan</a>
|
<a href=/people/j/jessica-macbride/>Jessica MacBride</a>
|
<a href=/people/b/benjamin-gyori/>Benjamin Gyori</a>
|
<a href=/people/a/andrew-zupon/>Andrew Zupon</a>
|
<a href=/people/z/zheng-tang/>Zheng Tang</a>
|
<a href=/people/h/haoling-qiu/>Haoling Qiu</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/y/yan-zverev/>Yan Zverev</a>
|
<a href=/people/c/caitlin-hilverman/>Caitlin Hilverman</a>
|
<a href=/people/m/max-thomas/>Max Thomas</a>
|
<a href=/people/w/walter-andrews/>Walter Andrews</a>
|
<a href=/people/k/keith-alcock/>Keith Alcock</a>
|
<a href=/people/z/zeyu-zhang/>Zeyu Zhang</a>
|
<a href=/people/m/michael-reynolds/>Michael Reynolds</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/e/egoitz-laparra/>Egoitz Laparra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--hcinlp-1--1><div class="card-body p-3 small">An existing domain taxonomy for normalizing content is often assumed when discussing approaches to information extraction, yet often in real-world scenarios there is none. When one does exist, as the information needs shift, it must be continually extended. This is a slow and tedious task, and one which does not scale well. Here we propose an interactive tool that allows a taxonomy to be built or extended <i>rapidly</i> and with a <i>human in the loop</i> to control precision. We apply insights from text summarization and information extraction to reduce the search space dramatically, then leverage modern pretrained language models to perform contextualized clustering of the remaining concepts to yield candidate nodes for the user to review. We show this allows a user to consider as many as 200 taxonomy concept candidates an hour, to quickly build or extend a taxonomy to better fit information needs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.hcinlp-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--hcinlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.hcinlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.hcinlp-1.2/>An Interactive Exploratory Tool for the Task of Hate Speech Detection</a></strong><br><a href=/people/a/angelina-mcmillan-major/>Angelina McMillan-Major</a>
|
<a href=/people/a/amandalynne-paullada/>Amandalynne Paullada</a>
|
<a href=/people/y/yacine-jernite/>Yacine Jernite</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--hcinlp-1--2><div class="card-body p-3 small">With the growth of Automatic Content Moderation (ACM) on widely used social media platforms, transparency into the design of moderation technology and policy is necessary for online communities to advocate for themselves when harms occur. In this work, we describe a suite of interactive modules to support the exploration of various aspects of this technology, and particularly of those components that rely on English models and datasets for hate speech detection, a subtask within ACM. We intend for this demo to support the various stakeholders of ACM in investigating the definitions and decisions that underpin current technologies such that those with technical knowledge and those with contextual knowledge may both better understand existing systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.hcinlp-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--hcinlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.hcinlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.hcinlp-1.3/>Design Considerations for an <span class=acl-fixed-case>NLP</span>-Driven Empathy and Emotion Interface for Clinician Training via Telemedicine</a></strong><br><a href=/people/r/roxana-girju/>Roxana Girju</a>
|
<a href=/people/m/marina-girju/>Marina Girju</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--hcinlp-1--3><div class="card-body p-3 small">As digital social platforms and mobile technologies become more prevalent and robust, the use of Artificial Intelligence (AI) in facilitating human communication will grow. This, in turn, will encourage development of intuitive, adaptive, and effective empathic AI interfaces that better address the needs of socially and culturally diverse communities. In this paper, we present several design considerations of an intelligent digital interface intended to guide the clinicians toward more empathetic communication. This approach allows various communities of practice to investigate how AI, on one side, and human communication and healthcare needs, on the other, can contribute to each other’s development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.hcinlp-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--hcinlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.hcinlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.hcinlp-1.4/>Human-centered computing in legal <span class=acl-fixed-case>NLP</span> - An application to refugee status determination</a></strong><br><a href=/people/c/claire-barale/>Claire Barale</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--hcinlp-1--4><div class="card-body p-3 small">This paper proposes an approach to the design of an ethical human-AI reasoning support system for decision makers in refugee law. In the context of refugee status determination, practitioners mostly rely on text data. We therefore investigate human-AI cooperation in legal natural language processing. Specifically, we want to determine which design methods can be transposed to legal text analytics. Although little work has been done so far on human-centered design methods applicable to the legal domain, we assume that introducing iterative cooperation and user engagement in the design process is (1) a method to reduce technical limitations of an NLP system and (2) that it will help design more ethical and effective applications by taking users’ preferences and feedback into account. The proposed methodology is based on three main design steps: cognitive process formalization in models understandable by both humans and computers, speculative design of prototypes, and semi-directed interviews with a sample of potential users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.hcinlp-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--hcinlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.hcinlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.hcinlp-1.5/>Let’s Chat: Understanding User Expectations in Socialbot Interactions</a></strong><br><a href=/people/e/elizabeth-soper/>Elizabeth Soper</a>
|
<a href=/people/e/erin-pacquetet/>Erin Pacquetet</a>
|
<a href=/people/s/sougata-saha/>Sougata Saha</a>
|
<a href=/people/s/souvik-das/>Souvik Das</a>
|
<a href=/people/r/rohini-k-srihari/>Rohini Srihari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--hcinlp-1--5><div class="card-body p-3 small">This paper analyzes data from the 2021 Amazon Alexa Prize Socialbot Grand Challenge 4, in order to better understand the differences between human-computer interactions (HCI) in a socialbot setting and conventional human-to-human interactions. We find that because socialbots are a new genre of HCI, we are still negotiating norms to guide interactions in this setting. We present several notable patterns in user behavior toward socialbots, which have important implications for guiding future work in the development of conversational agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.hcinlp-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--hcinlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.hcinlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.hcinlp-1.6/>Teaching Interactively to Learn Emotions in Natural Language</a></strong><br><a href=/people/r/rajesh-titung/>Rajesh Titung</a>
|
<a href=/people/c/cecilia-alm/>Cecilia Alm</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--hcinlp-1--6><div class="card-body p-3 small">Motivated by prior literature, we provide a proof of concept simulation study for an understudied interactive machine learning method, machine teaching (MT), for the text-based emotion prediction task. We compare this method experimentally against a more well-studied technique, active learning (AL). Results show the strengths of both approaches over more resource-intensive offline supervised learning. Additionally, applying AL and MT to fine-tune a pre-trained model offers further efficiency gain. We end by recommending research directions which aim to empower users in the learning process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.hcinlp-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--hcinlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.hcinlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.hcinlp-1.7/>Narrative Datasets through the Lenses of <span class=acl-fixed-case>NLP</span> and <span class=acl-fixed-case>HCI</span></a></strong><br><a href=/people/s/sharifa-sultana/>Sharifa Sultana</a>
|
<a href=/people/r/renwen-zhang/>Renwen Zhang</a>
|
<a href=/people/h/hajin-lim/>Hajin Lim</a>
|
<a href=/people/m/maria-antoniak/>Maria Antoniak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--hcinlp-1--7><div class="card-body p-3 small">In this short paper, we compare existing value systems and approaches in NLP and HCI for collecting narrative data. Building on these parallel discussions, we shed light on the challenges facing some popular NLP dataset types, which we discuss these in relation to widely-used narrative-based HCI research methods; and we highlight points where NLP methods can broaden qualitative narrative studies. In particular, we point towards contextuality, positionality, dataset size, and open research design as central points of difference and windows for collaboration when studying narratives. Through the use case of narratives, this work contributes to a larger conversation regarding the possibilities for bridging NLP and HCI through speculative mixed-methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.hcinlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.hcinlp-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--hcinlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.hcinlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.hcinlp-1.8/>Towards a Deep Multi-layered Dialectal Language Analysis: A Case Study of <span class=acl-fixed-case>A</span>frican-<span class=acl-fixed-case>A</span>merican <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/j/jamell-dacon/>Jamell Dacon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--hcinlp-1--8><div class="card-body p-3 small">Currently, natural language processing (NLP) models proliferate language discrimination leading to potentially harmful societal impacts as a result of biased outcomes. For example, part-of-speech taggers trained on Mainstream American English (MAE) produce non-interpretable results when applied to African American English (AAE) as a result of language features not seen during training. In this work, we incorporate a human-in-the-loop paradigm to gain a better understanding of AAE speakers’ behavior and their language use, and highlight the need for dialectal language inclusivity so that native AAE speakers can extensively interact with NLP systems while reducing feelings of disenfranchisement.</div></div></div><hr><div id=2022mia-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.mia-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.mia-1/>Proceedings of the Workshop on Multilingual Information Access (MIA)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.mia-1.0/>Proceedings of the Workshop on Multilingual Information Access (MIA)</a></strong><br><a href=/people/a/akari-asai/>Akari Asai</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/j/jonathan-h-clark/>Jonathan H. Clark</a>
|
<a href=/people/j/junjie-hu/>Junjie Hu</a>
|
<a href=/people/c/chia-hsuan-lee/>Chia-Hsuan Lee</a>
|
<a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/s/shayne-longpre/>Shayne Longpre</a>
|
<a href=/people/i/ikuya-yamada/>Ikuya Yamada</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.mia-1.1/>Geographical Distance Is The New Hyperparameter: A Case Study Of Finding The Optimal Pre-trained Language For <span class=acl-fixed-case>E</span>nglish-isi<span class=acl-fixed-case>Z</span>ulu Machine Translation.</a></strong><br><a href=/people/m/muhammad-umair-nasir/>Muhammad Umair Nasir</a>
|
<a href=/people/i/innocent-mchechesi/>Innocent Mchechesi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--1><div class="card-body p-3 small">Stemming from the limited availability of datasets and textual resources for low-resource languages such as isiZulu, there is a significant need to be able to harness knowledge from pre-trained models to improve low resource machine translation. Moreover, a lack of techniques to handle the complexities of morphologically rich languages has compounded the unequal development of translation models, with many widely spoken African languages being left behind. This study explores the potential benefits of transfer learning in an English-isiZulu translation framework. The results indicate the value of transfer learning from closely related languages to enhance the performance of low-resource translation models, thus providing a key strategy for low-resource translation going forward. We gathered results from 8 different language corpora, including one multi-lingual corpus, and saw that isiXhosa-isiZulu outperformed all languages, with a BLEU score of 8.56 on the test set which was better from the multi-lingual corpora pre-trained model by 2.73. We also derived a new coefficient, Nasir’s Geographical Distance Coefficient (NGDC) which provides an easy selection of languages for the pre-trained models. NGDC also indicated that isiXhosa should be selected as the language for the pre-trained model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.mia-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.mia-1.2/>An Annotated Dataset and Automatic Approaches for Discourse Mode Identification in Low-resource <span class=acl-fixed-case>B</span>engali Language</a></strong><br><a href=/people/s/salim-sazzed/>Salim Sazzed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--2><div class="card-body p-3 small">The modes of discourse aid in comprehending the convention and purpose of various forms of languages used during communication. In this study, we introduce a discourse mode annotated corpus for the low-resource Bangla (also referred to as Bengali) language. The corpus consists of sentence-level annotation of three different discourse modes, narrative, descriptive, and informative of the text excerpted from a number of Bangla novels. We analyze the annotated corpus to expose various linguistic aspects of discourse modes, such as class distributions and average sentence lengths. To automatically determine the mode of discourse, we apply CML (classical machine learning) classifiers with n-gram based statistical features and a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) based language model. We observe that fine-tuned BERT-based approach yields more promising results than n-gram based CML classifiers. Our created discourse mode annotated dataset, the first of its kind in Bangla, and the evaluation, provide baselines for the automatic discourse mode identification in Bangla and can assist various downstream natural language processing tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.mia-1.3/>Pivot Through <span class=acl-fixed-case>E</span>nglish: Reliably Answering Multilingual Questions without Document Retrieval</a></strong><br><a href=/people/i/ivan-montero/>Ivan Montero</a>
|
<a href=/people/s/shayne-longpre/>Shayne Longpre</a>
|
<a href=/people/n/ni-lao/>Ni Lao</a>
|
<a href=/people/a/andrew-frank/>Andrew Frank</a>
|
<a href=/people/c/christopher-dubois/>Christopher DuBois</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--3><div class="card-body p-3 small">Existing methods for open-retrieval question answering in lower resource languages (LRLs) lag significantly behind English. They not only suffer from the shortcomings of non-English document retrieval, but are reliant on language-specific supervision for either the task or translation. We formulate a task setup more realistic to available resources, that circumvents document retrieval to reliably transfer knowledge from English to lower resource languages. Assuming a strong English question answering model or database, we compare and analyze methods that pivot through English: to map foreign queries to English and then English answers back to target language answers. Within this task setup we propose Reranked Multilingual Maximal Inner Product Search (RM-MIPS), akin to semantic similarity retrieval over the English training set with reranking, which outperforms the strongest baselines by 2.7% on XQuAD and 6.2% on MKQA. Analysis demonstrates the particular efficacy of this strategy over state-of-the-art alternatives in challenging settings: low-resource languages, with extensive distractor data and query distribution misalignment. Circumventing retrieval, our analysis shows this approach offers rapid answer generation to many other languages off-the-shelf, without necessitating additional training data in the target language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.mia-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.mia-1.4/>Cross-Lingual <span class=acl-fixed-case>QA</span> as a Stepping Stone for Monolingual Open <span class=acl-fixed-case>QA</span> in <span class=acl-fixed-case>I</span>celandic</a></strong><br><a href=/people/v/vesteinn-snaebjarnarson/>Vésteinn Snæbjarnarson</a>
|
<a href=/people/h/hafsteinn-einarsson/>Hafsteinn Einarsson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--4><div class="card-body p-3 small">It can be challenging to build effective open question answering (open QA) systems for languages other than English, mainly due to a lack of labeled data for training. We present a data efficient method to bootstrap such a system for languages other than English. Our approach requires only limited QA resources in the given language, along with machine-translated data, and at least a bilingual language model. To evaluate our approach, we build such a system for the Icelandic language and evaluate performance over trivia style datasets. The corpora used for training are English in origin but machine translated into Icelandic. We train a bilingual Icelandic/English language model to embed English context and Icelandic questions following methodology introduced with DensePhrases (Lee et al., 2021). The resulting system is an open domain cross-lingual QA system between Icelandic and English. Finally, the system is adapted for Icelandic only open QA, demonstrating how it is possible to efficiently create an open QA system with limited access to curated datasets in the language of interest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.mia-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.mia-1.5/>Multilingual Event Linking to <span class=acl-fixed-case>W</span>ikidata</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/r/rishubh-gupta/>Rishubh Gupta</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--5><div class="card-body p-3 small">We present a task of multilingual linking of events to a knowledge base. We automatically compile a large-scale dataset for this task, comprising of 1.8M mentions across 44 languages referring to over 10.9K events from Wikidata. We propose two variants of the event linking task: 1) multilingual, where event descriptions are from the same language as the mention, and 2) crosslingual, where all event descriptions are in English. On the two proposed tasks, we compare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and multilingual adaptations of the biencoder and crossencoder architectures from BLINK (Wu et al., 2020). In our experiments on the two task variants, we find both biencoder and crossencoder models significantly outperform the BM25+ baseline. Our results also indicate that the crosslingual task is in general more challenging than the multilingual task. To test the out-of-domain generalization of the proposed linking systems, we additionally create a Wikinews-based evaluation set. We present qualitative analysis highlighting various aspects captured by the proposed dataset, including the need for temporal reasoning over context and tackling diverse event descriptions across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.mia-1.6/>Complex Word Identification in <span class=acl-fixed-case>V</span>ietnamese: Towards <span class=acl-fixed-case>V</span>ietnamese Text Simplification</a></strong><br><a href=/people/p/phuong-nguyen/>Phuong Nguyen</a>
|
<a href=/people/d/david-kauchak/>David Kauchak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--6><div class="card-body p-3 small">Text Simplification has been an extensively researched problem in English, but has not been investigated in Vietnamese. We focus on the Vietnamese-specific Complex Word Identification task, often the first step in Lexical Simplification (Shardlow, 2013). We examine three different Vietnamese datasets constructed for other Natural Language Processing tasks and show that, like in other languages, frequency is a strong signal in determining whether a word is complex, with a mean accuracy of 86.87%. Across the datasets, we find that the 10% most frequent words in many corpus can be labelled as simple, and the rest as complex, though this is more variable for smaller corpora. We also examine how human annotators perform at this task. Given the subjective nature, there is a fair amount of variability in which words are seen as difficult, though majority results are more consistent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.mia-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.mia-1.7/>Benchmarking Language-agnostic Intent Classification for Virtual Assistant Platforms</a></strong><br><a href=/people/g/gengyu-wang/>Gengyu Wang</a>
|
<a href=/people/c/cheng-qian/>Cheng Qian</a>
|
<a href=/people/l/lin-pan/>Lin Pan</a>
|
<a href=/people/h/haode-qi/>Haode Qi</a>
|
<a href=/people/l/ladislav-kunc/>Ladislav Kunc</a>
|
<a href=/people/s/saloni-potdar/>Saloni Potdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--7><div class="card-body p-3 small">Current virtual assistant (VA) platforms are beholden to the limited number of languages they support. Every component, such as the tokenizer and intent classifier, is engineered for specific languages in these intricate platforms. Thus, supporting a new language in such platforms is a resource-intensive operation requiring expensive re-training and re-designing. In this paper, we propose a benchmark for evaluating language-agnostic intent classification, the most critical component of VA platforms. To ensure the benchmarking is challenging and comprehensive, we include 29 public and internal datasets across 10 low-resource languages and evaluate various training and testing settings with consideration of both accuracy and training time. The benchmarking result shows that Watson Assistant, among 7 commercial VA platforms and pre-trained multilingual language models (LMs), demonstrates close-to-best accuracy with the best accuracy-training time trade-off.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.mia-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.mia-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.mia-1.8/><span class=acl-fixed-case>Z</span>usammen<span class=acl-fixed-case>QA</span>: Data Augmentation with Specialized Models for Cross-lingual Open-retrieval Question Answering System</a></strong><br><a href=/people/c/chia-chien-hung/>Chia-Chien Hung</a>
|
<a href=/people/t/tommaso-green/>Tommaso Green</a>
|
<a href=/people/r/robert-litschko/>Robert Litschko</a>
|
<a href=/people/t/tornike-tsereteli/>Tornike Tsereteli</a>
|
<a href=/people/s/sotaro-takeshita/>Sotaro Takeshita</a>
|
<a href=/people/m/marco-bombieri/>Marco Bombieri</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--8><div class="card-body p-3 small">This paper introduces our proposed system for the MIA Shared Task on Cross-lingual Openretrieval Question Answering (COQA). In this challenging scenario, given an input question the system has to gather evidence documents from a multilingual pool and generate from them an answer in the language of the question. We devised several approaches combining different model variants for three main components: Data Augmentation, Passage Retrieval, and Answer Generation. For passage retrieval, we evaluated the monolingual BM25 ranker against the ensemble of re-rankers based on multilingual pretrained language models (PLMs) and also variants of the shared task baseline, re-training it from scratch using a recently introduced contrastive loss that maintains a strong gradient signal throughout training by means of mixed negative samples. For answer generation, we focused on languageand domain-specialization by means of continued language model (LM) pretraining of existing multilingual encoders. Additionally, for both passage retrieval and answer generation, we augmented the training data provided by the task organizers with automatically generated question-answer pairs created from Wikipedia passages to mitigate the issue of data scarcity, particularly for the low-resource languages for which no training data were provided. Our results show that language- and domain-specialization as well as data augmentation help, especially for low-resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.mia-1.9/>Zero-shot cross-lingual open domain question answering</a></strong><br><a href=/people/s/sumit-agarwal/>Sumit Agarwal</a>
|
<a href=/people/s/suraj-tripathi/>Suraj Tripathi</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Penstein Rose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--9><div class="card-body p-3 small">People speaking different kinds of languages search for information in a cross-lingual manner. They tend to ask questions in their language and expect the answer to be in the same language, despite the evidence lying in another language. In this paper, we present our approach for this task of cross-lingual open-domain question-answering. Our proposed method employs a passage reranker, the fusion-in-decoder technique for generation, and a wiki data entity-based post-processing system to tackle the inability to generate entities across all languages. Our end-2-end pipeline shows an improvement of 3 and 4.6 points on F1 and EM metrics respectively, when compared with the baseline CORA model on the XOR-TyDi dataset. We also evaluate the effectiveness of our proposed techniques in the zero-shot setting using the MKQA dataset and show an improvement of 5 points in F1 for high-resource and 3 points improvement for low-resource zero-shot languages. Our team, CMUmQA’s submission in the MIA-Shared task ranked 1st in the constrained setup for the dev and 2nd in the test setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.mia-1.10/><span class=acl-fixed-case>MIA</span> 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering</a></strong><br><a href=/people/z/zhucheng-tu/>Zhucheng Tu</a>
|
<a href=/people/s/sarguna-janani-padmanabhan/>Sarguna Janani Padmanabhan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--10><div class="card-body p-3 small">We describe our two-stage system for the Multilingual Information Access (MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The first stage consists of multilingual passage retrieval with a hybrid dense and sparse retrieval strategy. The second stage consists of a reader which outputs the answer from the top passages returned by the first stage. We show the efficacy of using entity representations, sparse retrieval signals to help dense retrieval, and Fusion-in-Decoder. On the development set, we obtain 43.46 F1 on XOR-TyDi QA and 21.99 F1 on MKQA, for an average F1 score of 32.73. On the test set, we obtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an average F1 score of 31.61. We improve over the official baseline by over 4 F1 points on both the development and test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.mia-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.mia-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--mia-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.mia-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.mia-1.11/><span class=acl-fixed-case>MIA</span> 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages</a></strong><br><a href=/people/a/akari-asai/>Akari Asai</a>
|
<a href=/people/s/shayne-longpre/>Shayne Longpre</a>
|
<a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/c/chia-hsuan-lee/>Chia-Hsuan Lee</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/j/junjie-hu/>Junjie Hu</a>
|
<a href=/people/i/ikuya-yamada/>Ikuya Yamada</a>
|
<a href=/people/j/jonathan-h-clark/>Jonathan H. Clark</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--mia-1--11><div class="card-body p-3 small">We present the results of the Workshop on Multilingual Information Access (MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question answering (QA) systems in 16 typologically diverse languages. In this task, we adapted two large-scale cross-lingual open-retrieval QA datasets in 14 typologically diverse languages, and newly annotated open-retrieval QA data in 2 underrepresented languages: Tagalog and Tamil. Four teams submitted their systems. The best constrained system uses entity-aware contextualized representations for document retrieval, thereby achieving an average F1 score of 31.6, which is 4.1 F1 absolute higher than the challenging baseline. The best system obtains particularly significant improvements in Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores. The best unconstrained system achieves 32.2 F1, outperforming our baseline by 4.5 points.</div></div></div><hr><div id=2022privatenlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.privatenlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.privatenlp-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.privatenlp-1/>Proceedings of the Fourth Workshop on Privacy in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.privatenlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.privatenlp-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.privatenlp-1.0/>Proceedings of the Fourth Workshop on Privacy in Natural Language Processing</a></strong><br><a href=/people/o/oluwaseyi-feyisetan/>Oluwaseyi Feyisetan</a>
|
<a href=/people/s/sepideh-ghanavati/>Sepideh Ghanavati</a>
|
<a href=/people/p/patricia-thaine/>Patricia Thaine</a>
|
<a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/f/fatemehsadat-mireshghallah/>Fatemehsadat Mireshghallah</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.privatenlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.privatenlp-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--privatenlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.privatenlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.privatenlp-1.1/>Differential Privacy in Natural Language Processing The Story So Far</a></strong><br><a href=/people/o/oleksandra-klymenko/>Oleksandra Klymenko</a>
|
<a href=/people/s/stephen-meisenbacher/>Stephen Meisenbacher</a>
|
<a href=/people/f/florian-matthes/>Florian Matthes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--privatenlp-1--1><div class="card-body p-3 small">As the tide of Big Data continues to influence the landscape of Natural Language Processing (NLP), the utilization of modern NLP methods has grounded itself in this data, in order to tackle a variety of text-based tasks. These methods without a doubt can include private or otherwise personally identifiable information. As such, the question of privacy in NLP has gained fervor in recent years, coinciding with the development of new Privacy- Enhancing Technologies (PETs). Among these PETs, Differential Privacy boasts several desirable qualities in the conversation surrounding data privacy. Naturally, the question becomes whether Differential Privacy is applicable in the largely unstructured realm of NLP. This topic has sparked novel research, which is unified in one basic goal how can one adapt Differential Privacy to NLP methods? This paper aims to summarize the vulnerabilities addressed by Differential Privacy, the current thinking, and above all, the crucial next steps that must be considered.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.privatenlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.privatenlp-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--privatenlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.privatenlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.privatenlp-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.privatenlp-1.2/>The Impact of Differential Privacy on Group Disparity Mitigation</a></strong><br><a href=/people/v/victor-petren-bach-hansen/>Victor Petren Bach Hansen</a>
|
<a href=/people/a/atula-tejaswi-neerkaje/>Atula Tejaswi Neerkaje</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/l/lucie-flek/>Lucie Flek</a>
|
<a href=/people/a/anders-sogaard/>Anders Sogaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--privatenlp-1--2><div class="card-body p-3 small">The performance cost of differential privacy has, for some applications, been shown to be higher for minority groups fairness, conversely, has been shown to disproportionally compromise the privacy of members of such groups. Most work in this area has been restricted to computer vision and risk assessment. In this paper, we evaluate the impact of differential privacy on fairness across four tasks, focusing on how attempts to mitigate privacy violations and between-group performance differences interact Does privacy inhibit attempts to ensure fairness? To this end, we train epsilon, delta-differentially private models with empirical risk minimization and group distributionally robust training objectives. Consistent with previous findings, we find that differential privacy increases between-group performance differences in the baseline setting but more interestingly, differential privacy reduces between-group performance differences in the robust setting. We explain this by reinterpreting differential privacy as regularization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.privatenlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.privatenlp-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--privatenlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.privatenlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.privatenlp-1.3/>Privacy Leakage in Text Classification A Data Extraction Approach</a></strong><br><a href=/people/a/adel-elmahdy/>Adel Elmahdy</a>
|
<a href=/people/h/huseyin-a-inan/>Huseyin A. Inan</a>
|
<a href=/people/r/robert-sim/>Robert Sim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--privatenlp-1--3><div class="card-body p-3 small">Recent work has demonstrated the successful extraction of training data from generative language models. However, it is not evident whether such extraction is feasible in text classification models since the training objective is to predict the class label as opposed to next-word prediction. This poses an interesting challenge and raises an important question regarding the privacy of training data in text classification settings. Therefore, we study the potential privacy leakage in the text classification domain by investigating the problem of unintended memorization of training data that is not pertinent to the learning task. We propose an algorithm to extract missing tokens of a partial text by exploiting the likelihood of the class label provided by the model. We test the effectiveness of our algorithm by inserting canaries into the training set and attempting to extract tokens in these canaries post-training. In our experiments, we demonstrate that successful extraction is possible to some extent. This can also be used as an auditing strategy to assess any potential unauthorized use of personal data without consent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.privatenlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.privatenlp-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--privatenlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.privatenlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.privatenlp-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.privatenlp-1.4/>Training Text-to-Text Transformers with Privacy Guarantees</a></strong><br><a href=/people/n/natalia-ponomareva/>Natalia Ponomareva</a>
|
<a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/s/sergei-vassilvitskii/>Sergei Vassilvitskii</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--privatenlp-1--4><div class="card-body p-3 small">Recent advances in NLP often stem from large transformer-based pre-trained models, which rapidly grow in size and use more and more training data. Such models are often released to the public so that end users can fine-tune them on a task dataset. While it is common to treat pre-training data as public, it may still contain personally identifiable information (PII), such as names, phone numbers, and copyrighted material. Recent findings show that the capacity of these models allows them to memorize parts of the training data, and suggest differentially private (DP) training as a potential mitigation. While there is recent work on DP fine-tuning of NLP models, the effects of DP pre-training are less well understood it is not clear how downstream performance is affected by DP pre-training, and whether DP pre-training mitigates some of the memorization concerns. We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracy on downstream tasks (e.g. GLUE). Moreover, we show that T5s span corruption is a good defense against data memorization.</div></div></div><hr><div id=2022semeval-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.semeval-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.semeval-1/>Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.0/>Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</a></strong><br><a href=/people/g/guy-emerson/>Guy Emerson</a>
|
<a href=/people/n/natalie-schluter/>Natalie Schluter</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/s/siddharth-singh/>Siddharth Singh</a>
|
<a href=/people/s/shyam-ratan/>Shyam Ratan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.1/><span class=acl-fixed-case>S</span>emeval-2022 Task 1: <span class=acl-fixed-case>CODWOE</span> – Comparing Dictionaries and Word Embeddings</a></strong><br><a href=/people/t/timothee-mickus/>Timothee Mickus</a>
|
<a href=/people/k/kees-van-deemter/>Kees Van Deemter</a>
|
<a href=/people/m/matthieu-constant/>Mathieu Constant</a>
|
<a href=/people/d/denis-paperno/>Denis Paperno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--1><div class="card-body p-3 small">Word embeddings have advanced the state of the art in NLP across numerous tasks. Understanding the contents of dense neural representations is of utmost interest to the computational semantics community. We propose to focus on relating these opaque word vectors with human-readable definitions, as found in dictionaries This problem naturally divides into two subtasks: converting definitions into embeddings, and converting embeddings into definitions. This task was conducted in a multilingual setting, using comparable sets of embeddings trained homogeneously.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.2/>1<span class=acl-fixed-case>C</span>ademy at <span class=acl-fixed-case>S</span>emeval-2022 Task 1: Investigating the Effectiveness of Multilingual, Multitask, and Language-Agnostic Tricks for the Reverse Dictionary Task</a></strong><br><a href=/people/z/zhiyong-wang/>Zhiyong Wang</a>
|
<a href=/people/g/ge-zhang/>Ge Zhang</a>
|
<a href=/people/n/nineli-lashkarashvili/>Nineli Lashkarashvili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--2><div class="card-body p-3 small">This paper describes our system for the Se- mEval2022 task of matching dictionary glosses to word embeddings. We focus on the Reverse Dictionary Track of the competition, which maps multilingual glosses to reconstructed vector representations. More specifically, models convert the input of sentences to three types of embeddings: SGNS, Char, and Electra. We pro- pose several experiments for applying neural network cells, general multilingual and multi-task structures, and language-agnostic tricks to the task. We also provide comparisons over different types of word embeddings and ablation studies to suggest helpful strategies. Our initial transformer-based model achieves relatively low performance. However, trials on different retokenization methodologies indicate improved performance. Our proposed Elmo- based monolingual model achieves the highest outcome, and its multitask, and multilingual varieties show competitive results as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.3/><span class=acl-fixed-case>BLCU</span>-<span class=acl-fixed-case>ICALL</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 1: Cross-Attention Multitasking Framework for Definition Modeling</a></strong><br><a href=/people/c/cunliang-kong/>Cunliang Kong</a>
|
<a href=/people/y/yujie-wang/>Yujie Wang</a>
|
<a href=/people/r/ruining-chong/>Ruining Chong</a>
|
<a href=/people/l/liner-yang/>Liner Yang</a>
|
<a href=/people/h/hengyuan-zhang/>Hengyuan Zhang</a>
|
<a href=/people/e/erhong-yang/>Erhong Yang</a>
|
<a href=/people/y/yaping-huang/>Yaping Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--3><div class="card-body p-3 small">This paper describes the BLCU-ICALL system used in the SemEval-2022 Task 1 Comparing Dictionaries and Word Embeddings, the Definition Modeling subtrack, achieving 1st on Italian, 2nd on Spanish and Russian, and 3rd on English and French. We propose a transformer-based multitasking framework to explore the task. The framework integrates multiple embedding architectures through the cross-attention mechanism, and captures the structure of glosses through a masking language model objective. Additionally, we also investigate a simple but effective model ensembling strategy to further improve the robustness. The evaluation results show the effectiveness of our solution. We release our code at: <a href=https://github.com/blcuicall/SemEval2022-Task1-DM class=acl-markup-url>https://github.com/blcuicall/SemEval2022-Task1-DM</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.4/><span class=acl-fixed-case>L</span>ing<span class=acl-fixed-case>J</span>ing at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 1: Multi-task Self-supervised Pre-training for Multilingual Reverse Dictionary</a></strong><br><a href=/people/b/bin-li/>Bin Li</a>
|
<a href=/people/y/yixuan-weng/>Yixuan Weng</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/b/bin-sun/>Bin Sun</a>
|
<a href=/people/s/shutao-li/>Shutao Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--4><div class="card-body p-3 small">This paper introduces the approach of Team LingJing’s experiments on SemEval-2022 Task 1 Comparing Dictionaries and Word Embeddings (CODWOE). This task aims at comparing two types of semantic descriptions and including two sub-tasks: the definition modeling and reverse dictionary track. Our team focuses on the reverse dictionary track and adopts the multi-task self-supervised pre-training for multilingual reverse dictionaries. Specifically, the randomly initialized mDeBERTa-base model is used to perform multi-task pre-training on the multilingual training datasets. The pre-training step is divided into two stages, namely the MLM pre-training stage and the contrastive pre-training stage. The experimental results show that the proposed method has achieved good performance in the reverse dictionary track, where we rank the 1-st in the Sgns targets of the EN and RU languages. All the experimental codes are open-sourced at <a href=https://github.com/WENGSYX/Semeval class=acl-markup-url>https://github.com/WENGSYX/Semeval</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.5/><span class=acl-fixed-case>IRB</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations</a></strong><br><a href=/people/d/damir-korencic/>Damir Korenčić</a>
|
<a href=/people/i/ivan-grubisic/>Ivan Grubisic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--5><div class="card-body p-3 small">What is the relation between a word and its description, or a word and its embedding? Both descriptions and embeddings are semantic representations of words. But, what information from the original word remains in these representations? Or more importantly, which information about a word do these two representations share? Definition Modeling and Reverse Dictionary are two opposite learning tasks that address these questions. The goal of the Definition Modeling task is to investigate the power of information laying inside a word embedding to express the meaning of the word in a humanly understandable way – as a dictionary definition. Conversely, the Reverse Dictionary task explores the ability to predict word embeddings directly from its definition. In this paper, by tackling these two tasks, we are exploring the relationship between words and their semantic representations. We present our findings based on the descriptive, exploratory, and predictive data analysis conducted on the CODWOE dataset. We give a detailed overview of the systems that we designed for Definition Modeling and Reverse Dictionary tasks, and that achieved top scores on SemEval-2022 CODWOE challenge in several subtasks. We hope that our experimental results concerning the predictive models and the data analyses we provide will prove useful in future explorations of word representations and their relationships.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.6/><span class=acl-fixed-case>TLDR</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 1: Using Transformers to Learn Dictionaries and Representations</a></strong><br><a href=/people/a/aditya-srivastava/>Aditya Srivastava</a>
|
<a href=/people/h/harsha-vardhan-vemulapati/>Harsha Vardhan Vemulapati</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--6><div class="card-body p-3 small">We propose a pair of deep learning models, which employ unsupervised pretraining, attention mechanisms and contrastive learning for representation learning from dictionary definitions, and definition modeling from such representations. Our systems, the Transformers for Learning Dictionaries and Representations (TLDR), were submitted to the SemEval 2022 Task 1: Comparing Dictionaries and Word Embeddings (CODWOE), where they officially ranked first on the definition modeling subtask, and achieved competitive performance on the reverse dictionary subtask. In this paper we describe our methodology and analyse our system design hypotheses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.7/><span class=acl-fixed-case>MMG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 1: A Reverse Dictionary approach based on a review of the dataset from a lexicographic perspective</a></strong><br><a href=/people/a/alfonso-ardoiz/>Alfonso Ardoiz</a>
|
<a href=/people/m/miguel-ortega-martin/>Miguel Ortega-Martín</a>
|
<a href=/people/o/oscar-garcia-sierra/>Óscar García-Sierra</a>
|
<a href=/people/j/jorge-alvarez/>Jorge Álvarez</a>
|
<a href=/people/i/ignacio-arranz/>Ignacio Arranz</a>
|
<a href=/people/a/adrian-alonso/>Adrián Alonso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--7><div class="card-body p-3 small">This paper presents a novel and linguistic-driven system for the Spanish Reverse Dictionary task of SemEval-2022 Task 1. The aim of this task is the automatic generation of a word using its gloss. The conclusion is that this task results could improve if the quality of the dataset did as well by incorporating high-quality lexicographic data. Therefore, in this paper we analyze the main gaps in the proposed dataset and describe how these limitations could be tackled.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.8/><span class=acl-fixed-case>E</span>dinburgh at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 1: Jointly Fishing for Word Embeddings and Definitions</a></strong><br><a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/z/zheng-zhao/>Zheng Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--8><div class="card-body p-3 small">This paper presents a winning submission to the SemEval 2022 Task 1 on two sub-tasks: reverse dictionary and definition modelling. We leverage a recently proposed unified model with multi-task training. It utilizes data symmetrically and learns to tackle both tracks concurrently. Analysis shows that our system performs consistently on diverse languages, and works the best with sgns embeddings. Yet, char and electra carry intriguing properties. The two tracks’ best results are always in differing subsets grouped by linguistic annotations. In this task, the quality of definition generation lags behind, and BLEU scores might be misleading.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.9/><span class=acl-fixed-case>RIGA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 1: Scaling Recurrent Neural Networks for <span class=acl-fixed-case>CODWOE</span> Dictionary Modeling</a></strong><br><a href=/people/e/eduards-mukans/>Eduards Mukans</a>
|
<a href=/people/g/gus-strazds/>Gus Strazds</a>
|
<a href=/people/g/guntis-barzdins/>Guntis Barzdins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--9><div class="card-body p-3 small">Described are our two entries “emukans” and “guntis” for the definition modeling track of CODWOE SemEval-2022 Task 1. Our approach is based on careful scaling of a GRU recurrent neural network, which exhibits double descent of errors, corresponding to significant improvements also per human judgement. Our results are in the middle of the ranking table per official automatic metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.10/><span class=acl-fixed-case>U</span>ppsala <span class=acl-fixed-case>U</span>niversity at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 1: Can Foreign Entries Enhance an <span class=acl-fixed-case>E</span>nglish Reverse Dictionary?</a></strong><br><a href=/people/r/rafal-cerniavski/>Rafal Cerniavski</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--10><div class="card-body p-3 small">We present the Uppsala University system for SemEval-2022 Task 1: Comparing Dictionaries and Word Embeddings (CODWOE). We explore the performance of multilingual reverse dictionaries as well as the possibility of utilizing annotated data in other languages to improve the quality of a reverse dictionary in the target language. We mainly focus on character-based embeddings.In our main experiment, we train multilingual models by combining the training data from multiple languages. In an additional experiment, using resources beyond the shared task, we use the training data in Russian and French to improve the English reverse dictionary using unsupervised embeddings alignment and machine translation. The results show that multilingual models occasionally but not consistently can outperform the monolingual baselines. In addition, we demonstrate an improvement of an English reverse dictionary using translated entries from the Russian training data set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.11/><span class=acl-fixed-case>BL</span>.<span class=acl-fixed-case>R</span>esearch at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 1: Deep networks for Reverse Dictionary using embeddings and <span class=acl-fixed-case>LSTM</span> autoencoders</a></strong><br><a href=/people/n/nihed-bendahman/>Nihed Bendahman</a>
|
<a href=/people/j/julien-breton/>Julien Breton</a>
|
<a href=/people/l/lina-nicolaieff/>Lina Nicolaieff</a>
|
<a href=/people/m/mokhtar-boumedyen-billami/>Mokhtar Boumedyen Billami</a>
|
<a href=/people/c/christophe-bortolaso/>Christophe Bortolaso</a>
|
<a href=/people/y/youssef-miloudi/>Youssef Miloudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--11><div class="card-body p-3 small">This paper describes our two deep learning systems that competed at SemEval-2022 Task 1 “CODWOE: Comparing Dictionaries and WOrd Embeddings”. We participated in the subtask for the reverse dictionary which consists in generating vectors from glosses. We use sequential models that integrate several neural networks, starting from Embeddings networks until the use of Dense networks, Bidirectional Long Short-Term Memory (BiLSTM) networks and LSTM networks. All glosses have been preprocessed in order to consider the best representation form of the meanings for all words that appears. We achieved very competitive results in reverse dictionary with a second position in English and French languages when using contextualized embeddings, and the same position for English, French and Spanish languages when using char embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.12/><span class=acl-fixed-case>JSI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 1: <span class=acl-fixed-case>CODWOE</span> - Reverse Dictionary: Monolingual and cross-lingual approaches</a></strong><br><a href=/people/t/thi-hong-hanh-tran/>Thi Hong Hanh Tran</a>
|
<a href=/people/m/matej-martinc/>Matej Martinc</a>
|
<a href=/people/m/matthew-purver/>Matthew Purver</a>
|
<a href=/people/s/senja-pollak/>Senja Pollak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--12><div class="card-body p-3 small">The reverse dictionary task is a sequence-to-vector task in which a gloss is provided as input, and the output must be a semantically matching word vector. The reverse dictionary is useful in practical applications such as solving the tip-of-the-tongue problem, helping new language learners, etc. In this paper, we evaluate the effect of a Transformer-based model with cross-lingual zero-shot learning to improve the reverse dictionary performance. Our experiments are conducted in five languages in the CODWOE dataset, including English, French, Italian, Spanish, and Russian. Even if we did not achieve a good ranking in the CODWOE competition, we show that our work partially improves the current baseline from the organizers with a hypothesis on the impact of LSTM in monolingual, multilingual, and zero-shot learning. All the codes are available at <a href=https://github.com/honghanhh/codwoe2021 class=acl-markup-url>https://github.com/honghanhh/codwoe2021</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.13.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.13/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding</a></strong><br><a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a>
|
<a href=/people/e/edward-gow-smith/>Edward Gow-Smith</a>
|
<a href=/people/m/marcos-garcia/>Marcos Garcia</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/m/marco-idiart/>Marco Idiart</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--13><div class="card-body p-3 small">This paper presents the shared task on Multilingual Idiomaticity Detection and Sentence Embedding, which consists of two subtasks: (a) a binary classification task aimed at identifying whether a sentence contains an idiomatic expression, and (b) a task based on semantic text similarity which requires the model to adequately represent potentially idiomatic expressions in context. Each subtask includes different settings regarding the amount of training data. Besides the task description, this paper introduces the datasets in English, Portuguese, and Galician and their annotation procedure, the evaluation metrics, and a summary of the participant systems and their results. The task had close to 100 registered participants organised into twenty five teams making over 650 and 150 submissions in the practice and evaluation phases respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.14.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.14/><span class=acl-fixed-case>H</span>elsinki-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: A Feature-Based Approach to Multilingual Idiomaticity Detection</a></strong><br><a href=/people/s/sami-itkonen/>Sami Itkonen</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--14><div class="card-body p-3 small">This paper describes the University of Helsinki submission to the SemEval 2022 task on multilingual idiomaticity detection. Our system utilizes several models made available by HuggingFace, along with the baseline BERT model for the task. We focus on feature engineering based on properties that typically characterize idiomatic expressions. The additional features lead to improvements over the baseline and the final submission achieves 15th place out of 20 submissions. The paper provides error analysis of our model including visualisations of the contributions of individual features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.15.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.15/>Hitachi at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: On the Effectiveness of Span-based Classification Approaches for Multilingual Idiomaticity Detection</a></strong><br><a href=/people/a/atsuki-yamaguchi/>Atsuki Yamaguchi</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/y/yasuhiro-sogawa/>Yasuhiro Sogawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--15><div class="card-body p-3 small">In this paper, we describe our system for SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding. The task aims at detecting idiomaticity in an input sequence (Subtask A) and modeling representation of sentences that contain potential idiomatic multiword expressions (MWEs) (Subtask B) in three languages. We focus on the zero-shot setting of Subtask A and propose two span-based idiomaticity classification methods: MWE span-based classification and idiomatic MWE span prediction-based classification. We use several cross-lingual pre-trained language models (InfoXLM, XLM-R, and others) as our backbone network. Our best-performing system, fine-tuned with the span-based idiomaticity classification, ranked fifth in the zero-shot setting of Subtask A and exhibited a macro F1 score of 0.7466.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.16.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.16/><span class=acl-fixed-case>UA</span>lberta at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2022 Task 2: Leveraging Glosses and Translations for Multilingual Idiomaticity Detection</a></strong><br><a href=/people/b/bradley-hauer/>Bradley Hauer</a>
|
<a href=/people/s/seeratpal-jaura/>Seeratpal Jaura</a>
|
<a href=/people/t/talgat-omarov/>Talgat Omarov</a>
|
<a href=/people/g/grzegorz-kondrak/>Grzegorz Kondrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--16><div class="card-body p-3 small">We describe the University of Alberta systems for the SemEval-2022 Task 2 on multilingual idiomaticity detection. Working under the assumption that idiomatic expressions are noncompositional, our first method integrates information on the meanings of the individual words of an expression into a binary classifier. Further hypothesizing that literal and idiomatic expressions translate differently, our second method translates an expression in context, and uses a lexical knowledge base to determine if the translation is literal. Our approaches are grounded in linguistic phenomena, and leverage existing sources of lexical knowledge. Our results offer support for both approaches, particularly the former.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.17.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.17/><span class=acl-fixed-case>HYU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Effective Idiomaticity Detection with Consideration at Different Levels of Contextualization</a></strong><br><a href=/people/y/youngju-joung/>Youngju Joung</a>
|
<a href=/people/t/taeuk-kim/>Taeuk Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--17><div class="card-body p-3 small">We propose a unified framework that enables us to consider various aspects of contextualization at different levels to better identify the idiomaticity of multi-word expressions. Through extensive experiments, we demonstrate that our approach based on the inter- and inner-sentence context of a target MWE is effective in improving the performance of related models. We also share our experience in detail on the task of SemEval-2022 Tasks 2 such that future work on the same task can be benefited from this.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.18.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.18/>drsphelps at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Learning idiom representations using <span class=acl-fixed-case>BERTRAM</span></a></strong><br><a href=/people/d/dylan-phelps/>Dylan Phelps</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--18><div class="card-body p-3 small">This paper describes our system for SemEval-2022 Task 2 Multilingual Idiomaticity Detection and Sentence Embedding sub-task B. We modify a standard BERT sentence transformer by adding embeddings for each idiom, which are created using BERTRAM and a small number of contexts. We show that this technique increases the quality of idiom representations and leads to better performance on the task. We also perform analysis on our final results and show that the quality of the produced idiom embeddings is highly sensitive to the quality of the input contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.19.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.19/><span class=acl-fixed-case>JARV</span>ix at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One-Shot Learning</a></strong><br><a href=/people/y/yash-jakhotiya/>Yash Jakhotiya</a>
|
<a href=/people/v/vaibhav-kumar/>Vaibhav Kumar</a>
|
<a href=/people/a/ashwin-pathak/>Ashwin Pathak</a>
|
<a href=/people/r/raj-shah/>Raj Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--19><div class="card-body p-3 small">Large Language Models have been successful in a wide variety of Natural Language Processing tasks by capturing the compositionality of the text representations. In spite of their great success, these vector representations fail to capture meaning of idiomatic multi-word expressions (MWEs). In this paper, we focus on the detection of idiomatic expressions by using binary classification. We use a dataset consisting of the literal and idiomatic usage of MWEs in English and Portuguese. Thereafter, we perform the classification in two different settings: zero shot and one shot, to determine if a given sentence contains an idiom or not. N shot classification for this task is defined by N number of common idioms between the training and testing sets. In this paper, we train multiple Large Language Models in both the settings and achieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score (macro) of 0.85 for the one shot setting. An implementation of our work can be found at <a href=https://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning class=acl-markup-url>https://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.20.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.20/><span class=acl-fixed-case>C</span>ardiff<span class=acl-fixed-case>NLP</span>-Metaphor at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Targeted Fine-tuning of Transformer-based Language Models for Idiomaticity Detection</a></strong><br><a href=/people/j/joanne-boisson/>Joanne Boisson</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--20><div class="card-body p-3 small">This paper describes the experiments ran for SemEval-2022 Task 2, subtask A, zero-shot and one-shot settings for idiomaticity detection. Our main approach is based on fine-tuning transformer-based language models as a baseline to perform binary classification. Our system, CardiffNLP-Metaphor, ranked 8th and 7th (respectively on zero- and one-shot settings on this task. Our main contribution lies in the extensive evaluation of transformer-based language models and various configurations, showing, among others, the potential of large multilingual models over base monolingual models. Moreover, we analyse the impact of various input parameters, which offer interesting insights on how language models work in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.21.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.21/>kpfriends at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: <span class=acl-fixed-case>NEAMER</span> - Named Entity Augmented Multi-word Expression Recognizer</a></strong><br><a href=/people/m/min-sik-oh/>Min Sik Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--21><div class="card-body p-3 small">We present NEAMER - Named Entity Augmented Multi-word Expression Recognizer. This system is inspired by non-compositionality characteristics shared between Named Entity and Idiomatic Expressions. We utilize transfer learning and locality features to enhance idiom classification task. This system is our submission for SemEval Task 2: Multilingual Idiomaticity Detection and Sentence Embedding Subtask A OneShot shared task. We achieve SOTA with F1 0.9395 during post-evaluation phase. We also observe improvement in training stability. Lastly, we experiment with non-compositionality knowledge transfer, cross-lingual fine-tuning and locality features, which we also introduce in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.22.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.22/>daminglu123 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Using <span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>LSTM</span> to Do Text Classification</a></strong><br><a href=/people/d/daming-lu/>Daming Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--22><div class="card-body p-3 small">Multiword expressions (MWEs) or idiomaticity are common phenomenon in natural languages. Current pre-trained language models cannot effectively capture the meaning of these MWEs. The reason is that two normal words, after combining together, could have an abruptly different meaning than the compositionality of the meanings of each word, whereas pre-trained language models reply on words compositionality. We proposed an improved method of adding an LSTM layer to the BERT model in order to get better results on a text classification task (Subtask A). Our result is slightly better than the baseline. We also tried adding TextCNN to BERT and adding both LSTM and TextCNN to BERT. We find that adding only LSTM gives the best performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.23.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.23" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.23/><span class=acl-fixed-case>H</span>i<span class=acl-fixed-case>J</span>o<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Detecting Idiomaticity of Multiword Expressions using Multilingual Pretrained Language Models</a></strong><br><a href=/people/m/minghuan-tan/>Minghuan Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--23><div class="card-body p-3 small">This paper describes an approach to detect idiomaticity only from the contextualized representation of a MWE over multilingual pretrained language models. Our experiments find that larger models are usually more effective in idiomaticity detection. However, using a higher layer of the model may not guarantee a better performance. In multilingual scenarios, the convergence of different languages are not consistent and rich-resource languages have big advantages over other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.24/><span class=acl-fixed-case>Z</span>hichun<span class=acl-fixed-case>R</span>oad at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Adversarial Training and Contrastive Learning for Multiword Representations</a></strong><br><a href=/people/x/xuange-cui/>Xuange Cui</a>
|
<a href=/people/w/wei-xiong/>Wei Xiong</a>
|
<a href=/people/s/songlin-wang/>Songlin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--24><div class="card-body p-3 small">This paper presents our contribution to the SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding.We explore the impact of three different pre-trained multilingual language models in the SubTaskA.By enhancing the model generalization and robustness, we use the exponential moving average (EMA) method and the adversarial attack strategy. In SubTaskB, we add an effective cross-attention module for modeling the relationships of two sentences. We jointly train the model with a contrastive learning objective and employ a momentum contrast to enlarge the number of negative pairs. Additionally, we use the alignment and uniformity properties to measure the quality of sentence embeddings.Our approach obtained competitive results in both subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.25.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.25/><span class=acl-fixed-case>NER</span>4<span class=acl-fixed-case>ID</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Named Entity Recognition for Idiomaticity Detection</a></strong><br><a href=/people/s/simone-tedeschi/>Simone Tedeschi</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--25><div class="card-body p-3 small">Idioms are lexically-complex phrases whose meaning cannot be derived by compositionally interpreting their components. Although the automatic identification and understanding of idioms is essential for a wide range of Natural Language Understanding tasks, they are still largely under-investigated. This motivated the organization of the SemEval-2022 Task 2, which is divided into two multilingual subtasks: one about idiomaticity detection, and the other about sentence embeddings. In this work, we focus on the first subtask and propose a Transformer-based dual-encoder architecture to compute the semantic similarity between a potentially-idiomatic expression and its context and, based on this, predict idiomaticity. Then, we show how and to what extent Named Entity Recognition can be exploited to reduce the degree of confusion of idiom identification systems and, therefore, improve performance. Our model achieves 92.1 F1 in the one-shot setting and shows strong robustness towards unseen idioms achieving 77.4 F1 in the zero-shot setting. We release our code at <a href=https://github.com/Babelscape/ner4id class=acl-markup-url>https://github.com/Babelscape/ner4id</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.26.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.26/><span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Representing Multilingual Idiomaticity based on Contrastive Learning</a></strong><br><a href=/people/k/kuanghong-liu/>Kuanghong Liu</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--26><div class="card-body p-3 small">This paper will present the methods we use as the YNU-HPCC team in the SemEval-2022 Task 2, Multilingual Idiomaticity Detection and Sentence Embedding. We are involved in two subtasks, including four settings. In subtask B of sentence representation, we used novel approaches with ideas of contrastive learning to optimize model, where method of CoSENT was used in the pre-train setting, and triplet loss and multiple negatives ranking loss functions in fine-tune setting. We had achieved very competitive results on the final released test datasets. However, for subtask A of idiomaticity detection, we simply did a few explorations and experiments based on the xlm-RoBERTa model. Sentence concatenated with additional MWE as inputs did well in a one-shot setting. Sentences containing context had a poor performance on final released test data in zero-shot setting even if we attempted to extract effective information from CLS tokens of hidden layers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.27.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.27.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.27/><span class=acl-fixed-case>OCHADAI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Adversarial Training for Multilingual Idiomaticity Detection</a></strong><br><a href=/people/l/lis-pereira/>Lis Pereira</a>
|
<a href=/people/i/ichiro-kobayashi/>Ichiro Kobayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--27><div class="card-body p-3 small">We propose a multilingual adversarial training model for determining whether a sentence contains an idiomatic expression. Given that a key challenge with this task is the limited size of annotated data, our model relies on pre-trained contextual representations from different multi-lingual state-of-the-art transformer-based language models (i.e., multilingual BERT and XLM-RoBERTa), and on adversarial training, a training method for further enhancing model generalization and robustness. Without relying on any human-crafted features, knowledgebase, or additional datasets other than the target datasets, our model achieved competitive results and ranked 6thplace in SubTask A (zero-shot) setting and 15thplace in SubTask A (one-shot) setting</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.28.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.28/><span class=acl-fixed-case>HIT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 2: Pre-trained Language Model for Idioms Detection</a></strong><br><a href=/people/z/zheng-chu/>Zheng Chu</a>
|
<a href=/people/z/ziqing-yang/>Ziqing Yang</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/z/zhigang-chen/>Zhigang Chen</a>
|
<a href=/people/m/ming-liu/>Ming Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--28><div class="card-body p-3 small">The same multi-word expressions may have different meanings in different sentences. They can be mainly divided into two categories, which are literal meaning and idiomatic meaning. Non-contextual-based methods perform poorly on this problem, and we need contextual embedding to understand the idiomatic meaning of multi-word expressions correctly. We use a pre-trained language model, which can provide a context-aware sentence embedding, to detect whether multi-word expression in the sentence is idiomatic usage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.29.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.29.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.29/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 3: <span class=acl-fixed-case>P</span>re<span class=acl-fixed-case>TENS</span>-Evaluating Neural Networks on Presuppositional Semantic Knowledge</a></strong><br><a href=/people/r/roberto-zamparelli/>Roberto Zamparelli</a>
|
<a href=/people/s/shammur-chowdhury/>Shammur Chowdhury</a>
|
<a href=/people/d/dominique-brunato/>Dominique Brunato</a>
|
<a href=/people/c/cristiano-chesi/>Cristiano Chesi</a>
|
<a href=/people/f/felice-dellorletta/>Felice Dell’Orletta</a>
|
<a href=/people/m/md-arid-hasan/>Md. Arid Hasan</a>
|
<a href=/people/g/giulia-venturi/>Giulia Venturi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--29><div class="card-body p-3 small">We report the results of the SemEval 2022 Task 3, PreTENS, on evaluation the acceptability of simple sentences containing constructions whose two arguments are presupposed to be or not to be in an ordered taxonomic relation. The task featured two sub-tasks articulated as: <i>(i)</i> binary prediction task and <i>(ii)</i> regression task, predicting the acceptability in a continuous scale. The sentences were artificially generated in three languages (English, Italian and French). 21 systems, with 8 system papers were submitted for the task, all based on various types of fine-tuned transformer systems, often with ensemble methods and various data augmentation techniques. The best systems reached an F1-macro score of 94.49 (sub-task1) and a Spearman correlation coefficient of 0.80 (sub-task2), with interesting variations in specific constructions and/or languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.30.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.30.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.30/><span class=acl-fixed-case>L</span>ing<span class=acl-fixed-case>J</span>ing at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 3: Applying <span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>BERT</span>a to Lexical-level Presupposed Relation Taxonomy with Knowledge Transfer</a></strong><br><a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/b/bin-li/>Bin Li</a>
|
<a href=/people/y/yixuan-weng/>Yixuan Weng</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/b/bin-sun/>Bin Sun</a>
|
<a href=/people/s/shutao-li/>Shutao Li</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--30><div class="card-body p-3 small">This paper presents the results and main findings of our system on SemEval-2022 Task 3 Presupposed Taxonomies: Evaluating Neural Network Semantics (PreTENS). This task aims at semantic competence with specific attention on the evaluation of language models, which is a task with respect to the recognition of appropriate taxonomic relations between two nominal arguments. Two sub-tasks including binary classification and regression are designed for the evaluation. For the classification sub-task, we adopt the DeBERTa-v3 pre-trained model for fine-tuning datasets of different languages. Due to the small size of the training datasets of the regression sub-task, we transfer the knowledge of classification model (i.e., model parameters) to the regression task. The experimental results show that the proposed method achieves the best results on both sub-tasks. Meanwhile, we also report negative results of multiple training strategies for further discussion. All the experimental codes are open-sourced at <a href=https://github.com/WENGSYX/Semeval class=acl-markup-url>https://github.com/WENGSYX/Semeval</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.31.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.31.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.31/><span class=acl-fixed-case>RUG</span>-1-Pegasussers at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 3: Data Generation Methods to Improve Recognizing Appropriate Taxonomic Word Relations</a></strong><br><a href=/people/w/wessel-poelman/>Wessel Poelman</a>
|
<a href=/people/g/gijs-danoe/>Gijs Danoe</a>
|
<a href=/people/e/esther-ploeger/>Esther Ploeger</a>
|
<a href=/people/f/frank-van-den-berg/>Frank van den Berg</a>
|
<a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/l/lukas-edman/>Lukas Edman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--31><div class="card-body p-3 small">This paper describes our system created for the SemEval 2022 Task 3: Presupposed Taxonomies - Evaluating Neural-network Semantics. This task is focused on correctly recognizing taxonomic word relations in English, French and Italian. We developed various datageneration techniques that expand the originally provided train set and show that all methods increase the performance of modelstrained on these expanded datasets. Our final system outperformed the baseline system from the task organizers by achieving an average macro F1 score of 79.6 on all languages, compared to the baseline’s 67.4.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.32.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.32/><span class=acl-fixed-case>CSECU</span>-<span class=acl-fixed-case>DSG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 3: Investigating the Taxonomic Relationship Between Two Arguments using Fusion of Multilingual Transformer Models</a></strong><br><a href=/people/a/abdul-aziz/>Abdul Aziz</a>
|
<a href=/people/m/md-akram-hossain/>Md. Akram Hossain</a>
|
<a href=/people/a/abu-nowshed-chy/>Abu Nowshed Chy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--32><div class="card-body p-3 small">Recognizing lexical relationships between words is one of the formidable tasks in computational linguistics. It plays a vital role in the improvement of various NLP tasks. However, the diversity of word semantics, sentence structure as well as word order information make it challenging to distill the relationship effectively. To address these challenges, SemEval-2022 Task 3 introduced a shared task PreTENS focusing on semantic competence to determine the taxonomic relations between two nominal arguments. This paper presents our participation in this task where we proposed an approach through exploiting an ensemble of multilingual transformer methods. We employed two fine-tuned multilingual transformer models including XLM-RoBERTa and mBERT to train our model. To enhance the performance of individual models, we fuse the predicted probability score of these two models using weighted arithmetic mean to generate a unified probability score. The experimental results showed that our proposed method achieved competitive performance among the participants’ methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.33.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.33/><span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>R</span>-<span class=acl-fixed-case>NCL</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 3: Fine-Tuning the <span class=acl-fixed-case>BERT</span>-Based Models for Validating Taxonomic Relations</a></strong><br><a href=/people/t/thanet-markchom/>Thanet Markchom</a>
|
<a href=/people/h/huizhi-liang/>Huizhi Liang</a>
|
<a href=/people/j/jiaoyan-chen/>Jiaoyan Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--33><div class="card-body p-3 small">In human languages, there are many presuppositional constructions that impose a constrain on the taxonomic relations between two nouns depending on their order. These constructions create a challenge in validating taxonomic relations in real-world contexts. In SemEval2022-Task3 Presupposed Taxonomies: Evaluating Neural Network Semantics (PreTENS), the organizers introduced a task regarding validating the taxonomic relations within a variety of presuppositional constructions. This task is divided into two subtasks: classification and regression. Each subtask contains three datasets in multiple languages, i.e., English, Italian and French. To tackle this task, this work proposes to fine-tune different BERT-based models pre-trained on different languages. According to the experimental results, the fine-tuned BERT-based models are effective compared to the baselines in classification. For regression, the fine-tuned models show promising performance with the possibility of improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.34.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.34.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.34/><span class=acl-fixed-case>SPDB</span> Innovation Lab at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 3: Recognize Appropriate Taxonomic Relations Between Two Nominal Arguments with <span class=acl-fixed-case>ERNIE</span>-<span class=acl-fixed-case>M</span> Model</a></strong><br><a href=/people/y/yue-zhou/>Yue Zhou</a>
|
<a href=/people/b/bowei-wei/>Bowei Wei</a>
|
<a href=/people/j/jianyu-liu/>Jianyu Liu</a>
|
<a href=/people/y/yang-yang/>Yang Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--34><div class="card-body p-3 small">Synonym and antonym practice are the most common practices in our early childhood. It correlated our known words to a better place deep in our intuition. At the beginning of life for a machine, we would like to treat the machine as a baby and built a similar training for it as well to present a qualified performance. In this paper, we present an ensemble model for sentence logistics classification, which outperforms the state-of-art methods. Our approach essentially builds on two models including ERNIE-M and DeBERTaV3. With cross validation and random seeds tuning, we select the top performance models for the last soft ensemble and make them vote for the final answer, achieving the top 6 performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.35.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.35.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.35" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.35/><span class=acl-fixed-case>UU</span>-Tax at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 3: Improving the generalizability of language models for taxonomy classification through data augmentation</a></strong><br><a href=/people/i/injy-sarhan/>Injy Sarhan</a>
|
<a href=/people/p/pablo-mosteiro/>Pablo Mosteiro</a>
|
<a href=/people/m/marco-spruit/>Marco Spruit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--35><div class="card-body p-3 small">This paper presents our strategy to address the SemEval-2022 Task 3 PreTENS: Presupposed Taxonomies Evaluating Neural Network Semantics. The goal of the task is to identify if a sentence is deemed acceptable or not, depending on the taxonomic relationship that holds between a noun pair contained in the sentence. For sub-task 1—binary classification—we propose an effective way to enhance the robustness and the generalizability of language models for better classification on this downstream task. We design a two-stage fine-tuning procedure on the ELECTRA language model using data augmentation techniques. Rigorous experiments are carried out using multi-task learning and data-enriched fine-tuning. Experimental results demonstrate that our proposed model, UU-Tax, is indeed able to generalize well for our downstream task. For sub-task 2 —regression—we propose a simple classifier that trains on features obtained from Universal Sentence Encoder (USE). In addition to describing the submitted systems, we discuss other experiments that employ pre-trained language models and data augmentation techniques. For both sub-tasks, we perform error analysis to further understand the behaviour of the proposed models. We achieved a global F1<span class=tex-math>Binary</span> score of 91.25% in sub-task 1 and a rho score of 0.221 in sub-task 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.36.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.36/><span class=acl-fixed-case>K</span>a<span class=acl-fixed-case>M</span>i<span class=acl-fixed-case>K</span>la at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 3: <span class=acl-fixed-case>A</span>l<span class=acl-fixed-case>BERT</span>o, <span class=acl-fixed-case>BERT</span>, and <span class=acl-fixed-case>C</span>amem<span class=acl-fixed-case>BERT</span>—<span class=acl-fixed-case>B</span>e(r)tween Taxonomy Detection and Prediction</a></strong><br><a href=/people/k/karl-vetter/>Karl Vetter</a>
|
<a href=/people/m/miriam-segiet/>Miriam Segiet</a>
|
<a href=/people/k/klara-lennermann/>Klara Lennermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--36><div class="card-body p-3 small">This paper describes our system submitted for SemEval Task 3: Presupposed Taxonomies: Evaluating Neural Network Semantics (Zamparelli et al., 2022). We participated in both the binary classification and the regression subtask. Target sentences are classified according to their taxonomical relation in subtask 1 and according to their acceptability judgment in subtask 2. Our approach in both subtasks is based on a neural network BERT model. We used separate models for the three languages covered by the task, English, French, and Italian. For the second subtask, we used median averaging to construct an ensemble model. We ranked 15th out of 21 groups for subtask 1 (F1-score: 77.38%) and 11th out of 17 groups for subtask 2 (RHO: 0.078).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.37.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.37.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.37/><span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 3: A Unified Approach Fine-tuned on Multilingual Pretrained Model for <span class=acl-fixed-case>P</span>re<span class=acl-fixed-case>TENS</span></a></strong><br><a href=/people/y/yinglu-li/>Yinglu Li</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/x/xiaosong-qiao/>Xiaosong Qiao</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--37><div class="card-body p-3 small">In the paper, we describe a unified system for task 3 of SemEval-2022. The task aims to recognize the semantic structures of sentences by providing two nominal arguments and to evaluate the degree of taxonomic relations. We utilise the strategy that adding language prefix tag in the training set, which is effective for the model. We split the training set to avoid the translation information to be learnt by the model. For the task, we propose a unified model fine-tuned on the multilingual pretrained model, XLM-RoBERTa. The model performs well in subtask 1 (the binary classification subtask). In order to verify whether our model could also perform better in subtask 2 (the regression subtask), the ranking score is transformed into classification labels by an up-sampling strategy. With the ensemble strategy, the performance of our model can be also improved. As a result, the model obtained the second place for subtask 1 and subtask 2 in the competition evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.38.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.38.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.38/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Patronizing and Condescending Language Detection</a></strong><br><a href=/people/c/carla-perez-almendros/>Carla Perez-Almendros</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--38><div class="card-body p-3 small">This paper presents an overview of Task 4 at SemEval-2022, which was focused on detecting Patronizing and Condescending Language (PCL) towards vulnerable communities. Two sub-tasks were considered: a binary classification task, where participants needed to classify a given paragraph as containing PCL or not, and a multi-label classification task, where participants needed to identify which types of PCL are present (if any). The task attracted more than 300 participants, 77 teams and 229 valid submissions. We provide an overview of how the task was organized, discuss the techniques that were employed by the different participants, and summarize the main resulting insights about PCL detection and categorization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.39.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.39/><span class=acl-fixed-case>JUST</span>-<span class=acl-fixed-case>DEEP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Using Deep Learning Techniques to Reveal Patronizing and Condescending Language</a></strong><br><a href=/people/m/mohammad-makahleh/>Mohammad Makahleh</a>
|
<a href=/people/n/naba-bani-yaseen/>Naba Bani Yaseen</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--39><div class="card-body p-3 small">Classification of language that favors or condones vulnerable communities (e.g., refugees, homeless, widows) has been considered a challenging task and a critical step in NLP applications. Moreover, the spread of this language among people and on social media harms society and harms the people concerned. Therefore, the classification of this language is considered a significant challenge for researchers in the world. In this paper, we propose JUST-DEEP architecture to classify a text and determine if it contains any form of patronizing and condescending language (Task 4- Subtask 1). The architecture uses state-of-art pre-trained models and empowers ensembling techniques that outperform the baseline (RoBERTa) in the SemEval-2022 task4 with a 0.502 F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.40.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.40.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.40/><span class=acl-fixed-case>PINGAN</span> Omini-Sinitic at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Multi-prompt Training for Patronizing and Condescending Language Detection</a></strong><br><a href=/people/y/ye-wang/>Ye Wang</a>
|
<a href=/people/y/yanmeng-wang/>Yanmeng Wang</a>
|
<a href=/people/b/baishun-ling/>Baishun Ling</a>
|
<a href=/people/z/zexiang-liao/>Zexiang Liao</a>
|
<a href=/people/s/shaojun-wang/>Shaojun Wang</a>
|
<a href=/people/j/jing-xiao/>Jing Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--40><div class="card-body p-3 small">This paper describes the second-placed system for subtask 2 and the ninth-placed system for subtask 1 in SemEval 2022 Task 4: Patronizing and Condescending Language Detection. We propose an ensemble of prompt training and label attention mechanism for multi-label classification tasks. Transfer learning is introduced to transfer the knowledge from binary classification to multi-label classification. The experimental results proved the effectiveness of our proposed method. The ablation study is also conducted to show the validity of each technique.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.41.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.41/><span class=acl-fixed-case>BEIKE</span> <span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Prompt-Based Paragraph Classification for Patronizing and Condescending Language Detection</a></strong><br><a href=/people/y/yong-deng/>Yong Deng</a>
|
<a href=/people/c/chenxiao-dou/>Chenxiao Dou</a>
|
<a href=/people/l/liang-yu-chen/>Liangyu Chen</a>
|
<a href=/people/d/deqiang-miao/>Deqiang Miao</a>
|
<a href=/people/x/xianghui-sun/>Xianghui Sun</a>
|
<a href=/people/b/baochang-ma/>Baochang Ma</a>
|
<a href=/people/x/xiangang-li/>Xiangang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--41><div class="card-body p-3 small">PCL detection task is aimed at identifying and categorizing language that is patronizing or condescending towards vulnerable communities in the general media. Compared to other NLP tasks of paragraph classification, the negative language presented in the PCL detection task is usually more implicit and subtle to be recognized, making the performance of common text classification approaches disappointed. Targeting the PCL detection problem in SemEval-2022 Task 4, in this paper, we give an introduction to our team’s solution, which exploits the power of prompt-based learning on paragraph classification. We reformulate the task as an appropriate cloze prompt and use pre2trained Masked Language Models to fill the cloze slot. For the two subtasks, binary classification and multi-label classification, DeBERTa model is adopted and fine-tuned to predict masked label words of task-specific prompts. On the evaluation dataset, for binary classification, our approach achieves an F1-score of 0.6406; for multi-label classification, our approach achieves an macro-F1-score of 0.4689 and ranks first in the leaderboard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.42.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.42.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.42" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.42/><span class=acl-fixed-case>DH</span>-<span class=acl-fixed-case>FBK</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Leveraging Annotators’ Disagreement and Multiple Data Views for Patronizing Language Detection</a></strong><br><a href=/people/a/alan-ramponi/>Alan Ramponi</a>
|
<a href=/people/e/elisa-leonardelli/>Elisa Leonardelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--42><div class="card-body p-3 small">The subtle and typically unconscious use of patronizing and condescending language (PCL) in large-audience media outlets undesirably feeds stereotypes and strengthens power-knowledge relationships, perpetuating discrimination towards vulnerable communities. Due to its subjective and subtle nature, PCL detection is an open and challenging problem, both for computational methods and human annotators. In this paper we describe the systems submitted by the DH-FBK team to SemEval-2022 Task 4, aiming at detecting PCL towards vulnerable communities in English media texts. Motivated by the subjectivity of human interpretation, we propose to leverage annotators’ uncertainty and disagreement to better capture the shades of PCL in a multi-task, multi-view learning framework. Our approach achieves competitive results, largely outperforming baselines and ranking on the top-left side of the leaderboard on both PCL identification and classification. Noticeably, our approach does not rely on any external data or model ensemble, making it a viable and attractive solution for real-world use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.43.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.43.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.43/><span class=acl-fixed-case>PALI</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Discriminative Fine-tuning of Transformers for Patronizing and Condescending Language Detection</a></strong><br><a href=/people/d/dou-hu/>Dou Hu</a>
|
<a href=/people/z/zhou-mengyuan/>Zhou Mengyuan</a>
|
<a href=/people/x/xiyang-du/>Xiyang Du</a>
|
<a href=/people/m/mengfei-yuan/>Mengfei Yuan</a>
|
<a href=/people/j/jin-zhi/>Jin Zhi</a>
|
<a href=/people/l/lianxin-jiang/>Lianxin Jiang</a>
|
<a href=/people/m/mo-yang/>Mo Yang</a>
|
<a href=/people/x/xiaofeng-shi/>Xiaofeng Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--43><div class="card-body p-3 small">Patronizing and condescending language (PCL) has a large harmful impact and is difficult to detect, both for human judges and existing NLP systems. At SemEval-2022 Task 4, we propose a novel Transformer-based model and its ensembles to accurately understand such language context for PCL detection. To facilitate comprehension of the subtle and subjective nature of PCL, two fine-tuning strategies are applied to capture discriminative features from diverse linguistic behaviour and categorical distribution. The system achieves remarkable results on the official ranking, including 1st in Subtask 1 and 5th in Subtask 2. Extensive experiments on the task demonstrate the effectiveness of our system and its strategies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.44.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.44/><span class=acl-fixed-case>ASR</span>trans at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Ensemble of Tuned Transformer-based Models for <span class=acl-fixed-case>PCL</span> Detection</a></strong><br><a href=/people/a/ailneni-rakshitha-rao/>Ailneni Rakshitha Rao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--44><div class="card-body p-3 small">Patronizing behavior is a subtle form of bullying and when directed towards vulnerable communities, it can arise inequalities. This paper describes our system for Task 4 of SemEval-2022: Patronizing and Condescending Language Detection (PCL). We participated in both the sub-tasks and conducted extensive experiments to analyze the effects of data augmentation and loss functions used, to tackle the problem of class imbalance. We explore whether large transformer-based models can capture the intricacies associated with PCL detection. Our solution consists of an ensemble of the RoBERTa model which is further trained on external data and other language models such as XLNeT, Ernie-2.0, and BERT. We also present the results of several problem transformation techniques such as Classifier Chains, Label Powerset, and Binary relevance for multi-label classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.45.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.45/><span class=acl-fixed-case>L</span>ast<span class=acl-fixed-case>R</span>esort at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Towards Patronizing and Condescending Language Detection using Pre-trained Transformer Based Models Ensembles</a></strong><br><a href=/people/s/samyak-agrawal/>Samyak Agrawal</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--45><div class="card-body p-3 small">This paper presents our solutions systems for Task4 at SemEval2022: Patronizing and Condescending Language Detection. This shared task contains two sub-tasks. The first sub-task is a binary classification task whose goal is to predict whether a given paragraph contains any form of patronising or condescending language(PCL). For the second sub-task, given a paragraph, we have to find which PCL categories express the condescension. Here we have a total of 7 overlapping sub-categories for PCL. Our proposed solution uses BERT based ensembled models with hard voting and techniques applied to take care of class imbalances. Our paper describes the system architecture of the submitted solution and other experiments that we conducted.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.46.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.46.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.46/>Felix&amp;Julia at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Patronizing and Condescending Language Detection</a></strong><br><a href=/people/f/felix-herrmann/>Felix Herrmann</a>
|
<a href=/people/j/julia-krebs/>Julia Krebs</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--46><div class="card-body p-3 small">This paper describes the authors’ submission to the SemEval-2022 task 4: Patronizing and Condescending Language (PCL) Detection. The aim of the task is the detection and classification of PCL in an annotated dataset. Subtask 1 includes a binary classification task (PCL or not PCL). Subtask 2 is a multi label classification task where the system identifies different categories of PCL. The authors of this paper submitted two different models: one RoBERTa model and one DistilBERT model. Both systems performed better than the random and RoBERTA baseline given by the task organizers. The RoBERTA model finetuned by the authors performed better in both subtasks than the DistilBERT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.47.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.47.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.47/><span class=acl-fixed-case>MS</span>@<span class=acl-fixed-case>IW</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Patronising and Condescending Language Detection with Synthetically Generated Data</a></strong><br><a href=/people/s/selina-meyer/>Selina Meyer</a>
|
<a href=/people/m/maximilian-schmidhuber/>Maximilian Schmidhuber</a>
|
<a href=/people/u/udo-kruschwitz/>Udo Kruschwitz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--47><div class="card-body p-3 small">In this description paper we outline the system architecture submitted to Task 4, Subtask 1 at SemEval-2022. We leverage the generative power of state of the art generative pretrained transformer models to increase training set size and remedy class imbalance issues. Our best submitted system is trained on a synthetically enhanced dataset with 10.3 times as many positive samples as the original dataset and reaches an F1 score of 50.62%, which is 10 percentage points higher than our initial system trained on an undersampled version of the original dataset. We explore possible reasons for the comparably low score in the overall task ranking and report on experiments conducted during the post-evaluation phase.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.48.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.48.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--48 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.48 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.48/>Team <span class=acl-fixed-case>LEGO</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Machine Learning Methods for <span class=acl-fixed-case>PCL</span> Detection</a></strong><br><a href=/people/a/abhishek-singh/>Abhishek Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--48><div class="card-body p-3 small">In this paper, we present our submission to the SemEval 2022 - Task 4 on Patronizing and Condescending Language (PCL) detection. Weapproach this problem as a traditional text classification problem with machine learning (ML)methods. We experiment and investigate theuse of various ML algorithms for detecting PCL in news articles. Our best methodology achieves an F1- Score of 0.39 for subtask1 witha rank of 63 out of 80, and F1-score of 0.082for subtask2 with a rank of 41 out of 48 on the blind dataset provided in the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.49.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.49/><span class=acl-fixed-case>RNRE</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Patronizing and Condescending Language Detection</a></strong><br><a href=/people/r/rylan-yang/>Rylan Yang</a>
|
<a href=/people/e/ethan-chi/>Ethan Chi</a>
|
<a href=/people/n/nathan-chi/>Nathan Chi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--49><div class="card-body p-3 small">An understanding of patronizing and condescending language detection is an important part of identifying and addressing discrimination and prejudice in various forms of communication. In this paper, we investigate several methods for detecting patronizing and condescending language in short statements as part of SemEval-2022 Task 4. For Task 1a, we investigate applying both lightweight (tree-based and linear) machine learning classification models and fine-tuned pre-trained large language models. Our final system achieves an F1-score of 0.4321, recall-score of 0.5016, and a precision-score of 0.3795 (ranked 53 / 78) on Task 1a.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.50.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.50.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.50/><span class=acl-fixed-case>UTSA</span> <span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: An Exploration of Simple Ensembles of Transformers, Convolutional, and Recurrent Neural Networks</a></strong><br><a href=/people/x/xingmeng-zhao/>Xingmeng Zhao</a>
|
<a href=/people/a/anthony-rios/>Anthony Rios</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--50><div class="card-body p-3 small">The act of appearing kind or helpful via the use of but having a feeling of superiority condescending and patronizing language can have have serious mental health implications to those that experience it. Thus, detecting this condescending and patronizing language online can be useful for online moderation systems. Thus, in this manuscript, we describe the system developed by Team UTSA SemEval-2022 Task 4, Detecting Patronizing and Condescending Language. Our approach explores the use of several deep learning architectures including RoBERTa, convolutions neural networks, and Bidirectional Long Short-Term Memory Networks. Furthermore, we explore simple and effective methods to create ensembles of neural network models. Overall, we experimented with several ensemble models and found that the a simple combination of five RoBERTa models achieved an F-score of .6441 on the development dataset and .5745 on the final test dataset. Finally, we also performed a comprehensive error analysis to better understand the limitations of the model and provide ideas for further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.51.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.51" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.51/><span class=acl-fixed-case>A</span>li<span class=acl-fixed-case>E</span>dalat at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Patronizing and Condescending Language Detection using Fine-tuned Language Models, <span class=acl-fixed-case>BERT</span>+<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>GRU</span>, and Ensemble Models</a></strong><br><a href=/people/a/ali-edalat/>Ali Edalat</a>
|
<a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/b/behnam-bahrak/>Behnam Bahrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--51><div class="card-body p-3 small">This paper presents the AliEdalat team’s methodology and results in SemEval-2022 Task 4: Patronizing and Condescending Language (PCL) Detection. This task aims to detect the presence of PCL and PCL categories in text in order to prevent further discrimination against vulnerable communities. We use an ensemble of three basic models to detect the presence of PCL: fine-tuned bigbird, fine-tuned mpnet, and BERT+BiGRU. The ensemble model performs worse than the baseline due to overfitting and achieves an F1-score of 0.3031. We offer another solution to resolve the submitted model’s problem. We consider the different categories of PCL separately. To detect each category of PCL, we act like a PCL detector. Instead of BERT+BiGRU, we use fine-tuned roberta in the models. In PCL category detection, our model outperforms the baseline model and achieves an F1-score of 0.2531. We also present new models for detecting two categories of PCL that outperform the submitted models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.52.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.52.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.52/>Tesla at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Patronizing and Condescending Language Detection using Transformer-based Models with Data Augmentation</a></strong><br><a href=/people/s/sahil-bhatt/>Sahil Bhatt</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--52><div class="card-body p-3 small">This paper describes our system for Task 4 of SemEval 2022: Patronizing and Condescending Language (PCL) Detection. For sub-task 1, where the objective is to classify a text as PCL or non-PCL, we use a T5 Model fine-tuned on the dataset. For sub-task 2, which is a multi-label classification problem, we use a RoBERTa model fine-tuned on the dataset. Given that the key challenge in this task is classification on an imbalanced dataset, our models rely on an augmented dataset that we generate using paraphrasing. We found that these two models yield the best results out of all the other approaches we tried.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.53.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.53/><span class=acl-fixed-case>SSN</span>_<span class=acl-fixed-case>NLP</span>_<span class=acl-fixed-case>MLRG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Ensemble Learning strategies to detect Patronizing and Condescending Language</a></strong><br><a href=/people/k/kalaivani-adaikkan/>Kalaivani Adaikkan</a>
|
<a href=/people/t/thenmozhi-durairaj/>Thenmozhi Durairaj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--53><div class="card-body p-3 small">In this paper, we describe our efforts at SemEval 2022 Shared Task 4 on Patronizing and Condescending Language (PCL) Detection. This is the first shared task to detect PCL which is to identify and categorize PCL language towards vulnerable communities. The shared task consists of two subtasks: Patronizing and Condescending language detection (Subtask A) which is the binary task classification and identifying the PCL categories that express the condescension (Subtask B) which is the multi-label text classification. For PCL language detection, We proposed the ensemble strategies of a system combination of BERT, Roberta, Distilbert, Roberta large, Albert achieved the official results for Subtask A with a macro f1 score of 0.5172 on the test set which is improved by baseline score. For PCL Category identification, We proposed a multi-label classification model to ensemble the various Bert-based models and the official results for Subtask B with a macro f1 score of 0.2117 on the test set which is improved by baseline score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.54.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.54/>Sapphire at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: A Patronizing and Condescending Language Detection Model Based on Capsule Networks</a></strong><br><a href=/people/s/sihui-li/>Sihui Li</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--54><div class="card-body p-3 small">This paper introduces the related work and the results of Team Sapphire’s system for SemEval-2022 Task 4: Patronizing and Condescending Language Detection. We only participated in subtask 1. The task goal is to judge whether a news text contains PCL. This task can be considered as a task of binary classification of news texts. In this binary classification task, the BERT-base model is adopted as the pre-trained model used to represent textual information in vector form and encode it. Capsule networks is adopted to extract features from the encoded vectors. The official evaluation metric for subtask 1 is the F1 score over the positive class. Finally, our system’s submitted prediction results on test set achieved the score of 0.5187.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.55.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.55" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.55/><span class=acl-fixed-case>M</span>c<span class=acl-fixed-case>R</span>ock at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Patronizing and Condescending Language Detection using Multi-Channel <span class=acl-fixed-case>CNN</span>, Hybrid <span class=acl-fixed-case>LSTM</span>, <span class=acl-fixed-case>D</span>istil<span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>XLN</span>et</a></strong><br><a href=/people/m/marco-siino/>Marco Siino</a>
|
<a href=/people/m/marco-cascia/>Marco Cascia</a>
|
<a href=/people/i/ilenia-tinnirello/>Ilenia Tinnirello</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--55><div class="card-body p-3 small">In this paper we propose four deep learning models for the task of detecting and classifying Patronizing and Condescending Language (PCL) using a corpus of over 13,000 annotated paragraphs in English. The task, hosted at SemEval-2022, consists of two different subtasks. The Subtask 1 is a binary classification problem. Namely, given a paragraph, a system must predict whether or not it contains any form of PCL. The Subtask 2 is a multi-label classification task. Given a paragraph, a system must identify which PCL categories express the condescension. A paragraph might contain one or more categories of PCL. To face with the first subtask we propose a multi-channel Convolutional Neural Network (CNN) and an Hybrid LSTM. Using the multi-channel CNN we explore the impact of parallel word emebeddings and convolutional layers involving different kernel sizes. With Hybrid LSTM we focus on extracting features in advance, thanks to a convolutional layer followed by two bidirectional LSTM layers. For the second subtask a Transformer BERT-based model (i.e. DistilBERT) and an XLNet-based model are proposed. The multi-channel CNN model is able to reach an F1 score of 0.2928, the Hybrid LSTM modelis able to reach an F1 score of 0.2815, the DistilBERT-based one an average F1 of 0.2165 and the XLNet an average F1 of 0.2296. In this paper, in addition to system descriptions, we also provide further analysis of the results, highlighting strengths and limitations. We make all the code publicly available and reusable on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.56.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.56/>Team <span class=acl-fixed-case>S</span>tanford <span class=acl-fixed-case>ACML</span>ab at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2022 Task 4: Textual Analysis of <span class=acl-fixed-case>PCL</span> Using Contextual Word Embeddings</a></strong><br><a href=/people/u/upamanyu-dass-vattam/>Upamanyu Dass-Vattam</a>
|
<a href=/people/s/spencer-wallace/>Spencer Wallace</a>
|
<a href=/people/r/rohan-sikand/>Rohan Sikand</a>
|
<a href=/people/z/zach-witzel/>Zach Witzel</a>
|
<a href=/people/j/jillian-tang/>Jillian Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--56><div class="card-body p-3 small">We propose the use of a contextual embedding based-neural model on strictly textual inputs to detect the presence of patronizing or condescending language (PCL). We finetuned a pre-trained BERT model to detect whether or not a paragraph contained PCL (Subtask 1), and furthermore finetuned another pre-trained BERT model to identify the linguistic techniques used to convey the PCL (Subtask 2). Results show that this approach is viable for binary classification of PCL, but breaks when attempting to identify the PCL techniques. Our system placed 32/79 for subtask 1, and 40/49 for subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.57.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.57.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.57/>Team <span class=acl-fixed-case>LRL</span>_<span class=acl-fixed-case>NC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Binary and Multi-label Classification of <span class=acl-fixed-case>PCL</span> using Fine-tuned Transformer-based Models</a></strong><br><a href=/people/k/kushagri-tandon/>Kushagri Tandon</a>
|
<a href=/people/n/niladri-chatterjee/>Niladri Chatterjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--57><div class="card-body p-3 small">Patronizing and condescending language (PCL) can find its way into many mediums of public discourse. Presence of PCL in text can produce negative effects in the society. The challenge presented by the task emerges from the subtleties of PCL and various data dependent constraints. Hence, developing techniques to detect PCL in text, before it is propagated is vital. The aim of this paper is twofold, a) to present systems that can be used to classify a text as containing PCL or not, and b) to present systems that assign the different categories of PCL present in text. The proposed systems are primarily rooted in transformer-based pre-trained language models. Among the models submitted for Subtask 1, the best F1-Score of 0.5436 was achieved by a deep learning based ensemble model. This system secured the rank 29 in the official task ranking. For Subtask 2, the best macro-average F1-Score of 0.339 was achieved by an ensemble model combining transformer-based neural architecture with gradient boosting label-balanced classifiers. This system secured the rank 21 in the official task ranking. Among subsequently carried out experiments a variation in architecture of a system for Subtask 2 achieved a macro-average F1-Score of 0.3527.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.58.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.58.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.58/><span class=acl-fixed-case>GUTS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Adversarial Training and Balancing Methods for Patronizing and Condescending Language Detection</a></strong><br><a href=/people/j/junyu-lu/>Junyu Lu</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/t/tongyue-zhang/>Tongyue Zhang</a>
|
<a href=/people/h/hongbo-wang/>Hongbo Wang</a>
|
<a href=/people/h/haohao-zhu/>Haohao Zhu</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a>
|
<a href=/people/h/hongfei-lin/>Hongfei Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--58><div class="card-body p-3 small">Patronizing and Condescending Language (PCL) towards vulnerable communities in general media has been shown to have potentially harmful effects. Due to its subtlety and the good intentions behind its use, the audience is not aware of the language’s toxicity. In this paper, we present our method for the SemEval-2022 Task4 titled “Patronizing and Condescending Language Detection”. In Subtask A, a binary classification task, we introduce adversarial training based on Fast Gradient Method (FGM) and employ pre-trained model in a unified architecture. For Subtask B, framed as a multi-label classification problem, we utilize various improved multi-label cross-entropy loss functions and analyze the performance of our method. In the final evaluation, our system achieved official rankings of 17/79 and 16/49 on Subtask A and Subtask B, respectively. In addition, we explore the relationship between PCL and emotional polarity and intensity it contains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.59.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.59/><span class=acl-fixed-case>HITMI</span>&<span class=acl-fixed-case>T</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Investigating Task-Adaptive Pretraining And Attention Mechanism On <span class=acl-fixed-case>PCL</span> Detection</a></strong><br><a href=/people/z/zihang-liu/>Zihang Liu</a>
|
<a href=/people/y/yancheng-he/>Yancheng He</a>
|
<a href=/people/f/feiqing-zhuang/>Feiqing Zhuang</a>
|
<a href=/people/b/bing-xu/>Bing Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--59><div class="card-body p-3 small">This paper describes the system for the Semeval-2022 Task4 ”Patronizing and Condescending Language Detection”.An entity engages in Patronizing and Condescending Language(PCL) when its language use shows a superior attitude towards others or depicts them in a compassionate way. The task contains two parts. The first one is to identify whether the sentence is PCL, and the second one is to categorize PCL. Through experimental verification, the Roberta-based model will be used in our system. Respectively, for subtask 1, that is, to judge whether a sentence is PCL, the method of retraining the model with specific task data is adopted, and the method of splicing [CLS] and the keyword representation of the last three layers as the representation of the sentence; for subtask 2, that is, to judge the PCL type of the sentence, in addition to using the same method as task1, the method of selecting a special loss for Multi-label text classification is applied. We give a clear ablation experiment and give the effect of each method on the final result. Our project ranked 11th out of 79 teams participating in subtask 1 and 6th out of 49 teams participating in subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.60.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.60/><span class=acl-fixed-case>UM</span>ass <span class=acl-fixed-case>PCL</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Pre-trained Language Model Ensembles for Detecting Patronizing and Condescending Language</a></strong><br><a href=/people/d/david-koleczek/>David Koleczek</a>
|
<a href=/people/a/alexander-scarlatos/>Alexander Scarlatos</a>
|
<a href=/people/p/preshma-linet-pereira/>Preshma Linet Pereira</a>
|
<a href=/people/s/siddha-makarand-karkare/>Siddha Makarand Karkare</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--60><div class="card-body p-3 small">Patronizing and condescending language (PCL) is everywhere, but rarely is the focus on its use by media towards vulnerable communities. Accurately detecting PCL of this form is a difficult task due to limited labeled data and how subtle it can be. In this paper, we describe our system for detecting such language which was submitted to SemEval 2022 Task 4: Patronizing and Condescending Language Detection. Our approach uses an ensemble of pre-trained language models, data augmentation, and optimizing the threshold for detection. Experimental results on the evaluation dataset released by the competition hosts show that our work is reliably able to detect PCL, achieving an F1 score of 55.47% on the binary classification task and a macro F1 score of 36.25% on the fine-grained, multi-label detection task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.61.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.61/><span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Finetuning Pretrained Language Models for Patronizing and Condescending Language Detection</a></strong><br><a href=/people/w/wenqiang-bai/>Wenqiang Bai</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--61><div class="card-body p-3 small">This paper describes a system built in the SemEval-2022 competition. As participants in Task 4: Patronizing and Condescending Language Detection, we implemented the text sentiment classification system for two subtasks in English. Both subtasks involve determining emotions; subtask 1 requires us to determine whether the text belongs to the PCL category (single-label classification), and subtask 2 requires us to determine to which PCL category the text belongs (multi-label classification). Our system is based on the bidirectional encoder representations from transformers (BERT) model. For the single-label classification, our system applies a BertForSequenceClassification model to classify the input text. For the multi-label classification, we use the fine-tuned BERT model to extract the sentiment score of the text and a fully connected layer to classify the text into the PCL categories. Our system achieved relatively good results on the competition’s official leaderboard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.62.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.62/><span class=acl-fixed-case>I</span>2<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Patronizing and Condescending Language Detection using Deep Learning Techniques</a></strong><br><a href=/people/l/laura-vazquez-ramos/>Laura Vázquez Ramos</a>
|
<a href=/people/a/adrian-moreno-monterde/>Adrián Moreno Monterde</a>
|
<a href=/people/v/victoria-pachon/>Victoria Pachón</a>
|
<a href=/people/j/jacinto-mata/>Jacinto Mata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--62><div class="card-body p-3 small">Patronizing and Condescending Language is an ever-present problem in our day-to-day lives. There has been a rise in patronizing language on social media platforms manifesting itself in various forms. This paper presents two performing deep learning algorithms and results for the “Task 4: Patronizing and Condescending Language Detection.” of SemEval 2022. The task incorporates an English dataset containing sentences from social media from around the world. The paper focuses on data augmentation to boost results on various deep learning methods as BERT and LSTM Neural Network.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.63.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.63.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.63/><span class=acl-fixed-case>P</span>i<span class=acl-fixed-case>C</span>k<span class=acl-fixed-case>L</span>e at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Boosting Pre-trained Language Models with Task Specific Metadata and Cost Sensitive Learning</a></strong><br><a href=/people/m/manan-suri/>Manan Suri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--63><div class="card-body p-3 small">This paper describes our system for Task 4 of SemEval 2022: Patronizing and Condescending Language Detection. Patronizing and Condescending Language (PCL) refers to language used with respect to vulnerable communities that portrays them in a pitiful way and is reflective of a sense of superiority. Task 4 involved binary classification (Subtask 1) and multi-label classification (Subtask 2) of Patronizing and Condescending Language (PCL). For our system, we experimented with fine-tuning different transformer-based pre-trained models including BERT, DistilBERT, RoBERTa and ALBERT. Further, we have used token separated metadata in order to improve our model by helping it contextualize different communities with respect to PCL. We faced the challenge of class imbalance, which we solved by experimenting with different class weighting schemes. Our models were effective in both subtasks, with the best performance coming out of models with Effective Number of Samples (ENS) class weighting and token separated metadata in both subtasks. For subtask 1 and subtask 2, our best models were finetuned BERT and RoBERTa models respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.64.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.64.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.64/><span class=acl-fixed-case>ML</span>_<span class=acl-fixed-case>LTU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: T5 Towards Identifying Patronizing and Condescending Language</a></strong><br><a href=/people/t/tosin-adewumi/>Tosin Adewumi</a>
|
<a href=/people/l/lama-alkhaled/>Lama Alkhaled</a>
|
<a href=/people/h/hamam-mokayed/>Hamam Mokayed</a>
|
<a href=/people/f/foteini-liwicki/>Foteini Liwicki</a>
|
<a href=/people/m/marcus-liwicki/>Marcus Liwicki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--64><div class="card-body p-3 small">This paper describes the system used by the Machine Learning Group of LTU in subtask 1 of the SemEval-2022 Task 4: Patronizing and Condescending Language (PCL) Detection. Our system consists of finetuning a pretrained text-to-text transfer transformer (T5) and innovatively reducing its out-of-class predictions. The main contributions of this paper are 1) the description of the implementation details of the T5 model we used, 2) analysis of the successes & struggles of the model in this task, and 3) ablation studies beyond the official submission to ascertain the relative importance of data split. Our model achieves an F1 score of 0.5452 on the official test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.65.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.65" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.65/>Xu at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Pre-<span class=acl-fixed-case>BERT</span> Neural Network Methods vs Post-<span class=acl-fixed-case>BERT</span> <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a Approach for Patronizing and Condescending Language Detection</a></strong><br><a href=/people/j/jinghua-xu/>Jinghua Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--65><div class="card-body p-3 small">This paper describes my participation in the SemEval-2022 Task 4: Patronizing and Condescending Language Detection. I participate in both subtasks: Patronizing and Condescending Language (PCL) Identification and Patronizing and Condescending Language Categorization, with the main focus put on subtask 1. The experiments compare pre-BERT neural network (NN) based systems against post-BERT pretrained language model RoBERTa. This research finds NN-based systems in the experiments perform worse on the task compared to the pretrained language models. The top-performing RoBERTa system is ranked 26 out of 78 teams (F1-score: 54.64) in subtask 1, and 23 out of 49 teams (F1-score: 30.03) in subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.66.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.66/>Amsqr at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Towards <span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>NLP</span> via Meta-Learning and Adversarial Data Augmentation for <span class=acl-fixed-case>PCL</span> Detection</a></strong><br><a href=/people/a/alejandro-mosquera/>Alejandro Mosquera</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--66><div class="card-body p-3 small">This paper describes the use of AutoNLP techniques applied to the detection of patronizing and condescending language (PCL) in a binary classification scenario. The proposed approach combines meta-learning, in order to identify the best performing combination of deep learning architectures, with the synthesis of adversarial training examples; thus boosting robustness and model generalization. A submission from this system was evaluated as part of the first sub-task of SemEval 2022 - Task 4 and achieved an F1 score of 0.57%, which is 16 percentage points higher than the RoBERTa baseline provided by the organizers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.67.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.67.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.67/><span class=acl-fixed-case>SATL</span>ab at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Trying to Detect Patronizing and Condescending Language with only Character and Word N-grams</a></strong><br><a href=/people/y/yves-bestgen/>Yves Bestgen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--67><div class="card-body p-3 small">A logistic regression model only fed with character and word n-grams is proposed for the SemEval-2022 Task 4 on Patronizing and Condescending Language Detection (PCL). It obtained an average level of performance, well above the performance of a system that tries to guess without using any knowledge about the task, but much lower than the best teams. To facilitate the interpretation of the performance scores, the F1 measure, the best level of performance of a system that tries to guess without using any knowledge is calculated and used to correct the F1 scores in the manner of a Kappa. As the proposed model is very similar to the one that performed well on a task requiring to automatically identify hate speech and offensive content, this paper confirms the difficulty of PCL detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.68.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.68/>Taygete at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a based models for detecting Patronising and Condescending Language</a></strong><br><a href=/people/j/jayant-chhillar/>Jayant Chhillar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--68><div class="card-body p-3 small">This work describes the development of different models to detect patronising and condescending language within extracts of news articles as part of the SemEval 2022 competition (Task-4). This work explores different models based on the pre-trained RoBERTa language model coupled with LSTM and CNN layers. The best models achieved 15<span class=tex-math><sup>th</sup></span> rank with an F1-score of 0.5924 for subtask-A and 12<span class=tex-math><sup>th</sup></span> in subtask-B with a macro-F1 score of 0.3763.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.69.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.69/><span class=acl-fixed-case>CS</span>/<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Effective Data Augmentation Methods for Patronizing Language Detection and Multi-label Classification with <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a and <span class=acl-fixed-case>GPT</span>3</a></strong><br><a href=/people/d/daniel-saeedi/>Daniel Saeedi</a>
|
<a href=/people/s/sirwe-saeedi/>Sirwe Saeedi</a>
|
<a href=/people/a/aliakbar-panahi/>Aliakbar Panahi</a>
|
<a href=/people/a/alvis-c-m-fong/>Alvis C.M. Fong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--69><div class="card-body p-3 small">This paper presents a combination of data augmentation methods to boost the performance of state-of-the-art transformer-based language models for Patronizing and Condescending Language (PCL) detection and multi-label PCL classification tasks. These tasks are inherently different from sentiment analysis because positive/negative hidden attitudes in the context will not necessarily be considered positive/negative for PCL tasks. The oblation study observes that the imbalance degree of PCL dataset is in the extreme range. This paper presents a modified version of the sentence paraphrasing deep learning model (PEGASUS) to tackle the limitation of maximum sequence length. The proposed algorithm has no specific maximum input length to paraphrase sequences. Our augmented underrepresented class of annotated data achieved competitive results among top-16 SemEval-2022 participants. This paper’s approaches rely on fine-tuning pretrained RoBERTa and GPT3 models such as Davinci and Curie engines with an extra-enriched PCL dataset. Furthermore, we discuss Few-Shot learning technique to overcome the limitation of low-resource NLP problems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.70.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.70/>University of Bucharest Team at <span class=acl-fixed-case>S</span>emeval-2022 Task4: Detection and Classification of Patronizing and Condescending Language</a></strong><br><a href=/people/t/tudor-dumitrascu/>Tudor Dumitrascu</a>
|
<a href=/people/r/raluca-andreea-ginga/>Raluca-Andreea Gînga</a>
|
<a href=/people/b/bogdan-dobre/>Bogdan Dobre</a>
|
<a href=/people/b/bogdan-radu-silviu-sielecki/>Bogdan Radu Silviu Sielecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--70><div class="card-body p-3 small">This paper details our implementations for finding Patronizing and Condescending Language in texts, as part of the SemEval Workshop Task 4. We have used a variety of methods from simple machine learning algorithms applied on bag of words, all the way to BERT models, in order to solve the binary classification and the multi-label multi-class classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.71.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.71.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.71/><span class=acl-fixed-case>A</span>mrita_<span class=acl-fixed-case>CEN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Oversampling-based Machine Learning Approach for Detecting Patronizing and Condescending Language</a></strong><br><a href=/people/b/bichu-george/>Bichu George</a>
|
<a href=/people/a/adarsh-s/>Adarsh S</a>
|
<a href=/people/n/nishitkumar-prajapati/>Nishitkumar Prajapati</a>
|
<a href=/people/p/premjith-b/>Premjith B</a>
|
<a href=/people/s/soman-kp/>Soman Kp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--71><div class="card-body p-3 small">This paper narrates the work of the team Amrita_CEN for the shared task on Patronizing and Condescending Language Detection at SemEval 2022. We implemented machine learning algorithms such as Support Vector Machine (SVV), Logistic regression, Naive Bayes, XG Boost and Random Forest for modelling the tasks. At the same time, we also applied a feature engineering method to solve the class imbalance problem with respect to training data. Among all the models, the logistic regression model outperformed all other models and we have submitted results based upon the same.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.72.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.72.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--72 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.72 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.72/><span class=acl-fixed-case>JCT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4-A: Patronism Detection in Posts Written in <span class=acl-fixed-case>E</span>nglish using Preprocessing Methods and various Machine Leaerning Methods</a></strong><br><a href=/people/y/yaakov-hacohen-kerner/>Yaakov HaCohen-Kerner</a>
|
<a href=/people/i/ilan-meyrowitsch/>Ilan Meyrowitsch</a>
|
<a href=/people/m/matan-fchima/>Matan Fchima</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--72><div class="card-body p-3 small">In this paper, we describe our submissions to SemEval-2022 subtask 4-A - “Patronizing and Condescending Language Detection: Binary Classification”. We developed different models for this subtask. We applied 11 supervised machine learning methods and 9 preprocessing methods. Our best submission was a model we built with BertForSequenceClassification. Our experiments indicate that pre-processing stage is a must for a successful model. The dataset for Subtask 1 is highly imbalanced dataset. The f1-scores on the oversampled imbalanced training dataset were higher the results on the original training dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.73.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.73/><span class=acl-fixed-case>ULFRI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 4: Leveraging uncertainty and additional knowledge for patronizing and condescending language detection</a></strong><br><a href=/people/m/matej-klemen/>Matej Klemen</a>
|
<a href=/people/m/marko-robnik-sikonja/>Marko Robnik-Šikonja</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--73><div class="card-body p-3 small">We describe the ULFRI system used in the Subtask 1 of SemEval-2022 Task 4 Patronizing and condescending language detection. Our models are based on the RoBERTa model, modified in two ways: (1) by injecting additional knowledge (coreferences, named entities, dependency relations, and sentiment) and (2) by leveraging the task uncertainty by using soft labels, Monte Carlo dropout, and threshold optimization. We find that the injection of additional knowledge is not helpful but the uncertainty management mechanisms lead to small but consistent improvements. Our final system based on these findings achieves F1 = 0.575 in the online evaluation, ranking 19th out of 78 systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.74.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.74.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.74/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Multimedia Automatic Misogyny Identification</a></strong><br><a href=/people/e/elisabetta-fersini/>Elisabetta Fersini</a>
|
<a href=/people/f/francesca-gasparini/>Francesca Gasparini</a>
|
<a href=/people/g/giulia-rizzi/>Giulia Rizzi</a>
|
<a href=/people/a/aurora-saibene/>Aurora Saibene</a>
|
<a href=/people/b/berta-chulvi/>Berta Chulvi</a>
|
<a href=/people/p/paolo-rosso/>Paolo Rosso</a>
|
<a href=/people/a/alyssa-lees/>Alyssa Lees</a>
|
<a href=/people/j/jeffrey-sorensen/>Jeffrey Sorensen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--74><div class="card-body p-3 small">The paper describes the SemEval-2022 Task 5: Multimedia Automatic Misogyny Identification (MAMI),which explores the detection of misogynous memes on the web by taking advantage of available texts and images. The task has been organised in two related sub-tasks: the first one is focused on recognising whether a meme is misogynous or not (Sub-task A), while the second one is devoted to recognising types of misogyny (Sub-task B). MAMI has been one of the most popular tasks at SemEval-2022 with more than 400 participants, 65 teams involved in Sub-task A and 41 in Sub-task B from 13 countries. The MAMI challenge received 4214 submitted runs (of which 166 uploaded on the leader-board), denoting an enthusiastic participation for the proposed problem. The collection and annotation is described for the task dataset. The paper provides an overview of the systems proposed for the challenge, reports the results achieved in both sub-tasks and outlines a description of the main errors for a comprehension of the systems capabilities and for detailing future research perspectives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.75.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.75/>Transformers at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: A Feature Extraction based Approach for Misogynous Meme Detection</a></strong><br><a href=/people/s/shankar-mahadevan/>Shankar Mahadevan</a>
|
<a href=/people/s/sean-benhur/>Sean Benhur</a>
|
<a href=/people/r/roshan-nayak/>Roshan Nayak</a>
|
<a href=/people/m/malliga-subramanian/>Malliga Subramanian</a>
|
<a href=/people/k/kogilavani-shanmugavadivel/>Kogilavani Shanmugavadivel</a>
|
<a href=/people/k/kanchana-sivanraju/>Kanchana Sivanraju</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--75><div class="card-body p-3 small">Social media is an idea created to make theworld smaller and more connected. Recently,it has become a hub of fake news and sexistmemes that target women. Social Media shouldensure proper women’s safety and equality. Filteringsuch information from social media is ofparamount importance to achieving this goal. In this paper, we describe the system developedby our team for SemEval-2022 Task 5: MultimediaAutomatic Misogyny Identification. Wepropose a multimodal training methodologythat achieves good performance on both thesubtasks, ranking 4th for Subtask A (0.718macro F1-score) and 9th for Subtask B (0.695macro F1-score) while exceeding the baselineresults by good margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.76.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.76/><span class=acl-fixed-case>PAIC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Multi-Modal Misogynous Detection in <span class=acl-fixed-case>MEMES</span> with Multi-Task Learning And Multi-model Fusion</a></strong><br><a href=/people/j/jin-zhi/>Jin Zhi</a>
|
<a href=/people/z/zhou-mengyuan/>Zhou Mengyuan</a>
|
<a href=/people/m/mengfei-yuan/>Mengfei Yuan</a>
|
<a href=/people/d/dou-hu/>Dou Hu</a>
|
<a href=/people/x/xiyang-du/>Xiyang Du</a>
|
<a href=/people/l/lianxin-jiang/>Lianxin Jiang</a>
|
<a href=/people/y/yang-mo/>Yang Mo</a>
|
<a href=/people/x/xiaofeng-shi/>XiaoFeng Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--76><div class="card-body p-3 small">This paper describes our system used in the SemEval-2022 Task 5: Multimedia Automatic Misogyny Identification (MAMI). Multimedia automatic misogyny recognition consists of the identification of misogynous memes, taking advantage of both text and images as sources of information. The task will be organized around two main subtasks: Task A is a binary classification task, which should be identified either as misogynous or not misogynous. Task B is a multi-label classification task, in which the types of misogyny should be identified in potential overlapping categories, such as stereotype, shaming, objectification, and violence. In this paper, we proposed a system based on multi-task learning for multi-modal misogynous detection in memes. Our system combined image features with text features to train a multi-label classification. The prediction results were obtained by the simple weighted average method of the results with different fusion models, and the results of Task A were corrected by Task B. Our system achieves a test accuracy of 0.755 on Task A (ranking 3rd on the final leaderboard) and the accuracy of 0.731 on Task B (ranking 1st on the final leaderboard).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.77.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.77/><span class=acl-fixed-case>DD</span>-<span class=acl-fixed-case>TIG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Investigating the Relationships Between Multimodal and Unimodal Information in Misogynous Memes Detection and Classification</a></strong><br><a href=/people/z/ziming-zhou/>Ziming Zhou</a>
|
<a href=/people/h/han-zhao/>Han Zhao</a>
|
<a href=/people/j/jingjing-dong/>Jingjing Dong</a>
|
<a href=/people/n/ning-ding/>Ning Ding</a>
|
<a href=/people/x/xiaolong-liu/>Xiaolong Liu</a>
|
<a href=/people/k/kangli-zhang/>Kangli Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--77><div class="card-body p-3 small">This paper describes our submission for task 5 Multimedia Automatic Misogyny Identification (MAMI) at SemEval-2022. The task is designed to detect and classify misogynous memes. To utilize both textual and visual information presented in a meme, we investigate several of the most recent visual language transformer-based multimodal models and choose ERNIE-ViL-Large as our base model. For subtask A, with observations of models’ overfitting on unimodal patterns, strategies are proposed to mitigate problems of biased words and template memes. For subtask B, we transform this multi-label problem into a multi-class one and experiment with oversampling and complementary techniques. Our approach places 2nd for subtask A and 5th for subtask B in this competition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.78.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.78.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--78 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.78 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.78/><span class=acl-fixed-case>T</span>ech<span class=acl-fixed-case>SSN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Multimedia Automatic Misogyny Identification using Deep Learning Models</a></strong><br><a href=/people/r/rajalakshmi-sivanaiah/>Rajalakshmi Sivanaiah</a>
|
<a href=/people/a/angel-s/>Angel S</a>
|
<a href=/people/s/sakaya-milton-rajendram/>Sakaya Milton Rajendram</a>
|
<a href=/people/t/t-t-mirnalinee/>Mirnalinee T T</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--78><div class="card-body p-3 small">Research is progressing in a fast manner in the field of offensive, hate speech, abusive and sarcastic data. Tackling hate speech against women is urgent and really needed to give respect to the lady of our life. This paper describes the system used for identifying misogynous content using images and text. The system developed by the team TECHSSN uses transformer models to detect the misogynous content from text and Convolutional Neural Network model for image data. Various models like BERT, ALBERT, XLNET and CNN are explored and the combination of ALBERT and CNN as an ensemble model provides better results than the rest. This system was developed for the task 5 of the competition, SemEval 2022.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.79.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.79/><span class=acl-fixed-case>L</span>ast<span class=acl-fixed-case>R</span>esort at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Towards Misogyny Identification using Visual Linguistic Model Ensembles And Task-Specific Pretraining</a></strong><br><a href=/people/s/samyak-agrawal/>Samyak Agrawal</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--79><div class="card-body p-3 small">In current times, memes have become one of the most popular mediums to share jokes and information with the masses over the internet. Memes can also be used as tools to spread hatred and target women through degrading content disguised as humour. The task, Multimedia Automatic Misogyny Identification (MAMI), is to detect misogyny in these memes. This task is further divided into two sub-tasks: (A) Misogynous meme identification, where a meme should be categorized either as misogynous or not misogynous and (B) Categorizing these misogynous memes into potential overlapping subcategories. In this paper, we propose models leveraging task-specific pretraining with transfer learning on Visual Linguistic models. Our best performing models scored 0.686 and 0.691 on sub-tasks A and B respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.80.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.80/><span class=acl-fixed-case>H</span>ate<span class=acl-fixed-case>U</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Multimedia Automatic Misogyny Identification</a></strong><br><a href=/people/a/ayme-arango/>Ayme Arango</a>
|
<a href=/people/j/jesus-perez-martin/>Jesus Perez-Martin</a>
|
<a href=/people/a/arniel-labrada/>Arniel Labrada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--80><div class="card-body p-3 small">Hate speech expressions in social media are not limited to textual messages; they can appear in videos, images, or multimodal formats like memes. Existing work towards detecting such expressions has been conducted almost exclusively over textual content, and the analysis of pictures and videos has been very scarce. This paper describes our team proposal in the Multimedia Automatic Misogyny Identification (MAMI) task at SemEval 2022. The challenge consisted of identifying misogynous memes from a dataset where images and text transcriptions were provided. We reported a 71% of F-score using a multimodal system based on the CLIP model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.81.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.81/><span class=acl-fixed-case>SRCB</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Pretraining Based Image to Text Late Sequential Fusion System for Multimodal Misogynous Meme Identification</a></strong><br><a href=/people/j/jing-zhang/>Jing Zhang</a>
|
<a href=/people/y/yujin-wang/>Yujin Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--81><div class="card-body p-3 small">Online misogyny meme detection is an image/text multimodal classification task, the complicated relation of image and text challenges the intelligent system’s modality fusion learning capability. In this paper, we investigate the single-stream UNITER and dual-stream CLIP multimodal pretrained models on their capability to handle strong and weakly correlated image/text pairs. The XGBoost classifier with image features extracted by the CLIP model has the highest performance and being robust on domain shift. Based on this, we propose the PBR system, an ensemble system of Pretraining models, Boosting method and Rule-based adjustment, text information is fused into the system using our late sequential fusion scheme. Our system ranks 1st place on both sub-task A and sub-task B of the SemEval-2022 Task 5 Multimedia Automatic Misogyny Identification, with 0.834/0.731 macro F1 scores for sub-task A/B correspondingly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.82.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.82/><span class=acl-fixed-case>ASR</span>trans at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Transformer-based Models for Meme Classification</a></strong><br><a href=/people/a/ailneni-rakshitha-rao/>Ailneni Rakshitha Rao</a>
|
<a href=/people/a/arjun-rao/>Arjun Rao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--82><div class="card-body p-3 small">Women are frequently targeted online with hate speech and misogyny using tweets, memes, and other forms of communication. This paper describes our system for Task 5 of SemEval-2022: Multimedia Automatic Misogyny Identification (MAMI). We participated in both the sub-tasks, where we used transformer-based architecture to combine features of images and text. We explore models with multi-modal pre-training (VisualBERT) and text-based pre-training (MMBT) while drawing comparative results. We also show how additional training with task-related external data can improve the model performance. We achieved sizable improvements over baseline models and the official evaluation ranked our system <span class=tex-math>3<sup>rd</sup></span> out of 83 teams on the binary classification task (Sub-task A) with an F1 score of 0.761, and <span class=tex-math>7<sup>th</sup></span> out of 48 teams on the multi-label classification task (Sub-task B) with an F1 score of 0.705.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.83.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.83.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.83/><span class=acl-fixed-case>UAEM</span>-<span class=acl-fixed-case>ITAM</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Vision-Language Approach to Recognize Misogynous Content in Memes</a></strong><br><a href=/people/e/edgar-roman-rangel/>Edgar Roman-Rangel</a>
|
<a href=/people/j/jorge-fuentes-pacheco/>Jorge Fuentes-Pacheco</a>
|
<a href=/people/j/jorge-hermosillo-valadez/>Jorge Hermosillo Valadez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--83><div class="card-body p-3 small">In the context of the Multimedia Automatic Misogyny Identification (MAMI) competition 2022, we developed a framework for extracting lexical-semantic features from text and combine them with semantic descriptions of images, together with image content representation. We enriched the text modality description by incorporating word representations for each object present within the images. Images and text are then described at two levels of detail, globally and locally, using standard dimensionality reduction techniques for images in order to obtain 4 embeddings for each meme. These embeddings are finally concatenated and passed to a classifier. Our results overcome the baseline by 4%, falling behind the best performance by 12% for Sub-task B.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.84.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.84.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.84" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.84/><span class=acl-fixed-case>JRLV</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: The Importance of Visual Elements for Misogyny Identification in Memes</a></strong><br><a href=/people/j/jason-ravagli/>Jason Ravagli</a>
|
<a href=/people/l/lorenzo-vaiani/>Lorenzo Vaiani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--84><div class="card-body p-3 small">Gender discrimination is a serious and widespread problem on social media and online in general. Besides offensive messages, memes are one of the main means of dissemination for such content. With these premises, the MAMI task was proposed at the SemEval-2022, which consists of identifying memes with misogynous characteristics. In this work, we propose a solution to this problem based on Mask R-CNN and VisualBERT that leverages the multimodal nature of the task. Our study focuses on observing how the two sources of data in memes (text and image) and their possible combinations impact performances. Our best result slightly exceeds the higher baseline, but the experiments allowed us to draw important considerations regarding the importance of correctly exploiting the visual information and the relevance of the elements present in the memes images.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.85.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.85.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.85" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.85/><span class=acl-fixed-case>UPB</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Enhancing <span class=acl-fixed-case>UNITER</span> with Image Sentiment and Graph Convolutional Networks for Multimedia Automatic Misogyny Identification</a></strong><br><a href=/people/a/andrei-paraschiv/>Andrei Paraschiv</a>
|
<a href=/people/m/mihai-dascalu/>Mihai Dascalu</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--85><div class="card-body p-3 small">In recent times, the detection of hate-speech, offensive, or abusive language in online media has become an important topic in NLP research due to the exponential growth of social media and the propagation of such messages, as well as their impact. Misogyny detection, even though it plays an important part in hate-speech detection, has not received the same attention. In this paper, we describe our classification systems submitted to the SemEval-2022 Task 5: MAMI - Multimedia Automatic Misogyny Identification. The shared task aimed to identify misogynous content in a multi-modal setting by analysing meme images together with their textual captions. To this end, we propose two models based on the pre-trained UNITER model, one enhanced with an image sentiment classifier, whereas the second leverages a Vocabulary Graph Convolutional Network (VGCN). Additionally, we explore an ensemble using the aforementioned models. Our best model reaches an F1-score of 71.4% in Sub-task A and 67.3% for Sub-task B positioning our team in the upper third of the leaderboard. We release the code and experiments for our models on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.86.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.86.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.86" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.86/><span class=acl-fixed-case>R</span>ub<span class=acl-fixed-case>CSG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Ensemble learning for identifying misogynous <span class=acl-fixed-case>MEME</span>s</a></strong><br><a href=/people/w/wentao-yu/>Wentao Yu</a>
|
<a href=/people/b/benedikt-boenninghoff/>Benedikt Boenninghoff</a>
|
<a href=/people/j/jonas-rohrig/>Jonas Röhrig</a>
|
<a href=/people/d/dorothea-kolossa/>Dorothea Kolossa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--86><div class="card-body p-3 small">This work presents an ensemble system based on various uni-modal and bi-modal model architectures developed for the SemEval 2022 Task 5: MAMI-Multimedia Automatic Misogyny Identification. The challenge organizers provide an English meme dataset to develop and train systems for identifying and classifying misogynous memes. More precisely, the competition is separated into two sub-tasks: sub-task A asks for a binary decision as to whether a meme expresses misogyny, while sub-task B is to classify misogynous memes into the potentially overlapping sub-categories of stereotype, shaming, objectification, and violence. For our submission, we implement a new model fusion network and employ an ensemble learning approach for better performance. With this structure, we achieve a 0.755 macro-average F1-score (11th) in sub-task A and a 0.709 weighted-average F1-score (10th) in sub-task B.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.87.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.87/><span class=acl-fixed-case>RIT</span> Boston at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Multimedia Misogyny Detection By Using Coherent Visual and Language Features from <span class=acl-fixed-case>CLIP</span> Model and Data-centric <span class=acl-fixed-case>AI</span> Principle</a></strong><br><a href=/people/l/lei-chen/>Lei Chen</a>
|
<a href=/people/h/hou-wei-chou/>Hou Wei Chou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--87><div class="card-body p-3 small">Detecting MEME images to be misogynous or not is an application useful on curbing online hateful information against women. In the SemEval-2022 Multimedia Automatic Misogyny Identification (MAMI) challenge, we designed a system using two simple but effective principles. First, we leverage on recently emerging Transformer models pre-trained (mostly in a self-supervised learning way) on massive data sets to obtain very effective visual (V) and language (L) features. In particular, we used the CLIP model provided by OpenAI to obtain coherent V and L features and then simply used a logistic regression model to make binary predictions. Second, we emphasized more on data rather than tweaking models by following the data-centric AI principle. These principles were proven to be useful and our final macro-F1 is 0.778 for the MAMI task A and ranked the third place among participant teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.88.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.88" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.88/><span class=acl-fixed-case>T</span>eam<span class=acl-fixed-case>O</span>tter at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Detecting Misogynistic Content in Multimodal Memes</a></strong><br><a href=/people/p/paridhi-maheshwari/>Paridhi Maheshwari</a>
|
<a href=/people/s/sharmila-reddy-nangi/>Sharmila Reddy Nangi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--88><div class="card-body p-3 small">We describe our system for the SemEval 2022 task on detecting misogynous content in memes. This is a pressing problem and we explore various methods ranging from traditional machine learning to deep learning models such as multimodal transformers. We propose a multimodal BERT architecture that uses information from both image and text. We further incorporate common world knowledge from pretrained CLIP and Urban dictionary. We also provide qualitative analysis to support out model. Our best performing model achieves an F1 score of 0.679 on Task A (Rank 5) and 0.680 on Task B (Rank 13) of the hidden test set. Our code is available at <a href=https://github.com/paridhimaheshwari2708/MAMI class=acl-markup-url>https://github.com/paridhimaheshwari2708/MAMI</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.89.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.89.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.89/>taochen at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Multimodal Multitask Learning and Ensemble Learning</a></strong><br><a href=/people/c/chen-tao/>Chen Tao</a>
|
<a href=/people/j/jung-jae-kim/>Jung-jae Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--89><div class="card-body p-3 small">We present a multi-modal deep learning system for the Multimedia Automatic Misogyny Identification (MAMI) challenge, a SemEval task of identifying and classifying misogynistic messages in online memes. We adapt multi-task learning for the multimodal subtasks of the MAMI challenge to transfer knowledge among the correlated subtasks. We also leverage on ensemble learning for synergistic integration of models individually trained for the subtasks. We finally discuss about errors of the system to provide useful insights for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.90.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.90.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.90" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.90/><span class=acl-fixed-case>M</span>ila<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Using Perceiver <span class=acl-fixed-case>IO</span> for Detecting Misogynous Memes with Text and Image Modalities</a></strong><br><a href=/people/g/giuseppe-attanasio/>Giuseppe Attanasio</a>
|
<a href=/people/d/debora-nozza/>Debora Nozza</a>
|
<a href=/people/f/federico-bianchi/>Federico Bianchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--90><div class="card-body p-3 small">In this paper, we describe the system proposed by the MilaNLP team for the Multimedia Automatic Misogyny Identification (MAMI) challenge. We use Perceiver IO as a multimodal late fusion over unimodal streams to address both sub-tasks A and B. We build unimodal embeddings using Vision Transformer (image) and RoBERTa (text transcript). We enrich the input representation using face and demographic recognition, image captioning, and detection of adult content and web entities. To the best of our knowledge, this work is the first to use Perceiver IO combining text and image modalities. The proposed approach outperforms unimodal and multimodal baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.91.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.91.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.91" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.91/><span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>BO</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: A Multimodal bi-Transformer Approach to the Binary and Fine-grained Identification of Misogyny in Memes</a></strong><br><a href=/people/a/arianna-muti/>Arianna Muti</a>
|
<a href=/people/k/katerina-korre/>Katerina Korre</a>
|
<a href=/people/a/alberto-barron-cedeno/>Alberto Barrón-Cedeño</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--91><div class="card-body p-3 small">We present our submission to SemEval 2022 Task 5 on Multimedia Automatic Misogyny Identification. We address the two tasks: Task A consists of identifying whether a meme is misogynous. If so, Task B attempts to identify its kind among shaming, stereotyping, objectification, and violence. Our approach combines a BERT Transformer with CLIP for the textual and visual representations. Both textual and visual encoders are fused in an early-fusion fashion through a Multimodal Bidirectional Transformer with unimodally pretrained components. Our official submissions obtain macro-averaged F<span class=tex-math><sub>1</sub></span>=0.727 in Task A (4th position out of 69 participants)and weighted F<span class=tex-math><sub>1</sub></span>=0.710 in Task B (4th position out of 42 participants).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.92.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.92/><span class=acl-fixed-case>IIITH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: A comparative study of deep learning models for identifying misogynous memes</a></strong><br><a href=/people/t/tathagata-raha/>Tathagata Raha</a>
|
<a href=/people/s/sagar-joshi/>Sagar Joshi</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--92><div class="card-body p-3 small">This paper provides a comparison of different deep learning methods for identifying misogynous memes for SemEval-2022 Task 5: Multimedia Automatic Misogyny Identification. In this task, we experiment with architectures in the identification of misogynous content in memes by making use of text and image-based information. The different deep learning methods compared in this paper are: (i) unimodal image or text models (ii) fusion of unimodal models (iii) multimodal transformers models and (iv) transformers further pretrained on a multimodal task. From our experiments, we found pretrained multimodal transformer architectures to strongly outperform the models involving the fusion of representation from both the modalities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.93.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.93" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.93/>Codec at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Multi-Modal Multi-Transformer Misogynous Meme Classification Framework</a></strong><br><a href=/people/a/ahmed-mahran/>Ahmed Mahran</a>
|
<a href=/people/c/carlo-alessandro-borella/>Carlo Alessandro Borella</a>
|
<a href=/people/k/konstantinos-perifanos/>Konstantinos Perifanos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--93><div class="card-body p-3 small">In this paper we describe our work towards building a generic framework for both multi-modal embedding and multi-label binary classification tasks, while participating in task 5 (Multimedia Automatic Misogyny Identification) of SemEval 2022 competition. Since pretraining deep models from scratch is a resource and data hungry task, our approach is based on three main strategies. We combine different state-of-the-art architectures to capture a wide spectrum of semantic signals from the multi-modal input. We employ a multi-task learning scheme to be able to use multiple datasets from the same knowledge domain to help increase the model’s performance. We also use multiple objectives to regularize and fine tune different system components.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.94.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.94/><span class=acl-fixed-case>I</span>2<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Identification of misogyny in internet memes</a></strong><br><a href=/people/p/pablo-cordon/>Pablo Cordon</a>
|
<a href=/people/p/pablo-gonzalez-diaz/>Pablo Gonzalez Diaz</a>
|
<a href=/people/j/jacinto-mata/>Jacinto Mata</a>
|
<a href=/people/v/victoria-pachon/>Victoria Pachón</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--94><div class="card-body p-3 small">In this paper we present our approach and system description on Task 5 A in MAMI: Multimedia Automatic Misogyny Identification. In our experiments we compared several architectures based on deep learning algorithms with various other approaches to binary classification using Transformers, combined with a nudity image detection algorithm to provide better results. With this approach, we achieved an F1-score of 0.665 in the evaluation process</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.95.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.95.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.95/><span class=acl-fixed-case>INF</span>-<span class=acl-fixed-case>UFRGS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: analyzing the performance of multimodal models</a></strong><br><a href=/people/g/gustavo-lorentz/>Gustavo Lorentz</a>
|
<a href=/people/v/viviane-moreira/>Viviane Moreira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--95><div class="card-body p-3 small">This paper describes INF-UFRGS submission for SemEval-2022 Task 5 Multimodal Automatic Misogyny Identification (MAMI). Unprecedented levels of harassment came with the ever-growing internet usage as a mean of worldwide communication. The goal of the task is to improve the quality of existing methods for misogyny identification, many of which require dedicated personnel, hence the need for automation. We experimented with five existing models, including ViLBERT and Visual BERT - both uni and multimodally pretrained - and MMBT. The datasets consist of memes with captions in English. The results show that all models achieved Macro-F1 scores above 0.64. ViLBERT was the best performer with a score of 0.698.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.96.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.96.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.96" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.96/><span class=acl-fixed-case>MMVAE</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: A Multi-modal Multi-task <span class=acl-fixed-case>VAE</span> on Misogynous Meme Detection</a></strong><br><a href=/people/y/yimeng-gu/>Yimeng Gu</a>
|
<a href=/people/i/ignacio-castro/>Ignacio Castro</a>
|
<a href=/people/g/gareth-tyson/>Gareth Tyson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--96><div class="card-body p-3 small">Nowadays, memes have become quite common in day-to-day communications on social media platforms. They appear to be amusing, evoking and attractive to audiences. However, some memes containing malicious contents can be harmful to the targeted group and arouse public anger in the long run. In this paper, we study misogynous meme detection, a shared task in SemEval 2022 - Multimedia Automatic Misogyny Identification (MAMI). The challenge of misogynous meme detection is to co-represent multi-modal features. To tackle with this challenge, we propose a Multi-modal Multi-task Variational AutoEncoder (MMVAE) to learn an effective co-representation of visual and textual features in the latent space, and determine if the meme contains misogynous information and identify its fine-grained categories. Our model achieves 0.723 on sub-task A and 0.634 on sub-task B in terms of <span class=tex-math>F<sub>1</sub></span> scores. We carry out comprehensive experiments on our model’s architecture and show that our approach significantly outperforms several strong uni-modal and multi-modal approaches. Our code is released on github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.97.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.97/><span class=acl-fixed-case>AMS</span>_<span class=acl-fixed-case>ADRN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: A Suitable Image-text Multimodal Joint Modeling Method for Multi-task Misogyny Identification</a></strong><br><a href=/people/d/da-li/>Da Li</a>
|
<a href=/people/m/ming-yi/>Ming Yi</a>
|
<a href=/people/y/yukai-he/>Yukai He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--97><div class="card-body p-3 small">Women are influential online, especially in image-based social media such as Twitter and Instagram. However, many in the network environment contain gender discrimination and aggressive information, which magnify gender stereotypes and gender inequality. Therefore, the filtering of illegal content such as gender discrimination is essential to maintain a healthy social network environment. In this paper, we describe the system developed by our team for SemEval-2022Task 5: Multimedia Automatic Misogyny Identification. More specifically, we introduce two novel system to analyze these posts: a multimodal multi-task learning architecture that combines Bertweet for text encoding with ResNet-18 for image representation, and a single-flow transformer structure which combines text embeddings from BERT-Embeddings and image embeddings from several different modules such as EfficientNet and ResNet. In this manner, we show that the information behind them can be properly revealed. Our approach achieves good performance on each of the two subtasks of the current competition, ranking 15th for Subtask A (0.746 macro F1-score), 11th for Subtask B (0.706 macro F1-score) while exceeding the official baseline results by high margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.98.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.98/><span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>ildesheim at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 task 5: Combining Deep Text and Image Models for Multimedia Misogyny Detection</a></strong><br><a href=/people/m/milan-kalkenings/>Milan Kalkenings</a>
|
<a href=/people/t/thomas-mandl/>Thomas Mandl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--98><div class="card-body p-3 small">This paper describes the participation of the University of Hildesheim at the SemEval task 5. The task deals with Multimedia Automatic Misogyny Identification (MAMI). Hateful memes need to be detected within a data collection. For this task, we implemented six models for text and image analysis and tested the effectiveness of their combinations. A fusion system implements a multi-modal transformer to integrate the embeddings of these models. The best performing models included BERT for the text of the meme, manually derived associations for words in the memes and a Faster R-CNN network for the image. We evaluated the performance of our approach also with the data of the Facebook Hateful Memes challenge in order to analyze the generalisation capabilities of the approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.99.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.99/>Mitra Behzadi at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5 : Multimedia Automatic Misogyny Identification method based on <span class=acl-fixed-case>CLIP</span></a></strong><br><a href=/people/m/mitra-behzadi/>Mitra Behzadi</a>
|
<a href=/people/a/ali-derakhshan/>Ali Derakhshan</a>
|
<a href=/people/i/ian-harris/>Ian Harris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--99><div class="card-body p-3 small">Everyday more users are using memes on social media platforms to convey a message with text and image combined. Although there are many fun and harmless memes being created and posted, there are also ones that are hateful and offensive to particular groups of people. In this article present a novel approach based on the CLIP network to detect misogynous memes and find out the types of misogyny in that meme. We participated in Task A and Task B of the Multimedia Automatic Misogyny Identification (MaMi) challenge and our best scores are 0.694 and 0.681 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.100.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--100 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.100/><span class=acl-fixed-case>IITR</span> <span class=acl-fixed-case>C</span>ode<span class=acl-fixed-case>B</span>usters at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Misogyny Identification using Transformers</a></strong><br><a href=/people/g/gagan-sharma/>Gagan Sharma</a>
|
<a href=/people/g/gajanan-sunil-gitte/>Gajanan Sunil Gitte</a>
|
<a href=/people/s/shlok-goyal/>Shlok Goyal</a>
|
<a href=/people/r/raksha-sharma/>Raksha Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--100><div class="card-body p-3 small">This paper presents our submission to task 5 ( Multimedia Automatic Misogyny Identification) of the SemEval 2022 competition. The purpose of the task is to identify given memes as misogynistic or not and further label the type of misogyny involved. In this paper, we present our approach based on language processing tools. We embed meme texts using GloVe embedding and classify misogyny using BERT model. Our model obtains an F1-score of 66.24% and 63.5% in misogyny classification and misogyny labels, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.101.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.101/><span class=acl-fixed-case>IIT</span> <span class=acl-fixed-case>DHANBAD</span> <span class=acl-fixed-case>CODECHAMPS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: <span class=acl-fixed-case>MAMI</span> - Multimedia Automatic Misogyny Identification</a></strong><br><a href=/people/s/shubham-barnwal/>Shubham Barnwal</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/r/rajendra-pamula/>Rajendra Pamula</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--101><div class="card-body p-3 small">With the growth of the internet, the use of social media based on images has drastically increased like Twitter, Instagram, etc. In these social media, women have a very high contribution as of 75% women use social media multiple times compared to men which is only 65% of men uses social media multiple times a day. However, with this much contribution, it also increases systematic inequality and discrimination offline is replicated in online spaces in the form of MEMEs. A meme is essentially an image characterized by pictorial content with an overlaying text a posteriori introduced by humans, with the main goal of being funny and/or ironic. Although most of them are created with the intent of making funny jokes, in a short time people started to use them as a form of hate and prejudice against women, landing to sexist and aggressive messages in online environments that subsequently amplify the sexual stereotyping and gender inequality of the offline world. This leads to the need for automatic detection of Misogyny MEMEs. Specifically, I described the model submitted for the shared task on Multimedia Automatic Misogyny Identification (MAMI) and my team name is IIT DHANBAD CODECHAMPS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.102.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.102/><span class=acl-fixed-case>Q</span>i<span class=acl-fixed-case>N</span>i<span class=acl-fixed-case>A</span>n at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Multi-Modal Misogyny Detection and Classification</a></strong><br><a href=/people/q/qin-gu/>Qin Gu</a>
|
<a href=/people/n/nino-meisinger/>Nino Meisinger</a>
|
<a href=/people/a/anna-katharina-dick/>Anna-Katharina Dick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--102><div class="card-body p-3 small">In this paper, we describe our submission to the misogyny classification challenge at SemEval-2022. We propose two models for the two subtasks of the challenge: The first uses joint image and text classification to classify memes as either misogynistic or not. This model uses a majority voting ensemble structure built on traditional classifiers and additional image information such as age, gender and nudity estimations. The second model uses a RoBERTa classifier on the text transcriptions to additionally identify the type of problematic ideas the memes perpetuate. Our submissions perform above all organizer submitted baselines. For binary misogyny classification, our system achieved the fifth place on the leaderboard, with a macro F1-score of 0.665. For multi-label classification identifying the type of misogyny, our model achieved place 19 on the leaderboard, with a weighted F1-score of 0.637.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.103.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.103/><span class=acl-fixed-case>UMUT</span>eam at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Combining image and textual embeddings for multi-modal automatic misogyny identification</a></strong><br><a href=/people/j/jose-garcia-diaz/>José García-Díaz</a>
|
<a href=/people/c/camilo-caparros-laiz/>Camilo Caparros-Laiz</a>
|
<a href=/people/r/rafael-valencia-garcia/>Rafael Valencia-García</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--103><div class="card-body p-3 small">In this manuscript we describe the participation of the UMUTeam on the MAMI shared task proposed at SemEval 2022. This task is concerning the identification of misogynous content from a multi-modal perspective. Our participation is grounded on the combination of different feature sets within the same neural network. Specifically, we combine linguistic features with contextual transformers based on text (BERT) and images (BEiT). Besides, we also evaluate other ensemble learning strategies and the usage of non-contextual pretrained embeddings. Although our results are limited, we outperform all the baselines proposed, achieving position 36 in the binary classification task with a macro F1-score of 0.687, and position 28 in the multi-label task of misogynous categorisation, with an macro F1-score of 0.663.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.104.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.104/><span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Multi-Modal and Multi-label Emotion Classification Based on <span class=acl-fixed-case>LXMERT</span></a></strong><br><a href=/people/c/chao-han/>Chao Han</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--104><div class="card-body p-3 small">This paper describes our system used in the SemEval-2022 Task5 Multimedia Automatic Misogyny Identification (MAMI). This task is to use the provided text-image pairs to classify emotions. In this paper, We propose a multi-label emotion classification model based on pre-trained LXMERT. We use Faster-RCNN to extract visual representation and utilize LXMERT’s cross-attention for multi-modal alignment. Then we use the Bilinear-interaction layer to fuse these features. Our experimental results surpass the <span class=tex-math>F<sub>1</sub></span> score of baseline. For Sub-task A, our <span class=tex-math>F<sub>1</sub></span> score is 0.662 and Sub-task B’s <span class=tex-math>F<sub>1</sub></span> score is 0.633. The code of this study is available on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.105.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.105.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.105" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.105/><span class=acl-fixed-case>TIB</span>-<span class=acl-fixed-case>VA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: A Multimodal Architecture for the Detection and Classification of Misogynous Memes</a></strong><br><a href=/people/s/sherzod-hakimov/>Sherzod Hakimov</a>
|
<a href=/people/g/gullal-singh-cheema/>Gullal Singh Cheema</a>
|
<a href=/people/r/ralph-ewerth/>Ralph Ewerth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--105><div class="card-body p-3 small">The detection of offensive, hateful content on social media is a challenging problem that affects many online users on a daily basis. Hateful content is often used to target a group of people based on ethnicity, gender, religion and other factors. The hate or contempt toward women has been increasing on social platforms. Misogynous content detection is especially challenging when textual and visual modalities are combined to form a single context, e.g., an overlay text embedded on top of an image, also known as <i>meme</i>. In this paper, we present a multimodal architecture that combines textual and visual features to detect misogynous memes. The proposed architecture is evaluated in the SemEval-2022 Task 5: MAMI - Multimedia Automatic Misogyny Identification challenge under the team name TIB-VA. We obtained the best result in the Task-B where the challenge is to classify whether a given document is misogynous and further identify the following sub-classes: shaming, stereotype, objectification, and violence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.106.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.106/><span class=acl-fixed-case>R</span>2<span class=acl-fixed-case>D</span>2 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Attention is only as good as its Values! A multimodal system for identifying misogynist memes</a></strong><br><a href=/people/m/mayukh-sharma/>Mayukh Sharma</a>
|
<a href=/people/i/ilanthenral-kandasamy/>Ilanthenral Kandasamy</a>
|
<a href=/people/v/vasantha-w-b/>Vasantha W B</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--106><div class="card-body p-3 small">This paper describes the multimodal deep learning system proposed for SemEval 2022 Task 5: MAMI - Multimedia Automatic Misogyny Identification. We participated in both Subtasks, i.e. Subtask A: Misogynous meme identification, and Subtask B: Identifying type of misogyny among potential overlapping categories (stereotype, shaming, objectification, violence). The proposed architecture uses pre-trained models as feature extractors for text and images. We use these features to learn multimodal representation using methods like concatenation and scaled dot product attention. Classification layers are used on fused features as per the subtask definition. We also performed experiments using unimodal models for setting up comparative baselines. Our best performing system achieved an F1 score of 0.757 and was ranked <span class=tex-math>3<sup>rd</sup></span> in Subtask A. On Subtask B, our system performed well with an F1 score of 0.690 and was ranked <span class=tex-math>10<sup>th</sup></span> on the leaderboard. We further show extensive experiments using combinations of different pre-trained models which will be helpful as baselines for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.107.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--107 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.107" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.107/><span class=acl-fixed-case>AIDA</span>-<span class=acl-fixed-case>UPM</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Exploring Multimodal Late Information Fusion for Multimedia Automatic Misogyny Identification</a></strong><br><a href=/people/a/alvaro-huertas-garcia/>Álvaro Huertas-García</a>
|
<a href=/people/h/helena-liz/>Helena Liz</a>
|
<a href=/people/g/guillermo-villar-rodriguez/>Guillermo Villar-Rodríguez</a>
|
<a href=/people/a/alejandro-martin/>Alejandro Martín</a>
|
<a href=/people/j/javier-huertas-tato/>Javier Huertas-Tato</a>
|
<a href=/people/d/david-camacho/>David Camacho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--107><div class="card-body p-3 small">This paper describes the multimodal late fusion model proposed in the SemEval-2022 Multimedia Automatic Misogyny Identification (MAMI) task. The main contribution of this paper is the exploration of different late fusion methods to boost the performance of the combination based on the Transformer-based model and Convolutional Neural Networks (CNN) for text and image, respectively. Additionally, our findings contribute to a better understanding of the effects of different image preprocessing methods for meme classification. We achieve 0.636 F1-macro average score for the binary subtask A, and 0.632 F1-macro average score for the multi-label subtask B. The present findings might help solve the inequality and discrimination women suffer on social media platforms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.108.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.108/><span class=acl-fixed-case>YMAI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Detecting Misogyny in Memes using <span class=acl-fixed-case>V</span>isual<span class=acl-fixed-case>BERT</span> and <span class=acl-fixed-case>MMBT</span> <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>M</span>odal Pre-trained Models</a></strong><br><a href=/people/m/mohammad-habash/>Mohammad Habash</a>
|
<a href=/people/y/yahya-daqour/>Yahya Daqour</a>
|
<a href=/people/m/malak-abdullah/>Malak Abdullah</a>
|
<a href=/people/m/mahmoud-al-ayyoub/>Mahmoud Al-Ayyoub</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--108><div class="card-body p-3 small">This paper presents a deep learning system that contends at SemEval-2022 Task 5. The goal is to detect the existence of misogynous memes in sub-task A. At the same time, the advanced multi-label sub-task B categorizes the misogyny of misogynous memes into one of four types: stereotype, shaming, objectification, and violence. The Ensemble technique has been used for three multi-modal deep learning models: two MMBT models and VisualBERT. Our proposed system ranked 17 place out of 83 participant teams with an F1-score of 0.722 in sub-task A, which shows a significant performance improvement over the baseline model’s F1-score of 0.65.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.109.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.109.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.109/>Exploring Contrastive Learning for Multimodal Detection of Misogynistic Memes</a></strong><br><a href=/people/c/charic-farinango-cuervo/>Charic Farinango Cuervo</a>
|
<a href=/people/n/natalie-parde/>Natalie Parde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--109><div class="card-body p-3 small">Misogynistic memes are rampant on social media, and often convey their messages using multimodal signals (e.g., images paired with derogatory text or captions). However, to date very few multimodal systems have been leveraged for the detection of misogynistic memes. Recently, researchers have turned to contrastive learning, and most notably OpenAI’s CLIP model, is an innovative solution to a variety of multimodal tasks. In this work, we experiment with contrastive learning to address the detection of misogynistic memes within the context of SemEval 2022 Task 5. Although our model does not achieve top results, these experiments provide important exploratory findings for this task. We conduct a detailed error analysis, revealing promising clues and offering a foundation for follow-up work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.110.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.110/>Poirot at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 5: Leveraging Graph Network for Misogynistic Meme Detection</a></strong><br><a href=/people/h/harshvardhan-srivastava/>Harshvardhan Srivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--110><div class="card-body p-3 small">In recent years, there has been an upsurge in a new form of entertainment medium called memes. These memes although seemingly innocuous have transcended the boundary of online harassment against women and created an unwanted bias against them. To help alleviate this problem, we propose an early fusion model for the prediction and identification of misogynistic memes and their type in this paper for which we participated in SemEval-2022 Task 5. The model receives as input meme image with its text transcription with a target vector. Given that a key challenge with this task is the combination of different modalities to predict misogyny, our model relies on pre-trained contextual representations from different state-of-the-art transformer-based language models and pre-trained image models to get an effective image representation. Our model achieved competitive results on both SubTask-A and SubTask-B with the other competingteams and significantly outperforms the baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.111.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.111.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.111/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: i<span class=acl-fixed-case>S</span>arcasm<span class=acl-fixed-case>E</span>val, Intended Sarcasm Detection in <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/i/ibrahim-abu-farha/>Ibrahim Abu Farha</a>
|
<a href=/people/s/silviu-vlad-oprea/>Silviu Vlad Oprea</a>
|
<a href=/people/s/steven-wilson/>Steven Wilson</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--111><div class="card-body p-3 small">iSarcasmEval is the first shared task to target intended sarcasm detection: the data for this task was provided and labelled by the authors of the texts themselves. Such an approach minimises the downfalls of other methods to collect sarcasm data, which rely on distant supervision or third-party annotations. The shared task contains two languages, English and Arabic, and three subtasks: sarcasm detection, sarcasm category classification, and pairwise sarcasm identification given a sarcastic sentence and its non-sarcastic rephrase. The task received submissions from 60 different teams, with the sarcasm detection task being the most popular. Most of the participating teams utilised pre-trained language models. In this paper, we provide an overview of the task, data, and participating teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.112.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.112/><span class=acl-fixed-case>PALI</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: i<span class=acl-fixed-case>S</span>arcasm<span class=acl-fixed-case>E</span>val- Fine-tuning the Pre-trained Model for Detecting Intended Sarcasm</a></strong><br><a href=/people/x/xiyang-du/>Xiyang Du</a>
|
<a href=/people/d/dou-hu/>Dou Hu</a>
|
<a href=/people/j/jin-zhi/>Jin Zhi</a>
|
<a href=/people/l/lianxin-jiang/>Lianxin Jiang</a>
|
<a href=/people/x/xiaofeng-shi/>Xiaofeng Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--112><div class="card-body p-3 small">This paper describes the method we utilized in the SemEval-2022 Task 6 iSarcasmEval: Intended Sarcasm Detection In English and Arabic. Our system has achieved 1st in SubtaskB, which is to identify the categories of intended sarcasm. The proposed system integrates multiple BERT-based, RoBERTa-based and BERTweet-based models with finetuning. In this task, we contributed the following: 1) we reveal several large pre-trained models’ performance on tasks coping with the tweet-like text. 2) Our methods prove that we can still achieve excellent results in this particular task without a complex classifier adopting some proper training method. 3) we found there is a hierarchical relationship of sarcasm types in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.113.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.113/>stce at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Sarcasm Detection in <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/m/mengfei-yuan/>Mengfei Yuan</a>
|
<a href=/people/z/zhou-mengyuan/>Zhou Mengyuan</a>
|
<a href=/people/l/lianxin-jiang/>Lianxin Jiang</a>
|
<a href=/people/y/yang-mo/>Yang Mo</a>
|
<a href=/people/x/xiaofeng-shi/>Xiaofeng Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--113><div class="card-body p-3 small">This paper describes the systematic approach applied in “SemEval-2022 Task 6 (iSarcasmEval) : Intended Sarcasm Detection in English and Arabic”. In particular, we illustrate the proposed system in detail for SubTask-A about determining a given text as sarcastic or non-sarcastic in English. We start with the training data from the officially released data and then experiment with different combinations of public datasets to improve the model generalization. Additional experiments conducted on the task demonstrate our strategies are effective in completing the task. Different transformer-based language models, as well as some popular plug-and-play proirs, are mixed into our system to enhance the model’s robustness. Furthermore, statistical and lexical-based text features are mined to improve the accuracy of the sarcasm detection. Our final submission achieves an F1-score for the sarcastic class of 0.6052 on the official test set (the top 1 of the 43 teams in “SubTask-A-English” on the leaderboard).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.114.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.114/><span class=acl-fixed-case>G</span>et<span class=acl-fixed-case>S</span>mart<span class=acl-fixed-case>MSEC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Sarcasm Detection using Contextual Word Embedding with <span class=acl-fixed-case>G</span>aussian model for Irony Type Identification</a></strong><br><a href=/people/d/diksha-krishnan/>Diksha Krishnan</a>
|
<a href=/people/j/jerin-mahibha-c/>Jerin Mahibha C</a>
|
<a href=/people/t/thenmozhi-durairaj/>Thenmozhi Durairaj</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--114><div class="card-body p-3 small">Sarcasm refers to the use of words that have different literal and intended meanings. It represents the usage of words that are opposite of what is literally said, especially in order to insult, mock, criticise or irritate someone. These types of statements may be funny or amusing to others but may hurt or annoy the person towards whom it is intended. Identification of sarcastic phrases from social media posts finds its application in different domains like sentiment analysis, opinion mining, author profiling, and harassment detection. We have proposed a model for the shared task iSarcasmEval - Intended Sarcasm Detection in English and Arabic <span class=tex-math>(CITATION)</span> by SemEval-2022 considering the language English based on ELmo embeddings for Subtasks A and C and TF-IDF vectors and Gaussian Naive bayes classifier for Subtask B. The proposed model resulted in a F1 score 0.2012 for sarcastic texts in Subtask A, macro-F1 score of 0.0387 and 0.2794 for Subtasks B and C respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.115.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.115.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.115/><span class=acl-fixed-case>A</span>mrita_<span class=acl-fixed-case>CEN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: A Machine Learning Approach for Detecting Intended Sarcasm using Oversampling</a></strong><br><a href=/people/a/aparna-k-ajayan/>Aparna K Ajayan</a>
|
<a href=/people/k/krishna-mohanan/>Krishna Mohanan</a>
|
<a href=/people/a/anugraha-s/>Anugraha S</a>
|
<a href=/people/p/premjith-b/>Premjith B</a>
|
<a href=/people/s/soman-kp/>Soman Kp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--115><div class="card-body p-3 small">This paper describes the submission of the team Amrita_CEN to the shared task on iSarcasm Eval: Intended Sarcasm Detection in English and Arabic at SemEval 2022. We employed machine learning algorithms towards sarcasm detection. Here, we used K-Nearest Neighbor (KNN), Support Vector Machine (SVM), Naïve Bayes, Logistic Regression, and Decision Tree along with the Random Forest ensemble method. Additionally, feature engineering techniques were applied to deal with the problems of class imbalance during training. Among the models considered, our study shows that the SVM, logistic regression and ensemble model Random Forest exhibited the best performance, which was submitted to the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.116.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.116/>High Tech team at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Intended Sarcasm Detection for <span class=acl-fixed-case>A</span>rabic texts</a></strong><br><a href=/people/h/hamza-alami/>Hamza Alami</a>
|
<a href=/people/a/abdessamad-benlahbib/>Abdessamad Benlahbib</a>
|
<a href=/people/a/ahmed-alami/>Ahmed Alami</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--116><div class="card-body p-3 small">This paper presents our proposed methods for the iSarcasmEval shared task. The shared task consists of three different subtasks. We participate in both subtask A and subtask C. The purpose of subtask A was to predict if a text is sarcastic while the aim of subtask C is to determine which text is sarcastic given a sarcastic text and its non-sarcastic rephrase. Both of the developed solutions used BERT pre-trained models. The proposed models are optimized on simple objectives and are easy to grasp. However, despite their simplicity, our methods ranked 4 and 2 in iSarcasmEval subtask A and subtask C for Arabic texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.117.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--117 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.117.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.117" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.117/><span class=acl-fixed-case>CS</span>-<span class=acl-fixed-case>UM</span>6<span class=acl-fixed-case>P</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Transformer-based Models for Intended Sarcasm Detection in <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/a/abdelkader-el-mahdaouy/>Abdelkader El Mahdaouy</a>
|
<a href=/people/a/abdellah-el-mekki/>Abdellah El Mekki</a>
|
<a href=/people/k/kabil-essefar/>Kabil Essefar</a>
|
<a href=/people/a/abderrahman-skiredj/>Abderrahman Skiredj</a>
|
<a href=/people/i/ismail-berrada/>Ismail Berrada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--117><div class="card-body p-3 small">Sarcasm is a form of figurative language where the intended meaning of a sentence differs from its literal meaning. This poses a serious challenge to several Natural Language Processing (NLP) applications such as Sentiment Analysis, Opinion Mining, and Author Profiling. In this paper, we present our participating system to the intended sarcasm detection task in English and Arabic languages. Our system consists of three deep learning-based models leveraging two existing pre-trained language models for Arabic and English. We have participated in all sub-tasks. Our official submissions achieve the best performance on sub-task A for Arabic language and rank second in sub-task B. For sub-task C, our system is ranked 7th and 11th on Arabic and English datasets, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.118.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.118.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.118/><span class=acl-fixed-case>T</span>ech<span class=acl-fixed-case>SSN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Intended Sarcasm Detection using Transformer Models</a></strong><br><a href=/people/r/ramdhanush-v/>Ramdhanush V</a>
|
<a href=/people/r/rajalakshmi-sivanaiah/>Rajalakshmi Sivanaiah</a>
|
<a href=/people/a/angel-s/>Angel S</a>
|
<a href=/people/s/sakaya-milton-rajendram/>Sakaya Milton Rajendram</a>
|
<a href=/people/t/t-t-mirnalinee/>Mirnalinee T T</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--118><div class="card-body p-3 small">Irony detection in the social media is an upcoming research which places a main role in sentiment analysis and offensive language identification. Sarcasm is one form of irony that is used to provide intended comments against realism. This paper describes a method to detect intended sarcasm in text (SemEval-2022 Task 6). The TECHSSN team used Bidirectional Encoder Representations from Transformers (BERT) models and its variants to classify the text as sarcastic or non-sarcastic in English and Arabic languages. The data is preprocessed and fed to the model for training. The transformer models learn the weights during the training phase from the given dataset and predicts the output class labels for the unseen test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.119.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.119/><span class=acl-fixed-case>I</span>2<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Intended Sarcasm in <span class=acl-fixed-case>E</span>nglish using Deep Learning Techniques</a></strong><br><a href=/people/a/adrian-moreno-monterde/>Adrián Moreno Monterde</a>
|
<a href=/people/l/laura-vazquez-ramos/>Laura Vázquez Ramos</a>
|
<a href=/people/j/jacinto-mata/>Jacinto Mata</a>
|
<a href=/people/v/victoria-pachon-alvarez/>Victoria Pachón Álvarez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--119><div class="card-body p-3 small">Sarcasm is often expressed through several verbal and non-verbal cues, e.g., a change of tone, overemphasis in a word, a drawn-out syllable, or a straight looking face. Most of the recent work in sarcasm detection has been carried out on textual data. This paper describes how the problem proposed in Task 6: Intended Sarcasm Detection in English (Abu Arfa et al. 2022) has been solved. Specifically, we participated in Subtask B: a binary multi-label classification task, where it is necessary to determine whether a tweet belongs to an ironic speech category, if any. Several approaches (classic machine learning and deep learning algorithms) were developed. The final submission consisted of a BERT based model and a macro-F1 score of 0.0699 was obtained.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.120.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.120/><span class=acl-fixed-case>NULL</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Intended Sarcasm Detection Using Stylistically Fused Contextualized Representation and Deep Learning</a></strong><br><a href=/people/m/mostafa-rahgouy/>Mostafa Rahgouy</a>
|
<a href=/people/h/hamed-babaei-giglou/>Hamed Babaei Giglou</a>
|
<a href=/people/t/taher-rahgooy/>Taher Rahgooy</a>
|
<a href=/people/c/cheryl-seals/>Cheryl Seals</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--120><div class="card-body p-3 small">The intended sarcasm cannot be understood until the listener observes that the text’s literal meaning violates truthfulness. Consequently, words and meanings play an essential role in specifying sarcasm. Enriched feature extraction techniques were proposed to capture both words and meanings in the contexts. Due to the overlapping features in sarcastic and non-sarcastic texts, a CNN model extracts local features from the combined class-dependent statistical embedding of sarcastic texts with contextualized embedding. Another component BiLSTM extracts long dependencies from combined non-sarcastic statistical and contextualized embeddings. This work combines a classifier that uses the combined high-level features of CNN and BiLSTM for sarcasm detection to produce the final predictions. The experimental analysis presented in this paper shows the effectiveness of the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.121.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--121 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.121/><span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>R</span>-<span class=acl-fixed-case>NCL</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Using ensemble loss with <span class=acl-fixed-case>BERT</span> for intended sarcasm detection</a></strong><br><a href=/people/e/emmanuel-osei-brefo/>Emmanuel Osei-Brefo</a>
|
<a href=/people/h/huizhi-liang/>Huizhi Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--121><div class="card-body p-3 small">Sarcasm has gained notoriety for being difficult to detect by machine learning systems due to its figurative nature. In this paper, Bidirectional Encoder Representations from Transformers (BERT) model has been used with ensemble loss made of cross-entropy loss and negative log-likelihood loss to classify whether a given sentence is in English and Arabic tweets are sarcastic or not. From the results obtained in the experiments, our proposed BERT with ensemble loss achieved superior performance when applied to English and Arabic test datasets. For the validation dataset, our model performed better on the Arabic dataset but failed to outperform the baseline method (made of BERT with only a single loss function) when applied on the English validation set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.122.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--122 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.122/><span class=acl-fixed-case>I</span>2<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Intended Sarcasm Detection on Social Networks with Deep Learning</a></strong><br><a href=/people/p/pablo-gonzalez-diaz/>Pablo Gonzalez Diaz</a>
|
<a href=/people/p/pablo-cordon/>Pablo Cordon</a>
|
<a href=/people/j/jacinto-mata/>Jacinto Mata</a>
|
<a href=/people/v/victoria-pachon/>Victoria Pachón</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--122><div class="card-body p-3 small">In this paper we present our approach and system description on iSarcasmEval: a SemEval task for intended sarcasm detection on social networks. This derives from our participation in SubTask A: Given a text, determine whether it is sarcastic or non-sarcastic. In our approach to complete the task, a comparison of several machine learning and deep learning algorithms using two datasets was conducted. The model which obtained the highest values of F1-score was a BERT-base-cased model. With this one, an F1-score of 0.2451 for the sarcastic class in the evaluation process was achieved. Finally, our team reached the 30th position.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.123.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--123 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.123/><span class=acl-fixed-case>BFCAI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Multi-Layer Perceptron for Sarcasm Detection in <span class=acl-fixed-case>A</span>rabic Texts</a></strong><br><a href=/people/n/nsrin-ashraf/>Nsrin Ashraf</a>
|
<a href=/people/f/fathy-elkazzaz/>Fathy Elkazzaz</a>
|
<a href=/people/m/mohamed-taha/>Mohamed Taha</a>
|
<a href=/people/h/hamada-nayel/>Hamada Nayel</a>
|
<a href=/people/t/tarek-elshishtawy/>Tarek Elshishtawy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--123><div class="card-body p-3 small">This paper describes the systems submitted to iSarcasm shared task. The aim of iSarcasm is to identify the sarcastic contents in Arabic and English text. Our team participated in iSarcasm for the Arabic language. A multi-Layer machine learning based model has been submitted for Arabic sarcasm detection. In this model, a vector space TF-IDF has been used as for feature representation. The submitted system is simple and does not need any external resources. The test results show encouraging results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.124.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.124.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.124/>aka<span class=acl-fixed-case>BERT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: An Ensemble Transformer-based Model for <span class=acl-fixed-case>A</span>rabic Sarcasm Detection</a></strong><br><a href=/people/a/abdulrahman-mohamed-kamr/>Abdulrahman Mohamed Kamr</a>
|
<a href=/people/e/ensaf-mohamed/>Ensaf Mohamed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--124><div class="card-body p-3 small">Due to the widespread usage of social media sites and the enormous number of users who utilize irony implicit words in most of their tweets and posts, it has become necessary to detect sarcasm, which strongly influences understanding and analyzing the crowd’s opinions. Detecting sarcasm is difficult due to the nature of sarcastic tweets, which vary based on the topic, region, the user’s attitude, culture, terminologies, and other criteria. In addition to these difficulties, detecting sarcasm in Arabic has its challenges due to its complexities, such as being morphologically rich, having many different dialects, and having low resources. In this research, we present our submission of (iSarcasmEval) sub-task A of the shared task on SemEval 2022. In Sub-task A; we determine whether the tweets are sarcastic or non-sarcastic. We implemented different approaches based on Transformers. First, we fine-tuned the AraBERT, MARABERT, and AraELECTRA. One of the challenges that faced us was that the data was not balanced. Non-sarcastic data is much more than sarcastic. We used data augmentation techniques to balance the two classes, significantly affecting the performance. The performance F1 score of the three models was 87%, 90%, and 91%, respectively. Then we boosted the three models by developing an ensemble model based on hard voting. The final performance F1 Score was 93%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.125.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.125.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.125" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.125/><span class=acl-fixed-case>A</span>lex<span class=acl-fixed-case>U</span>-<span class=acl-fixed-case>AL</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Detecting Sarcasm in <span class=acl-fixed-case>A</span>rabic Text Using Deep Learning Techniques</a></strong><br><a href=/people/a/aya-lotfy/>Aya Lotfy</a>
|
<a href=/people/m/marwan-torki/>Marwan Torki</a>
|
<a href=/people/n/nagwa-m-el-makky/>Nagwa El-Makky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--125><div class="card-body p-3 small">Sarcasm detection is an important task in Natural Language Understanding. Sarcasm is a form of verbal irony that occurs when there is a discrepancy between the literal and intended meanings of an expression. In this paper, we use the tweets of the Arabic dataset provided by SemEval-2022 task 6 to train deep learning classifiers to solve the sub-tasks A and C associated with the dataset. Sub-task A is to determine if the tweet is sarcastic or not. For sub-task C, given a sarcastic text and its non-sarcastic rephrase, i.e. two texts that convey the same meaning, determine which is the sarcastic one. In our solution, we utilize fine-tuned MARBERT (Abdul-Mageed et al., 2021) model with an added single linear layer on top for classification. The proposed solution achieved 0.5076 F1-sarcastic in Arabic sub-task A, accuracy of 0.7450 and F-score of 0.7442 in Arabic sub-task C. We achieved the <span class=tex-math>2<sup>nd</sup></span> and the <span class=tex-math>9<sup>th</sup></span> places for Arabic sub-tasks A and C respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.126.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--126 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.126.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.126" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.126/>reamtchka at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Investigating the effect of different loss functions for Sarcasm detection for unbalanced datasets</a></strong><br><a href=/people/r/reem-abdel-salam/>Reem Abdel-Salam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--126><div class="card-body p-3 small">This paper describes the system used in SemEval-2022 Task 6: Intended Sarcasm Detection in English and Arabic. Achieving 20th,3rd places with 34& 47 F1-Sarcastic score for task A, 16th place for task B with 0.0560 F1-macro score, and 10, 6th places for task C with72% and 80% accuracy on the leaderboard. A voting classifier between either multiple different BERT-based models or machine learningmodels is proposed, as our final model. Multiple key points has been extensively examined to overcome the problem of the unbalance ofthe dataset as: type of models, suitable architecture, augmentation, loss function, etc. In addition to that, we present an analysis of ourresults in this work, highlighting its strengths and shortcomings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.127.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--127 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.127/>niksss at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Are Traditionally Pre-Trained Contextual Embeddings Enough for Detecting Intended Sarcasm ?</a></strong><br><a href=/people/n/nikhil-singh/>Nikhil Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--127><div class="card-body p-3 small">This paper presents the 10th and 11th place system for Subtask A -English and Subtask A Arabic respectively of the SemEval 2022 -Task 6. The purpose of the Subtask A was to classify a given text sequence into sarcastic and nonsarcastic. We also breifly cover our method for Subtask B which performed subpar when compared with most of the submissions on the official leaderboard . All of the developed solutions used a transformers based language model for encoding the text sequences with necessary changes of the pretrained weights and classifier according to the language and subtask at hand .</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.128.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.128.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.128/><span class=acl-fixed-case>D</span>artmouth at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Detection of Sarcasm</a></strong><br><a href=/people/r/rishik-lad/>Rishik Lad</a>
|
<a href=/people/w/weicheng-ma/>Weicheng Ma</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--128><div class="card-body p-3 small">This paper introduces the result of Team Dartmouth’s experiments on each of the five subtasks for the detection of sarcasm in English and Arabic tweets. This detection was framed as a classification problem, and our contributions are threefold: we developed an English binary classifier system with RoBERTa, an Arabic binary classifier with XLM-RoBERTa, and an English multilabel classifier with BERT. Preprocessing steps are taken with labeled input data prior to tokenization, such as extracting and appending verbs/adjectives or representative/significant keywords to the end of an input tweet to help the models better understand and generalize sarcasm detection. We also discuss the results of simple data augmentation techniques to improve the quality of the given training dataset as well as an alternative approach to the question of multilabel sequence classification. Ultimately, our systems place us in the top 14 participants for each of the five subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.129.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.129/><span class=acl-fixed-case>ISD</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Sarcasm Detection Using Lightweight Models</a></strong><br><a href=/people/s/samantha-huang/>Samantha Huang</a>
|
<a href=/people/e/ethan-chi/>Ethan Chi</a>
|
<a href=/people/n/nathan-chi/>Nathan Chi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--129><div class="card-body p-3 small">A robust comprehension of sarcasm detection iscritical for creating artificial systems that can ef-fectively perform sentiment analysis in writtentext. In this work, we investigate AI approachesto identifying whether a text is sarcastic or notas part of SemEval-2022 Task 6. We focus oncreating systems for Task A, where we experi-ment with lightweight statistical classificationapproaches trained on both GloVe features andmanually-selected features. Additionally, weinvestigate fine-tuning the transformer modelBERT. Our final system for Task A is an Ex-treme Gradient Boosting Classifier trained onmanually-engineered features. Our final sys-tem achieved an F1-score of 0.2403 on SubtaskA and was ranked 32 of 43.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.130.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.130.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.130/>Plumeria at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Sarcasm Detection for <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>A</span>rabic Using Transformers and Data Augmentation</a></strong><br><a href=/people/m/mosab-shaheen/>Mosab Shaheen</a>
|
<a href=/people/s/shubham-nigam/>Shubham Nigam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--130><div class="card-body p-3 small">The paper describes our submission to SemEval-2022 Task 6 on sarcasm detection and its five subtasks for English and Arabic. Sarcasm conveys a meaning which contradicts the literal meaning, and it is mainly found on social networks. It has a significant role in understanding the intention of the user. For detecting sarcasm, we used deep learning techniques based on transformers due to its success in the field of Natural Language Processing (NLP) without the need for feature engineering. The datasets were taken from tweets. We created new datasets by augmenting with external data or by using word embeddings and repetition of instances. Experiments were done on the datasets with different types of preprocessing because it is crucial in this task. The rank of our team was consistent across four subtasks (fourth rank in three subtasks and sixth rank in one subtask); whereas other teams might be in the top ranks for some subtasks but rank drastically less in other subtasks. This implies the robustness and stability of the models and the techniques we used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.131.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--131 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.131.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.131/><span class=acl-fixed-case>IISERB</span> Brains at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: A Deep-learning Framework to Identify Intended Sarcasm in <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/t/tanuj-shekhawat/>Tanuj Shekhawat</a>
|
<a href=/people/m/manoj-kumar/>Manoj Kumar</a>
|
<a href=/people/u/udaybhan-rathore/>Udaybhan Rathore</a>
|
<a href=/people/a/aditya-joshi/>Aditya Joshi</a>
|
<a href=/people/j/jasabanta-patro/>Jasabanta Patro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--131><div class="card-body p-3 small">This paper describes the system architectures and the models submitted by our team “IISERB Brains” to SemEval 2022 Task 6 competition. We contested for all three sub-tasks floated for the English dataset. On the leader-board, we got 19th rank out of 43 teams for sub-task A, 8th rank out of 22 teams for sub-task B, and 13th rank out of 16 teams for sub-task C. Apart from the submitted results and models, we also report the other models and results that we obtained through our experiments after organizers published the gold labels of their evaluation data. All of our code and links to additional resources are present in GitHub for reproducibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.132.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.132/>connotation_clashers at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: The effect of sentiment analysis on sarcasm detection</a></strong><br><a href=/people/p/patrick-hantsch/>Patrick Hantsch</a>
|
<a href=/people/n/nadav-chkroun/>Nadav Chkroun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--132><div class="card-body p-3 small">We investigated the influence of contradictory connotations of words or phrases occurring in sarcastic statements, causing those statements to convey the opposite of their literal meaning. Our approach was to perform a sentiment analysis in order to capture potential opposite sentiments within one sentence and use its results as additional information for a further classifier extracting general text features, testing this for a Convolutional Neural Network, as well as for a Support Vector Machine classifier, respectively. We found that a more complex and sophisticated implementation of the sentiment analysis than just classifying the sentences as positive or negative is necessary, since our implementation showed a worse performance in both approaches than the respective classifier without using any sentiment analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.133.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.133/><span class=acl-fixed-case>TUG</span>-<span class=acl-fixed-case>CIC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: Two-stage Fine-tuning for Intended Sarcasm Detection</a></strong><br><a href=/people/j/jason-angel/>Jason Angel</a>
|
<a href=/people/s/segun-aroyehun/>Segun Aroyehun</a>
|
<a href=/people/a/alexander-gelbukh/>Alexander Gelbukh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--133><div class="card-body p-3 small">We present our systems and findings for the iSarcasmEval: Intended Sarcasm Detection In English and Arabic at SEMEVAL 2022. Specifically we take part in Subtask A for the English language. The task aims to determine whether a text from social media (a tweet) is sarcastic or not. We model the problem using knowledge sources, a pre-trained language model on sentiment/emotion data and a dataset focused on intended sarcasm. Our submission ranked third place among 43 teams. In addition, we show a brief error analysis of our best model to investigate challenging examples for detecting sarcasm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.134.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.134/><span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Transformer-based Model for Intended Sarcasm Detection in <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/g/guangmin-zheng/>Guangmin Zheng</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--134><div class="card-body p-3 small">In this paper, we (a YNU-HPCC team) describe the system we built in the SemEval-2022 competition. As participants in Task 6 (titled “iSarcasmEval: Intended Sarcasm Detection In English and Arabic”), we implement the sentiment system for all three subtasks in English and Arabic. All subtasks involve the detection of sarcasm (binary and multilabel classification) and the determination of the sarcastic text location (sentence pair classification). Our system primarily applies the sequence classification model of a bidirectional encoder representation from a transformer (BERT). The BERT is used to extract sentence information from both directions for downstream classification tasks. A single basic model is used for single-sentence and sentence-pair binary classification tasks. For the multilabel task, the Label-Powerset method and binary cross-entropy loss function with weights are used. Our system exhibits competitive performance, obtaining 12/43 (21/32), 11/22, and 3/16 (8/13) rankings in the three official rankings for English (Arabic).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.135.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.135.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.135" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.135/><span class=acl-fixed-case>UTNLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: A Comparative Analysis of Sarcasm Detection Using Generative-based and Mutation-based Data Augmentation</a></strong><br><a href=/people/a/amirhossein-abaskohi/>Amirhossein Abaskohi</a>
|
<a href=/people/a/arash-rasouli/>Arash Rasouli</a>
|
<a href=/people/t/tanin-zeraati/>Tanin Zeraati</a>
|
<a href=/people/b/behnam-bahrak/>Behnam Bahrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--135><div class="card-body p-3 small">Sarcasm is a term that refers to the use of words to mock, irritate, or amuse someone. It is commonly used on social media. The metaphorical and creative nature of sarcasm presents a significant difficulty for sentiment analysis systems based on affective computing. The methodology and results of our team, UTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in this paper. We put different models, and data augmentation approaches to the test and report on which one works best. The tests begin with traditional machine learning models and progress to transformer-based and attention-based models. We employed data augmentation based on data mutation and data generation. Using RoBERTa and mutation-based data augmentation, our best approach achieved an F1-score of 0.38 in the competition’s evaluation phase. After the competition, we fixed our model’s flaws and achieved anF1-score of 0.414.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.136.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.136/><span class=acl-fixed-case>FII</span> <span class=acl-fixed-case>UAIC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: i<span class=acl-fixed-case>S</span>arcasm<span class=acl-fixed-case>E</span>val - Intended Sarcasm Detection in <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/t/tudor-manoleasa/>Tudor Manoleasa</a>
|
<a href=/people/d/daniela-gifu/>Daniela Gifu</a>
|
<a href=/people/i/iustin-sandu/>Iustin Sandu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--136><div class="card-body p-3 small">The “iSarcasmEval - Intended Sarcasm Detection in English and Arabic” task at the SemEval 2022 competition focuses on detectingand rating the distinction between intendedand perceived sarcasm in the context of textual sarcasm detection, as well as the level ofirony contained in these texts. In the contextof SemEval, we present a binary classificationmethod which classifies the text as sarcasticor non-sarcastic (task A, for English) based onfive classical machine learning approaches bytrying to train the models based on this datasetsolely (i.e., no other datasets have been used).This process indicates low performance compared to previously studied datasets, which in2dicates that the previous ones might be biased.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.137.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--137 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.137.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.137/><span class=acl-fixed-case>M</span>ar<span class=acl-fixed-case>S</span>an at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: i<span class=acl-fixed-case>S</span>arcasm Detection via T5 and Sequence Learners</a></strong><br><a href=/people/m/maryam-najafi/>Maryam Najafi</a>
|
<a href=/people/e/ehsan-tavan/>Ehsan Tavan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--137><div class="card-body p-3 small">The paper describes SemEval-2022’s shared task “Intended Sarcasm Detection in English and Arabic.” This task includes English and Arabic tweets with sarcasm and non-sarcasm samples and irony speech labels. The first two subtasks predict whether a text is sarcastic and the ironic category the sarcasm sample belongs to. The third one is to find the sarcastic sample from its non-sarcastic paraphrase. Deep neural networks have recently achieved highly competitive performance in many tasks. Combining deep learning with language models has also resulted in acceptable accuracy. Inspired by this, we propose a novel deep learning model on top of language models. On top of T5, this architecture uses an encoder module of the transformer, followed by LSTM and attention to utilizing past and future information, concentrating on informative tokens. Due to the success of the proposed model, we used the same architecture with a few modifications to the output layer in all three subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.138.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.138.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.138" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.138/><span class=acl-fixed-case>LT</span>3 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Fuzzy-Rough Nearest Neighbor Classification for Sarcasm Detection</a></strong><br><a href=/people/o/olha-kaminska/>Olha Kaminska</a>
|
<a href=/people/c/chris-cornelis/>Chris Cornelis</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--138><div class="card-body p-3 small">This paper describes the approach developed by the LT3 team in the Intended Sarcasm Detection task at SemEval-2022 Task 6. We considered the binary classification subtask A for English data. The presented system is based on the fuzzy-rough nearest neighbor classification method using various text embedding techniques. Our solution reached 9th place in the official leader-board for English subtask A.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.139.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--139 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.139/><span class=acl-fixed-case>LISACT</span>eam at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: A Transformer based Approach for Intended Sarcasm Detection in <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/a/abdessamad-benlahbib/>Abdessamad Benlahbib</a>
|
<a href=/people/h/hamza-alami/>Hamza Alami</a>
|
<a href=/people/a/ahmed-alami/>Ahmed Alami</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--139><div class="card-body p-3 small">In this paper, we present our system and findings for SemEval-2022 Task 6 - iSarcasmEval: Intended Sarcasm Detection in English. The main objective of this task was to identify sarcastic tweets. This task was challenging mainly due to (1) the small training dataset that contains only 3468 tweets and (2) the imbalanced class distribution (25% sarcastic and 75% non-sarcastic). Our submitted model (ranked eighth on Sub-Task A and fifth on Sub-Task C) consists of a Transformer-based approach (BERTweet model).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.140.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.140.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.140/><span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>P</span>u<span class=acl-fixed-case>D</span>u at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Multilingual Learning for <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>A</span>rabic Sarcasm Detection</a></strong><br><a href=/people/y/yaqian-han/>Yaqian Han</a>
|
<a href=/people/y/yekun-chai/>Yekun Chai</a>
|
<a href=/people/s/shuohuan-wang/>Shuohuan Wang</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/h/hongyi-huang/>Hongyi Huang</a>
|
<a href=/people/g/guanghao-chen/>Guanghao Chen</a>
|
<a href=/people/y/yitong-xu/>Yitong Xu</a>
|
<a href=/people/y/yang-yang/>Yang Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--140><div class="card-body p-3 small">Detecting sarcasm and verbal irony from people’s subjective statements is crucial to understanding their intended meanings and real sentiments and positions in social scenarios. This paper describes the X-PuDu system that participated in SemEval-2022 Task 6, iSarcasmEval - Intended Sarcasm Detection in English and Arabic, which aims at detecting intended sarcasm in various settings of natural language understanding. Our solution finetunes pre-trained language models, such as ERNIE-M and DeBERTa, under the multilingual settings to recognize the irony from Arabic and English texts. Our system ranked second out of 43, and ninth out of 32 in Task A: one-sentence detection in English and Arabic; fifth out of 22 in Task B: binary multi-label classification in English; first out of 16, and fifth out of 13 in Task C: sentence-pair detection in English and Arabic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.141.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.141.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.141/><span class=acl-fixed-case>DUCS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Exploring Emojis and Sentiments for Sarcasm Detection</a></strong><br><a href=/people/v/vandita-grover/>Vandita Grover</a>
|
<a href=/people/p/prof-hema-banati/>Prof Hema Banati</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--141><div class="card-body p-3 small">This paper describes the participation of team DUCS at SemEval 2022 Task 6: iSarcasmEval - Intended Sarcasm Detection in English and Arabic. Team DUCS participated in SubTask A of iSarcasmEval which was to determine if the given English text was sarcastic or not. In this work, emojis were utilized to capture how they contributed to the sarcastic nature of a text. It is observed that emojis can augment or reverse the polarity of a given statement. Thus sentiment polarities and intensities of emojis, as well as those of text, were computed to determine sarcasm. Use of capitalization, word repetition, and use of punctuation marks like '!' were factored in as sentiment intensifiers. An NLP augmenter was used to tackle the imbalanced nature of the sarcasm dataset. Several architectures comprising of various ML and DL classifiers, and transformer models like BERT and Multimodal BERT were experimented with. It was observed that Multimodal BERT outperformed other architectures tested and achieved an F1-score of 30.71%. The key takeaway of this study was that sarcastic texts are usually positive sentences. In general emojis with positive polarity are used more than those with negative polarities in sarcastic texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.142.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.142/><span class=acl-fixed-case>UMUT</span>eam at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Evaluating Transformers for detecting Sarcasm in <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/j/jose-garcia-diaz/>José García-Díaz</a>
|
<a href=/people/c/camilo-caparros-laiz/>Camilo Caparros-Laiz</a>
|
<a href=/people/r/rafael-valencia-garcia/>Rafael Valencia-García</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--142><div class="card-body p-3 small">In this manuscript we detail the participation of the UMUTeam in the iSarcasm shared task (SemEval-2022). This shared task is related to the identification of sarcasm in English and Arabic documents. Our team achieve in the first challenge, a binary classification task, a F1 score of the sarcastic class of 17.97 for English and 31.75 for Arabic. For the second challenge, a multi-label classification, our results are not recorded due to an unknown problem. Therefore, we report the results of each sarcastic mechanism with the validation split. For our proposal, several neural networks that combine language-independent linguistic features with pre-trained embeddings are trained. The embeddings are based on different schemes, such as word and sentence embeddings, and contextual and non-contextual embeddings. Besides, we evaluate different techniques for the integration of the feature sets, such as ensemble learning and knowledge integration. In general, our best results are achieved using the knowledge integration strategy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.143.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.143/><span class=acl-fixed-case>R</span>2<span class=acl-fixed-case>D</span>2 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Are language models sarcastic enough? Finetuning pre-trained language models to identify sarcasm</a></strong><br><a href=/people/m/mayukh-sharma/>Mayukh Sharma</a>
|
<a href=/people/i/ilanthenral-kandasamy/>Ilanthenral Kandasamy</a>
|
<a href=/people/v/vasantha-w-b/>Vasantha W B</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--143><div class="card-body p-3 small">This paper describes our system used for SemEval 2022 Task 6: iSarcasmEval: Intended Sarcasm Detection in English and Arabic. We participated in all subtasks based on only English datasets. Pre-trained Language Models (PLMs) have become a de-facto approach for most natural language processing tasks. In our work, we evaluate the performance of these models for identifying sarcasm. For Subtask A and Subtask B, we used simple finetuning on PLMs. For Subtask C, we propose a Siamese network architecture trained using a combination of cross-entropy and distance-maximisation loss. Our model was ranked <span class=tex-math>7<sup>th</sup></span> in Subtask B, <span class=tex-math>8<sup>th</sup></span> in Subtask C (English), and performed well in Subtask A (English). In our work, we also present the comparative performance of different PLMs for each Subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.144.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.144/><span class=acl-fixed-case>S</span>arcasm<span class=acl-fixed-case>D</span>et at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6: Detecting Sarcasm using Pre-trained Transformers in <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>A</span>rabic Languages</a></strong><br><a href=/people/m/malak-abdullah/>Malak Abdullah</a>
|
<a href=/people/d/dalya-alnore/>Dalya Alnore</a>
|
<a href=/people/s/safa-swedat/>Safa Swedat</a>
|
<a href=/people/j/jumana-khrais/>Jumana Khrais</a>
|
<a href=/people/m/mahmoud-al-ayyoub/>Mahmoud Al-Ayyoub</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--144><div class="card-body p-3 small">This paper presents solution systems for task 6 at SemEval2022, iSarcasmEval: Intended Sarcasm Detection In English and Arabic. The shared task 6 consists of three sub-task. We participated in subtask A for both languages, Arabic and English. The goal of subtask A is to predict if a tweet would be considered sarcastic or not. The proposed solution SarcasmDet has been developed using the state-of-the-art Arabic and English pre-trained models AraBERT, MARBERT, BERT, and RoBERTa with ensemble techniques. The paper describes the SarcasmDet architecture with the fine-tuning of the best hyperparameter that led to this superior system. Our model ranked seventh out of 32 teams in subtask A- Arabic with an f1-sarcastic of 0.4305 and Seventeen out of 42 teams with f1-sarcastic 0.3561. However, we built another model to score f-1 sarcastic with 0.43 in English after the deadline. Both Models (Arabic and English scored 0.43 as f-1 sarcastic with ranking seventh).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.145.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.145/><span class=acl-fixed-case>JCT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 6-A: Sarcasm Detection in Tweets Written in <span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>A</span>rabic using Preprocessing Methods and Word N-grams</a></strong><br><a href=/people/y/yaakov-hacohen-kerner/>Yaakov HaCohen-Kerner</a>
|
<a href=/people/m/matan-fchima/>Matan Fchima</a>
|
<a href=/people/i/ilan-meyrowitsch/>Ilan Meyrowitsch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--145><div class="card-body p-3 small">In this paper, we describe our submissions to SemEval-2022 contest. We tackled subtask 6-A - “iSarcasmEval: Intended Sarcasm Detection In English and Arabic – Binary Classification”. We developed different models for two languages: English and Arabic. We applied 4 supervised machine learning methods, 6 preprocessing methods for English and 3 for Arabic, and 3 oversampling methods. Our best submitted model for the English test dataset was a SVC model that balanced the dataset using SMOTE and removed stop words. For the Arabic test dataset our best submitted model was a SVC model that preprocessed removed longation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.146.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--146 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.146.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.146" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.146/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts</a></strong><br><a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/t/talita-anthonio/>Talita Anthonio</a>
|
<a href=/people/a/anna-sauer/>Anna Sauer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--146><div class="card-body p-3 small">We describe SemEval-2022 Task 7, a shared task on rating the plausibility of clarifications in instructional texts. The dataset for this task consists of manually clarified how-to guides for which we generated alternative clarifications and collected human plausibility judgements. The task of participating systems was to automatically determine the plausibility of a clarification in the respective context. In total, 21 participants took part in this task, with the best system achieving an accuracy of 68.9%. This report summarizes the results and findings from 8 teams and their system descriptions. Finally, we show in an additional evaluation that predictions by the top participating team make it possible to identify contexts with multiple plausible clarifications with an accuracy of 75.2%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.147.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.147.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.147/><span class=acl-fixed-case>JBNU</span>-<span class=acl-fixed-case>CCL</span>ab at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 7: <span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>BERT</span>a for Identifying Plausible Clarifications in Instructional Texts</a></strong><br><a href=/people/d/daewook-kang/>Daewook Kang</a>
|
<a href=/people/s/sung-min-lee/>Sung-Min Lee</a>
|
<a href=/people/e/eunhwan-park/>Eunhwan Park</a>
|
<a href=/people/s/seung-hoon-na/>Seung-Hoon Na</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--147><div class="card-body p-3 small">In this study, we examine the ability of contextualized representations of pretrained language model to distinguish whether sequences from instructional articles are plausible or implausible. Towards this end, we compare the BERT, RoBERTa, and DeBERTa models using simple classifiers based on the sentence representations of the [CLS] tokens and perform a detailed analysis by visualizing the representations of the [CLS] tokens of the models. In the experimental results of Subtask A: Multi-Class Classification, DeBERTa exhibits the best performance and produces a more distinguishable representation across different labels. Submitting an ensemble of 10 DeBERTa-based models, our final system achieves an accuracy of 61.4% and is ranked fifth out of models submitted by eight teams. Further in-depth results suggest that the abilities of pretrained language models for the plausibility detection task are more strongly affected by their model structures or attention designs than by their model sizes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.148.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.148/><span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 7: Ensemble Model Based on Pretrained Models for Identifying Plausible Clarifications</a></strong><br><a href=/people/x/xiaosong-qiao/>Xiaosong Qiao</a>
|
<a href=/people/y/yinglu-li/>Yinglu Li</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/q/qin-ying/>Qin Ying</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--148><div class="card-body p-3 small">This paper describes the system for the identifying Plausible Clarifications of Implicit and Underspecified Phrases. This task was set up as an English cloze task, in which clarifications are presented as possible fillers and systems have to score how well each filler plausibly fits in a given context. For this shared task, we propose our own solutions, including supervised proaches, unsupervised approaches with pretrained models, and then we use these models to build an ensemble model. Finally we get the 2nd best result in the subtask1 which is a classification task, and the 3rd best result in the subtask2 which is a regression task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.149.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--149 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.149/><span class=acl-fixed-case>D</span>uluth<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 7: Classifying Plausible Alternatives with Pre–trained <span class=acl-fixed-case>ELECTRA</span></a></strong><br><a href=/people/s/samuel-akrah/>Samuel Akrah</a>
|
<a href=/people/t/ted-pedersen/>Ted Pedersen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--149><div class="card-body p-3 small">This paper describes the DuluthNLP system that participated in Task 7 of SemEval-2022 on Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts. Given an instructional text with an omitted token, the task requires models to classify or rank the plausibility of potential fillers. To solve the task, we fine–tuned the models BERT, RoBERTa, and ELECTRA on training data where potential fillers are rated for plausibility. This is a challenging problem, as shown by BERT-based models achieving accuracy less than 45%. However, our ELECTRA model with tuned class weights on CrossEntropyLoss achieves an accuracy of 53.3% on the official evaluation test data, which ranks 6 out of the 8 total submissions for Subtask A.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.150.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--150 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.150/><span class=acl-fixed-case>S</span>tanford <span class=acl-fixed-case>ML</span>ab at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2022 Task 7: Tree- and Transformer-Based Methods for Clarification Plausibility</a></strong><br><a href=/people/t/thomas-yim/>Thomas Yim</a>
|
<a href=/people/j/junha-lee/>Junha Lee</a>
|
<a href=/people/r/rishi-verma/>Rishi Verma</a>
|
<a href=/people/s/scott-hickmann/>Scott Hickmann</a>
|
<a href=/people/a/annie-zhu/>Annie Zhu</a>
|
<a href=/people/c/camron-sallade/>Camron Sallade</a>
|
<a href=/people/i/ian-ng/>Ian Ng</a>
|
<a href=/people/r/ryan-chi/>Ryan Chi</a>
|
<a href=/people/p/patrick-liu/>Patrick Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--150><div class="card-body p-3 small">In this paper, we detail the methods we used to determine the idiomaticity and plausibility of candidate words or phrases into an instructional text as part of the SemEval Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts. Given a set of steps in an instructional text, there are certain phrases that most plausibly fill that spot. We explored various possible architectures, including tree-based methods over GloVe embeddings, ensembled BERT and ELECTRA models, and GPT 2-based infilling methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.151.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.151" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.151/>Nowruz at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 7: Tackling Cloze Tests with Transformers and Ordinal Regression</a></strong><br><a href=/people/m/mohammadmahdi-nouriborji/>Mohammadmahdi Nouriborji</a>
|
<a href=/people/o/omid-rohanian/>Omid Rohanian</a>
|
<a href=/people/d/david-clifton/>David Clifton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--151><div class="card-body p-3 small">This paper outlines the system using which team Nowruz participated in SemEval 2022 Task 7 “Identifying Plausible Clarifications of Implicit and Underspecified Phrases” for both subtasks A and B. Using a pre-trained transformer as a backbone, the model targeted the task of multi-task classification and ranking in the context of finding the best fillers for a cloze task related to instructional texts on the website Wikihow. The system employed a combination of two ordinal regression components to tackle this task in a multi-task learning scenario. According to the official leaderboard of the shared task, this system was ranked 5th in the ranking and 7th in the classification subtasks out of 21 participating teams. With additional experiments, the models have since been further optimised. The code used in the experiments is going to be publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.152.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.152.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.152/><span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>P</span>u<span class=acl-fixed-case>D</span>u at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 7: A Replaced Token Detection Task Pre-trained Model with Pattern-aware Ensembling for Identifying Plausible Clarifications</a></strong><br><a href=/people/j/junyuan-shang/>Junyuan Shang</a>
|
<a href=/people/s/shuohuan-wang/>Shuohuan Wang</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/y/yanjun-yu/>Yanjun Yu</a>
|
<a href=/people/y/yue-zhou/>Yue Zhou</a>
|
<a href=/people/l/li-xiang/>Li Xiang</a>
|
<a href=/people/g/guixiu-yang/>Guixiu Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--152><div class="card-body p-3 small">This paper describes our winning system on SemEval 2022 Task 7: <span class=tex-math><em>Identifying Plausible Clarifications ofImplicit and Underspecified Phrases in Instructional Texts</em></span>. A replaced token detection pre-trained model is utilized with minorly different task-specific heads for SubTask-A: <span class=tex-math><em>Multi-class Classification</em></span> and SubTask-B: <span class=tex-math><em>Ranking</em></span>. Incorporating a pattern-aware ensemble method, our system achieves a 68.90% accuracy score and 0.8070 spearman’s rank correlation score surpassing the 2nd place with a large margin by 2.7 and 2.2 percent points for SubTask-A and SubTask-B, respectively. Our approach is simple and easy to implement, and we conducted ablation studies and qualitative and quantitative analyses for the working strategies used in our system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.153.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--153 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.153/><span class=acl-fixed-case>PALI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts</a></strong><br><a href=/people/z/zhou-mengyuan/>Zhou Mengyuan</a>
|
<a href=/people/d/dou-hu/>Dou Hu</a>
|
<a href=/people/m/mengfei-yuan/>Mengfei Yuan</a>
|
<a href=/people/j/jin-zhi/>Jin Zhi</a>
|
<a href=/people/x/xiyang-du/>Xiyang Du</a>
|
<a href=/people/l/lianxin-jiang/>Lianxin Jiang</a>
|
<a href=/people/y/yang-mo/>Yang Mo</a>
|
<a href=/people/x/xiaofeng-shi/>Xiaofeng Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--153><div class="card-body p-3 small">This paper describes our system used in the SemEval-2022 Task 7(Roth et al.): Identifying Plausible Clarifications of Implicit and Under-specified Phrases. Semeval Task7 is an more complex cloze task, different than normal cloze task, only requiring NLP system could find the best fillers for sentence. In Semeval Task7, NLP system not only need to choose the best fillers for each input instance, but also evaluate the quality of all possible fillers and give them a relative score according to context semantic information. We propose an ensemble of different state-of-the-art transformer-based language models(i.e., RoBERTa and Deberta) with some plug-and-play tricks, such as Grouped Layerwise Learning Rate Decay (GLLRD) strategy, contrastive learning loss, different pooling head and an external input data preprecess block before the information came into pretrained language models, which improve performance significantly. The main contributions of our sys-tem are 1) revealing the performance discrepancy of different transformer-based pretraining models on the downstream task; 2) presenting an efficient learning-rate and parameter attenuation strategy when fintuning pretrained language models; 3) adding different constrative learning loss to improve model performance; 4) showing the useful of the different pooling head structure. Our system achieves a test accuracy of 0.654 on subtask1(ranking 4th on the leaderboard) and a test Spearman’s rank correlation coefficient of 0.785 on subtask2(ranking 2nd on the leaderboard).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.154.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--154 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.154/>niksss at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task7:Transformers for Grading the Clarifications on Instructional Texts</a></strong><br><a href=/people/n/nikhil-singh/>Nikhil Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--154><div class="card-body p-3 small">This paper describes the 9th place system description for SemEval-2022 Task 7. The goal of this shared task was to develop computational models to predict how plausible a clarification made on an instructional text is. This shared task was divided into two Subtasks A and B. We attempted to solve these using various transformers-based architecture under different regime. We initially treated this as a text2text generation problem but comparing it with our recent approach we dropped it and treated this as a text-sequence classification and regression depending on the Subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.155.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.155/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Multilingual news article similarity</a></strong><br><a href=/people/x/xi-chen/>Xi Chen</a>
|
<a href=/people/a/ali-zeynali/>Ali Zeynali</a>
|
<a href=/people/c/chico-camargo/>Chico Camargo</a>
|
<a href=/people/f/fabian-flock/>Fabian Flöck</a>
|
<a href=/people/d/devin-gaffney/>Devin Gaffney</a>
|
<a href=/people/p/przemyslaw-grabowicz/>Przemyslaw Grabowicz</a>
|
<a href=/people/s/scott-hale/>Scott Hale</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/m/mattia-samory/>Mattia Samory</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--155><div class="card-body p-3 small">Thousands of new news articles appear daily in outlets in different languages. Understanding which articles refer to the same story can not only improve applications like news aggregation but enable cross-linguistic analysis of media consumption and attention. However, assessing the similarity of stories in news articles is challenging due to the different dimensions in which a story might vary, e.g., two articles may have substantial textual overlap but describe similar events that happened years apart. To address this challenge, we introduce a new dataset of nearly 10,000 news article pairs spanning 18 language combinations annotated for seven dimensions of similarity as SemEval 2022 Task 8. Here, we present an overview of the task, the best performing submissions, and the frontiers and challenges for measuring multilingual news article similarity. While the participants of this SemEval task contributed very strong models, achieving up to 0.818 correlation with gold standard labels across languages, human annotators are capable of reaching higher correlations, suggesting space for further progress.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.156.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--156 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.156.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.156" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.156/><span class=acl-fixed-case>EMBEDDIA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Investigating Sentence, Image, and Knowledge Graph Representations for Multilingual News Article Similarity</a></strong><br><a href=/people/e/elaine-zosa/>Elaine Zosa</a>
|
<a href=/people/e/emanuela-boros/>Emanuela Boros</a>
|
<a href=/people/b/boshko-koloski/>Boshko Koloski</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--156><div class="card-body p-3 small">In this paper, we present the participation of the EMBEDDIA team in the SemEval-2022 Task 8 (Multilingual News Article Similarity). We cover several techniques and propose different methods for finding the multilingual news article similarity by exploring the dataset in its entirety. We take advantage of the textual content of the articles, the provided metadata (e.g., titles, keywords, topics), the translated articles, the images (those that were available), and knowledge graph-based representations for entities and relations present in the articles. We, then, compute the semantic similarity between the different features and predict through regression the similarity scores. Our findings show that, while our proposed methods obtained promising results, exploiting the semantic textual similarity with sentence representations is unbeatable. Finally, in the official SemEval-2022 Task 8, we ranked fifth in the overall team ranking cross-lingual results, and second in the English-only results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.157.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.157.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.157" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.157/><span class=acl-fixed-case>HFL</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: A Linguistics-inspired Regression Model with Data Augmentation for Multilingual News Similarity</a></strong><br><a href=/people/z/zihang-xu/>Zihang Xu</a>
|
<a href=/people/z/ziqing-yang/>Ziqing Yang</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/z/zhigang-chen/>Zhigang Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--157><div class="card-body p-3 small">This paper describes our system designed for SemEval-2022 Task 8: Multilingual News Article Similarity. We proposed a linguistics-inspired model trained with a few task-specific strategies. The main techniques of our system are: 1) data augmentation, 2) multi-label loss, 3) adapted R-Drop, 4) samples reconstruction with the head-tail combination. We also present a brief analysis of some negative methods like two-tower architecture. Our system ranked 1st on the leaderboard while achieving a Pearson’s Correlation Coefficient of 0.818 on the official evaluation set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.158.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.158.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.158" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.158/><span class=acl-fixed-case>G</span>ate<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>US</span>hef at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Entity-Enriched <span class=acl-fixed-case>S</span>iamese Transformer for Multilingual News Article Similarity</a></strong><br><a href=/people/i/iknoor-singh/>Iknoor Singh</a>
|
<a href=/people/y/yue-li/>Yue Li</a>
|
<a href=/people/m/melissa-thong/>Melissa Thong</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--158><div class="card-body p-3 small">This paper describes the second-placed system on the leaderboard of SemEval-2022 Task 8: Multilingual News Article Similarity. We propose an entity-enriched Siamese Transformer which computes news article similarity based on different sub-dimensions, such as the shared narrative, entities, location and time of the event discussed in the news article. Our system exploits a Siamese network architecture using a Transformer encoder to learn document-level representations for the purpose of capturing the narrative together with the auxiliary entity-based features extracted from the news articles. The intuition behind using all these features together is to capture the similarity between news articles at different granularity levels and to assess the extent to which different news outlets write about “the same events”. Our experimental results and detailed ablation study demonstrate the effectiveness and the validity of our proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.159.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--159 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.159.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.159/>Wolfies at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Feature extraction pipeline with transformers for Multi-lingual news article similarity</a></strong><br><a href=/people/n/nikhil-goel/>Nikhil Goel</a>
|
<a href=/people/r/ranjith-reddy-bommidi/>Ranjith Reddy Bommidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--159><div class="card-body p-3 small">This work is about finding the similarity between a pair of news articles. There are seven different objective similarity metrics provided in the dataset for each pair and the news articles are in multiple different languages. On top of the pre-trained embedding model, we calculated cosine similarity for baseline results and feed-forward neural network was then trained on top of it to improve the results. We also built separate pipelines for each similarity metric for feature extraction. We could see significant improvement from baseline results using feature extraction and feed-forward neural network.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.160.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.160.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.160" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.160/><span class=acl-fixed-case>S</span>koltech<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Multilingual News Article Similarity via Exploration of News Texts to Vector Representations</a></strong><br><a href=/people/m/mikhail-kuimov/>Mikhail Kuimov</a>
|
<a href=/people/d/daryna-dementieva/>Daryna Dementieva</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--160><div class="card-body p-3 small">This paper describes our contribution to SemEval 2022 Task 8: Multilingual News Article Similarity. The aim was to test completely different approaches and distinguish the best performing. That is why we’ve considered systems based on Transformer-based encoders, NER-based, and NLI-based methods (and their combination with SVO dependency triplets representation). The results prove that Transformer models produce the best scores. However, there is space for research and approaches that give not yet comparable but more interpretable results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.161.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--161 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.161.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.161/><span class=acl-fixed-case>IIIT</span>-<span class=acl-fixed-case>MLNS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: <span class=acl-fixed-case>S</span>iamese Architecture for Modeling Multilingual News Similarity</a></strong><br><a href=/people/s/sagar-joshi/>Sagar Joshi</a>
|
<a href=/people/d/dhaval-taunk/>Dhaval Taunk</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--161><div class="card-body p-3 small">The task of multilingual news article similarity entails determining the degree of similarity of a given pair of news articles in a language-agnostic setting. This task aims to determine the extent to which the articles deal with the entities and events in question without much consideration of the subjective aspects of the discourse. Considering the superior representations being given by these models as validated on other tasks in NLP across an array of high and low-resource languages and this task not having any restricted set of languages to focus on, we adopted using the encoder representations from these models as our choice throughout our experiments. For modeling the similarity task by using the representations given by these models, a Siamese architecture was used as the underlying architecture. In experimentation, we investigated on several fronts including features passed to the encoder model, data augmentation and ensembling among our major experiments. We found data augmentation to be the most effective working strategy among our experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.162.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.162.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.162/><span class=acl-fixed-case>H</span>ua<span class=acl-fixed-case>AMS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Combining Translation and Domain Pre-training for Cross-lingual News Article Similarity</a></strong><br><a href=/people/s/sai-sandeep-sharma-chittilla/>Sai Sandeep Sharma Chittilla</a>
|
<a href=/people/t/talaat-khalil/>Talaat Khalil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--162><div class="card-body p-3 small">This paper describes our submission to SemEval-2022 Multilingual News Article Similarity task. We experiment with different approaches that utilize a pre-trained language model fitted with a regression head to predict similarity scores for a given pair of news articles. Our best performing systems include 2 key steps: 1) pre-training with in-domain data 2) training data enrichment through machine translation. Our final submission is an ensemble of predictions from our top systems. While we show the significance of pre-training and augmentation, we believe the issue of language coverage calls for more attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.163.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--163 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.163/><span class=acl-fixed-case>D</span>artmouth<span class=acl-fixed-case>CS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Predicting Multilingual News Article Similarity with Meta-Information and Translation</a></strong><br><a href=/people/j/joseph-hajjar/>Joseph Hajjar</a>
|
<a href=/people/w/weicheng-ma/>Weicheng Ma</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--163><div class="card-body p-3 small">This paper presents our approach for tackling SemEval-2022 Task 8: Multilingual News Article Similarity. Our experiments show that even by using multi-lingual pre-trained language models (LMs), translating the text into the same language yields the best evaluation performance. We also find that stylometric features of the text and meta-information of the news articles can be predicted based on the text with low error rates, and these predictions could be used to improve the predictions of the overall similarity scores. These findings suggest substantial correlations between authorship information and topical similarity estimation, which sheds light on future stylometric and topic modeling research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.164.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.164.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.164/>Team Innovators at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 for Task 8: Multi-Task Training with Hyperpartisan and Semantic Relation for Multi-Lingual News Article Similarity</a></strong><br><a href=/people/n/nidhir-bhavsar/>Nidhir Bhavsar</a>
|
<a href=/people/r/rishikesh-devanathan/>Rishikesh Devanathan</a>
|
<a href=/people/a/aakash-bhatnagar/>Aakash Bhatnagar</a>
|
<a href=/people/m/muskaan-singh/>Muskaan Singh</a>
|
<a href=/people/p/petr-motlicek/>Petr Motlicek</a>
|
<a href=/people/t/tirthankar-ghosal/>Tirthankar Ghosal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--164><div class="card-body p-3 small">This work represents the system proposed by team Innovators for SemEval 2022 Task 8: Multilingual News Article Similarity. Similar multilingual news articles should match irrespective of the style of writing, the language of conveyance, and subjective decisions and biases induced by medium/outlet. The proposed architecture includes a machine translation system that translates multilingual news articles into English and presents a multitask learning model trained simultaneously on three distinct datasets. The system leverages the PageRank algorithm for Long-form text alignment. Multitask learning approach allows simultaneous training of multiple tasks while sharing the same encoder during training, facilitating knowledge transfer between tasks. Our best model is ranked 16 with a Pearson score of 0.733.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.165.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--165 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.165.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.165" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.165/><span class=acl-fixed-case>O</span>versampled<span class=acl-fixed-case>ML</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: When multilingual news similarity met Zero-shot approaches</a></strong><br><a href=/people/m/mayank-jobanputra/>Mayank Jobanputra</a>
|
<a href=/people/l/lorena-martin-rodriguez/>Lorena Martín Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--165><div class="card-body p-3 small">We investigate the capabilities of pre-trained models, without any fine-tuning, for a document-level multilingual news similarity task of SemEval-2022. We utilize title and news content with appropriate pre-processing techniques. Our system derives 14 different similarity features using a combination of state-of-the-art methods (MPNet) with well-known statistical methods (i.e. TF-IDF, Word Mover’s distance). We formulate multilingual news similarity task as a regression task and approximate the overall similarity between two news articles using these features. Our best-performing system achieved a correlation score of 70.1% and was ranked 20th among the 34 participating teams. In this paper, in addition to a system description, we also provide further analysis of our results and an ablation study highlighting the strengths and limitations of our features. We make our code publicly available at <a href=https://github.com/cicl-iscl/multinewssimilarity class=acl-markup-url>https://github.com/cicl-iscl/multinewssimilarity</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.166.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--166 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.166.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.166/>Team <span class=acl-fixed-case>TMA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Lightweight and Language-Agnostic News Similarity Classifier</a></strong><br><a href=/people/n/nicolas-stefanovitch/>Nicolas Stefanovitch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--166><div class="card-body p-3 small">We present our contribution to the SemEval 22 Share Task 8: Multilingual news article similarity. The approach is lightweight and language-agnostic, it is based on the computation of several lexicographic and embedding-based features, and the use of a simple ML approach: random forests. In a notable departure from the task formulation, which is a ranking task, we tackled this task as a classification one. We present a detailed analysis of the behaviour of our system under different settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.167.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--167 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.167.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.167/><span class=acl-fixed-case>ITNLP</span>2022 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Pre-trained Model with Data Augmentation and Voting for Multilingual News Similarity</a></strong><br><a href=/people/z/zhongan-chen/>Zhongan Chen</a>
|
<a href=/people/w/weiwei-chen/>Weiwei Chen</a>
|
<a href=/people/y/yunlong-sun/>YunLong Sun</a>
|
<a href=/people/h/hongqing-xu/>Hongqing Xu</a>
|
<a href=/people/s/shuzhe-zhou/>Shuzhe Zhou</a>
|
<a href=/people/b/bohan-chen/>Bohan Chen</a>
|
<a href=/people/c/cheng-jie-sun/>Chengjie Sun</a>
|
<a href=/people/y/yuanchao-liu/>Yuanchao Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--167><div class="card-body p-3 small">This article introduces a system to solve the SemEval 2022 Task 8: Multilingual News Article Similarity. The task focuses on the consistency of events reported in two news articles. The system consists of a pre-trained model(e.g., INFOXLM and XLM-RoBERTa) to extract multilingual news features, following fully-connected networks to measure the similarity. In addition, data augmentation and Ten Fold Voting are used to enhance the model. Our final submitted model is an ensemble of three base models, with a Pearson value of 0.784 on the test dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.168.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--168 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.168.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.168/><span class=acl-fixed-case>LSX</span>_team5 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Multilingual News Article Similarity Assessment based on Word- and Sentence Mover’s Distance</a></strong><br><a href=/people/s/stefan-heil/>Stefan Heil</a>
|
<a href=/people/k/karina-kopp/>Karina Kopp</a>
|
<a href=/people/a/albin-zehe/>Albin Zehe</a>
|
<a href=/people/k/konstantin-kobs/>Konstantin Kobs</a>
|
<a href=/people/a/andreas-hotho/>Andreas Hotho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--168><div class="card-body p-3 small">This paper introduces our submission for the SemEval 2022 Task 8: Multilingual News Article Similarity. The task of the competition consisted of the development of a model, capable of determining the similarity between pairs of multilingual news articles. To address this challenge, we evaluated the Word Mover’s Distance in conjunction with word embeddings from ConceptNet Numberbatch and term frequencies of WorldLex, as well the Sentence Mover’s Distance based on sentence embeddings generated by pretrained transformer models of Sentence-BERT. To facilitate the comparison of multilingual articles with Sentence-BERT models, we deployed a Neural Machine Translation system. All our models achieve stable results in multilingual similarity estimation without learning parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.169.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.169/>Team dina at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Pre-trained Language Models as Baselines for Semantic Similarity</a></strong><br><a href=/people/d/dina-pisarevskaya/>Dina Pisarevskaya</a>
|
<a href=/people/a/arkaitz-zubiaga/>Arkaitz Zubiaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--169><div class="card-body p-3 small">This paper describes the participation of the team “dina” in the Multilingual News Similarity task at SemEval 2022. To build our system for the task, we experimented with several multilingual language models which were originally pre-trained for semantic similarity but were not further fine-tuned. We use these models in combination with state-of-the-art packages for machine translation and named entity recognition with the expectation of providing valuable input to the model. Our work assesses the applicability of such “pure” models to solve the multilingual semantic similarity task in the case of news articles. Our best model achieved a score of 0.511, but shows that there is room for improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.170.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.170/><span class=acl-fixed-case>TCU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: A Stacking Ensemble Transformer Model for Multilingual News Article Similarity</a></strong><br><a href=/people/x/xiang-luo/>Xiang Luo</a>
|
<a href=/people/y/yanqing-niu/>Yanqing Niu</a>
|
<a href=/people/b/boer-zhu/>Boer Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--170><div class="card-body p-3 small">Previous studies focus on measuring the degree of similarity of textsby using traditional machine learning methods, such as Support Vector Regression (SVR). Based on Transformers, this paper describes our contribution to SemEval-2022 Task 8 Multilingual News Article Similarity. The similarity of multilingual news articles requires a regression prediction on the similarity of multilingual articles, rather than a classification for judging text similarity. This paper mainly describes the architecture of the model and how to adjust the parameters in the experiment and strengthen the generalization ability. In this paper, we implement and construct different models through transformer-based models. We applied different transformer-based models, as well as ensemble them together by using ensemble learning. To avoid the overfit, we focus on the adjustment of parameters and the increase of generalization ability in our experiments. In the last submitted contest, we achieve a score of 0.715 and rank the 21st place.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.171.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--171 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.171.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.171/><span class=acl-fixed-case>N</span>ikkei at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Exploring <span class=acl-fixed-case>BERT</span>-based Bi-Encoder Approach for Pairwise Multilingual News Article Similarity</a></strong><br><a href=/people/s/shotaro-ishihara/>Shotaro Ishihara</a>
|
<a href=/people/h/hono-shirai/>Hono Shirai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--171><div class="card-body p-3 small">This paper describes our system in SemEval-2022 Task 8, where participants were required to predict the similarity of two multilingual news articles. In the task of pairwise sentence and document scoring, there are two main approaches: Cross-Encoder, which inputs pairs of texts into a single encoder, and Bi-Encoder, which encodes each input independently. The former method often achieves higher performance, but the latter gave us a better result in SemEval-2022 Task 8. This paper presents our exploration of BERT-based Bi-Encoder approach for this task, and there are several findings such as pretrained models, pooling methods, translation, data separation, and the number of tokens. The weighted average ensemble of the four models achieved the competitive result and ranked in the top 12.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.172.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--172 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.172/><span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Transformer-based Ensemble Model for Multilingual News Article Similarity</a></strong><br><a href=/people/z/zihan-nai/>Zihan Nai</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--172><div class="card-body p-3 small">This paper describes the system submitted by our team (YNU-HPCC) to SemEval-2022 Task 8: Multilingual news article similarity. This task requires participants to develop a system which could evaluate the similarity between multilingual news article pairs. We propose an approach that relies on Transformers to compute the similarity between pairs of news. We tried different models namely BERT, ALBERT, ELECTRA, RoBERTa, M-BERT and Compared their results. At last, we chose M-BERT as our System, which has achieved the best Pearson Correlation Coefficient score of 0.738.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.173.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--173 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.173" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.173/><span class=acl-fixed-case>BL</span>.<span class=acl-fixed-case>R</span>esearch at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Using various Semantic Information to evaluate document-level Semantic Textual Similarity</a></strong><br><a href=/people/s/sebastien-dufour/>Sebastien Dufour</a>
|
<a href=/people/m/mohamed-mehdi-kandi/>Mohamed Mehdi Kandi</a>
|
<a href=/people/k/karim-boutamine/>Karim Boutamine</a>
|
<a href=/people/c/camille-gosse/>Camille Gosse</a>
|
<a href=/people/m/mokhtar-boumedyen-billami/>Mokhtar Boumedyen Billami</a>
|
<a href=/people/c/christophe-bortolaso/>Christophe Bortolaso</a>
|
<a href=/people/y/youssef-miloudi/>Youssef Miloudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--173><div class="card-body p-3 small">This paper presents our system for document-level semantic textual similarity (STS) evaluation at SemEval-2022 Task 8: “Multilingual News Article Similarity”. The semantic information used is obtained by using different semantic models ranging from the extraction of key terms and named entities to the document classification and obtaining similarity from automatic summarization of documents. All these semantic information’s are then used as features to feed a supervised system in order to evaluate the degree of similarity of a pair of documents. We obtained a Pearson correlation score of 0.706 compared to the best score of 0.818 from teams that participated in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.174.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.174.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.174/><span class=acl-fixed-case>D</span>ata<span class=acl-fixed-case>S</span>cience-Polimi at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Stacking Language Models to Predict News Article Similarity</a></strong><br><a href=/people/m/marco-di-giovanni/>Marco Di Giovanni</a>
|
<a href=/people/t/thomas-tasca/>Thomas Tasca</a>
|
<a href=/people/m/marco-brambilla/>Marco Brambilla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--174><div class="card-body p-3 small">In this paper, we describe the approach we designed to solve SemEval-2022 Task 8: Multilingual News Article Similarity. We collect and use exclusively textual features (title, description and body) of articles. Our best model is a stacking of 14 Transformer-based Language models fine-tuned on single or multiple fields, using data in the original language or translated to English. It placed fourth on the original leaderboard, sixth on the complete official one and fourth on the English-subset official one. We observe the data collection as our principal source of error due to a relevant fraction of missing or wrong fields.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.175.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--175 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.175.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.175/><span class=acl-fixed-case>W</span>ue<span class=acl-fixed-case>D</span>evils at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 8: Multilingual News Article Similarity via Pair-Wise Sentence Similarity Matrices</a></strong><br><a href=/people/d/dirk-wangsadirdja/>Dirk Wangsadirdja</a>
|
<a href=/people/f/felix-heinickel/>Felix Heinickel</a>
|
<a href=/people/s/simon-trapp/>Simon Trapp</a>
|
<a href=/people/a/albin-zehe/>Albin Zehe</a>
|
<a href=/people/k/konstantin-kobs/>Konstantin Kobs</a>
|
<a href=/people/a/andreas-hotho/>Andreas Hotho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--175><div class="card-body p-3 small">We present a system that creates pair-wise cosine and arccosine sentence similarity matrices using multilingual sentence embeddings obtained from pre-trained SBERT and Universal Sentence Encoder (USE) models respectively. For each news article sentence, it searches the most similar sentence from the other article and computes an average score. Further, a convolutional neural network calculates a total similarity score for the article pairs on these matrices. Finally, a random forest regressor merges the previous results to a final score that can optionally be extended with a publishing date score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.176.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.176.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.176/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 9: <span class=acl-fixed-case>R</span>2<span class=acl-fixed-case>VQ</span> – Competence-based Multimodal Question Answering</a></strong><br><a href=/people/j/jingxuan-tu/>Jingxuan Tu</a>
|
<a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/m/marco-maru/>Marco Maru</a>
|
<a href=/people/s/simone-conia/>Simone Conia</a>
|
<a href=/people/k/kyeongmin-rim/>Kyeongmin Rim</a>
|
<a href=/people/k/kelley-lynch/>Kelley Lynch</a>
|
<a href=/people/r/richard-brutti/>Richard Brutti</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--176><div class="card-body p-3 small">In this task, we identify a challenge that is reflective of linguistic and cognitive competencies that humans have when speaking and reasoning. Particularly, given the intuition that textual and visual information mutually inform each other for semantic reasoning, we formulate a Competence-based Question Answering challenge, designed to involve rich semantic annotation and aligned text-video objects. The task is to answer questions from a collection of cooking recipes and videos, where each question belongs to a “question family” reflecting a specific reasoning competence. The data and task result is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.177.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--177 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.177/><span class=acl-fixed-case>HIT</span>&<span class=acl-fixed-case>QMUL</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 9: Label-Enclosed Generative Question Answering (<span class=acl-fixed-case>LEG</span>-<span class=acl-fixed-case>QA</span>)</a></strong><br><a href=/people/w/weihe-zhai/>Weihe Zhai</a>
|
<a href=/people/m/mingqiang-feng/>Mingqiang Feng</a>
|
<a href=/people/a/arkaitz-zubiaga/>Arkaitz Zubiaga</a>
|
<a href=/people/b/bingquan-liu/>Bingquan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--177><div class="card-body p-3 small">This paper presents the second place system for the R2VQ: competence-based multimodal question answering shared task. The purpose of this task is to involve semantic&amp;cooking roles and text-images objects when querying how well a system understands the procedure of a recipe. This task is approached with text-to-text generative model based on transformer architecture. As a result, the model can well generalise to soft constrained and other competence-based question answering problem. We propose label enclosed input method which help the model achieve significant improvement from 65.34 (baseline) to 91.3. In addition to describing the submitted system, the impact of model architecture and label selection are investigated along with remarks regarding error analysis. Finally, future works are presented.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.178.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.178.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.178/><span class=acl-fixed-case>S</span>amsung Research <span class=acl-fixed-case>P</span>oland (<span class=acl-fixed-case>SRPOL</span>) at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 9: Hybrid Question Answering Using Semantic Roles</a></strong><br><a href=/people/t/tomasz-dryjanski/>Tomasz Dryjański</a>
|
<a href=/people/m/monika-zaleska/>Monika Zaleska</a>
|
<a href=/people/b/bartek-kuzma/>Bartek Kuźma</a>
|
<a href=/people/a/artur-blazejewski/>Artur Błażejewski</a>
|
<a href=/people/z/zuzanna-bordzicka/>Zuzanna Bordzicka</a>
|
<a href=/people/p/pawel-bujnowski/>Paweł Bujnowski</a>
|
<a href=/people/k/klaudia-firlag/>Klaudia Firlag</a>
|
<a href=/people/c/christian-goltz/>Christian Goltz</a>
|
<a href=/people/m/maciej-grabowski/>Maciej Grabowski</a>
|
<a href=/people/j/jakub-jonczyk/>Jakub Jończyk</a>
|
<a href=/people/g/grzegorz-klosinski/>Grzegorz Kłosiński</a>
|
<a href=/people/b/bartlomiej-paziewski/>Bartłomiej Paziewski</a>
|
<a href=/people/n/natalia-paszkiewicz/>Natalia Paszkiewicz</a>
|
<a href=/people/j/jaroslaw-piersa/>Jarosław Piersa</a>
|
<a href=/people/p/piotr-andruszkiewicz/>Piotr Andruszkiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--178><div class="card-body p-3 small">In this work we present an overview of our winning system for the R2VQ - Competence-based Multimodal Question Answering task, with the final exact match score of 92.53%.The task is structured as question-answer pairs, querying how well a system is capable of competence-based comprehension of recipes. We propose a hybrid of a rule-based system, Question Answering Transformer, and a neural classifier for N/A answers recognition. The rule-based system focuses on intent identification, data extraction and response generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.179.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--179 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.179/><span class=acl-fixed-case>PINGAN</span>_<span class=acl-fixed-case>AI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 9: Recipe knowledge enhanced model applied in Competence-based Multimodal Question Answering</a></strong><br><a href=/people/z/zhihao-ruan/>Zhihao Ruan</a>
|
<a href=/people/x/xiaolong-hou/>Xiaolong Hou</a>
|
<a href=/people/l/lianxin-jiang/>Lianxin Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--179><div class="card-body p-3 small">This paper describes our system used in the SemEval-2022 Task 09: R2VQ - Competence-based Multimodal Question Answering. We propose a knowledge-enhanced model for predicting answer in QA task, this model use BERT as the backbone. We adopted two knowledge-enhanced methods in this model: the knowledge auxiliary text method and the knowledge embedding method. We also design an answer extraction task pipeline, which contains an extraction-based model, an automatic keyword labeling module, and an answer generation module. Our system ranked 3rd in task 9 and achieved an exact match score of 78.21 and a word-level F1 score of 82.62.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.180.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.180/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2022 Task 10: Structured Sentiment Analysis</a></strong><br><a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/l/laura-oberlaender/>Laura Oberlaender</a>
|
<a href=/people/e/enrica-troiano/>Enrica Troiano</a>
|
<a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/j/jan-buchmann/>Jan Buchmann</a>
|
<a href=/people/r/rodrigo-agerri/>Rodrigo Agerri</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--180><div class="card-body p-3 small">In this paper, we introduce the first SemEval shared task on Structured Sentiment Analysis, for which participants are required to predict all sentiment graphs in a text, where a single sentiment graph is composed of a sentiment holder, target, expression and polarity. This new shared task includes two subtracks (monolingual and cross-lingual) with seven datasets available in five languages, namely Norwegian, Catalan, Basque, Spanish and English. Participants submitted their predictions on a held-out test set and were evaluated on Sentiment Graph F1 . Overall, the task received over 200 submissions from 32 participating teams. We present the results of the 15 teams that provided system descriptions and our own expanded analysis of the test predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.181.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.181/><span class=acl-fixed-case>AMEX</span> <span class=acl-fixed-case>AI</span> Labs at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Contextualized fine-tuning of <span class=acl-fixed-case>BERT</span> for Structured Sentiment Analysis</a></strong><br><a href=/people/p/pratyush-sarangi/>Pratyush Sarangi</a>
|
<a href=/people/s/shamika-ganesan/>Shamika Ganesan</a>
|
<a href=/people/p/piyush-arora/>Piyush Arora</a>
|
<a href=/people/s/salil-joshi/>Salil Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--181><div class="card-body p-3 small">We describe the work carried out by AMEX AI Labs on the structured sentiment analysis task at SemEval-2022. This task focuses on extracting fine grained information w.r.t. to source, target and polar expressions in a given text. We propose a BERT based encoder, which utilizes a novel concatenation mechanism for combining syntactic and pretrained embeddings with BERT embeddings. Our system achieved an average rank of 14/32 systems, based on the average scores across seven datasets for five languages provided for the monolingual task. The proposed BERT based approaches outperformed BiLSTM based approaches used for structured sentiment extraction problem. We provide an in-depth analysis based on our post submission analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.182.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.182" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.182/><span class=acl-fixed-case>ISCAS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: An Extraction-Validation Pipeline for Structured Sentiment Analysis</a></strong><br><a href=/people/x/xinyu-lu/>Xinyu Lu</a>
|
<a href=/people/m/mengjie-ren/>Mengjie Ren</a>
|
<a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--182><div class="card-body p-3 small">ISCAS participated in both sub-tasks in SemEval-2022 Task 10: Structured Sentiment competition. We design an extraction-validation pipeline architecture to tackle both monolingual and cross-lingual sub-tasks. Experimental results show the multilingual effectiveness and cross-lingual robustness of our system. Our system is openly released on: <a href=https://github.com/luxinyu1/SemEval2022-Task10/ class=acl-markup-url>https://github.com/luxinyu1/SemEval2022-Task10/</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.183.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--183 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.183.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.183/><span class=acl-fixed-case>S</span>en<span class=acl-fixed-case>P</span>oi at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Point me to your Opinion, <span class=acl-fixed-case>S</span>en<span class=acl-fixed-case>P</span>oi</a></strong><br><a href=/people/j/jan-pfister/>Jan Pfister</a>
|
<a href=/people/s/sebastian-wankerl/>Sebastian Wankerl</a>
|
<a href=/people/a/andreas-hotho/>Andreas Hotho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--183><div class="card-body p-3 small">Structured Sentiment Analysis is the task of extracting sentiment tuples in a graph structure commonly from review texts. We adapt the Aspect-Based Sentiment Analysis pointer network BARTABSA to model this tuple extraction as a sequence prediction task and extend their output grammar to account for the increased complexity of Structured Sentiment Analysis. To predict structured sentiment tuples in languages other than English we swap BART for a multilingual mT5 and introduce a novel Output Length Regularization to mitigate overfitting to common target sequence lengths, thereby improving the performance of the model by up to 70%. We evaluate our approach on seven datasets in five languages including a zero shot crosslingual setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.184.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.184.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.184/><span class=acl-fixed-case>SSN</span>_<span class=acl-fixed-case>MLRG</span>1 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Structured Sentiment Analysis using 2-layer <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/k/karun-anantharaman/>Karun Anantharaman</a>
|
<a href=/people/d/divyasri-k/>Divyasri K</a>
|
<a href=/people/j/jayannthan-pt/>Jayannthan Pt</a>
|
<a href=/people/a/angel-s/>Angel S</a>
|
<a href=/people/r/rajalakshmi-sivanaiah/>Rajalakshmi Sivanaiah</a>
|
<a href=/people/s/sakaya-milton-rajendram/>Sakaya Milton Rajendram</a>
|
<a href=/people/t/t-t-mirnalinee/>Mirnalinee T T</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--184><div class="card-body p-3 small">Task 10 in SemEval 2022 is a composite task which entails analysis of opinion tuples, and recognition and demarcation of their nature. In this paper, we will elaborate on how such a methodology is implemented, how it is undertaken for a Structured Sentiment Analysis, and the results obtained thereof. To achieve this objective, we have adopted a bi-layered BiLSTM approach. In our research, a variation on the norm has been effected towards enhancement of accuracy, by basing the categorization meted out to an individual member as a by-product of its adjacent members, using specialized algorithms to ensure the veracity of the output, which has been modelled to be the holistically most accurate label for the entire sequence. Such a strategy is superior in terms of its parsing accuracy and requires less time. This manner of action has yielded an SF1 of 0.33 in the highest-performing configuration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.185.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.185.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.185/><span class=acl-fixed-case>MT</span>-Speech at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Incorporating Data Augmentation and Auxiliary Task with Cross-Lingual Pretrained Language Model for Structured Sentiment Analysis</a></strong><br><a href=/people/c/cong-chen/>Cong Chen</a>
|
<a href=/people/j/jiansong-chen/>Jiansong Chen</a>
|
<a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/f/fan-yang/>Fan Yang</a>
|
<a href=/people/g/guanglu-wan/>Guanglu Wan</a>
|
<a href=/people/j/jinxiong-xia/>Jinxiong Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--185><div class="card-body p-3 small">Sentiment analysis is a fundamental task, and structure sentiment analysis (SSA) is an important component of sentiment analysis. However, traditional SSA is suffering from some important issues: (1) lack of interactive knowledge of different languages; (2) small amount of annotation data or even no annotation data. To address the above problems, we incorporate data augment and auxiliary tasks within a cross-lingual pretrained language model into SSA. Specifically, we employ XLM-Roberta to enhance mutually interactive information when parallel data is available in the pretraining stage. Furthermore, we leverage two data augment strategies and auxiliary tasks to improve the performance on few-label data and zero-shot cross-lingual settings. Experiments demonstrate the effectiveness of our models. Our models rank first on the cross-lingual sub-task and rank second on the monolingual sub-task of SemEval-2022 task 10.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.186.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--186 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.186.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.186/><span class=acl-fixed-case>ECNU</span>_<span class=acl-fixed-case>ICA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: A Simple and Unified Model for Monolingual and Crosslingual Structured Sentiment Analysis</a></strong><br><a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/q/qin-chen/>Qin Chen</a>
|
<a href=/people/q/qingchun-bai/>Qingchun Bai</a>
|
<a href=/people/j/jun-xiao/>Jun Xiao</a>
|
<a href=/people/l/liang-he/>Liang He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--186><div class="card-body p-3 small">Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. In this paper, we focus on the structured sentiment analysis task that is released on SemEval-2022 Task 10. The task aims to extract the structured sentiment information (e.g., holder, target, expression and sentiment polarity) in a text. We propose a simple and unified model for both the monolingual and crosslingual structured sentiment analysis tasks. We translate this task into an event extraction task by regrading the expression as the trigger word and the other elements as the arguments of the event. Particularly, we first extract the expression by judging its start and end indices. Then, to consider the expression, we design a conditional layer normalization algorithm to extract the holder and target based on the extracted expression. Finally, we infer the sentiment polarity based on the extracted structured information. Pre-trained language models are utilized to obtain the text representation. We conduct the experiments on seven datasets in five languages. It attracted 233 submissions in monolingual subtask and crosslingual subtask from 32 teams. Finally, we obtain the top 5 place on crosslingual tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.187.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--187 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.187.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.187/><span class=acl-fixed-case>ZHIXIAOBAO</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Apporoaching Structured Sentiment with Graph Parsing</a></strong><br><a href=/people/y/yangkun-lin/>Yangkun Lin</a>
|
<a href=/people/c/chen-liang/>Chen Liang</a>
|
<a href=/people/j/jing-xu/>Jing Xu</a>
|
<a href=/people/c/chong-yang/>Chong Yang</a>
|
<a href=/people/y/yongliang-wang/>Yongliang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--187><div class="card-body p-3 small">This paper presents our submission to task 10, Structured Sentiment Analysis of the SemEval 2022 competition. The task aims to extract all elements of the fine-grained sentiment in a text. We cast structured sentiment analysis to the prediction of the sentiment graphs following (Barnes et al., 2021), where nodes are spans of sentiment holders, targets and expressions, and directed edges denote the relation types between them. Our approach closely follows that of semantic dependency parsing (Dozat and Manning, 2018). The difference is that we use pre-trained language models (e.g., BERT and RoBERTa) as text encoder to solve the problem of limited annotated data. Additionally, we make improvements on the computation of cross attention and present the suffix masking technique to make further performance improvement. Substantially, our model achieved the Top-1 average Sentiment Graph F1 score on seven datasets in five different languages in the monolingual subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.188.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--188 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.188.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.188/>Hitachi at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Comparing Graph- and <span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq-based Models Highlights Difficulty in Structured Sentiment Analysis</a></strong><br><a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/a/atsuki-yamaguchi/>Atsuki Yamaguchi</a>
|
<a href=/people/y/yasuhiro-sogawa/>Yasuhiro Sogawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--188><div class="card-body p-3 small">This paper describes our participation in SemEval-2022 Task 10, a structured sentiment analysis. In this task, we have to parse opinions considering both structure- and context-dependent subjective aspects, which is different from typical dependency parsing. Some of the major parser types have recently been used for semantic and syntactic parsing, while it is still unknown which type can capture structured sentiments well due to their subjective aspects. To this end, we compared two different types of state-of-the-art parser, namely graph-based and seq2seq-based. Our in-depth analyses suggest that, even though graph-based parser generally outperforms the seq2seq-based one, with strong pre-trained language models both parsers can essentially output acceptable and reasonable predictions. The analyses highlight that the difficulty derived from subjective aspects in structured sentiment analysis remains an essential challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.189.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.189.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.189/><span class=acl-fixed-case>UFRGS</span>ent at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Structured Sentiment Analysis using a Question Answering Model</a></strong><br><a href=/people/l/lucas-pessutto/>Lucas Pessutto</a>
|
<a href=/people/v/viviane-moreira/>Viviane Moreira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--189><div class="card-body p-3 small">This paper describes the system submitted by our team (UFRGSent) to SemEval-2022 Task 10: Structured Sentiment Analysis. We propose a multilingual approach that relies on a Question Answering model to find tuples consisting of aspect, opinion, and holder. The approach starts from general questions and uses the extracted tuple elements to find the remaining components. Finally, we employ an aspect sentiment classification model to classify the polarity of the entire tuple. Despite our method being in a mid-rank position on SemEval competition, we show that the question-answering approach can achieve good coverage retrieving sentiment tuples, allowing room for improvements in the technique.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.190.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--190 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.190" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.190/><span class=acl-fixed-case>OPI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Transformer-based Sequence Tagging with Relation Classification for Structured Sentiment Analysis</a></strong><br><a href=/people/r/rafal-poswiata/>Rafał Poświata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--190><div class="card-body p-3 small">This paper presents our solution for SemEval-2022 Task 10: Structured Sentiment Analysis. The solution consisted of two modules: the first for sequence tagging and the second for relation classification. In both modules we used transformer-based language models. In addition to utilizing language models specific to each of the five competition languages, we also adopted multilingual models. This approach allowed us to apply the solution to both monolingual and cross-lingual sub-tasks, where we obtained average Sentiment Graph F1 of 54.5% and 53.1%, respectively. The source code of the prepared solution is available at <a href=https://github.com/rafalposwiata/structured-sentiment-analysis class=acl-markup-url>https://github.com/rafalposwiata/structured-sentiment-analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.191.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.191.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.191/><span class=acl-fixed-case>ETMS</span>@<span class=acl-fixed-case>IITKGP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Structured Sentiment Analysis Using A Generative Approach</a></strong><br><a href=/people/r/raghav-r/>Raghav R</a>
|
<a href=/people/a/adarsh-vemali/>Adarsh Vemali</a>
|
<a href=/people/r/rajdeep-mukherjee/>Rajdeep Mukherjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--191><div class="card-body p-3 small">Structured Sentiment Analysis (SSA) deals with extracting opinion tuples in a text, where each tuple (h, e, t, p) consists of h, the holder, who expresses a sentiment polarity p towards a target t through a sentiment expression e. While prior works explore graph-based or sequence labeling-based approaches for the task, we in this paper present a novel unified generative method to solve SSA, a SemEval2022 shared task. We leverage a BART-based encoder-decoder architecture and suitably modify it to generate, given a sentence, a sequence of opinion tuples. Each generated tuple consists of seven integers respectively representing the indices corresponding to the start and end positions of the holder, target, and expression spans, followed by the sentiment polarity class associated between the target and the sentiment expression. We perform rigorous experiments for both Monolingual and Cross-lingual subtasks, and achieve competitive Sentiment F1 scores on the leaderboard in both settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.192.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--192 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.192.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.192/><span class=acl-fixed-case>SLPL</span>-Sentiment at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Making Use of Pre-Trained Model’s Attention Values in Structured Sentiment Analysis</a></strong><br><a href=/people/s/sadrodin-barikbin/>Sadrodin Barikbin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--192><div class="card-body p-3 small">Sentiment analysis is a useful problem which could serve a variety of fields from business intelligence to social studies and even health studies. Using SemEval 2022 Task 10 formulation of this problem and taking sequence labeling as our approach, we propose a model which learns the task by finetuning a pretrained transformer, introducing as few parameters (~150k) as possible and making use of precomputed attention values in the transformer. Our model improves shared task baselines on all task datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.193.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--193 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.193/><span class=acl-fixed-case>L</span>y<span class=acl-fixed-case>S</span>_<span class=acl-fixed-case>AC</span>oruña at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: Repurposing Off-the-Shelf Tools for Sentiment Analysis as Semantic Dependency Parsing</a></strong><br><a href=/people/i/iago-alonso-alonso/>Iago Alonso-Alonso</a>
|
<a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--193><div class="card-body p-3 small">This paper addressed the problem of structured sentiment analysis using a bi-affine semantic dependency parser, large pre-trained language models, and publicly available translation models. For the monolingual setup, we considered: (i) training on a single treebank, and (ii) relaxing the setup by training on treebanks coming from different languages that can be adequately processed by cross-lingual language models. For the zero-shot setup and a given target treebank, we relied on: (i) a word-level translation of available treebanks in other languages to get noisy, unlikely-grammatical, but annotated data (we release as much of it as licenses allow), and (ii) merging those translated treebanks to obtain training data. In the post-evaluation phase, we also trained cross-lingual models that simply merged all the English treebanks and did not use word-level translations, and yet obtained better results. According to the official results, we ranked 8th and 9th in the monolingual and cross-lingual setups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.194.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--194 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.194.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.194/><span class=acl-fixed-case>SPDB</span> Innovation Lab at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: A Novel End-to-End Structured Sentiment Analysis Model based on the <span class=acl-fixed-case>ERNIE</span>-<span class=acl-fixed-case>M</span></a></strong><br><a href=/people/y/yalong-jia/>Yalong Jia</a>
|
<a href=/people/z/zhenghui-ou/>Zhenghui Ou</a>
|
<a href=/people/y/yang-yang/>Yang Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--194><div class="card-body p-3 small">Sentiment analysis is a classical problem of natural language processing. SemEval 2022 sets a problem on the structured sentiment analysis in task 10, which is also a study-worthy topic in research area. In this paper, we propose a method which can predict structured sentiment information on multiple languages with limited data. The ERNIE-M pretrained language model is employed as a lingual feature extractor which works well on multiple language processing, followed by a graph parser as a opinion extractor. The method can predict structured sentiment information with high interpretability. We apply data augmentation as the given datasets are so small. Furthermore, we use K-fold cross-validation and DeBERTaV3 pretrained model as extra English embedding generator to train multiple models as our ensemble strategies. Experimental results show that the proposed model has considerable performance on both monolingual and cross-lingual tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.195.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.195/><span class=acl-fixed-case>HITSZ</span>-<span class=acl-fixed-case>HLT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 10: A Span-Relation Extraction Framework for Structured Sentiment Analysis</a></strong><br><a href=/people/y/yihui-li/>Yihui Li</a>
|
<a href=/people/y/yifan-yang/>Yifan Yang</a>
|
<a href=/people/y/yice-zhang/>Yice Zhang</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--195><div class="card-body p-3 small">This paper describes our system that participated in the SemEval-2022 Task 10: Structured Sentiment Analysis, which aims to extract opinion tuples from texts.A full opinion tuple generally contains an opinion holder, an opinion target, the sentiment expression, and the corresponding polarity. The complex structure of the opinion tuple makes the task challenging. To address this task, we formalize it as a span-relation extraction problem and propose a two-stage extraction framework accordingly. In the first stage, we employ the span module to enumerate spans and then recognize the type of every span. In the second stage, we employ the relation module to determine the relation between spans. Our system achieves competitive results and ranks among the top-10 systems in almost subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.196.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--196 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.196/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Multilingual Complex Named Entity Recognition (<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NER</span>)</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/a/anjie-fang/>Anjie Fang</a>
|
<a href=/people/b/besnik-fetahu/>Besnik Fetahu</a>
|
<a href=/people/s/sudipta-kar/>Sudipta Kar</a>
|
<a href=/people/o/oleg-rokhlenko/>Oleg Rokhlenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--196><div class="card-body p-3 small">We present the findings of SemEval-2022 Task 11 on Multilingual Complex Named Entity Recognition MULTICONER. Divided into 13 tracks, the task focused on methods to identify complex named entities (like names of movies, products and groups) in 11 languages in both monolingual and multi-lingual scenarios. Eleven tracks required building monolingual NER models for individual languages, one track focused on multilingual models able to work on all languages, and the last track featured code-mixed texts within any of these languages. The task is based on the MULTICONER dataset comprising of 2.3 millions instances in Bangla, Chinese, Dutch, English, Farsi, German, Hindi, Korean, Russian, Spanish, and Turkish. Results showed that methods fusing external knowledge into transformer models achieved the best results. However, identifying entities like creative works is still challenging even with external knowledge. MULTICONER was one of the most popular tasks in SemEval-2022 and it attracted 377 participants during the practice phase. 236 participants signed up for the final test phase and 55 teams submitted their systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.197.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.197/><span class=acl-fixed-case>LMN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: A Transformer-based System for <span class=acl-fixed-case>E</span>nglish Named Entity Recognition</a></strong><br><a href=/people/n/ngoc-lai/>Ngoc Lai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--197><div class="card-body p-3 small">Processing complex and ambiguous named entities is a challenging research problem, but it has not received sufficient attention from the natural language processing community. In this short paper, we present our participation in the English track of SemEval-2022 Task 11: Multilingual Complex Named Entity Recognition. Inspired by the recent advances in pretrained Transformer language models, we propose a simple yet effective Transformer-based baseline for the task. Despite its simplicity, our proposed approach shows competitive results in the leaderboard as we ranked 12 over 30 teams. Our system achieved a macro F1 score of 72.50% on the held-out test set. We have also explored a data augmentation approach using entity linking. While the approach does not improve the final performance, we also discuss it in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.198.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.198/><span class=acl-fixed-case>PA</span> Ph&amp;Tech at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: <span class=acl-fixed-case>NER</span> Task with Ensemble Embedding from Reinforcement Learning</a></strong><br><a href=/people/q/qizhi-lin/>Qizhi Lin</a>
|
<a href=/people/c/changyu-hou/>Changyu Hou</a>
|
<a href=/people/x/xiaopeng-wang/>Xiaopeng Wang</a>
|
<a href=/people/j/jun-wang/>Jun Wang</a>
|
<a href=/people/y/yixuan-qiao/>Yixuan Qiao</a>
|
<a href=/people/p/peng-jiang/>Peng Jiang</a>
|
<a href=/people/x/xiandi-jiang/>Xiandi Jiang</a>
|
<a href=/people/b/benqi-wang/>Benqi Wang</a>
|
<a href=/people/q/qifeng-xiao/>Qifeng Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--198><div class="card-body p-3 small">From pretrained contextual embedding to document-level embedding, the selection and construction of embedding have drawn more and more attention in the NER domain in recent research. This paper aims to discuss the performance of ensemble embeddings on complex NER tasks. Enlightened by Wang’s methodology, we try to replicate the dominating power of ensemble models with reinforcement learning optimizor on plain NER tasks to complex ones. Based on the composition of semeval dataset, the performance of the applied model is tested on lower-context, QA, and search query scenarios together with its zero-shot learning ability. Results show that with abundant training data, the model can achieve similar performance on lower-context cases compared to plain NER cases, but can barely transfer the performance to other scenarios in the test phase.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.199.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.199.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.199/><span class=acl-fixed-case>UC</span>3<span class=acl-fixed-case>M</span>-<span class=acl-fixed-case>PUCPR</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: An Ensemble Method of Transformer-based Models for Complex Named Entity Recognition</a></strong><br><a href=/people/e/elisa-schneider/>Elisa Schneider</a>
|
<a href=/people/r/renzo-m-rivera-zavala/>Renzo M. Rivera-Zavala</a>
|
<a href=/people/p/paloma-martinez/>Paloma Martinez</a>
|
<a href=/people/c/claudia-moro/>Claudia Moro</a>
|
<a href=/people/e/emerson-cabrera-paraiso/>Emerson Paraiso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--199><div class="card-body p-3 small">This study introduces the system submitted to the SemEval 2022 Task 11: MultiCoNER (Multilingual Complex Named Entity Recognition) by the UC3M-PUCPR team. We proposed an ensemble of transformer-based models for entity recognition in cross-domain texts. Our deep learning method benefits from the transformer architecture, which adopts the attention mechanism to handle the long-range dependencies of the input text. Also, the ensemble approach for named entity recognition (NER) improved the results over baselines based on individual models on two of the three tracks we participated in. The ensemble model for the code-mixed task achieves an overall performance of 76.36% F1-score, a 2.85 percentage point increase upon our individually best model for this task, XLM-RoBERTa-large (73.51%), outperforming the baseline provided for the shared task by 18.26 points. Our preliminary results suggest that contextualized language models ensembles can, even if modestly, improve the results in extracting information from unstructured data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.200.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--200 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.200.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.200" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.200/><span class=acl-fixed-case>DAMO</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yongliang-shen/>Yongliang Shen</a>
|
<a href=/people/j/jiong-cai/>Jiong Cai</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/x/xiaobin-wang/>Xiaobin Wang</a>
|
<a href=/people/p/pengjun-xie/>Pengjun Xie</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/w/weiming-lu/>Weiming Lu</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--200><div class="card-body p-3 small">The MultiCoNER shared task aims at detecting semantically ambiguous and complex named entities in short and low-context settings for multiple languages. The lack of contexts makes the recognition of ambiguous named entities challenging. To alleviate this issue, our team DAMO-NLP proposes a knowledge-based system, where we build a multilingual knowledge base based on Wikipedia to provide related context information to the named entity recognition (NER) model. Given an input sentence, our system effectively retrieves related contexts from the knowledge base. The original input sentences are then augmented with such context information, allowing significantly better contextualized token representations to be captured. Our system wins 10 out of 13 tracks in the MultiCoNER shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.201.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--201 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.201" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.201/>Multilinguals at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Complex <span class=acl-fixed-case>NER</span> in Semantically Ambiguous Settings for Low Resource Languages</a></strong><br><a href=/people/a/amit-pandey/>Amit Pandey</a>
|
<a href=/people/s/swayatta-daw/>Swayatta Daw</a>
|
<a href=/people/n/narendra-unnam/>Narendra Unnam</a>
|
<a href=/people/v/vikram-pudi/>Vikram Pudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--201><div class="card-body p-3 small">We leverage pre-trained language models to solve the task of complex NER for two low-resource languages: Chinese and Spanish. We use the technique of Whole Word Masking (WWM) to boost the performance of masked language modeling objective on large and unsupervised corpora. We experiment with multiple neural network architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on top of a fine-tuned BERT layer. All our models outperform the baseline by a significant margin and our best performing model obtains a competitive position on the evaluation leaderboard for the blind test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.202.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--202 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.202/><span class=acl-fixed-case>A</span>alto<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Ensembling Task-adaptive Pretrained Transformers for Multilingual Complex <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/a/aapo-pietilainen/>Aapo Pietiläinen</a>
|
<a href=/people/s/shaoxiong-ji/>Shaoxiong Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--202><div class="card-body p-3 small">This paper presents the system description of team AaltoNLP for SemEval-2022 shared task 11: MultiCoNER. Transformer-based models have produced high scores on standard Named Entity Recognition (NER) tasks. However, accuracy on complex named entities is still low. Complex and ambiguous named entities have been identified as a major error source in NER tasks. The shared task is about multilingual complex named entity recognition. In this paper, we describe an ensemble approach, which increases accuracy across all tested languages. The system ensembles output from multiple same architecture task-adaptive pretrained transformers trained with different random seeds. We notice a large discrepancy between performance on development and test data. Model selection based on limited development data may not yield optimal results on large test data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.203.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.203/><span class=acl-fixed-case>DANGNT</span>-<span class=acl-fixed-case>SGU</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Using Pre-trained Language Model for Complex Named Entity Recognition</a></strong><br><a href=/people/d/dang-nguyen/>Dang Nguyen</a>
|
<a href=/people/h/huy-khac-nguyen-huynh/>Huy Khac Nguyen Huynh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--203><div class="card-body p-3 small">In this paper, we describe a system that we built to participate in the SemEval 2022 Task 11: MultiCoNER Multilingual Complex Named Entity Recognition, specifically the track Mono-lingual in English. To construct this system, we used Pre-trained Language Models (PLMs). Especially, the Pre-trained Model base on BERT is applied for the task of recognizing named entities by fine-tuning method. We performed the evaluation on two test datasets of the shared task: the Practice Phase and the Evaluation Phase of the competition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.204.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.204.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.204/><span class=acl-fixed-case>OPDAI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: A hybrid approach for <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>NER</span> using outside <span class=acl-fixed-case>W</span>ikipedia knowledge</a></strong><br><a href=/people/z/ze-chen/>Ze Chen</a>
|
<a href=/people/k/kangxu-wang/>Kangxu Wang</a>
|
<a href=/people/j/jiewen-zheng/>Jiewen Zheng</a>
|
<a href=/people/z/zijian-cai/>Zijian Cai</a>
|
<a href=/people/j/jiarong-he/>Jiarong He</a>
|
<a href=/people/j/jin-gao/>Jin Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--204><div class="card-body p-3 small">This article describes the OPDAI submission to SemEval-2022 Task 11 on Chinese complex NER. First, we explore the performance of model-based approaches and their ensemble, finding that fine-tuning the pre-trained Chinese RoBERTa-wwm model with word semantic representation and contextual gazetteer representation performs best among single models. However, the model-based approach performs poorly on test data because of low-context and unseen-entity cases. Then, we extend our system into two stages: (1) generating entity candidates by using neural model, soft-templates and Wikipedia lexicon. (2) predicting the final entity results within a feature-based rank model. For the evaluation, our best submission achieves an <span class=tex-math>F<sub>1</sub></span> score of 0.7954 and attains the third-best score in the Chinese sub-track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.205.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.205.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.205/>Sliced at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Bigger, Better? Massively Multilingual <span class=acl-fixed-case>LM</span>s for Multilingual Complex <span class=acl-fixed-case>NER</span> on an Academic <span class=acl-fixed-case>GPU</span> Budget</a></strong><br><a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--205><div class="card-body p-3 small">Massively multilingual language models (MMLMs) have become a widely-used representation method, and multiple large MMLMs were proposed in recent years. A trend is to train MMLMs on larger text corpora or with more layers. In this paper we set out to test recent popular MMLMs on detecting semantically ambiguous and complex named entities with an academic GPU budget. Our submission of a single model for 11 languages on the SemEval Task 11 MultiCoNER shows that a vanilla transformer-CRF with XLM-R<span class=tex-math><sub>large</sub></span> outperforms the more recent RemBERT, ranking 9th from 26 submissions in the multilingual track. Compared to RemBERT, the XLM-R model has the additional advantage to fit on a slice of a multi-instance GPU. As contrary to expectations and recent findings, we found RemBERT to not be the best MMLM, we further set out to investigate this discrepancy with additional experiments on multilingual Wikipedia NER data. While we expected RemBERT to have an edge on that dataset as it is closer to its pre-training data, surprisingly, our results show that this is not the case, suggesting that text domain match does not explain the discrepancy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.206.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--206 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.206.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.206/>Infrrd.ai at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: A system for named entity recognition using data augmentation, transformer-based sequence labeling model, and <span class=acl-fixed-case>E</span>nsemble<span class=acl-fixed-case>CRF</span></a></strong><br><a href=/people/j/jianglong-he/>Jianglong He</a>
|
<a href=/people/a/akshay-uppal/>Akshay Uppal</a>
|
<a href=/people/m/mamatha-n/>Mamatha N</a>
|
<a href=/people/s/shiv-vignesh/>Shiv Vignesh</a>
|
<a href=/people/d/deepak-kumar/>Deepak Kumar</a>
|
<a href=/people/a/aditya-kumar-sarda/>Aditya Kumar Sarda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--206><div class="card-body p-3 small">In low-resource languages, the amount of training data is limited. Hence, the model has to perform well in unseen sentences and syntax on which the model has not trained. We propose a method that addresses the problem through an encoder and an ensemble of language models. A language-specific language model performed poorly when compared to a multilingual language model. So, the multilingual language model checkpoint is fine-tuned to a specific language. A novel approach of one hot encoder is introduced between the model outputs and the CRF to combine the results in an ensemble format. Our team, <b>Infrrd.ai</b>, competed in the MultiCoNER competition. The results are encouraging where the team is positioned within the top 10 positions. There is less than a 4% percent difference from the third position in most of the tracks that we participated in. The proposed method shows that the ensemble of models with a multilingual language model as the base with the help of an encoder performs better than a single language-specific model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.207.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.207.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.207/><span class=acl-fixed-case>UM</span>6<span class=acl-fixed-case>P</span>-<span class=acl-fixed-case>CS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Enhancing Multilingual and Code-Mixed Complex Named Entity Recognition via Pseudo Labels using Multilingual Transformer</a></strong><br><a href=/people/a/abdellah-el-mekki/>Abdellah El Mekki</a>
|
<a href=/people/a/abdelkader-el-mahdaouy/>Abdelkader El Mahdaouy</a>
|
<a href=/people/m/mohammed-akallouch/>Mohammed Akallouch</a>
|
<a href=/people/i/ismail-berrada/>Ismail Berrada</a>
|
<a href=/people/a/ahmed-khoumsi/>Ahmed Khoumsi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--207><div class="card-body p-3 small">Building real-world complex Named Entity Recognition (NER) systems is a challenging task. This is due to the complexity and ambiguity of named entities that appear in various contexts such as short input sentences, emerging entities, and complex entities. Besides, real-world queries are mostly malformed, as they can be code-mixed or multilingual, among other scenarios. In this paper, we introduce our submitted system to the Multilingual Complex Named Entity Recognition (MultiCoNER) shared task. We approach the complex NER for multilingual and code-mixed queries, by relying on the contextualized representation provided by the multilingual Transformer XLM-RoBERTa. In addition to the CRF-based token classification layer, we incorporate a span classification loss to recognize named entities spans. Furthermore, we use a self-training mechanism to generate weakly-annotated data from a large unlabeled dataset. Our proposed system is ranked 6th and 8th in the multilingual and code-mixed MultiCoNER’s tracks respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.208.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.208.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.208/><span class=acl-fixed-case>CASIA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: <span class=acl-fixed-case>C</span>hinese Named Entity Recognition for Complex and Ambiguous Entities</a></strong><br><a href=/people/j/jia-fu/>Jia Fu</a>
|
<a href=/people/z/zhen-gan/>Zhen Gan</a>
|
<a href=/people/z/zhucong-li/>Zhucong Li</a>
|
<a href=/people/s/sirui-li/>Sirui Li</a>
|
<a href=/people/d/dianbo-sui/>Dianbo Sui</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--208><div class="card-body p-3 small">This paper describes our approach to develop a complex named entity recognition system in SemEval 2022 Task 11: MultiCoNER Multilingual Complex Named Entity Recognition,Track 9 - Chinese. In this task, we need to identify the entity boundaries and categorylabels for the six identified categories of CW,LOC, PER, GRP, CORP, and PORD.The task focuses on detecting semantically ambiguous and complex entities in short and low-context settings. We constructed a hybrid system based on Roberta-large model with three training mechanisms and a series of data gugmentation.Three training mechanisms include adversarial training, Child-Tuning training, and continued pre-training. The core idea of the hybrid system is to improve the performance of the model in complex environments by introducing more domain knowledge through data augmentation and continuing pre-training domain adaptation of the model. Our proposed method in this paper achieves a macro-F1 of 0.797 on the final test set, ranking second.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.209.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--209 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.209/><span class=acl-fixed-case>TEAM</span>-Atreides at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: On leveraging data augmentation and ensemble to recognize complex Named Entities in <span class=acl-fixed-case>B</span>angla</a></strong><br><a href=/people/n/nazia-tasnim/>Nazia Tasnim</a>
|
<a href=/people/m/md-istiak-shihab/>Md. Istiak Shihab</a>
|
<a href=/people/a/asif-shahriyar-sushmit/>Asif Shahriyar Sushmit</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/f/farig-sadeque/>Farig Sadeque</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--209><div class="card-body p-3 small">Many areas, such as the biological and healthcare domain, artistic works, and organization names, have nested, overlapping, discontinuous entity mentions that may even be syntactically or semantically ambiguous in practice. Traditional sequence tagging algorithms are unable to recognize these complex mentions because they may violate the assumptions upon which sequence tagging schemes are founded. In this paper, we describe our contribution to SemEval 2022 Task 11 on identifying such complex Named Entities. We have leveraged the ensemble of multiple ELECTRA-based models that were exclusively pretrained on the Bangla language with the performance of ELECTRA-based models pretrained on English to achieve competitive performance on the Track-11. Besides providing a system description, we will also present the outcomes of our experiments on architectural decisions, dataset augmentations, and post-competition findings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.210.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--210 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.210.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.210/><span class=acl-fixed-case>KDDIE</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Using <span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>BERT</span>a for Named Entity Recognition</a></strong><br><a href=/people/c/caleb-martin/>Caleb Martin</a>
|
<a href=/people/h/huichen-yang/>Huichen Yang</a>
|
<a href=/people/w/william-hsu/>William Hsu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--210><div class="card-body p-3 small">In this work, we introduce our system to the SemEval 2022 Task 11: Multilingual Complex Named Entity Recognition (MultiCoNER) competition. Our team (KDDIE) attempted the sub-task of Named Entity Recognition (NER) for the language of English in the challenge and reported our results. For this task, we use transfer learning method: fine-tuning the pre-trained language models (PLMs) on the competition dataset. Our two approaches are the BERT-based PLMs and PLMs with additional layer such as Condition Random Field. We report our finding and results in this report.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.211.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--211 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.211.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.211/>silpa_nlp at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Tasks 11: Transformer based <span class=acl-fixed-case>NER</span> models for <span class=acl-fixed-case>H</span>indi and <span class=acl-fixed-case>B</span>angla languages</a></strong><br><a href=/people/s/sumit-singh/>Sumit Singh</a>
|
<a href=/people/p/pawankumar-jawale/>Pawankumar Jawale</a>
|
<a href=/people/u/uma-tiwary/>Uma Tiwary</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--211><div class="card-body p-3 small">We present Transformer based pretrained models, which are fine-tuned for Named Entity Recognition (NER) task. Our team participated in SemEval-2022 Task 11 MultiCoNER: Multilingual Complex Named Entity Recognition task for Hindi and Bangla. Result comparison of six models (mBERT, IndicBERT, MuRIL (Base), MuRIL (Large), XLM-RoBERTa (Base) and XLM-RoBERTa (Large) ) has been performed. It is found that among these models MuRIL (Large) model performs better for both the Hindi and Bangla languages. Its F1-Scores for Hindi and Bangla are 0.69 and 0.59 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.212.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--212 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.212.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.212/><span class=acl-fixed-case>DS</span>4<span class=acl-fixed-case>DH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Multilingual Named Entity Recognition Using an Ensemble of Transformer-based Language Models</a></strong><br><a href=/people/h/hossein-rouhizadeh/>Hossein Rouhizadeh</a>
|
<a href=/people/d/douglas-teodoro/>Douglas Teodoro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--212><div class="card-body p-3 small">In this paper, we describe our proposed method for the SemEval 2022 Task 11: Multilingual Complex Named Entity Recognition (MultiCoNER). The goal of this task is to locate and classify named entities in unstructured short complex texts in 11 different languages. After training a variety of contextual language models on the NER dataset, we used an ensemble strategy based on a majority vote to finalize our model. We evaluated our proposed approach on the multilingual NER dataset at SemEval-2022. The ensemble model provided consistent improvements against the individual models on the multilingual track, achieving a macro F1 performance of 65.2%. However, our results were significantly outperformed by the top ranking systems, achieving thus a baseline performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.213.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.213/><span class=acl-fixed-case>CSECU</span>-<span class=acl-fixed-case>DSG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Identifying the Multilingual Complex Named Entity in Text Using Stacked Embeddings and Transformer based Approach</a></strong><br><a href=/people/a/abdul-aziz/>Abdul Aziz</a>
|
<a href=/people/m/md-akram-hossain/>Md. Akram Hossain</a>
|
<a href=/people/a/abu-nowshed-chy/>Abu Nowshed Chy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--213><div class="card-body p-3 small">Recognizing complex and ambiguous named entities (NEs) is one of the formidable tasks in the NLP domain. However, the diversity of linguistic constituents, syntactic structure, semantic ambiguity as well as differences from traditional NEs make it challenging to identify the complex NEs. To address these challenges, SemEval-2022 Task 11 introduced a shared task MultiCoNER focusing on complex named entity recognition in multilingual settings. This paper presents our participation in this task where we propose two different approaches including a BiLSTM-CRF model with stacked-embedding strategy and a transformer-based approach. Our proposed method achieved competitive performance among the participants’ methods in a few languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.214.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--214 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.214/><span class=acl-fixed-case>CMNERO</span>ne at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Code-Mixed Named Entity Recognition by leveraging multilingual data</a></strong><br><a href=/people/s/suman-dowlagar/>Suman Dowlagar</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--214><div class="card-body p-3 small">Identifying named entities is, in general, a practical and challenging task in the field of Natural Language Processing. Named Entity Recognition on the code-mixed text is further challenging due to the linguistic complexity resulting from the nature of the mixing. This paper addresses the submission of team CMNEROne to the SEMEVAL 2022 shared task 11 MultiCoNER. The Code-mixed NER task aimed to identify named entities on the code-mixed dataset. Our work consists of Named Entity Recognition (NER) on the code-mixed dataset by leveraging the multilingual data. We achieved a weighted average F1 score of 0.7044, i.e., 6% greater than the NER baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.215.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--215 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.215.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.215/><span class=acl-fixed-case>RACAI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Complex named entity recognition using a lateral inhibition mechanism</a></strong><br><a href=/people/v/vasile-pais/>Vasile Pais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--215><div class="card-body p-3 small">This paper presents RACAI’s system used for the shared task of “Multilingual Complex Named Entity Recognition (MultiCoNER)”, organized as part of the “The 16th International Workshop on Semantic Evaluation (SemEval 2022)”. The system employs a novel layer inspired by the biological mechanism of lateral inhibition. This allowed the system to achieve good results without any additional resources apart from the provided training data. In addition to the system’s architecture, results are provided as well as observations regarding the provided dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.216.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--216 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.216.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.216/><span class=acl-fixed-case>N</span>amed<span class=acl-fixed-case>E</span>ntity<span class=acl-fixed-case>R</span>angers at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Transformer-based Approaches for Multilingual Complex Named Entity Recognition</a></strong><br><a href=/people/a/amina-miftahova/>Amina Miftahova</a>
|
<a href=/people/a/alexander-pugachev/>Alexander Pugachev</a>
|
<a href=/people/a/artem-skiba/>Artem Skiba</a>
|
<a href=/people/k/katya-artemova/>Katya Artemova</a>
|
<a href=/people/t/tatiana-batura/>Tatiana Batura</a>
|
<a href=/people/p/pavel-braslavski/>Pavel Braslavski</a>
|
<a href=/people/v/vladimir-ivanov/>Vladimir Ivanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--216><div class="card-body p-3 small">This paper presents the two submissions of NamedEntityRangers Team to the MultiCoNER Shared Task, hosted at SemEval-2022. We evaluate two state-of-the-art approaches, of which both utilize pre-trained multi-lingual language models differently. The first approach follows the token classification schema, in which each token is assigned with a tag. The second approach follows a recent template-free paradigm, in which an encoder-decoder model translates the input sequence of words to a special output, encoding named entities with predefined labels. We utilize RemBERT and mT5 as backbone models for these two approaches, respectively. Our results show that the oldie but goodie token classification outperforms the template-free method by a wide margin. Our code is available at: <a href=https://github.com/Abiks/MultiCoNER class=acl-markup-url>https://github.com/Abiks/MultiCoNER</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.217.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.217/>Raccoons at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Leveraging Concatenated Word Embeddings for Named Entity Recognition</a></strong><br><a href=/people/a/atharvan-dogra/>Atharvan Dogra</a>
|
<a href=/people/p/prabsimran-kaur/>Prabsimran Kaur</a>
|
<a href=/people/g/guneet-kohli/>Guneet Kohli</a>
|
<a href=/people/j/jatin-bedi/>Jatin Bedi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--217><div class="card-body p-3 small">Named Entity Recognition (NER), an essential subtask in NLP that identifies text belonging to predefined semantics such as a person, location, organization, drug, time, clinical procedure, biological protein, etc. NER plays a vital role in various fields such as informationextraction, question answering, and machine translation. This paper describes our participating system run to the Named entity recognitionand classification shared task SemEval-2022. The task is motivated towards detecting semantically ambiguous and complex entities in shortand low-context settings. Our team focused on improving entity recognition by improving the word embeddings. We concatenated the word representations from State-of-the-art language models and passed them to find the best representation through a reinforcement trainer. Our results highlight the improvements achieved by various embedding concatenations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.218.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--218 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.218.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.218/><span class=acl-fixed-case>S</span>eq<span class=acl-fixed-case>L</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: An Ensemble of Transformer Based Models for Complex Named Entity Recognition Task</a></strong><br><a href=/people/f/fadi-hassan/>Fadi Hassan</a>
|
<a href=/people/w/wondimagegnhue-tufa/>Wondimagegnhue Tufa</a>
|
<a href=/people/g/guillem-collell/>Guillem Collell</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a>
|
<a href=/people/l/lisa-beinborn/>Lisa Beinborn</a>
|
<a href=/people/a/adrian-flanagan/>Adrian Flanagan</a>
|
<a href=/people/k/kuan-eeik-tan/>Kuan Eeik Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--218><div class="card-body p-3 small">This paper presents our system used to participate in task 11 (MultiCONER) of the SemEval 2022 competition. Our system ranked fourth place in track 12 (Multilingual) and fifth place in track 13 (Code-Mixed). The goal of track 12 is to detect complex named entities in a multilingual setting, while track 13 is dedicated to detecting complex named entities in a code-mixed setting. Both systems were developed using transformer-based language models. We used an ensemble of XLM-RoBERTa-large and Microsoft/infoxlm-large with a Conditional Random Field (CRF) layer. In addition, we describe the algorithms employed to train our models and our hyper-parameter selection. We furthermore study the impact of different methods to aggregate the outputs of the individual models that compose our ensemble. Finally, we present an extensive analysis of the results and errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.219.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--219 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.219/><span class=acl-fixed-case>SFE</span>-<span class=acl-fixed-case>AI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Low-Resource Named Entity Recognition using Large Pre-trained Language Models</a></strong><br><a href=/people/c/changyu-hou/>Changyu Hou</a>
|
<a href=/people/j/jun-wang/>Jun Wang</a>
|
<a href=/people/y/yixuan-qiao/>Yixuan Qiao</a>
|
<a href=/people/p/peng-jiang/>Peng Jiang</a>
|
<a href=/people/p/peng-gao/>Peng Gao</a>
|
<a href=/people/g/guotong-xie/>Guotong Xie</a>
|
<a href=/people/q/qizhi-lin/>Qizhi Lin</a>
|
<a href=/people/x/xiaopeng-wang/>Xiaopeng Wang</a>
|
<a href=/people/x/xiandi-jiang/>Xiandi Jiang</a>
|
<a href=/people/b/benqi-wang/>Benqi Wang</a>
|
<a href=/people/q/qifeng-xiao/>Qifeng Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--219><div class="card-body p-3 small">Large scale pre-training models have been widely used in named entity recognition (NER) tasks. However, model ensemble through parameter averaging or voting can not give full play to the differentiation advantages of different models, especially in the open domain. This paper describes our NER system in the SemEval 2022 task11: MultiCoNER. We proposed an effective system to adaptively ensemble pre-trained language models by a Transformer layer. By assigning different weights to each model for different inputs, we adopted the Transformer layer to integrate the advantages of diverse models effectively. Experimental results show that our method achieves superior performances in Farsi and Dutch.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.220.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.220/><span class=acl-fixed-case>NCUEE</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: <span class=acl-fixed-case>C</span>hinese Named Entity Recognition Using the <span class=acl-fixed-case>BERT</span>-<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> Model</a></strong><br><a href=/people/l/lung-hao-lee/>Lung-Hao Lee</a>
|
<a href=/people/c/chien-huan-lu/>Chien-Huan Lu</a>
|
<a href=/people/t/tzu-mi-lin/>Tzu-Mi Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--220><div class="card-body p-3 small">This study describes the model design of the NCUEE-NLP system for the Chinese track of the SemEval-2022 MultiCoNER task. We use the BERT embedding for character representation and train the BiLSTM-CRF model to recognize complex named entities. A total of 21 teams participated in this track, with each team allowed a maximum of six submissions. Our best submission, with a macro-averaging F1-score of 0.7418, ranked the seventh position out of 21 teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.221.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--221 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.221/><span class=acl-fixed-case>CMB</span> <span class=acl-fixed-case>AI</span> Lab at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: A Two-Stage Approach for Complex Named Entity Recognition via Span Boundary Detection and Span Classification</a></strong><br><a href=/people/k/keyu-pu/>Keyu Pu</a>
|
<a href=/people/h/hongyi-liu/>Hongyi Liu</a>
|
<a href=/people/y/yixiao-yang/>Yixiao Yang</a>
|
<a href=/people/j/jiangzhou-ji/>Jiangzhou Ji</a>
|
<a href=/people/w/wenyi-lv/>Wenyi Lv</a>
|
<a href=/people/y/yaohan-he/>Yaohan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--221><div class="card-body p-3 small">This paper presents a solution for the SemEval-2022 Task 11 Multilingual Complex Named Entity Recognition. What is challenging in this task is detecting semantically ambiguous and complex entities in short and low-context settings. Our team (CMB AI Lab) propose a two-stage method to recognize the named entities: first, a model based on biaffine layer is built to predict span boundaries, and then a span classification model based on pooling layer is built to predict semantic tags of the spans. The basic pre-trained models we choose are XLM-RoBERTa and mT5. The evaluation result of our approach achieves an F1 score of 84.62 on sub-task 13, which ranks the third on the learder board.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.222.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--222 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.222.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.222/><span class=acl-fixed-case>UA</span>-<span class=acl-fixed-case>KO</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Data Augmentation and Ensembles for <span class=acl-fixed-case>K</span>orean Named Entity Recognition</a></strong><br><a href=/people/h/hyunju-song/>Hyunju Song</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--222><div class="card-body p-3 small">This paper presents the approaches and systems of the UA-KO team for the Korean portion of SemEval-2022 Task 11 on Multilingual Complex Named Entity Recognition.We fine-tuned Korean and multilingual BERT and RoBERTA models, conducted experiments on data augmentation, ensembles, and task-adaptive pretraining. Our final system ranked 8th out of 17 teams with an F1 score of 0.6749 F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.223.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--223 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.223" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.223/><span class=acl-fixed-case>USTC</span>-<span class=acl-fixed-case>NELSLIP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Gazetteer-Adapted Integration Network for Multilingual Complex Named Entity Recognition</a></strong><br><a href=/people/b/beiduo-chen/>Beiduo Chen</a>
|
<a href=/people/j/jun-yu-ma/>Jun-Yu Ma</a>
|
<a href=/people/j/jiajun-qi/>Jiajun Qi</a>
|
<a href=/people/w/wu-guo/>Wu Guo</a>
|
<a href=/people/z/zhen-hua-ling/>Zhen-Hua Ling</a>
|
<a href=/people/q/quan-liu/>Quan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--223><div class="card-body p-3 small">This paper describes the system developed by the USTC-NELSLIP team for SemEval-2022 Task 11 Multilingual Complex Named Entity Recognition (MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to improve the performance of language models for recognizing complex named entities. The method first adapts the representations of gazetteer networks to those of language models by minimizing the KL divergence between them. After adaptation, these two networks are then integrated for backend supervised named entity recognition (NER) training. The proposed method is applied to several state-of-the-art Transformer-based NER models with a gazetteer built from Wikidata, and shows great generalization ability across them. The final predictions are derived from an ensemble of these trained models. Experimental results and detailed analysis verify the effectiveness of the proposed method. The official results show that our system ranked 1st on three tracks (Chinese, Code-mixed and Bangla) and 2nd on the other ten tracks in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.224.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.224" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.224/>Multilinguals at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Transformer Based Architecture for Complex <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/a/amit-pandey/>Amit Pandey</a>
|
<a href=/people/s/swayatta-daw/>Swayatta Daw</a>
|
<a href=/people/v/vikram-pudi/>Vikram Pudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--224><div class="card-body p-3 small">We investigate the task of complex NER for the English language. The task is non-trivial due to the semantic ambiguity of the textual structure and the rarity of occurrence of such entities in the prevalent literature. Using pre-trained language models such as BERT, we obtain a competitive performance on this task. We qualitatively analyze the performance of multiple architectures for this task. All our models are able to outperform the baseline by a significant margin. Our best performing model beats the baseline F1-score by over 9%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.225.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--225 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.225.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.225/>L3i at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Straightforward Additional Context for Multilingual Named Entity Recognition</a></strong><br><a href=/people/e/emanuela-boros/>Emanuela Boros</a>
|
<a href=/people/c/carlos-emiliano-gonzalez-gallardo/>Carlos-Emiliano González-Gallardo</a>
|
<a href=/people/j/jose-g-moreno/>Jose Moreno</a>
|
<a href=/people/a/antoine-doucet/>Antoine Doucet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--225><div class="card-body p-3 small">This paper summarizes the participation of the L3i laboratory of the University of La Rochelle in the SemEval-2022 Task 11, Multilingual Complex Named Entity Recognition (MultiCoNER). The task focuses on detecting semantically ambiguous and complex entities in short and low-context monolingual and multilingual settings. We argue that using a language-specific and a multilingual language model could improve the performance of multilingual and mixed NER. Also, we consider that using additional contexts from the training set could improve the performance of a NER on short texts. Thus, we propose a straightforward technique for generating additional contexts with and without the presence of entities. Our findings suggest that, in our internal experimental setup, this approach is promising. However, we ranked above average for the high-resource languages and lower than average for low-resource and multilingual models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.226.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--226 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.226.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.226/><span class=acl-fixed-case>M</span>ar<span class=acl-fixed-case>S</span>an at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Multilingual complex named entity recognition using T5 and transformer encoder</a></strong><br><a href=/people/e/ehsan-tavan/>Ehsan Tavan</a>
|
<a href=/people/m/maryam-najafi/>Maryam Najafi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--226><div class="card-body p-3 small">The multilingual complex named entity recognition task of SemEval2020 required participants to detect semantically ambiguous and complex entities in 11 languages. In order to participate in this competition, a deep learning model is being used with the T5 text-to-text language model and its multilingual version, MT5, along with the transformer’s encoder module. The subtoken check has also been introduced, resulting in a 4% increase in the model F1-score in English. We also examined the use of the BPEmb model for converting input tokens to representation vectors in this research. A performance evaluation of the proposed entity detection model is presented at the end of this paper. Six different scenarios were defined, and the proposed model was evaluated in each scenario within the English development set. Our model is also evaluated in other languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.227.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.227.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.227/><span class=acl-fixed-case>SU</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Complex Named Entity Recognition with Entity Linking</a></strong><br><a href=/people/b/buse-carik/>Buse Çarık</a>
|
<a href=/people/f/fatih-beyhan/>Fatih Beyhan</a>
|
<a href=/people/r/reyyan-yeniterzi/>Reyyan Yeniterzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--227><div class="card-body p-3 small">This paper describes the system proposed by Sabancı University Natural Language Processing Group in the SemEval-2022 MultiCoNER task. We developed an unsupervised entity linking pipeline that detects potential entity mentions with the help of Wikipedia and also uses the corresponding Wikipedia context to help the classifier in finding the named entity type of that mention. The proposed pipeline significantly improved the performance, especially for complex entities in low-context settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.228.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.228.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.228/>Qtrade <span class=acl-fixed-case>AI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: An Unified Framework for Multilingual <span class=acl-fixed-case>NER</span> Task</a></strong><br><a href=/people/w/weichao-gan/>Weichao Gan</a>
|
<a href=/people/y/yuanping-lin/>Yuanping Lin</a>
|
<a href=/people/g/guangbo-yu/>Guangbo Yu</a>
|
<a href=/people/g/guimin-chen/>Guimin Chen</a>
|
<a href=/people/q/qian-ye/>Qian Ye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--228><div class="card-body p-3 small">This paper describes our system, which placed third in the Multilingual Track (subtask 11), fourth in the Code-Mixed Track (subtask 12), and seventh in the Chinese Track (subtask 9) in the SemEval 2022 Task 11: MultiCoNER Multilingual Complex Named Entity Recognition. Our system’s key contributions are as follows: 1) For multilingual NER tasks, we offered a unified framework with which one can easily execute single-language or multilingual NER tasks, 2) for low-resource mixed-code NER task, one can easily enhanced his or her dataset through implementing several simple data augmentation methods and 3) for Chinese tasks, we proposed a model that can capture Chinese lexical semantic, lexical border, and lexical graph structural information. Finally, in the test phase, our system received macro-f1 scores of 77.66, 84.35, and 74 on task 12, task 13, and task 9.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.229.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.229/><span class=acl-fixed-case>PAI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 11: Name Entity Recognition with Contextualized Entity Representations and Robust Loss Functions</a></strong><br><a href=/people/l/long-ma/>Long Ma</a>
|
<a href=/people/x/xiaorong-jian/>Xiaorong Jian</a>
|
<a href=/people/x/xuan-li/>Xuan Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--229><div class="card-body p-3 small">This paper describes our system used in the SemEval-2022 Task 11 Multilingual Complex Named Entity Recognition, achieving 3rd for track 1 on the leaderboard. We propose Dictionary-fused BERT, a flexible approach for entity dictionaries integration. The main ideas of our systems are:1) integrating external knowledge (an entity dictionary) into pre-trained models to obtain contextualized word and entity representations 2) designing a robust loss function leveraging a logit matrix 3) adding an auxiliary task, which is an on-top binary classification to decide whether the token is a mention word or not, makes the main task easier to learn. It is worth noting that our system achieves an F1 of 0.914 in the post-evaluation stage by updating the entity dictionary to the one of (CITATION), which is higher than the score of 1st on the leaderboard of the evaluation stage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.230.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--230 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.semeval-1.230/><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2022 Task 12: Symlink - Linking Mathematical Symbols to their Descriptions</a></strong><br><a href=/people/v/viet-lai/>Viet Lai</a>
|
<a href=/people/a/amir-pouran-ben-veyseh/>Amir Pouran Ben Veyseh</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/thien-nguyen/>Thien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--230><div class="card-body p-3 small">We describe Symlink, a SemEval shared task of extracting mathematical symbols and their descriptions from LaTeX source of scientific documents. This is a new task in SemEval 2022, which attracted 180 individual registrations and 59 final submissions from 7 participant teams. We expect the data developed for this task and the findings reported to be valuable for the scientific knowledge extraction and automated knowledge base construction communities. The data used in this task is publicly accessible at <a href=https://github.com/nlp-oregon/symlink class=acl-markup-url>https://github.com/nlp-oregon/symlink</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.231.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--231 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.231.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.231" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.231/><span class=acl-fixed-case>JBNU</span>-<span class=acl-fixed-case>CCL</span>ab at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 12: Machine Reading Comprehension and Span Pair Classification for Linking Mathematical Symbols to Their Descriptions</a></strong><br><a href=/people/s/sung-min-lee/>Sung-Min Lee</a>
|
<a href=/people/s/seung-hoon-na/>Seung-Hoon Na</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--231><div class="card-body p-3 small">This paper describes our system in the SemEval-2022 Task 12: ‘linking mathematical symbols to their descriptions’, achieving first on the leaderboard for all the subtasks comprising named entity extraction (NER) and relation extraction (RE). Our system is a two-stage pipeline model based on SciBERT that detects symbols, descriptions, and their relationships in scientific documents. The system consists of 1) machine reading comprehension(MRC)-based NER model, where each entity type is represented as a question and its entity mention span is extracted as an answer using an MRC model, and 2) span pair classification for RE, where two entity mentions and their type markers are encoded into span representations that are then fed to a Softmax classifier. In addition, we deploy a rule-based symbol tokenizer to improve the detection of the exact boundary of symbol entities. Regularization and ensemble methods are further explored to improve the RE model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.232.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--232 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.232.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.232" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.232/><span class=acl-fixed-case>AIFB</span>-<span class=acl-fixed-case>W</span>eb<span class=acl-fixed-case>S</span>cience at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Task 12: Relation Extraction First - Using Relation Extraction to Identify Entities</a></strong><br><a href=/people/n/nicholas-popovic/>Nicholas Popovic</a>
|
<a href=/people/w/walter-laurito/>Walter Laurito</a>
|
<a href=/people/m/michael-farber/>Michael Färber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--232><div class="card-body p-3 small">In this paper, we present an end-to-end joint entity and relation extraction approach based on transformer-based language models. We apply the model to the task of linking mathematical symbols to their descriptions in LaTeX documents. In contrast to existing approaches, which perform entity and relation extraction in sequence, our system incorporates information from relation extraction into entity extraction. This means that the system can be trained even on data sets where only a subset of all valid entity spans is annotated. We provide an extensive evaluation of the proposed system and its strengths and weaknesses. Our approach, which can be scaled dynamically in computational complexity at inference time, produces predictions with high precision and reaches 3rd place in the leaderboard of SemEval-2022 Task 12. For inputs in the domain of physics and math, it achieves high relation extraction macro F1 scores of 95.43% and 79.17%, respectively. The code used for training and evaluating our models is available at: <a href=https://github.com/nicpopovic/RE1st class=acl-markup-url>https://github.com/nicpopovic/RE1st</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.semeval-1.233.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--semeval-1--233 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.semeval-1.233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.semeval-1.233.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.semeval-1.233" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.semeval-1.233/><span class=acl-fixed-case>M</span>a<span class=acl-fixed-case>C</span>h<span class=acl-fixed-case>A</span>mp at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2022 Tasks 2, 3, 4, 6, 10, 11, and 12: Multi-task Multi-lingual Learning for a Pre-selected Set of Semantic Datasets</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--semeval-1--233><div class="card-body p-3 small">Previous work on multi-task learning in Natural Language Processing (NLP) oftenincorporated carefully selected tasks as well as carefully tuning ofarchitectures to share information across tasks. Recently, it has shown thatfor autoregressive language models, a multi-task second pre-training step on awide variety of NLP tasks leads to a set of parameters that more easily adaptfor other NLP tasks. In this paper, we examine whether a similar setup can beused in autoencoder language models using a restricted set of semanticallyoriented NLP tasks, namely all SemEval 2022 tasks that are annotated at theword, sentence or paragraph level. We first evaluate a multi-task model trainedon all SemEval 2022 tasks that contain annotation on the word, sentence orparagraph level (7 tasks, 11 sub-tasks), and then evaluate whetherre-finetuning the resulting model for each task specificially leads to furtherimprovements. Our results show that our mono-task baseline, our multi-taskmodel and our re-finetuned multi-task model each outperform the other modelsfor a subset of the tasks. Overall, huge gains can be observed by doingmulti-task learning: for three tasks we observe an error reduction of more than40%.</div></div></div><hr><div id=2022sigmorphon-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.sigmorphon-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.sigmorphon-1/>Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.0/>Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/e/eleanor-chodroff/>Eleanor Chodroff</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.1/>On Building Spoken Language Understanding Systems for Low Resourced Languages</a></strong><br><a href=/people/a/akshat-gupta/>Akshat Gupta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--1><div class="card-body p-3 small">Spoken dialog systems are slowly becoming an integral part of the human experience due to their various advantages over textual interfaces. Spoken language understanding (SLU) systems are fundamental building blocks of spoken dialog systems. But creating SLU systems for low resourced languages is still a challenge. In a large number of low resourced language, we don’t have access to enough data to build automatic speech recognition (ASR) technologies, which are fundamental to any SLU system. Also, ASR based SLU systems do not generalize to unwritten languages. In this paper, we present a series of experiments to explore extremely low-resourced settings where we perform intent classification with systems trained on as low as one data-point per intent and with only one speaker in the dataset. We also work in a low-resourced setting where we do not use language specific ASR systems to transcribe input speech, which compounds the challenge of building SLU systems to simulate a true low-resourced setting. We test our system on Belgian Dutch (Flemish) and English and find that using phonetic transcriptions to make intent classification systems in such low-resourced setting performs significantly better than using speech features. Specifically, when using a phonetic transcription based system over a feature based system, we see average improvements of 12.37% and 13.08% for binary and four-class classification problems respectively, when averaged over 49 different experimental settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.sigmorphon-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.2/>Unsupervised morphological segmentation in a language with reduplication</a></strong><br><a href=/people/s/simon-todd/>Simon Todd</a>
|
<a href=/people/a/annie-huang/>Annie Huang</a>
|
<a href=/people/j/jeremy-needle/>Jeremy Needle</a>
|
<a href=/people/j/jennifer-hay/>Jennifer Hay</a>
|
<a href=/people/j/jeanette-king/>Jeanette King</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--2><div class="card-body p-3 small">We present an extension of the Morfessor Baseline model of unsupervised morphological segmentation (Creutz and Lagus, 2007) that incorporates abstract templates for reduplication, a typologically common but computationally underaddressed process. Through a detailed investigation that applies the model to Maori, the ̄ Indigenous language of Aotearoa New Zealand, we show that incorporating templates improves Morfessor’s ability to identify instances of reduplication, and does so most when there are multiple minimally-overlapping templates. We present an error analysis that reveals important factors to consider when applying the extended model and suggests useful future directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.3/>Investigating phonological theories with crowd-sourced data: The Inventory Size Hypothesis in the light of Lingua Libre</a></strong><br><a href=/people/m/mathilde-hutin/>Mathilde Hutin</a>
|
<a href=/people/m/marc-allassonniere-tang/>Marc Allassonnière-Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--3><div class="card-body p-3 small">Data-driven research in phonetics and phonology relies massively on oral resources, and access thereto. We propose to explore a question in comparative linguistics using an open-source crowd-sourced corpus, Lingua Libre, Wikimedia’s participatory linguistic library, to show that such corpora may offer a solution to typologists wishing to explore numerous languages at once. For the present proof of concept, we compare the realizations of Italian and Spanish vowels (sample size = 5000) to investigate whether vowel production is influenced by the size of the phonemic inventory (the Inventory Size Hypothesis), by the exact shape of the inventory (the Vowel Quality Hypothesis) or by none of the above. Results show that the size of the inventory does not seem to influence vowel production, thus supporting previous research, but also that the shape of the inventory may well be a factor determining the extent of variation in vowel production. Most of all, these results show that Lingua Libre has the potential to provide valuable data for linguistic inquiry.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.4/>Logical Transductions for the Typology of Ditransitive Prosody</a></strong><br><a href=/people/m/mai-ha-vu/>Mai Ha Vu</a>
|
<a href=/people/a/aniello-de-santo/>Aniello De Santo</a>
|
<a href=/people/h/hossep-dolatian/>Hossep Dolatian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--4><div class="card-body p-3 small">Given the empirical landscape of possible prosodic parses, this paper examines the computations required to formalize the mapping from syntactic structure to prosodic structure. In particular, we use logical tree transductions to define the prosodic mapping of ditransitive verb phrases in SVO languages, building off of the typology described in Kalivoda (2018). Explicit formalization of syntax-prosody mapping revealed a number of unanswered questions relating to the fine details of theoretical assumptions behind prosodic mapping.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.sigmorphon-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.5/>A Masked Segmental Language Model for Unsupervised Natural Language Segmentation</a></strong><br><a href=/people/c/c-m-downey/>C.m. Downey</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/g/gina-anne-levow/>Gina-Anne Levow</a>
|
<a href=/people/s/shane-steinert-threlkeld/>Shane Steinert-Threlkeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--5><div class="card-body p-3 small">We introduce a Masked Segmental Language Model (MSLM) for joint language modeling and unsupervised segmentation. While near-perfect supervised methods have been developed for segmenting human-like linguistic units in resource-rich languages such as Chinese, many of the world’s languages are both morphologically complex, and have no large dataset of “gold” segmentations for supervised training. Segmental Language Models offer a unique approach by conducting unsupervised segmentation as the byproduct of a neural language modeling objective. However, current SLMs are limited in their scalability due to their recurrent architecture. We propose a new type of SLM for use in both unsupervised and lightly supervised segmentation tasks. The MSLM is built on a span-masking transformer architecture, harnessing a masked bidirectional modeling context and attention, as well as adding the potential for model scalability. In a series of experiments, our model outperforms the segmentation quality of recurrent SLMs on Chinese, and performs similarly to the recurrent model on English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.6/>Trees probe deeper than strings: an argument from allomorphy</a></strong><br><a href=/people/h/hossep-dolatian/>Hossep Dolatian</a>
|
<a href=/people/s/shiori-ikawa/>Shiori Ikawa</a>
|
<a href=/people/t/thomas-graf/>Thomas Graf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--6><div class="card-body p-3 small">Linguists disagree on whether morphological representations should be strings or trees. We argue that tree-based views of morphology can provide new insights into morphological complexity even in cases where the posited tree structure closely matches the surface string. Our argument is based on a subregular case study of morphologically conditioned allomorphy, where the phonological form of some morpheme (the target) is conditioned by the presence of some other morpheme (the trigger) somewhere within the morphosyntactic context. The trigger and target can either be linearly adjacent or non-adjacent, and either the trigger precedes the target (inwardly sensitive) or the target precedes the trigger (outwardly sensitive). When formalized as string transductions, the only complexity difference is between local and non-local allomorphy. Over trees, on the other hand, we also see a complexity difference between inwardly sensitive and outwardly sensitive allomorphy. Just as unboundedness assumptions can sometimes tease apart patterns that are equally complex in the finitely bounded case, tree-based representations can reveal differences that disappear over strings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.7/>Subword-based Cross-lingual Transfer of Embeddings from <span class=acl-fixed-case>H</span>indi to <span class=acl-fixed-case>M</span>arathi and <span class=acl-fixed-case>N</span>epali</a></strong><br><a href=/people/n/niyati-bafna/>Niyati Bafna</a>
|
<a href=/people/z/zdenek-zabokrtsky/>Zdeněk Žabokrtský</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--7><div class="card-body p-3 small">Word embeddings are growing to be a crucial resource in the field of NLP for any language. This work introduces a novel technique for static subword embeddings transfer for Indic languages from a relatively higher resource language to a genealogically related low resource language. We primarily work with HindiMarathi, simulating a low-resource scenario for Marathi, and confirm observed trends on Nepali. We demonstrate the consistent benefits of unsupervised morphemic segmentation on both source and target sides over the treatment performed by fastText. Our best-performing approach uses an EM-style approach to learning bilingual subword embeddings; we also show, for the first time, that a trivial “copyand-paste” embeddings transfer based on even perfect bilingual lexicons is inadequate in capturing language-specific relationships. We find that our approach substantially outperforms the fastText baselines for both Marathi and Nepali on the Word Similarity task as well as WordNetBased Synonymy Tests; on the former task, its performance for Marathi is close to that of pretrained fastText embeddings that use three orders of magnitude more Marathi data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.8/>Multidimensional acoustic variation in vowels across <span class=acl-fixed-case>E</span>nglish dialects</a></strong><br><a href=/people/j/james-tanner/>James Tanner</a>
|
<a href=/people/m/morgan-sonderegger/>Morgan Sonderegger</a>
|
<a href=/people/j/jane-stuart-smith/>Jane Stuart-Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--8><div class="card-body p-3 small">Vowels are typically characterized in terms of their static position in formant space, though vowels have also been long-known to undergo dynamic formant change over their timecourse. Recent studies have demonstrated that this change is highly informative for distinguishing vowels within a system, as well as providing additional resolution in characterizing differences between dialects. It remains unclear, however, how both static and dynamic representations capture the main dimensions of vowel variation across a large number of dialects. This study examines the role of static, dynamic, and duration information for 5 vowels across 21 British and North American English dialects, and observes that vowels exhibit highly structured variation across dialects, with dialects displaying similar patterns within a given vowel, broadly corresponding to a spectrum between traditional ‘monophthong’ and ‘diphthong’ characterizations. These findings highlight the importance of dynamic and duration information in capturing how vowels can systematically vary across a large number of dialects, and provide the first large-scale description of formant dynamics across many dialects of a single language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.9/>Domain-Informed Probing of wav2vec 2.0 Embeddings for Phonetic Features</a></strong><br><a href=/people/p/patrick-cormac-english/>Patrick Cormac English</a>
|
<a href=/people/j/john-kelleher/>John D. Kelleher</a>
|
<a href=/people/j/julie-carson-berndsen/>Julie Carson-Berndsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--9><div class="card-body p-3 small">In recent years large transformer model architectures have become available which provide a novel means of generating high-quality vector representations of speech audio. These transformers make use of an attention mechanism to generate representations enhanced with contextual and positional information from the input sequence. Previous works have explored the capabilities of these models with regard to performance in tasks such as speech recognition and speaker verification, but there has not been a significant inquiry as to the manner in which the contextual information provided by the transformer architecture impacts the representation of phonetic information within these models. In this paper, we report the results of a number of probing experiments on the representations generated by the wav2vec 2.0 model’s transformer component, with regard to the encoding of phonetic categorization information within the generated embeddings. We find that the contextual information generated by the transformer’s operation results in enhanced capture of phonetic detail by the model, and allows for distinctions to emerge in acoustic data that are otherwise difficult to separate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.sigmorphon-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.10/>Morphotactic Modeling in an Open-source Multi-dialectal <span class=acl-fixed-case>A</span>rabic Morphological Analyzer and Generator</a></strong><br><a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/r/reham-marzouk/>Reham Marzouk</a>
|
<a href=/people/c/christian-khairallah/>Christian Khairallah</a>
|
<a href=/people/s/salam-khalifa/>Salam Khalifa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--10><div class="card-body p-3 small">Arabic is a morphologically rich and complex language, with numerous dialectal variants. Previous efforts on Arabic morphology modeling focused on specific variants and specific domains using a range of techniques with different degrees of linguistic modeling transparency. In this paper we propose a new approach to modeling Arabic morphology with an eye towards multi-dialectness, resource openness, and easy extensibility and use. We demonstrate our approach by modeling verbs from Standard Arabic and Egyptian Arabic, within a common framework, and with high coverage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.sigmorphon-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.11/>The <span class=acl-fixed-case>SIGMORPHON</span> 2022 Shared Task on Morpheme Segmentation</a></strong><br><a href=/people/k/khuyagbaatar-batsuren/>Khuyagbaatar Batsuren</a>
|
<a href=/people/g/gabor-bella/>Gábor Bella</a>
|
<a href=/people/a/aryaman-arora/>Aryaman Arora</a>
|
<a href=/people/v/viktor-martinovic/>Viktor Martinovic</a>
|
<a href=/people/k/kyle-gorman/>Kyle Gorman</a>
|
<a href=/people/z/zdenek-zabokrtsky/>Zdeněk Žabokrtský</a>
|
<a href=/people/a/amarsanaa-ganbold/>Amarsanaa Ganbold</a>
|
<a href=/people/s/sarka-dohnalova/>Šárka Dohnalová</a>
|
<a href=/people/m/magda-sevcikova/>Magda Ševčíková</a>
|
<a href=/people/k/katerina-pelegrinova/>Kateřina Pelegrinová</a>
|
<a href=/people/f/fausto-giunchiglia/>Fausto Giunchiglia</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--11><div class="card-body p-3 small">The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems to decompose a word into a sequence of morphemes and covered most types of morphology: compounds, derivations, and inflections. Subtask 1, word-level morpheme segmentation, covered 5 million words in 9 languages (Czech, English, Spanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13 system submissions from 7 teams and the best system averaged 97.29% F1 score across all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2, sentence-level morpheme segmentation, covered 18,735 sentences in 3 languages (Czech, English, Mongolian), received 10 system submissions from 3 teams, and the best systems outperformed all three state-of-the-art subword tokenization methods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis and support any type of future studies, we released all system predictions, the evaluation script, and all gold standard datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.12/>Sharing Data by Language Family: Data Augmentation for <span class=acl-fixed-case>R</span>omance Language Morpheme Segmentation</a></strong><br><a href=/people/l/lauren-levine/>Lauren Levine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--12><div class="card-body p-3 small">This paper presents a basic character level sequence-to-sequence approach to morpheme segmentation for the following Romance languages: French, Italian, and Spanish. We experiment with adding a small set of additional linguistic features, as well as with sharing training data between sister languages for morphological categories with low performance in single language base models. We find that while the additional linguistic features were generally not helpful in this instance, data augmentation between sister languages did help to raise the scores of some individual morphological categories, but did not consistently result in an overall improvement when considering the aggregate of the categories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.13/><span class=acl-fixed-case>SIGMORPHON</span> 2022 Shared Task on Morpheme Segmentation Submission Description: Sequence Labelling for Word-Level Morpheme Segmentation</a></strong><br><a href=/people/l/leander-girrbach/>Leander Girrbach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--13><div class="card-body p-3 small">We propose a sequence labelling approach to word-level morpheme segmentation. Segmentation labels are edit operations derived from a modified minimum edit distance alignment. We show that sequence labelling performs well for “shallow segmentation” and “canonical segmentation”, achieving 96.06 f1 score (macroaveraged over all languages in the shared task) and ranking 3rd among all participating teams. Therefore, we conclude that sequence labelling is a promising approach to morpheme segmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.14/>Beyond Characters: Subword-level Morpheme Segmentation</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/a/andre-f-t-martins/>Andre F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--14><div class="card-body p-3 small">This paper presents DeepSPIN’s submissions to the SIGMORPHON 2022 Shared Task on Morpheme Segmentation. We make three submissions, all to the word-level subtask. First, we show that entmax-based sparse sequence-tosequence models deliver large improvements over conventional softmax-based models, echoing results from other tasks. Then, we challenge the assumption that models for morphological tasks should be trained at the character level by building a transformer that generates morphemes as sequences of unigram language model-induced subwords. This subword transformer outperforms all of our character-level models and wins the word-level subtask. Although we do not submit an official submission to the sentence-level subtask, we show that this subword-based approach is highly effective there as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.15/>Word-level Morpheme segmentation using Transformer neural network</a></strong><br><a href=/people/t/tsolmon-zundi/>Tsolmon Zundi</a>
|
<a href=/people/c/chinbat-avaajargal/>Chinbat Avaajargal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--15><div class="card-body p-3 small">This paper presents the submission of team NUM DI to the SIGMORPHON 2022 Task on Morpheme Segmentation Part 1, word-level morpheme segmentation. We explore the transformer neural network approach to the shared task. We develop monolingual models for world-level morpheme segmentation and focus on improving the model by using various training strategies to improve accuracy and generalization across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.16/><span class=acl-fixed-case>M</span>orfessor-enriched features and multilingual training for canonical morphological segmentation</a></strong><br><a href=/people/a/aku-rouhe/>Aku Rouhe</a>
|
<a href=/people/s/stig-arne-gronroos/>Stig-Arne Grönroos</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a>
|
<a href=/people/m/mikko-kurimo/>Mikko Kurimo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--16><div class="card-body p-3 small">In our submission to the SIGMORPHON 2022 Shared Task on Morpheme Segmentation, we study whether an unsupervised morphological segmentation method, Morfessor, can help in a supervised setting. Previous research has shown the effectiveness of the approach in semisupervised settings with small amounts of labeled data. The current tasks vary in data size: the amount of word-level annotated training data is much larger, but the amount of sentencelevel annotated training data remains small. Our approach is to pre-segment the input data for a neural sequence-to-sequence model with the unsupervised method. As the unsupervised method can be trained with raw text data, we use Wikipedia to increase the amount of training data. In addition, we train multilingual models for the sentence-level task. The results for the Morfessor-enriched features are mixed, showing benefit for all three sentencelevel tasks but only some of the word-level tasks. The multilingual training yields considerable improvements over the monolingual sentence-level models, but it negates the effect of the enriched features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.17/><span class=acl-fixed-case>JB</span>132 submission to the <span class=acl-fixed-case>SIGMORPHON</span> 2022 Shared Task 3 on Morphological Segmentation</a></strong><br><a href=/people/j/jan-bodnar/>Jan Bodnár</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--17><div class="card-body p-3 small">This paper describes the JB132 submission to the SIGMORPHON 2022 Shared Task 3 on Morpheme Segmentation. In this paper we describe probabilistic model trained with the Expectation-Maximization algorithm, we provide the results and analyze sources of errors and general limitations of our approach. The model was implemented within our own modular probabilistic framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.sigmorphon-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.18/><span class=acl-fixed-case>SIGMORPHON</span>–<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>M</span>orph 2022 Shared Task 0: Modeling Inflection in Language Acquisition</a></strong><br><a href=/people/j/jordan-kodner/>Jordan Kodner</a>
|
<a href=/people/s/salam-khalifa/>Salam Khalifa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--18><div class="card-body p-3 small">This year’s iteration of the SIGMORPHONUniMorph shared task on “human-like” morphological inflection generation focuses on generalization and errors in language acquisition. Systems are trained on data sets extracted from corpora of child-directed speech in order to simulate a natural learning setting, and their predictions are evaluated against what is known about children’s developmental trajectories for three well-studied patterns: English past tense, German noun plurals, and Arabic noun plurals. Three submitted neural systems were evaluated together with two baselines. Performance was generally good, and all systems were prone to human-like over-regularization. However, all systems were also prone to non-human-like over-irregularization and nonsense productions to varying degrees. We situate this behavior in a discussion of the Past Tense Debate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.sigmorphon-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.19/><span class=acl-fixed-case>SIGMORPHON</span>–<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>M</span>orph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection</a></strong><br><a href=/people/j/jordan-kodner/>Jordan Kodner</a>
|
<a href=/people/s/salam-khalifa/>Salam Khalifa</a>
|
<a href=/people/k/khuyagbaatar-batsuren/>Khuyagbaatar Batsuren</a>
|
<a href=/people/h/hossep-dolatian/>Hossep Dolatian</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/f/faruk-akkus/>Faruk Akkus</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/t/taras-andrushko/>Taras Andrushko</a>
|
<a href=/people/a/aryaman-arora/>Aryaman Arora</a>
|
<a href=/people/n/nona-atanalov/>Nona Atanalov</a>
|
<a href=/people/g/gabor-bella/>Gábor Bella</a>
|
<a href=/people/e/elena-budianskaya/>Elena Budianskaya</a>
|
<a href=/people/y/yustinus-ghanggo-ate/>Yustinus Ghanggo Ate</a>
|
<a href=/people/o/omer-goldman/>Omer Goldman</a>
|
<a href=/people/d/david-guriel/>David Guriel</a>
|
<a href=/people/s/simon-guriel/>Simon Guriel</a>
|
<a href=/people/s/silvia-guriel-agiashvili/>Silvia Guriel-Agiashvili</a>
|
<a href=/people/w/witold-kieras/>Witold Kieraś</a>
|
<a href=/people/a/andrew-krizhanovsky/>Andrew Krizhanovsky</a>
|
<a href=/people/n/natalia-krizhanovsky/>Natalia Krizhanovsky</a>
|
<a href=/people/i/igor-marchenko/>Igor Marchenko</a>
|
<a href=/people/m/magdalena-markowska/>Magdalena Markowska</a>
|
<a href=/people/p/polina-mashkovtseva/>Polina Mashkovtseva</a>
|
<a href=/people/m/maria-nepomniashchaya/>Maria Nepomniashchaya</a>
|
<a href=/people/d/daria-rodionova/>Daria Rodionova</a>
|
<a href=/people/k/karina-scheifer/>Karina Scheifer</a>
|
<a href=/people/a/alexandra-sorova/>Alexandra Sorova</a>
|
<a href=/people/a/anastasia-yemelina/>Anastasia Yemelina</a>
|
<a href=/people/j/jeremiah-young/>Jeremiah Young</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--19><div class="card-body p-3 small">The 2022 SIGMORPHON–UniMorph shared task on large scale morphological inflection generation included a wide range of typologically diverse languages: 33 languages from 11 top-level language families: Arabic (Modern Standard), Assamese, Braj, Chukchi, Eastern Armenian, Evenki, Georgian, Gothic, Gujarati, Hebrew, Hungarian, Itelmen, Karelian, Kazakh, Ket, Khalkha Mongolian, Kholosi, Korean, Lamahalot, Low German, Ludic, Magahi, Middle Low German, Old English, Old High German, Old Norse, Polish, Pomak, Slovak, Turkish, Upper Sorbian, Veps, and Xibe. We emphasize generalization along different dimensions this year by evaluating test items with unseen lemmas and unseen features separately under small and large training conditions. Across the five submitted systems and two baselines, the prediction of inflections with unseen features proved challenging, with average performance decreased substantially from last year. This was true even for languages for which the forms were in principle predictable, which suggests that further work is needed in designing systems that capture the various types of generalization required for the world’s languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.20/><span class=acl-fixed-case>SIGMORPHON</span> 2022 Task 0 Submission Description: Modelling Morphological Inflection with Data-Driven and Rule-Based Approaches</a></strong><br><a href=/people/t/tatiana-merzhevich/>Tatiana Merzhevich</a>
|
<a href=/people/n/nkonye-gbadegoye/>Nkonye Gbadegoye</a>
|
<a href=/people/l/leander-girrbach/>Leander Girrbach</a>
|
<a href=/people/j/jingwen-li/>Jingwen Li</a>
|
<a href=/people/r/ryan-soh-eun-shim/>Ryan Soh-Eun Shim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--20><div class="card-body p-3 small">This paper describes our participation in the 2022 SIGMORPHON-UniMorph Shared Task on Typologically Diverse and AcquisitionInspired Morphological Inflection Generation. We present two approaches: one being a modification of the neural baseline encoderdecoder model, the other being hand-coded morphological analyzers using finite-state tools (FST) and outside linguistic knowledge. While our proposed modification of the baseline encoder-decoder model underperforms the baseline for almost all languages, the FST methods outperform other systems in the respective languages by a large margin. This confirms that purely data-driven approaches have not yet reached the maturity to replace trained linguists for documentation and analysis especially considering low-resource and endangered languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.sigmorphon-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.21/><span class=acl-fixed-case>CLUZH</span> at <span class=acl-fixed-case>SIGMORPHON</span> 2022 Shared Tasks on Morpheme Segmentation and Inflection Generation</a></strong><br><a href=/people/s/silvan-wehrli/>Silvan Wehrli</a>
|
<a href=/people/s/simon-clematide/>Simon Clematide</a>
|
<a href=/people/p/peter-makarov/>Peter Makarov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--21><div class="card-body p-3 small">This paper describes the submissions of the team of the Department of Computational Linguistics, University of Zurich, to the SIGMORPHON 2022 Shared Tasks on Morpheme Segmentation and Inflection Generation. Our submissions use a character-level neural transducer that operates over traditional edit actions. While this model has been found particularly wellsuited for low-resource settings, using it with large data quantities has been difficult. Existing implementations could not fully profit from GPU acceleration and did not efficiently implement mini-batch training, which could be tricky for a transition-based system. For this year’s submission, we have ported the neural transducer to PyTorch and implemented true mini-batch training. This has allowed us to successfully scale the approach to large data quantities and conduct extensive experimentation. We report competitive results for morpheme segmentation (including sharing first place in part 2 of the challenge). We also demonstrate that reducing sentence-level morpheme segmentation to a word-level problem is a simple yet effective strategy. Additionally, we report strong results in inflection generation (the overall best result for large training sets in part 1, the best results in low-resource learning trajectories in part 2). Our code is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.22.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.sigmorphon-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.22/><span class=acl-fixed-case>OSU</span> at <span class=acl-fixed-case>S</span>ig<span class=acl-fixed-case>M</span>orphon 2022: Analogical Inflection With Rule Features</a></strong><br><a href=/people/m/micha-elsner/>Micha Elsner</a>
|
<a href=/people/s/sara-court/>Sara Court</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--22><div class="card-body p-3 small">OSU’s inflection system is a transformer whose input is augmented with an analogical exemplar showing how to inflect a different word into the target cell. In addition, alignment-based heuristic features indicate how well the exemplar is likely to match the output. OSU’s scores substantially improve over the baseline transformer for instances where an exemplar is available, though not quite matching the challenge winner. In Part 2, the system shows a tendency to over-apply the majority pattern in English, but not Arabic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.23/>Generalizing Morphological Inflection Systems to Unseen Lemmas</a></strong><br><a href=/people/c/changbing-yang/>Changbing Yang</a>
|
<a href=/people/r/ruixin-ray-yang/>Ruixin (Ray) Yang</a>
|
<a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--23><div class="card-body p-3 small">This paper presents experiments on morphological inflection using data from the SIGMORPHON-UniMorph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection. We present a transformer inflection system, which enriches the standard transformer architecture with reverse positional encoding and type embeddings. We further apply data hallucination and lemma copying to augment training data. We train models using a two-stage procedure: (1) We first train on the augmented training data using standard backpropagation and teacher forcing. (2) We then continue training with a variant of the scheduled sampling algorithm dubbed student forcing. Our system delivers competitive performance under the small and large data conditions on the shared task datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.24/><span class=acl-fixed-case>H</span>ei<span class=acl-fixed-case>M</span>orph at <span class=acl-fixed-case>SIGMORPHON</span> 2022 Shared Task on Morphological Acquisition Trajectories</a></strong><br><a href=/people/a/akhilesh-kakolu-ramarao/>Akhilesh Kakolu Ramarao</a>
|
<a href=/people/y/yulia-zinova/>Yulia Zinova</a>
|
<a href=/people/k/kevin-tang/>Kevin Tang</a>
|
<a href=/people/r/ruben-van-de-vijver/>Ruben van de Vijver</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--24><div class="card-body p-3 small">This paper presents the submission by the HeiMorph team to the SIGMORPHON 2022 task 2 of Morphological Acquisition Trajectories. Across all experimental conditions, we have found no evidence for the so-called Ushaped development trajectory. Our submitted systems achieve an average test accuracies of 55.5% on Arabic, 67% on German and 73.38% on English. We found that, bigram hallucination provides better inferences only for English and Arabic and only when the number of hallucinations remains low.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigmorphon-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigmorphon-1.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigmorphon-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigmorphon-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigmorphon-1.25/>Morphology is not just a naive <span class=acl-fixed-case>B</span>ayes – <span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>M</span>elb Submission to <span class=acl-fixed-case>SIGMORPHON</span> 2022 <span class=acl-fixed-case>ST</span> on Morphological Inflection</a></strong><br><a href=/people/a/andreas-sherbakov/>Andreas Sherbakov</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigmorphon-1--25><div class="card-body p-3 small">The paper describes the Flexica team’s submission to the SIGMORPHON 2022 Shared Task 1 Part 1: Typologically Diverse Morphological Inflection. Our team submitted a nonneural system that extracted transformation patterns from alignments between a lemma and inflected forms. For each inflection category, we chose a pattern based on its abstractness score. The system outperformed the non-neural baseline, the extracted patterns covered a substantial part of possible inflections. However, we discovered that such score that does not account for all possible combinations of string segments as well as morphosyntactic features is not sufficient for a certain proportion of inflection cases.</div></div></div><hr><div id=2022sigtyp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.sigtyp-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.sigtyp-1/>Proceedings of the 4th Workshop on Research in Computational Linguistic Typology and Multilingual NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.0/>Proceedings of the 4th Workshop on Research in Computational Linguistic Typology and Multilingual NLP</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/e/edoardo-ponti/>Edoardo Ponti</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.1/>Multilingualism Encourages Recursion: a Transfer Study with m<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/andrea-de-varda/>Andrea De Varda</a>
|
<a href=/people/r/roberto-zamparelli/>Roberto Zamparelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--1><div class="card-body p-3 small">The present work constitutes an attempt to investigate the relational structures learnt by mBERT, a multilingual transformer-based network, with respect to different cross-linguistic regularities proposed in the fields of theoretical and quantitative linguistics. We pursued this objective by relying on a zero-shot transfer experiment, evaluating the model’s ability to generalize its native task to artificial languages that could either respect or violate some proposed language universal, and comparing its performance to the output of BERT, a monolingual model with an identical configuration. We created four artificial corpora through a Probabilistic Context-Free Grammar by manipulating the distribution of tokens and the structure of their dependency relations. We showed that while both models were favoured by a Zipfian distribution of the tokens and by the presence of head-dependency type structures, the multilingual transformer network exhibited a stronger reliance on hierarchical cues compared to its monolingual counterpart.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.2/>Word-order Typology in Multilingual <span class=acl-fixed-case>BERT</span>: A Case Study in Subordinate-Clause Detection</a></strong><br><a href=/people/d/dmitry-nikolaev/>Dmitry Nikolaev</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Pado</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--2><div class="card-body p-3 small">The capabilities and limitations of BERT and similar models are still unclear when it comes to learning syntactic abstractions, in particular across languages. In this paper, we use the task of subordinate-clause detection within and across languages to probe these properties. We show that this task is deceptively simple, with easy gains offset by a long tail of harder cases, and that BERT’s zero-shot performance is dominated by word-order effects, mirroring the SVO/VSO/SOV typology.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.sigtyp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.3/>Typological Word Order Correlations with Logistic Brownian Motion</a></strong><br><a href=/people/k/kai-hartung/>Kai Hartung</a>
|
<a href=/people/g/gerhard-jager/>Gerhard Jäger</a>
|
<a href=/people/s/soren-grottrup/>Sören Gröttrup</a>
|
<a href=/people/m/munir-georges/>Munir Georges</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--3><div class="card-body p-3 small">In this study we address the question to what extent syntactic word-order traits of different languages have evolved under correlation and whether such dependencies can be found universally across all languages or restricted to specific language families. To do so, we use logistic Brownian Motion under a Bayesian framework to model the trait evolution for 768 languages from 34 language families. We test for trait correlations both in single families and universally over all families. Separate models reveal no universal correlation patterns and Bayes Factor analysis of models over all covered families also strongly indicate lineage specific correlation patters instead of universal dependencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.4/>Cross-linguistic Comparison of Linguistic Feature Encoding in <span class=acl-fixed-case>BERT</span> Models for Typologically Different Languages</a></strong><br><a href=/people/j/julia-otmakhova/>Yulia Otmakhova</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--4><div class="card-body p-3 small">Though recently there have been an increased interest in how pre-trained language models encode different linguistic features, there is still a lack of systematic comparison between languages with different morphology and syntax. In this paper, using BERT as an example of a pre-trained model, we compare how three typologically different languages (English, Korean, and Russian) encode morphology and syntax features across different layers. In particular, we contrast languages which differ in a particular aspect, such as flexibility of word order, head directionality, morphological type, presence of grammatical gender, and morphological richness, across four different tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.5/>Tweaking <span class=acl-fixed-case>UD</span> Annotations to Investigate the Placement of Determiners, Quantifiers and Numerals in the Noun Phrase</a></strong><br><a href=/people/l/luigi-talamo/>Luigi Talamo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--5><div class="card-body p-3 small">We describe a methodology to extract with finer accuracy word order patterns from texts automatically annotated with Universal Dependency (UD) trained parsers. We use the methodology to quantify the word order entropy of determiners, quantifiers and numerals in ten Indo-European languages, using UD-parsed texts from a parallel corpus of prosaic texts. Our results suggest that the combinations of different UD annotation layers, such as UD Relations, Universal Parts of Speech and lemma, and the introduction of language-specific lists of closed-category lemmata has the two-fold effect of improving the quality of analysis and unveiling hidden areas of variability in word order patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.6/>A Database for Modal Semantic Typology</a></strong><br><a href=/people/q/qingxia-guo/>Qingxia Guo</a>
|
<a href=/people/n/nathaniel-imel/>Nathaniel Imel</a>
|
<a href=/people/s/shane-steinert-threlkeld/>Shane Steinert-Threlkeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--6><div class="card-body p-3 small">This paper introduces a database for crosslinguistic modal semantics. The purpose of this database is to (1) enable ongoing consolidation of modal semantic typological knowledge into a repository according to uniform data standards and to (2) provide data for investigations in crosslinguistic modal semantic theory and experiments explaining such theories. We describe the kind of semantic variation that the database aims to record, the format of the data, and a current snapshot of the database, emphasizing access and contribution to the database in light of the goals above. We release the database at <a href=https://clmbr.shane.st/modal-typology class=acl-markup-url>https://clmbr.shane.st/modal-typology</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.7/>The <span class=acl-fixed-case>SIGTYP</span> 2022 Shared Task on the Prediction of Cognate Reflexes</a></strong><br><a href=/people/j/johann-mattis-list/>Johann-Mattis List</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/robert-forkel/>Robert Forkel</a>
|
<a href=/people/n/nathan-hill/>Nathan Hill</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--7><div class="card-body p-3 small">This study describes the structure and the results of the SIGTYP 2022 shared task on the prediction of cognate reflexes from multilingual wordlists. We asked participants to submit systems that would predict words in individual languages with the help of cognate words from related languages. Training and surprise data were based on standardized multilingual wordlists from several language families. Four teams submitted a total of eight systems, including both neural and non-neural systems, as well as systems adjusted to the task and systems using more general settings. While all systems showed a rather promising performance, reflecting the overwhelming regularity of sound change, the best performance throughout was achieved by a system based on convolutional networks originally designed for image restoration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.8/><span class=acl-fixed-case>B</span>ayesian Phylogenetic Cognate Prediction</a></strong><br><a href=/people/g/gerhard-jager/>Gerhard Jäger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--8><div class="card-body p-3 small">In Jäger (2019) a computational framework was defined to start from parallel word lists of related languages and infer the corresponding vocabulary of the shared proto-language. The SIGTYP 2022 Shared Task is closely related. The main difference is that what is to be reconstructed is not the proto-form but an unknown word from an extant language. The system described here is a re-implementation of the tools used in the mentioned paper, adapted to the current task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.9/>Mockingbird at the <span class=acl-fixed-case>SIGTYP</span> 2022 Shared Task: Two Types of Models for the Prediction of Cognate Reflexes</a></strong><br><a href=/people/c/christo-kirov/>Christo Kirov</a>
|
<a href=/people/r/richard-sproat/>Richard Sproat</a>
|
<a href=/people/a/alexander-gutkin/>Alexander Gutkin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--9><div class="card-body p-3 small">The SIGTYP 2022 shared task concerns the problem of word reflex generation in a target language, given cognate words from a subset of related languages. We present two systems to tackle this problem, covering two very different modeling approaches. The first model extends transformer-based encoder-decoder sequence-to-sequence modeling, by encoding all available input cognates in parallel, and having the decoder attend to the resulting joint representation during inference. The second approach takes inspiration from the field of image restoration, where models are tasked with recovering pixels in an image that have been masked out. For reflex generation, the missing reflexes are treated as “masked pixels” in an “image” which is a representation of an entire cognate set across a language family. As in the image restoration case, cognate restoration is performed with a convolutional network.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.10/>A Transformer Architecture for the Prediction of Cognate Reflexes</a></strong><br><a href=/people/g/giuseppe-g-a-celano/>Giuseppe G. A. Celano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--10><div class="card-body p-3 small">This paper presents the transformer model built to participate in the SIGTYP 2022 Shared Task on the Prediction of Cognate Reflexes. It consists of an encoder-decoder architecture with multi-head attention mechanism. Its output is concatenated with the one hot encoding of the language label of an input character sequence to predict a target character sequence. The results show that the transformer outperforms the baseline rule-based system only partially.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.11.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.11/>Approaching Reflex Predictions as a Classification Problem Using Extended Phonological Alignments</a></strong><br><a href=/people/t/tiago-tresoldi/>Tiago Tresoldi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--11><div class="card-body p-3 small">This work describes an implementation of the “extended alignment” model for cognate reflex prediction submitted to the “SIGTYP 2022 Shared Task on the Prediction of Cognate Reflexes”. Similarly to List et al. (2022a), the technique involves an automatic extension of sequence alignments with multilayered vectors that encode informational tiers on both site-specific traits, such as sound classes and distinctive features, as well as contextual and suprasegmental ones, conveyed by cross-site referrals and replication. The method allows to generalize the problem of cognate reflex prediction as a classification problem, with models trained using a parallel corpus of cognate sets. A model using random forests is trained and evaluated on the shared task for reflex prediction, and the experimental results are presented and discussed along with some differences to other implementations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.12/>Investigating Information-Theoretic Properties of the Typology of Spatial Demonstratives</a></strong><br><a href=/people/s/sihan-chen/>Sihan Chen</a>
|
<a href=/people/r/richard-futrell/>Richard Futrell</a>
|
<a href=/people/k/kyle-mahowald/>Kyle Mahowald</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--12><div class="card-body p-3 small">Using data from Nintemann et al. (2020), we explore the variability in complexity and informativity across spatial demonstrative systems using spatial deictic lexicons from 223 languages. We argue from an information-theoretic perspective (Shannon, 1948) that spatial deictic lexicons are efficient in communication, balancing informativity and complexity. Specifically, we find that under an appropriate choice of cost function and need probability over meanings, among all the 21146 theoretically possible spatial deictic lexicons, those adopted by real languages lie near an efficient frontier. Moreover, we find that the conditions that the need probability and the cost function need to satisfy are consistent with the cognitive science literature regarding the source-goal asymmetry. We also show that the data are better explained by introducing a notion of systematicity, which is not currently accounted for in Information Bottleneck approaches to linguistic efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.13.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.13/>How Universal is Metonymy? Results from a Large-Scale Multilingual Analysis</a></strong><br><a href=/people/t/temuulen-khishigsuren/>Temuulen Khishigsuren</a>
|
<a href=/people/g/gabor-bella/>Gábor Bella</a>
|
<a href=/people/t/thomas-brochhagen/>Thomas Brochhagen</a>
|
<a href=/people/d/daariimaa-marav/>Daariimaa Marav</a>
|
<a href=/people/f/fausto-giunchiglia/>Fausto Giunchiglia</a>
|
<a href=/people/k/khuyagbaatar-batsuren/>Khuyagbaatar Batsuren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--13><div class="card-body p-3 small">Metonymy is regarded by most linguists as a universal cognitive phenomenon, especially since the emergence of the theory of conceptual mappings. However, the field data backing up claims of universality has not been large enough so far to provide conclusive evidence. We introduce a large-scale analysis of metonymy based on a lexical corpus of over 20 thousand metonymy instances from 189 languages and 69 genera. No prior study, to our knowledge, is based on linguistic coverage as broad as ours. Drawing on corpus analysis, evidence of universality is found at three levels: systematic metonymy in general, particular metonymy patterns, and specific metonymy concepts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.14.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.14/><span class=acl-fixed-case>P</span>a<span class=acl-fixed-case>V</span>e<span class=acl-fixed-case>D</span>a - <span class=acl-fixed-case>P</span>avia Verbs Database: Challenges and Perspectives</a></strong><br><a href=/people/c/chiara-zanchi/>Chiara Zanchi</a>
|
<a href=/people/s/silvia-luraghi/>Silvia Luraghi</a>
|
<a href=/people/c/claudia-roberta-combei/>Claudia Roberta Combei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--14><div class="card-body p-3 small">This paper describes an ongoing endeavor to construct Pavia Verbs Database (PaVeDa) – an open-access typological resource that builds upon previous work on verb argument structure, in particular the Valency Patterns Leipzig (ValPaL) project (Hartmann et al., 2013). The PaVeDa database features four major innovations as compared to the ValPaL database: (i) it includes data from ancient languages enabling diachronic research; (ii) it expands the language sample to language families that are not represented in the ValPaL; (iii) it is linked to external corpora that are used as sources of usage-based examples of stored patterns; (iv) it introduces a new cross-linguistic layer of annotation for valency patterns which allows for contrastive data visualization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.sigtyp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.sigtyp-1.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--sigtyp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.sigtyp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.sigtyp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.sigtyp-1.15/><span class=acl-fixed-case>P</span>ara<span class=acl-fixed-case>N</span>ames: A Massively Multilingual Entity Name Corpus</a></strong><br><a href=/people/j/jonne-saleva/>Jonne Sälevä</a>
|
<a href=/people/c/constantine-lignos/>Constantine Lignos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--sigtyp-1--15><div class="card-body p-3 small">We present ParaNames, a Wikidata-derived multilingual parallel name resource consisting of names for approximately 14 million entities spanning over 400 languages. ParaNames is useful for multilingual language processing, both in defining tasks for name translation tasks and as supplementary data for other tasks. We demonstrate an application of ParaNames by training a multilingual model for canonical name translation to and from English.</div></div></div><hr><div id=2022socialnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.socialnlp-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.socialnlp-1/>Proceedings of the Tenth International Workshop on Natural Language Processing for Social Media</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.socialnlp-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.socialnlp-1.0/>Proceedings of the Tenth International Workshop on Natural Language Processing for Social Media</a></strong><br><a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a>
|
<a href=/people/c/cheng-te-li/>Cheng-Te Li</a>
|
<a href=/people/y/yu-che-tsai/>Yu-Che Tsai</a>
|
<a href=/people/w/wei-yao-wang/>Wei-Yao Wang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.socialnlp-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--socialnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.socialnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.socialnlp-1.1/>Mask and Regenerate: A Classifier-based Approach for Unpaired Sentiment Transformation of Reviews for Electronic Commerce Websites.</a></strong><br><a href=/people/s/shuo-yang/>Shuo Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--socialnlp-1--1><div class="card-body p-3 small">Style transfer is the task of transferring a sentence into the target style while keeping its content. The major challenge is that parallel corpora are not available for various domains. In this paper, we propose a Mask-And-Regenerate approach (MAR). It learns from unpaired sentences by modifying the word-level style attributes. We cautiously integrate the deletion, insertion and substitution operations into our model. This enables our model to automatically apply different edit operations for different sentences. Specifically, we train a multilayer perceptron (MLP) as a style classifier to find out and mask style-characteristic words in the source inputs. Then we learn a language model on non-parallel data sets to score sentences and remove unnecessary masks. Finally, the masked source sentences are input to a Transformer to perform style transfer. The final results show that our proposed model exceeds baselines by about 2 per cent of accuracy for both sentiment and style transfer tasks with comparable or better content retention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.socialnlp-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--socialnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.socialnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.socialnlp-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.socialnlp-1.2/>Exploiting Social Media Content for Self-Supervised Style Transfer</a></strong><br><a href=/people/d/dana-ruiter/>Dana Ruiter</a>
|
<a href=/people/t/thomas-kleinbauer/>Thomas Kleinbauer</a>
|
<a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--socialnlp-1--2><div class="card-body p-3 small">Recent research on style transfer takes inspiration from unsupervised neural machine translation (UNMT), learning from large amounts of non-parallel data by exploiting cycle consistency loss, back-translation, and denoising autoencoders. By contrast, the use of selfsupervised NMT (SSNMT), which leverages (near) parallel instances hidden in non-parallel data more efficiently than UNMT, has not yet been explored for style transfer. In this paper we present a novel Self-Supervised Style Transfer (3ST) model, which augments SSNMT with UNMT methods in order to identify and efficiently exploit supervisory signals in non-parallel social media posts. We compare 3ST with state-of-the-art (SOTA) style transfer models across civil rephrasing, formality and polarity tasks. We show that 3ST is able to balance the three major objectives (fluency, content preservation, attribute transfer accuracy) the best, outperforming SOTA models on averaged performance across their tested tasks in automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.socialnlp-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--socialnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.socialnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.socialnlp-1.3/>Detecting Rumor Veracity with Only Textual Information by Double-Channel Structure</a></strong><br><a href=/people/a/alex-gunwoo-kim/>Alex Gunwoo Kim</a>
|
<a href=/people/s/sangwon-yoon/>Sangwon Yoon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--socialnlp-1--3><div class="card-body p-3 small">Kyle (1985) proposes two types of rumors: informed rumors which are based on some private information and uninformed rumors which are not based on any information (i.e. bluffing). Also, prior studies find that when people have credible source of information, they are likely to use a more confident textual tone in their spreading of rumors. Motivated by these theoretical findings, we propose a double-channel structure to determine the ex-ante veracity of rumors on social media. Our ultimate goal is to classify each rumor into true, false, or unverifiable category. We first assign each text into either certain (informed rumor) or uncertain (uninformed rumor) category. Then, we apply lie detection algorithm to informed rumors and thread-reply agreement detection algorithm to uninformed rumors. Using the dataset of SemEval 2019 Task 7, which requires ex-ante threefold classification (true, false, or unverifiable) of social media rumors, our model yields a macro-F1 score of 0.4027, outperforming all the baseline models and the second-place winner (Gorrell et al., 2019). Furthermore, we empirically validate that the double-channel structure outperforms single-channel structures which use either lie detection or agreement detection algorithm to all posts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.socialnlp-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--socialnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.socialnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.socialnlp-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.socialnlp-1.4/>Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks</a></strong><br><a href=/people/d/divyam-goel/>Divyam Goel</a>
|
<a href=/people/r/raksha-sharma/>Raksha Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--socialnlp-1--4><div class="card-body p-3 small">The last few years have witnessed an exponential rise in the propagation of offensive text on social media. Identification of this text with high precision is crucial for the well-being of society. Most of the existing approaches tend to give high toxicity scores to innocuous statements (e.g., “I am a gay man”). These false positives result from over-generalization on the training data where specific terms in the statement may have been used in a pejorative sense (e.g., “gay”). Emphasis on such words alone can lead to discrimination against the classes these systems are designed to protect. In this paper, we address the problem of offensive language detection on Twitter, while also detecting the type and the target of the offense. We propose a novel approach called SyLSTM, which integrates syntactic features in the form of the dependency parse tree of a sentence and semantic features in the form of word embeddings into a deep learning architecture using a Graph Convolutional Network. Results show that the proposed approach significantly outperforms the state-of-the-art BERT model with orders of magnitude fewer number of parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.socialnlp-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--socialnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.socialnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.socialnlp-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.socialnlp-1.5/>A Comparative Study on Word Embeddings and Social <span class=acl-fixed-case>NLP</span> Tasks</a></strong><br><a href=/people/f/fatma-elsafoury/>Fatma Elsafoury</a>
|
<a href=/people/s/steven-r-wilson/>Steven R. Wilson</a>
|
<a href=/people/n/naeem-ramzan/>Naeem Ramzan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--socialnlp-1--5><div class="card-body p-3 small">In recent years, gray social media platforms, those with a loose moderation policy on cyberbullying, have been attracting more users. Recently, data collected from these types of platforms have been used to pre-train word embeddings (social-media-based), yet these word embeddings have not been investigated for social NLP related tasks. In this paper, we carried out a comparative study between social-media-based and non-social-media-based word embeddings on two social NLP tasks: Detecting cyberbullying and Measuring social bias. Our results show that using social-media-based word embeddings as input features, rather than non-social-media-based embeddings, leads to better cyberbullying detection performance. We also show that some word embeddings are more useful than others for categorizing offensive words. However, we do not find strong evidence that certain word embeddings will necessarily work best when identifying certain categories of cyberbullying within our datasets. Finally, We show even though most of the state-of-the-art bias metrics ranked social-media-based word embeddings as the most socially biased, these results remain inconclusive and further research is required.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.socialnlp-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--socialnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.socialnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.socialnlp-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.socialnlp-1.6/>Identifying Human Needs through Social Media: A study on <span class=acl-fixed-case>I</span>ndian cities during <span class=acl-fixed-case>COVID</span>-19</a></strong><br><a href=/people/s/sunny-rai/>Sunny Rai</a>
|
<a href=/people/r/rohan-joseph/>Rohan Joseph</a>
|
<a href=/people/p/prakruti-singh-thakur/>Prakruti Singh Thakur</a>
|
<a href=/people/m/mohammed-abdul-khaliq/>Mohammed Abdul Khaliq</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--socialnlp-1--6><div class="card-body p-3 small">In this paper, we present a minimally-supervised approach to identify human needs expressed in tweets. Taking inspiration from Frustration-Aggression theory, we trained RoBERTa model to classify tweets expressing frustration which serves as an indicator of unmet needs. Although the notion of frustration is highly subjective and complex, the findings support the use of pretrained language model in identifying tweets with unmet needs. Our study reveals the major causes behind feeling frustrated during the lockdown and the second wave of the COVID-19 pandemic in India. Our proposed approach can be useful in timely identification and prioritization of emerging human needs in the event of a crisis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.socialnlp-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--socialnlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.socialnlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.socialnlp-1.7/>Towards Toxic Positivity Detection</a></strong><br><a href=/people/i/ishan-sanjeev-upadhyay/>Ishan Sanjeev Upadhyay</a>
|
<a href=/people/k/kv-aditya-srivatsa/>KV Aditya Srivatsa</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--socialnlp-1--7><div class="card-body p-3 small">Over the past few years, there has been a growing concern around toxic positivity on social media which is a phenomenon where positivity is used to minimize one’s emotional experience. In this paper, we create a dataset for toxic positivity classification from Twitter and an inspirational quote website. We then perform benchmarking experiments using various text classification models and show the suitability of these models for the task. We achieved a macro F1 score of 0.71 and a weighted F1 score of 0.85 by using an ensemble model. To the best of our knowledge, our dataset is the first such dataset created.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.socialnlp-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--socialnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.socialnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.socialnlp-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.socialnlp-1.8/><span class=acl-fixed-case>OK</span> Boomer: Probing the socio-demographic Divide in Echo Chambers</a></strong><br><a href=/people/h/henri-jacques-geiss/>Henri-Jacques Geiss</a>
|
<a href=/people/f/flora-sakketou/>Flora Sakketou</a>
|
<a href=/people/l/lucie-flek/>Lucie Flek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--socialnlp-1--8><div class="card-body p-3 small">Social media platforms such as Twitter or Reddit have become an integral part in political opinion formation and discussions, accompanied by potential echo chamber forming. In this paper, we examine the relationships between the interaction patterns, the opinion polarity, and the socio-demographic characteristics in discussion communities on Reddit. On a dataset of over 2 million posts coming from over 20k users, we combine network community detection algorithms, reliable stance polarity annotations, and NLP-based socio-demographic estimations, to identify echo chambers and understand their properties at scale. We show that the separability of the interaction communities is more strongly correlated to the relative socio-demographic divide, rather than the stance polarity gap size. We further demonstrate that the socio-demographic classifiers have a strong topical bias and should be used with caution, merely for the relative community difference comparisons within a topic, rather than for any absolute labeling.</div></div></div><hr><div id=2022starsem-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.starsem-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.starsem-1/>Proceedings of the 11th Joint Conference on Lexical and Computational Semantics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.0/>Proceedings of the 11th Joint Conference on Lexical and Computational Semantics</a></strong><br><a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.1/>What do Large Language Models Learn about Scripts?</a></strong><br><a href=/people/a/abhilasha-sancheti/>Abhilasha Sancheti</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--1><div class="card-body p-3 small">Script Knowledge (Schank and Abelson, 1975) has long been recognized as crucial for language understanding as it can help in filling in unstated information in a narrative. However, such knowledge is expensive to produce manually and difficult to induce from text due to reporting bias (Gordon and Van Durme, 2013). In this work, we are interested in the scientific question of whether explicit script knowledge is present and accessible through pre-trained generative language models (LMs). To this end, we introduce the task of generating full event sequence descriptions (ESDs) given a scenario as a natural language prompt. Through zero-shot probing, we find that generative LMs produce poor ESDs with mostly omitted, irrelevant, repeated or misordered events. To address this, we propose a pipeline-based script induction framework (SIF) which can generate good quality ESDs for unseen scenarios (e.g., bake a cake). SIF is a two-staged framework that fine-tunes LM on a small set of ESD examples in the first stage. In the second stage, ESD generated for an unseen scenario is post-processed using RoBERTa-based models to filter irrelevant events, remove repetitions, and reorder the temporally misordered events. Through automatic and manual evaluations, we demonstrate that SIF yields substantial improvements (1-3 BLEU points) over a fine-tuned LM. However, manual analysis shows that there is great room for improvement, offering a new research direction for inducing script knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.2/><span class=acl-fixed-case>D</span>eep<span class=acl-fixed-case>A</span>2: A Modular Framework for Deep Argument Analysis with Pretrained Neural <span class=acl-fixed-case>T</span>ext2<span class=acl-fixed-case>T</span>ext Language Models</a></strong><br><a href=/people/g/gregor-betz/>Gregor Betz</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--2><div class="card-body p-3 small">In this paper, we present and implement a multi-dimensional, modular framework for performing deep argument analysis (DeepA2) using current pre-trained language models (PTLMs). ArgumentAnalyst – a T5 model [Raffel et al. 2020] set up and trained within DeepA2 – reconstructs argumentative texts, which advance an informal argumentation, as valid arguments: It inserts, e.g., missing premises and conclusions, formalizes inferences, and coherently links the logical reconstruction to the source text. We create a synthetic corpus for deep argument analysis, and evaluate ArgumentAnalyst on this new dataset as well as on existing data, specifically EntailmentBank [Dalvi et al. 2021]. Our empirical findings vindicate the overall framework and highlight the advantages of a modular design, in particular its ability to emulate established heuristics (such as hermeneutic cycles), to explore the model’s uncertainty, to cope with the plurality of correct solutions (underdetermination), and to exploit higher-order evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.3/>Semantics-aware Attention Improves Neural Machine Translation</a></strong><br><a href=/people/a/aviv-slobodkin/>Aviv Slobodkin</a>
|
<a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--3><div class="card-body p-3 small">The integration of syntactic structures into Transformer machine translation has shown positive results, but to our knowledge, no work has attempted to do so with semantic structures. In this work we propose two novel parameter-free methods for injecting semantic information into Transformers, both rely on semantics-aware masking of (some of) the attention heads. One such method operates on the encoder, through a Scene-Aware Self-Attention (SASA) head. Another on the decoder, through a Scene-Aware Cross-Attention (SACrA) head. We show a consistent improvement over the vanilla Transformer and syntax-aware models for four language pairs. We further show an additional gain when using both semantic and syntactic structures in some language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.4/>Compositional generalization with a broad-coverage semantic parser</a></strong><br><a href=/people/p/pia-weissenhorn/>Pia Weißenhorn</a>
|
<a href=/people/l/lucia-donatelli/>Lucia Donatelli</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--4><div class="card-body p-3 small">We show how the AM parser, a compositional semantic parser (Groschwitz et al., 2018) can solve compositional generalization on the COGS dataset. It is the first semantic parser that achieves high accuracy on both naturally occurring language and the synthetic COGS dataset. We discuss implications for corpus and model design for learning human-like generalization. Our results suggest that compositional generalization can be best achieved by building compositionality into semantic parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.5/><span class=acl-fixed-case>A</span>na<span class=acl-fixed-case>L</span>og: Testing Analytical and Deductive Logic Learnability in Language Models</a></strong><br><a href=/people/s/samuel-ryb/>Samuel Ryb</a>
|
<a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/a/arabella-sinclair/>Arabella Sinclair</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--5><div class="card-body p-3 small">We investigate the extent to which pre-trained language models acquire analytical and deductive logical reasoning capabilities as a side effect of learning word prediction. We present AnaLog, a natural language inference task designed to probe models for these capabilities, controlling for different invalid heuristics the models may adopt instead of learning the desired generalisations. We test four languagemodels on AnaLog, finding that they have all learned, to a different extent, to encode information that is predictive of entailment beyond shallow heuristics such as lexical overlap and grammaticality. We closely analyse the best performing language model and show that while it performs more consistently than other language models across logical connectives and reasoning domains, it still is sensitive to lexical and syntactic variations in the realisation of logical statements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.6/>Pairwise Representation Learning for Event Coreference</a></strong><br><a href=/people/x/xiaodong-yu/>Xiaodong Yu</a>
|
<a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--6><div class="card-body p-3 small">Natural Language Processing tasks such as resolving the coreference of events require understanding the relations between two text snippets. These tasks are typically formulated as (binary) classification problems over independently induced representations of the text snippets. In this work, we develop a Pairwise Representation Learning (PairwiseRL) scheme for the event mention pairs, in which we jointly encode a pair of text snippets so that the representation of each mention in the pair is induced in the context of the other one. Furthermore, our representation supports a finer, structured representation of the text snippet to facilitate encoding events and their arguments. We show that PairwiseRL, despite its simplicity, outperforms the prior state-of-the-art event coreference systems on both cross-document and within-document event coreference benchmarks. We also conduct in-depth analysis in terms of the improvement and the limitation of pairwise representation so as to provide insights for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.7/>A Simple Unsupervised Approach for Coreference Resolution using Rule-based Weak Supervision</a></strong><br><a href=/people/a/alessandro-stolfo/>Alessandro Stolfo</a>
|
<a href=/people/c/chris-tanner/>Chris Tanner</a>
|
<a href=/people/v/vikram-gupta/>Vikram Gupta</a>
|
<a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--7><div class="card-body p-3 small">Labeled data for the task of Coreference Resolution is a scarce resource, requiring significant human effort. While state-of-the-art coreference models rely on such data, we propose an approach that leverages an end-to-end neural model in settings where labeled data is unavailable. Specifically, using weak supervision, we transfer the linguistic knowledge encoded by Stanford?s rule-based coreference system to the end-to-end model, which jointly learns rich, contextualized span representations and coreference chains. Our experiments on the English OntoNotes corpus demonstrate that our approach effectively benefits from the noisy coreference supervision, producing an improvement over Stanford?s rule-based system (+3.7 F1) and outperforming the previous best unsupervised model (+0.9 F1). Additionally, we validate the efficacy of our method on two other datasets: PreCo and Litbank (+2.5 and +5 F1 on Stanford’s system, respectively).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.8/>Multilingual Extraction and Categorization of Lexical Collocations with Graph-aware Transformers</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/a/alexander-shvets/>Alexander Shvets</a>
|
<a href=/people/a/alireza-mohammadshahi/>Alireza Mohammadshahi</a>
|
<a href=/people/j/james-henderson/>James Henderson</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--8><div class="card-body p-3 small">Recognizing and categorizing lexical collocations in context is useful for language learning, dictionary compilation and downstream NLP. However, it is a challenging task due to the varying degrees of frozenness lexical collocations exhibit. In this paper, we put forward a sequence tagging BERT-based model enhanced with a graph-aware transformer architecture, which we evaluate on the task of collocation recognition in context. Our results suggest that explicitly encoding syntactic dependencies in the model architecture is helpful, and provide insights on differences in collocation typification in English, Spanish and French.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.9/><span class=acl-fixed-case>D</span>yna-b<span class=acl-fixed-case>A</span>b<span class=acl-fixed-case>I</span>: unlocking b<span class=acl-fixed-case>A</span>b<span class=acl-fixed-case>I</span>’s potential with dynamic synthetic benchmarking</a></strong><br><a href=/people/r/ronen-tamari/>Ronen Tamari</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a>
|
<a href=/people/n/noam-kahlon/>Noam Kahlon</a>
|
<a href=/people/a/aviad-sar-shalom/>Aviad Sar-shalom</a>
|
<a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a>
|
<a href=/people/d/dafna-shahaf/>Dafna Shahaf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--9><div class="card-body p-3 small">While neural language models often perform surprisingly well on natural language understanding (NLU) tasks, their strengths and limitations remain poorly understood. Controlled synthetic tasks are thus an increasingly important resource for diagnosing model behavior. In this work we focus on story understanding, a core competency for NLU systems. However, the main synthetic resource for story understanding, the bAbI benchmark, lacks such a systematic mechanism for controllable task generation. We develop Dyna-bAbI, a dynamic framework providing fine-grained control over task generation in bAbI. We demonstrate our ideas by constructing three new tasks requiring compositional generalization, an important evaluation setting absent from the original benchmark. We tested both special-purpose models developed for bAbI as well as state-of-the-art pre-trained methods, and found that while both approaches solve the original tasks (99% accuracy), neither approach succeeded in the compositional generalization setting, indicating the limitations of the original training data. We explored ways to augment the original data, and found that though diversifying training data was far more useful than simply increasing dataset size, it was still insufficient for driving robust compositional generalization (with 70% accuracy for complex compositions). Our results underscore the importance of highly controllable task generators for creating robust NLU systems through a virtuous cycle of model and data development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.10/>When Polysemy Matters: Modeling Semantic Categorization with Word Embeddings</a></strong><br><a href=/people/e/elizabeth-soper/>Elizabeth Soper</a>
|
<a href=/people/j/jean-pierre-koenig/>Jean-pierre Koenig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--10><div class="card-body p-3 small">Recent work using word embeddings to model semantic categorization have indicated that static models outperform the more recent contextual class of models (Majewska et al, 2021). In this paper, we consider polysemy as a possible confounding factor, comparing sense-level embeddings with previously studied static embeddings on both coarse- and fine-grained categorization tasks. We find that the effect of polysemy depends on how one defines semantic categorization; while sense-level embeddings dramatically outperform static embeddings in predicting coarse-grained categories derived from a word sorting task, they perform approximately equally in predicting fine-grained categories derived from context-free similarity judgments. Our findings highlight the different processes underlying human behavior on different types of semantic tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.11/>Word-Label Alignment for Event Detection: A New Perspective via Optimal Transport</a></strong><br><a href=/people/a/amir-pouran-ben-veyseh/>Amir Pouran Ben Veyseh</a>
|
<a href=/people/t/thien-nguyen/>Thien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--11><div class="card-body p-3 small">Event Detection (ED) aims to identify mentions/triggers of real world events in text. In the literature, this task is modeled as a sequence-labeling or word-prediction problem. In this work, we present a novel formulation in which ED is modeled as a word-label alignment task. In particular, given the words in a sentence and possible event types, the objective is to infer an alignment matrix in which event trigger words are aligned with the most likely event types. Moreover, we show that this new perspective facilitates the incorporation of word-label alignment biases to improve alignment matrix for ED. Novel alignment biases and Optimal Transport are introduced to solve our alignment problem for ED. We conduct experiments on a benchmark dataset to demonstrate the effectiveness of the proposed model for ED.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.12/>Comparison and Combination of Sentence Embeddings Derived from Different Supervision Signals</a></strong><br><a href=/people/h/hayato-tsukagoshi/>Hayato Tsukagoshi</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/k/koichi-takeda/>Koichi Takeda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--12><div class="card-body p-3 small">There have been many successful applications of sentence embedding methods. However, it has not been well understood what properties are captured in the resulting sentence embeddings depending on the supervision signals. In this paper, we focus on two types of sentence embedding methods with similar architectures and tasks: one fine-tunes pre-trained language models on the natural language inference task, and the other fine-tunes pre-trained language models on word prediction task from its definition sentence, and investigate their properties. Specifically, we compare their performances on semantic textual similarity (STS) tasks using STS datasets partitioned from two perspectives: 1) sentence source and 2) superficial similarity of the sentence pairs, and compare their performances on the downstream and probing tasks. Furthermore, we attempt to combine the two methods and demonstrate that combining the two methods yields substantially better performance than the respective methods on unsupervised STS tasks and downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.13/>Distilling Hypernymy Relations from Language Models: On the Effectiveness of Zero-Shot Taxonomy Induction</a></strong><br><a href=/people/d/devansh-jain/>Devansh Jain</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--13><div class="card-body p-3 small">In this paper, we analyze zero-shot taxonomy learning methods which are based on distilling knowledge from language models via prompting and sentence scoring. We show that, despite their simplicity, these methods outperform some supervised strategies and are competitive with the current state-of-the-art under adequate conditions. We also show that statistical and linguistic properties of prompts dictate downstream performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.14/>A Dynamic, Interpreted <span class=acl-fixed-case>C</span>heck<span class=acl-fixed-case>L</span>ist for Meaning-oriented <span class=acl-fixed-case>NLG</span> Metric Evaluation – through the Lens of Semantic Similarity Rating</a></strong><br><a href=/people/l/laura-zeidler/>Laura Zeidler</a>
|
<a href=/people/j/juri-opitz/>Juri Opitz</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--14><div class="card-body p-3 small">Evaluating the quality of generated text is difficult, since traditional NLG evaluation metrics, focusing more on surface form than meaning, often fail to assign appropriate scores. This is especially problematic for AMR-to-text evaluation, given the abstract nature of AMR.Our work aims to support the development and improvement of NLG evaluation metrics that focus on meaning by developing a dynamic CheckList for NLG metrics that is interpreted by being organized around meaning-relevant linguistic phenomena. Each test instance consists of a pair of sentences with their AMR graphs and a human-produced textual semantic similarity or relatedness score. Our CheckList facilitates comparative evaluation of metrics and reveals strengths and weaknesses of novel and traditional metrics. We demonstrate the usefulness of CheckList by designing a new metric GraCo that computes lexical cohesion graphs over AMR concepts. Our analysis suggests that GraCo presents an interesting NLG metric worth future investigation and that meaning-oriented NLG metrics can profit from graph-based metric components using AMR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.15/>Assessing the Limits of the Distributional Hypothesis in Semantic Spaces: Trait-based Relational Knowledge and the Impact of Co-occurrences</a></strong><br><a href=/people/m/mark-anderson/>Mark Anderson</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-collados</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--15><div class="card-body p-3 small">The increase in performance in NLP due to the prevalence of distributional models and deep learning has brought with it a reciprocal decrease in interpretability. This has spurred a focus on what neural networks learn about natural language with less of a focus on how. Some work has focused on the data used to develop data-driven models, but typically this line of work aims to highlight issues with the data, e.g. highlighting and offsetting harmful biases. This work contributes to the relatively untrodden path of what is required in data for models to capture meaningful representations of natural language. This is entails evaluating how well English and Spanish semantic spaces capture a particular type of relational knowledge, namely the traits associated with concepts (e.g. bananas-yellow), and exploring the role of co-occurrences in this context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.16/>A Generative Approach for Mitigating Structural Biases in Natural Language Inference</a></strong><br><a href=/people/d/dimion-asael/>Dimion Asael</a>
|
<a href=/people/z/zachary-ziegler/>Zachary Ziegler</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--16><div class="card-body p-3 small">Many natural language inference (NLI) datasets contain biases that allow models to perform well by only using a biased subset of the input, without considering the remainder features. For instance, models are able to classify samples by only using the hypothesis, without learning the true relationship between it and the premise. These structural biases lead discriminative models to learn unintended superficial features and generalize poorly out of the training distribution. In this work, we reformulate the NLI task as a generative task, where a model is conditioned on the biased subset of the input and the label and generates the remaining subset of the input. We show that by imposing a uniform prior, we obtain a provably unbiased model. Through synthetic experiments, we find that this approach is highly robust to large amounts of bias. We then demonstrate empirically on two types of natural bias that this approach leads to fully unbiased models in practice. However, we find that generative models are difficult to train and generally perform worse than discriminative baselines. We highlight the difficulty of the generative modeling task in the context of NLI as a cause for this worse performance. Finally, by fine-tuning the generative model with a discriminative objective, we reduce the performance gap between the generative model and the discriminative baseline, while allowing for a small amount of bias.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.17/>Measuring Alignment Bias in Neural Seq2seq Semantic Parsers</a></strong><br><a href=/people/d/davide-locatelli/>Davide Locatelli</a>
|
<a href=/people/a/ariadna-quattoni/>Ariadna Quattoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--17><div class="card-body p-3 small">Prior to deep learning the semantic parsing community has been interested in understanding and modeling the range of possible word alignments between natural language sentences and their corresponding meaning representations. Sequence-to-sequence models changed the research landscape suggesting that we no longer need to worry about alignments since they can be learned automatically by means of an attention mechanism. More recently, researchers have started to question such premise. In this work we investigate whether seq2seq models can handle both simple and complex alignments. To answer this question we augment the popular Geo semantic parsing dataset with alignment annotations and create Geo-Aligned. We then study the performance of standard seq2seq models on the examples that can be aligned monotonically versus examples that require more complex alignments. Our empirical study shows that performance is significantly better over monotonic alignments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.18/>Improved Induction of Narrative Chains via Cross-Document Relations</a></strong><br><a href=/people/a/andrew-blair-stanek/>Andrew Blair-stanek</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--18><div class="card-body p-3 small">The standard approach for inducing narrative chains considers statistics gathered per individual document. We consider whether statistics gathered using cross-document relations can lead to improved chain induction. Our study is motivated by legal narratives, where cases typically cite thematically similar cases. We consider four novel variations on pointwise mutual information (PMI), each accounting for cross-document relations in a different way. One proposed PMI variation performs 58% better relative to standard PMI on recall@50 and induces qualitatively better narrative chains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.19/><span class=acl-fixed-case>DRS</span> Parsing as Sequence Labeling</a></strong><br><a href=/people/m/minxing-shen/>Minxing Shen</a>
|
<a href=/people/k/kilian-evang/>Kilian Evang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--19><div class="card-body p-3 small">We present the first fully trainable semantic parser for English, German, Italian, and Dutch discourse representation structures (DRSs) that is competitive in accuracy with recent sequence-to-sequence models and at the same time <i>compositional</i> in the sense that the output maps each token to one of a finite set of meaning <i>fragments</i>, and the meaning of the utterance is a function of the meanings of its parts. We argue that this property makes the system more transparent and more useful for human-in-the-loop annotation. We achieve this simply by casting DRS parsing as a sequence labeling task, where tokens are labeled with both fragments (lists of abstracted clauses with relative referent indices indicating unification) and <i>symbols</i> like word senses or names. We give a comprehensive error analysis that highlights areas for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.20/>How Does Data Corruption Affect Natural Language Understanding Models? A Study on <span class=acl-fixed-case>GLUE</span> datasets</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--20><div class="card-body p-3 small">A central question in natural language understanding (NLU) research is whether high performance demonstrates the models’ strong reasoning capabilities. We present an extensive series of controlled experiments where pre-trained language models are exposed to data that have undergone specific corruption transformations. These involve removing instances of specific word classes and often lead to non-sensical sentences. Our results show that performance remains high on most GLUE tasks when the models are fine-tuned or tested on corrupted data, suggesting that they leverage other cues for prediction even in non-sensical contexts. Our proposed data transformations can be used to assess the extent to which a specific dataset constitutes a proper testbed for evaluating models’ language understanding capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.21/>Leveraging Three Types of Embeddings from Masked Language Models in Idiom Token Classification</a></strong><br><a href=/people/r/ryosuke-takahashi/>Ryosuke Takahashi</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/k/koichi-takeda/>Koichi Takeda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--21><div class="card-body p-3 small">Many linguistic expressions have idiomatic and literal interpretations, and the automatic distinction of these two interpretations has been studied for decades. Recent research has shown that contextualized word embeddings derived from masked language models (MLMs) can give promising results for idiom token classification. This indicates that contextualized word embedding alone contains information about whether the word is being used in a literal sense or not. However, we believe that more types of information can be derived from MLMs and that leveraging such information can improve idiom token classification. In this paper, we leverage three types of embeddings from MLMs; uncontextualized token embeddings and masked token embeddings in addition to the standard contextualized word embeddings and show that the newly added embeddings significantly improve idiom token classification for both English and Japanese datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.22/>“What makes a question inquisitive?” A Study on Type-Controlled Inquisitive Question Generation</a></strong><br><a href=/people/l/lingyu-gao/>Lingyu Gao</a>
|
<a href=/people/d/debanjan-ghosh/>Debanjan Ghosh</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--22><div class="card-body p-3 small">We propose a type-controlled framework for inquisitive question generation. We annotate an inquisitive question dataset with question types, train question type classifiers, and finetune models for type-controlled question generation. Empirical results demonstrate that we can generate a variety of questions that adhere to specific types while drawing from the source texts. We also investigate strategies for selecting a single question from a generated set, considering both an informative vs. inquisitive question classifier and a pairwise ranker trained from a small set of expert annotations. Question selection using the pairwise ranker yields strong results in automatic and manual evaluation. Our human evaluation assesses multiple aspects of the generated questions, finding that the ranker chooses questions with the best syntax (4.59), semantics (4.37), and inquisitiveness (3.92) on a scale of 1-5, even rivaling the performance of human-written questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.23" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.23/>Pretraining on Interactions for Learning Grounded Affordance Representations</a></strong><br><a href=/people/j/jack-merullo/>Jack Merullo</a>
|
<a href=/people/d/dylan-ebert/>Dylan Ebert</a>
|
<a href=/people/c/carsten-eickhoff/>Carsten Eickhoff</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--23><div class="card-body p-3 small">Lexical semantics and cognitive science point to affordances (i.e. the actions that objects support) as critical for understanding and representing nouns and verbs. However, study of these semantic features has not yet been integrated with the ?foundation? models that currently dominate language representation research. We hypothesize that predictive modeling of object state over time will result in representations that encode object affordance information ?for free?. We train a neural network to predict objects? trajectories in a simulated interaction and show that our network?s latent representations differentiate between both observed and unobserved affordances. We find that models trained using 3D simulations outperform conventional 2D computer vision models trained on a similar task, and, on initial inspection, that differences between concepts correspond to expected features (e.g., roll entails rotation) . Our results suggest a way in which modern deep learning approaches to grounded language learning can be integrated with traditional formal semantic notions of lexical representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.24/><span class=acl-fixed-case>P</span>rop<span class=acl-fixed-case>B</span>ank Comes of <span class=acl-fixed-case>A</span>ge—<span class=acl-fixed-case>L</span>arger, Smarter, and more Diverse</a></strong><br><a href=/people/s/sameer-pradhan/>Sameer Pradhan</a>
|
<a href=/people/j/julia-bonn/>Julia Bonn</a>
|
<a href=/people/s/skatje-myers/>Skatje Myers</a>
|
<a href=/people/k/kathryn-conger/>Kathryn Conger</a>
|
<a href=/people/t/tim-ogorman/>Tim O’gorman</a>
|
<a href=/people/j/james-gung/>James Gung</a>
|
<a href=/people/k/kristin-wright-bettner/>Kristin Wright-bettner</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--24><div class="card-body p-3 small">This paper describes the evolution of the PropBank approach to semantic role labeling over the last two decades. During this time the PropBank frame files have been expanded to include non-verbal predicates such as adjectives, prepositions and multi-word expressions. The number of domains, genres and languages that have been PropBanked has also expanded greatly, creating an opportunity for much more challenging and robust testing of the generalization capabilities of PropBank semantic role labeling systems. We also describe the substantial effort that has gone into ensuring the consistency and reliability of the various annotated datasets and resources, to better support the training and evaluation of such systems</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.25.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.25/>Speech acts and Communicative Intentions for Urgency Detection</a></strong><br><a href=/people/l/laurenti-enzo/>Laurenti Enzo</a>
|
<a href=/people/b/bourgon-nils/>Bourgon Nils</a>
|
<a href=/people/f/farah-benamara/>Farah Benamara</a>
|
<a href=/people/m/mari-alda/>Mari Alda</a>
|
<a href=/people/v/veronique-moriceau/>Véronique Moriceau</a>
|
<a href=/people/c/courgeon-camille/>Courgeon Camille</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--25><div class="card-body p-3 small">Recognizing speech acts (SA) is crucial for capturing meaning beyond what is said, making communicative intentions particularly relevant to identify urgent messages. This paper attempts to measure for the first time the impact of SA on urgency detection during crises,006in tweets. We propose a new dataset annotated for both urgency and SA, and develop several deep learning architectures to inject SA into urgency detection while ensuring models generalisability. Our results show that taking speech acts into account in tweet analysis improves information type detection in an out-of-type configuration where models are evaluated in unseen event types during training. These results are encouraging and constitute a first step towards SA-aware disaster management in social media.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.26.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.26/>What Drives the Use of Metaphorical Language? Negative Insights from Abstractness, Affect, Discourse Coherence and Contextualized Word Representations</a></strong><br><a href=/people/p/prisca-piccirilli/>Prisca Piccirilli</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte Im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--26><div class="card-body p-3 small">Given a specific discourse, which discourse properties trigger the use of metaphorical language, rather than using literal alternatives? For example, what drives people to say grasp the meaning rather than understand the meaning within a specific context? Many NLP approaches to metaphorical language rely on cognitive and (psycho-)linguistic insights and have successfully defined models of discourse coherence, abstractness and affect. In this work, we build five simple models relying on established cognitive and linguistic properties ? frequency, abstractness, affect, discourse coherence and contextualized word representations ? to predict the use of a metaphorical vs. synonymous literal expression in context. By comparing the models? outputs to human judgments, our study indicates that our selected properties are not sufficient to systematically explain metaphorical vs. literal language choices.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.27.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.27/>Unsupervised Reinforcement Adaptation for Class-Imbalanced Text Classification</a></strong><br><a href=/people/y/yuexin-wu/>Yuexin Wu</a>
|
<a href=/people/x/xiaolei-huang/>Xiaolei Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--27><div class="card-body p-3 small">Class imbalance naturally exists when label distributions are not aligned across source and target domains. However, existing state-of-the-art UDA models learn domain-invariant representations across domains and evaluate primarily on class-balanced data. In this work, we propose an unsupervised domain adaptation approach via reinforcement learning that jointly leverages feature variants and imbalanced labels across domains. We experiment with the text classification task for its easily accessible datasets and compare the proposed method with five baselines. Experiments on three datasets prove that our proposed method can effectively learn robust domain-invariant representations and successfully adapt text classifiers on imbalanced classes over domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.28.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.starsem-1.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.28/>Event Causality Identification via Generation of Important Context Words</a></strong><br><a href=/people/h/hieu-man/>Hieu Man</a>
|
<a href=/people/m/minh-nguyen/>Minh Nguyen</a>
|
<a href=/people/t/thien-nguyen/>Thien Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--28><div class="card-body p-3 small">An important problem of Information Extraction involves Event Causality Identification (ECI) that seeks to identify causal relation between pairs of event mentions. Prior models for ECI have mainly solved the problem using the classification framework that does not explore prediction/generation of important context words from input sentences for causal recognition. In this work, we consider the words along the dependency path between the two event mentions in the dependency tree as the important context words for ECI. We introduce dependency path generation as a complementary task for ECI, which can be solved jointly with causal label prediction to improve the performance. To facilitate the multi-task learning, we cast ECI into a generation problem that aims to generate both causal relation and dependency path words from input sentence. In addition, we propose to use the REINFORCE algorithm to train our generative model where novel reward functions are designed to capture both causal prediction accuracy and generation quality. The experiments on two benchmark datasets demonstrate state-of-the-art performance of the proposed model for ECI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.29.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.29/>Capturing the Content of a Document through Complex Event Identification</a></strong><br><a href=/people/z/zheng-qi/>Zheng Qi</a>
|
<a href=/people/e/elior-sulem/>Elior Sulem</a>
|
<a href=/people/h/haoyu-wang/>Haoyu Wang</a>
|
<a href=/people/x/xiaodong-yu/>Xiaodong Yu</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--29><div class="card-body p-3 small">Granular events, instantiated in a document by predicates, can usually be grouped into more general events, called complex events. Together, they capture the major content of the document. Recent work grouped granular events by defining event regions, filtering out sentences that are irrelevant to the main content. However, this approach assumes that a given complex event is always described in consecutive sentences, which does not always hold in practice. In this paper, we introduce the task of complex event identification. We address this task as a pipeline, first predicting whether two granular events mentioned in the text belong to the same complex event, independently of their position in the text, and then using this to cluster them into complex events. Due to the difficulty of predicting whether two granular events belong to the same complex event in isolation, we propose a context-augmented representation learning approach CONTEXTRL that adds additional context to better model the pairwise relation between granular events. We show that our approach outperforms strong baselines on the complex event identification task and further present a promising case study exploring the effectiveness of using complex events as input for document-level argument extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.starsem-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.starsem-1.30.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--starsem-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.starsem-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.starsem-1.30/>Online Coreference Resolution for Dialogue Processing: Improving Mention-Linking on Real-Time Conversations</a></strong><br><a href=/people/l/liyan-xu/>Liyan Xu</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--starsem-1--30><div class="card-body p-3 small">This paper suggests a direction of coreference resolution for online decoding on actively generated input such as dialogue, where the model accepts an utterance and its past context, then finds mentions in the current utterance as well as their referents, upon each dialogue turn. A baseline and four incremental updated models adapted from the mention linking paradigm are proposed for this new setting, which address different aspects including the singletons, speaker-grounded encoding and cross-turn mention contextualization. Our approach is assessed on three datasets: Friends, OntoNotes, and BOLT. Results show that each aspect brings out steady improvement, and our best models outperform the baseline by over 10%, presenting an effective system for this setting. Further analysis highlights the task characteristics, such as the significance of addressing the mention recall.</div></div></div><hr><div id=2022suki-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.suki-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.suki-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.suki-1/>Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.suki-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.suki-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.suki-1.0/>Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI)</a></strong><br><a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/x/xinyun-chen/>Xinyun Chen</a>
|
<a href=/people/z/zhiyu-chen/>Zhiyu Chen</a>
|
<a href=/people/z/ziyu-yao/>Ziyu Yao</a>
|
<a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.suki-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.suki-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--suki-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.suki-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.suki-1.1/><span class=acl-fixed-case>F</span>ab<span class=acl-fixed-case>KG</span>: A Knowledge graph of Manufacturing Science domain utilizing structured and unconventional unstructured knowledge source</a></strong><br><a href=/people/a/aman-kumar/>Aman Kumar</a>
|
<a href=/people/a/akshay-bharadwaj/>Akshay Bharadwaj</a>
|
<a href=/people/b/binil-starly/>Binil Starly</a>
|
<a href=/people/c/collin-lynch/>Collin Lynch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--suki-1--1><div class="card-body p-3 small">As the demands for large-scale information processing have grown, knowledge graph-based approaches have gained prominence for representing general and domain knowledge. The development of such general representations is essential, particularly in domains such as manufacturing which intelligent processes and adaptive education can enhance. Despite the continuous accumulation of text in these domains, the lack of structured data has created information extraction and knowledge transfer barriers. In this paper, we report on work towards developing robust knowledge graphs based upon entity and relation data for both commercial and educational uses. To create the FabKG (Manufacturing knowledge graph), we have utilized textbook index words, research paper keywords, FabNER (manufacturing NER), to extract a sub knowledge base contained within Wikidata. Moreover, we propose a novel crowdsourcing method for KG creation by leveraging student notes, which contain invaluable information but are not captured as meaningful information, excluding their use in personal preparation for learning and written exams. We have created a knowledge graph containing 65000+ triples using all data sources. We have also shown the use case of domain-specific question answering and expression/formula-based question answering for educational purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.suki-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.suki-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--suki-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.suki-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.suki-1.2/>Modeling Compositionality with Dependency Graph for Dialogue Generation</a></strong><br><a href=/people/x/xiaofeng-chen/>Xiaofeng Chen</a>
|
<a href=/people/y/yirong-chen/>Yirong Chen</a>
|
<a href=/people/x/xiaofen-xing/>Xiaofen Xing</a>
|
<a href=/people/x/xiangmin-xu/>Xiangmin Xu</a>
|
<a href=/people/w/wenjing-han/>Wenjing Han</a>
|
<a href=/people/q/qianfeng-tie/>Qianfeng Tie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--suki-1--2><div class="card-body p-3 small">Because of the compositionality of natural language, syntactic structure which contains the information about the relationship between words is a key factor for semantic understanding. However, the widely adopted Transformer is hard to learn the syntactic structure effectively in dialogue generation tasks. To explicitly model the compositionaity of language in Transformer Block, we restrict the information flow between words by constructing directed dependency graph and propose Dependency Relation Attention (DRA). Experimental results demonstrate that DRA can further improve the performance of state-of-the-art models for dialogue generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.suki-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.suki-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--suki-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.suki-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.suki-1.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.suki-1.3/>Strategies to Improve Few-shot Learning for Intent Classification and Slot-Filling</a></strong><br><a href=/people/s/samyadeep-basu/>Samyadeep Basu</a>
|
<a href=/people/a/amr-sharaf/>Amr Sharaf</a>
|
<a href=/people/k/karine-ip-kiun-chong/>Karine Ip Kiun Chong</a>
|
<a href=/people/a/alex-fischer/>Alex Fischer</a>
|
<a href=/people/v/vishal-rohra/>Vishal Rohra</a>
|
<a href=/people/m/michael-amoake/>Michael Amoake</a>
|
<a href=/people/h/hazem-el-hammamy/>Hazem El-Hammamy</a>
|
<a href=/people/e/ehi-nosakhare/>Ehi Nosakhare</a>
|
<a href=/people/v/vijay-ramani/>Vijay Ramani</a>
|
<a href=/people/b/benjamin-han/>Benjamin Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--suki-1--3><div class="card-body p-3 small">Intent classification (IC) and slot filling (SF) are two fundamental tasks in modern Natural Language Understanding (NLU) systems. Collecting and annotating large amounts of data to train deep learning models for such systems are not scalable. This problem can be addressed by learning from few examples using fast supervised meta-learning techniques such as prototypical networks. In this work, we systematically investigate how contrastive learning and data augmentation methods can benefit these existing meta-learning pipelines for jointly modelled IC/SF tasks. Through extensive experiments across standard IC/SF benchmarks (SNIPS and ATIS), we show that our proposed approaches outperform standard meta-learning methods: contrastive losses as a regularizer in conjunction with prototypical networks consistently outperform the existing state-of-the-art for both IC and SF tasks, while data augmentation strategies primarily improve few-shot IC by a significant margin</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.suki-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.suki-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--suki-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.suki-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.suki-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.suki-1.4/>Learning Open Domain Multi-hop Search Using Reinforcement Learning</a></strong><br><a href=/people/e/enrique-noriega-atala/>Enrique Noriega-Atala</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/c/clayton-morrison/>Clayton Morrison</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--suki-1--4><div class="card-body p-3 small">We propose a method to teach an automated agent to learn how to search for multi-hop paths of relations between entities in an open domain. The method learns a policy for directing existing information retrieval and machine reading resources to focus on relevant regions of a corpus. The approach formulates the learning problem as a Markov decision process with a state representation that encodes the dynamics of the search process and a reward structure that minimizes the number of documents that must be processed while still finding multi-hop paths. We implement the method in an actor-critic reinforcement learning algorithm and evaluate it on a dataset of search problems derived from a subset of English Wikipedia. The algorithm finds a family of policies that succeeds in extracting the desired information while processing fewer documents compared to several baseline heuristic algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.suki-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.suki-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--suki-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.suki-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.suki-1.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.suki-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.suki-1.5/>Table Retrieval May Not Necessitate Table-specific Model Design</a></strong><br><a href=/people/z/zhiruo-wang/>Zhiruo Wang</a>
|
<a href=/people/z/zhengbao-jiang/>Zhengbao Jiang</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--suki-1--5><div class="card-body p-3 small">Tables are an important form of structured data for both human and machine readers alike, providing answers to questions that cannot, or cannot easily, be found in texts. Recent work has designed special models and training paradigms for table-related tasks such as table-based question answering and table retrieval. Though effective, they add complexity in both modeling and data acquisition compared to generic text solutions and obscure which elements are truly beneficial. In this work, we focus on the task of table retrieval, and ask: “is table-specific model design necessary for table retrieval, or can a simpler text-based model be effectively used to achieve a similar result?’’ First, we perform an analysis on a table-based portion of the Natural Questions dataset (NQ-table), and find that structure plays a negligible role in more than 70% of the cases. Based on this, we experiment with a general Dense Passage Retriever (DPR) based on text and a specialized Dense Table Retriever (DTR) that uses table-specific model designs. We find that DPR performs well without any table-specific design and training, and even achieves superior results compared to DTR when fine-tuned on properly linearized tables. We then experiment with three modules to explicitly encode table structures, namely auxiliary row/column embeddings, hard attention masks, and soft relation-based attention biases. However, none of these yielded significant improvements, suggesting that table-specific model design may not be necessary for table retrieval.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.suki-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.suki-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--suki-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.suki-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.suki-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.suki-1.6/>Transfer Learning and Masked Generation for Answer Verbalization</a></strong><br><a href=/people/s/sebastien-montella/>Sebastien Montella</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina Rojas-Barahona</a>
|
<a href=/people/f/frederic-bechet/>Frederic Bechet</a>
|
<a href=/people/j/johannes-heinecke/>Johannes Heinecke</a>
|
<a href=/people/a/alexis-nasr/>Alexis Nasr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--suki-1--6><div class="card-body p-3 small">Structured Knowledge has recently emerged as an essential component to support fine-grained Question Answering (QA). In general, QA systems query a Knowledge Base (KB) to detect and extract the raw answers as final prediction. However, as lacking of context, language generation can offer a much informative and complete response. In this paper, we propose to combine the power of transfer learning and the advantage of entity placeholders to produce high-quality verbalization of extracted answers from a KB. We claim that such approach is especially well-suited for answer generation. Our experiments show 44.25%, 3.26% and 29.10% relative gain in BLEU over the state-of-the-art on the VQuAnDA, ParaQA and VANiLLa datasets, respectively. We additionally provide minor hallucinations corrections in VANiLLa standing for 5% of each of the training and testing set. We witness a median absolute gain of 0.81 SacreBLEU. This strengthens the importance of data quality when using automated evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.suki-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.suki-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--suki-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.suki-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.suki-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.suki-1.7/>Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering</a></strong><br><a href=/people/l/lingbo-mo/>Lingbo Mo</a>
|
<a href=/people/z/zhen-wang/>Zhen Wang</a>
|
<a href=/people/j/jie-zhao/>Jie Zhao</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--suki-1--7><div class="card-body p-3 small">Multi-hop question answering (QA) combines multiple pieces of evidence to search for the correct answer. Reasoning over a text corpus (TextQA) and/or a knowledge base (KBQA) has been extensively studied and led to distinct system architectures. However, knowledge transfer between such two QA systems has been under-explored. Research questions like what knowledge is transferred or whether the transferred knowledge can help answer over one source using another one, are yet to be answered. In this paper, therefore, we study the knowledge transfer of multi-hop reasoning between structured and unstructured sources. We first propose a unified QA framework named SimultQA to enable knowledge transfer and bridge the distinct supervisions from KB and text sources. Then, we conduct extensive analyses to explore how knowledge is transferred by leveraging the pre-training and fine-tuning paradigm. We focus on the low-resource fine-tuning to show that pre-training SimultQA on one source can substantially improve its performance on the other source. More fine-grained analyses on transfer behaviors reveal the types of transferred knowledge and transfer patterns. We conclude with insights into how to construct better QA datasets and systems to exploit knowledge transfer for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.suki-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.suki-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--suki-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.suki-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.suki-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.suki-1.8/>Hierarchical Control of Situated Agents through Natural Language</a></strong><br><a href=/people/s/shuyan-zhou/>Shuyan Zhou</a>
|
<a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--suki-1--8><div class="card-body p-3 small">When humans perform a particular task, they do so hierarchically: splitting higher-level tasks into smaller sub-tasks. However, most works on natural language (NL) command of situated agents have treated the procedures to be executed as flat sequences of simple actions, or any hierarchies of procedures have been shallow at best. In this paper, we propose a formalism of procedures as programs, a method for representing hierarchical procedural knowledge for agent command and control aimed at enabling easy application to various scenarios. We further propose a modeling paradigm of hierarchical modular networks, which consist of a planner and reactors that convert NL intents to predictions of executable programs and probe the environment for information necessary to complete the program execution. We instantiate this framework on the IQA and ALFRED datasets for NL instruction following. Our model outperforms reactive baselines by a large margin on both datasets. We also demonstrate that our framework is more data-efficient, and that it allows for fast iterative development.</div></div></div><hr><div id=2022trustnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.trustnlp-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.trustnlp-1/>Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.trustnlp-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.trustnlp-1.0/>Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022)</a></strong><br><a href=/people/a/apurv-verma/>Apurv Verma</a>
|
<a href=/people/y/yada-pruksachatkun/>Yada Pruksachatkun</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a>
|
<a href=/people/j/jwala-dhamala/>Jwala Dhamala</a>
|
<a href=/people/y/yang-trista-cao/>Yang Trista Cao</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.trustnlp-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--trustnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.trustnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.trustnlp-1.1/>An Encoder Attribution Analysis for Dense Passage Retriever in Open-Domain Question Answering</a></strong><br><a href=/people/m/minghan-li/>Minghan Li</a>
|
<a href=/people/x/xueguang-ma/>Xueguang Ma</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--trustnlp-1--1><div class="card-body p-3 small">The bi-encoder design of dense passage retriever (DPR) is a key factor to its success in open-domain question answering (QA), yet it is unclear how DPR’s question encoder and passage encoder individually contributes to overall performance, which we refer to as the encoder attribution problem. The problem is important as it helps us identify the factors that affect individual encoders to further improve overall performance. In this paper, we formulate our analysis under a probabilistic framework called encoder marginalization, where we quantify the contribution of a single encoder by marginalizing other variables. First, we find that the passage encoder contributes more than the question encoder to in-domain retrieval accuracy. Second, we demonstrate how to find the affecting factors for each encoder, where we train DPR with different amounts of data and use encoder marginalization to analyze the results. We find that positive passage overlap and corpus coverage of training data have big impacts on the passage encoder, while the question encoder is mainly affected by training sample complexity under this setting. Based on this framework, we can devise data-efficient training regimes: for example, we manage to train a passage encoder on SQuAD using 60% less training data without loss of accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.trustnlp-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--trustnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.trustnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.trustnlp-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.trustnlp-1.2/>Attributing Fair Decisions with Attention Interventions</a></strong><br><a href=/people/n/ninareh-mehrabi/>Ninareh Mehrabi</a>
|
<a href=/people/u/umang-gupta/>Umang Gupta</a>
|
<a href=/people/f/fred-morstatter/>Fred Morstatter</a>
|
<a href=/people/g/greg-ver-steeg/>Greg Ver Steeg</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--trustnlp-1--2><div class="card-body p-3 small">The widespread use of Artificial Intelligence (AI) in consequential domains, such as health-care and parole decision-making systems, has drawn intense scrutiny on the fairness of these methods. However, ensuring fairness is often insufficient as the rationale for a contentious decision needs to be audited, understood, and defended. We propose that the attention mechanism can be used to ensure fair outcomes while simultaneously providing feature attributions to account for how a decision was made. Toward this goal, we design an attention-based model that can be leveraged as an attribution framework. It can identify features responsible for both performance and fairness of the model through attention interventions and attention weight manipulation. Using this attribution framework, we then design a post-processing bias mitigation strategy and compare it with a suite of baselines. We demonstrate the versatility of our approach by conducting experiments on two distinct data types, tabular and textual.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.trustnlp-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--trustnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.trustnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.trustnlp-1.3/>Does Moral Code have a Moral Code? Probing Delphi’s Moral Philosophy</a></strong><br><a href=/people/k/kathleen-c-fraser/>Kathleen C. Fraser</a>
|
<a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a>
|
<a href=/people/e/esma-balkir/>Esma Balkir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--trustnlp-1--3><div class="card-body p-3 small">In an effort to guarantee that machine learning model outputs conform with human moral values, recent work has begun exploring the possibility of explicitly training models to learn the difference between right and wrong. This is typically done in a bottom-up fashion, by exposing the model to different scenarios, annotated with human moral judgements. One question, however, is whether the trained models actually learn any consistent, higher-level ethical principles from these datasets – and if so, what? Here, we probe the Allen AI Delphi model with a set of standardized morality questionnaires, and find that, despite some inconsistencies, Delphi tends to mirror the moral principles associated with the demographic groups involved in the annotation process. We question whether this is desirable and discuss how we might move forward with this knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.trustnlp-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--trustnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.trustnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.trustnlp-1.4/>The Cycle of Trust and Responsibility in Outsourced <span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/m/maximilian-castelli/>Maximilian Castelli</a>
|
<a href=/people/l/linda-c-moreau/>Linda C. Moreau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--trustnlp-1--4><div class="card-body p-3 small">Artificial Intelligence (AI) and Machine Learning (ML) are rapidly becoming must-have capabilities. According to a 2019 Forbes Insights Report, “seventy-nine percent [of executives] agree that AI is already having a transformational impact on workflows and tools for knowledge workers, but only 5% of executives consider their companies to be industry-leading in terms of taking advantage of AI-powered processes.” (Forbes 2019) A major reason for this may be a shortage of on-staff expertise in AI/ML. This paper explores the intertwined issues of trust, adoption, training, and ethics of outsourcing AI development to a third party. We describe our experiences as a provider of outsourced natural language processing (NLP). We discuss how trust and accountability co-evolve as solutions mature from proof-of-concept to production-ready.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.trustnlp-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--trustnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.trustnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.trustnlp-1.5/>Explaining Neural <span class=acl-fixed-case>NLP</span> Models for the Joint Analysis of Open-and-Closed-Ended Survey Answers</a></strong><br><a href=/people/e/edoardo-mosca/>Edoardo Mosca</a>
|
<a href=/people/k/katharina-harmann/>Katharina Harmann</a>
|
<a href=/people/t/tobias-eder/>Tobias Eder</a>
|
<a href=/people/g/georg-groh/>Georg Groh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--trustnlp-1--5><div class="card-body p-3 small">Large-scale surveys are a widely used instrument to collect data from a target audience. Beyond the single individual, an appropriate analysis of the answers can reveal trends and patterns and thus generate new insights and knowledge for researchers. Current analysis practices employ shallow machine learning methods or rely on (biased) human judgment. This work investigates the usage of state-of-the-art NLP models such as BERT to automatically extract information from both open- and closed-ended questions. We also leverage explainability methods at different levels of granularity to further derive knowledge from the analysis model. Experiments on EMS—a survey-based study researching influencing factors affecting a student’s career goals—show that the proposed approach can identify such factors both at the input- and higher concept-level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.trustnlp-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--trustnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.trustnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.trustnlp-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.trustnlp-1.6/>The Irrationality of Neural Rationale Models</a></strong><br><a href=/people/y/yiming-zheng/>Yiming Zheng</a>
|
<a href=/people/s/serena-booth/>Serena Booth</a>
|
<a href=/people/j/julie-shah/>Julie Shah</a>
|
<a href=/people/y/yilun-zhou/>Yilun Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--trustnlp-1--6><div class="card-body p-3 small">Neural rationale models are popular for interpretable predictions of NLP tasks. In these, a selector extracts segments of the input text, called rationales, and passes these segments to a classifier for prediction. Since the rationale is the only information accessible to the classifier, it is plausibly defined as the explanation. Is such a characterization unconditionally correct? In this paper, we argue to the contrary, with both philosophical perspectives and empirical evidence suggesting that rationale models are, perhaps, less rational and interpretable than expected. We call for more rigorous evaluations of these models to ensure desired properties of interpretability are indeed achieved. The code for our experiments is at <a href=https://github.com/yimingz89/Neural-Rationale-Analysis class=acl-markup-url>https://github.com/yimingz89/Neural-Rationale-Analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.trustnlp-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--trustnlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.trustnlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.trustnlp-1.7/>An Empirical Study on Pseudo-log-likelihood Bias Measures for Masked Language Models Using Paraphrased Sentences</a></strong><br><a href=/people/b/bum-chul-kwon/>Bum Chul Kwon</a>
|
<a href=/people/n/nandana-mihindukulasooriya/>Nandana Mihindukulasooriya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--trustnlp-1--7><div class="card-body p-3 small">In this paper, we conduct an empirical study on a bias measure, log-likelihood Masked Language Model (MLM) scoring, on a benchmark dataset. Previous work evaluates whether MLMs are biased or not for certain protected attributes (e.g., race) by comparing the log-likelihood scores of sentences that contain stereotypical characteristics with one category (e.g., black) versus another (e.g., white). We hypothesized that this approach might be too sensitive to the choice of contextual words than the meaning of the sentence. Therefore, we computed the same measure after paraphrasing the sentences with different words but with same meaning. Our results demonstrate that the log-likelihood scoring can be more sensitive to utterance of specific words than to meaning behind a given sentence. Our paper reveals a shortcoming of the current log-likelihood-based bias measures for MLMs and calls for new ways to improve the robustness of it</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.trustnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.trustnlp-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--trustnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.trustnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.trustnlp-1.8/>Challenges in Applying Explainability Methods to Improve the Fairness of <span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/e/esma-balkir/>Esma Balkir</a>
|
<a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a>
|
<a href=/people/i/isar-nejadgholi/>Isar Nejadgholi</a>
|
<a href=/people/k/kathleen-c-fraser/>Kathleen Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--trustnlp-1--8><div class="card-body p-3 small">Motivations for methods in explainable artificial intelligence (XAI) often include detecting, quantifying and mitigating bias, and contributing to making machine learning models fairer. However, exactly how an XAI method can help in combating biases is often left unspecified. In this paper, we briefly review trends in explainability and fairness in NLP research, identify the current practices in which explainability methods are applied to detect and mitigate bias, and investigate the barriers preventing XAI methods from being used more widely in tackling fairness issues.</div></div></div><hr><div id=2022unimplicit-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.unimplicit-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.unimplicit-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.unimplicit-1/>Proceedings of the Second Workshop on Understanding Implicit and Underspecified Language</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.unimplicit-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.unimplicit-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.unimplicit-1.0/>Proceedings of the Second Workshop on Understanding Implicit and Underspecified Language</a></strong><br><a href=/people/v/valentina-pyatkin/>Valentina Pyatkin</a>
|
<a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/t/talita-anthonio/>Talita Anthonio</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.unimplicit-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.unimplicit-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--unimplicit-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.unimplicit-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.unimplicit-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.unimplicit-1.1/>Pre-trained Language Models’ Interpretation of Evaluativity Implicature: Evidence from Gradable Adjectives Usage in Context</a></strong><br><a href=/people/y/yan-cong/>Yan Cong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--unimplicit-1--1><div class="card-body p-3 small">By saying Maria is tall, a human speaker typically implies that Maria is evaluatively tall from the speaker’s perspective. However, by using a different construction Maria is taller than Sophie, we cannot infer from Maria and Sophie’s relative heights that Maria is evaluatively tall because it is possible for Maria to be taller than Sophie in a context in which they both count as short. Can pre-trained language models (LMs) “understand” evaulativity (EVAL) inference? To what extent can they discern the EVAL salience of different constructions in a conversation? Will it help LMs’ implicitness performance if we give LMs a persona such as chill, social, and pragmatically skilled? Our study provides an approach to probing LMs’ interpretation of EVAL inference by incorporating insights from experimental pragmatics and sociolinguistics. We find that with the appropriate prompt, LMs can succeed in some pragmatic level language understanding tasks. Our study suggests that socio-pragmatics methodology can shed light on the challenging questions in NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.unimplicit-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.unimplicit-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--unimplicit-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.unimplicit-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.unimplicit-1.2/>Pragmatic and Logical Inferences in <span class=acl-fixed-case>NLI</span> Systems: The Case of Conjunction Buttressing</a></strong><br><a href=/people/p/paolo-pedinotti/>Paolo Pedinotti</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--unimplicit-1--2><div class="card-body p-3 small">An intelligent system is expected to perform reasonable inferences, accounting for both the literal meaning of a word and the meanings a word can acquire in different contexts. A specific kind of inference concerns the connective and, which in some cases gives rise to a temporal succession or causal interpretation in contrast with the logic, commutative one (Levinson, 2000). In this work, we investigate the phenomenon by creating a new dataset for evaluating the interpretation of and by NLI systems, which we use to test three Transformer-based models. Our results show that all systems generalize patterns that are consistent with both the logical and the pragmatic interpretation, perform inferences that are inconsistent with each other, and show clear divergences with both theoretical accounts and humans’ behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.unimplicit-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.unimplicit-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--unimplicit-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.unimplicit-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.unimplicit-1.3/>“Devils Are in the Details”: Annotating Specificity of Clinical Advice from Medical Literature</a></strong><br><a href=/people/y/yingya-li/>Yingya Li</a>
|
<a href=/people/b/bei-yu/>Bei Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--unimplicit-1--3><div class="card-body p-3 small">Prior studies have raised concerns over specificity issues in clinical advice. Lacking specificity — explicitly discussed detailed information — may affect the quality and implementation of clinical advice in medical practice. In this study, we developed and validated a fine-grained annotation schema to describe different aspects of specificity in clinical advice extracted from medical research literature. We also presented our initial annotation effort and discussed future directions towards an NLP-based specificity analysis tool for summarizing and verifying the details in clinical advice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.unimplicit-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.unimplicit-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--unimplicit-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.unimplicit-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.unimplicit-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.unimplicit-1.4/>Searching for <span class=acl-fixed-case>PET</span>s: Using Distributional and Sentiment-Based Methods to Find Potentially Euphemistic Terms</a></strong><br><a href=/people/p/patrick-lee/>Patrick Lee</a>
|
<a href=/people/m/martha-gavidia/>Martha Gavidia</a>
|
<a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/j/jing-peng/>Jing Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--unimplicit-1--4><div class="card-body p-3 small">This paper presents a linguistically driven proof of concept for finding potentially euphemistic terms, or PETs. Acknowledging that PETs tend to be commonly used expressions for a certain range of sensitive topics, we make use of distri- butional similarities to select and filter phrase candidates from a sentence and rank them using a set of simple sentiment-based metrics. We present the results of our approach tested on a corpus of sentences containing euphemisms, demonstrating its efficacy for detecting single and multi-word PETs from a broad range of topics. We also discuss future potential for sentiment-based methods on this task.</div></div></div><hr><div id=2022wnu-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.wnu-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.wnu-1/>Proceedings of the 4th Workshop of Narrative Understanding (WNU2022)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wnu-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.wnu-1.0/>Proceedings of the 4th Workshop of Narrative Understanding (WNU2022)</a></strong><br><a href=/people/e/elizabeth-clark/>Elizabeth Clark</a>
|
<a href=/people/f/faeze-brahman/>Faeze Brahman</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wnu-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wnu-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wnu-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.wnu-1.1/>Uncovering Surprising Event Boundaries in Narratives</a></strong><br><a href=/people/z/zhilin-wang/>Zhilin Wang</a>
|
<a href=/people/a/anna-jafarpour/>Anna Jafarpour</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wnu-1--1><div class="card-body p-3 small">When reading stories, people can naturally identify sentences in which a new event starts, i.e., event boundaries, using their knowledge of how events typically unfold, but a computational model to detect event boundaries is not yet available. We characterize and detect sentences with expected or surprising event boundaries in an annotated corpus of short diary-like stories, using a model that combines commonsense knowledge and narrative flow features with a RoBERTa classifier. Our results show that, while commonsense and narrative features can help improve performance overall, detecting event boundaries that are more subjective remains challenging for our model. We also find that sentences marking surprising event boundaries are less likely to be causally related to the preceding sentence, but are more likely to express emotional reactions of story characters, compared to sentences with no event boundary.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wnu-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wnu-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wnu-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.wnu-1.2/>Compositional Generalization for Kinship Prediction through Data Augmentation</a></strong><br><a href=/people/k/kangda-wei/>Kangda Wei</a>
|
<a href=/people/s/sayan-ghosh/>Sayan Ghosh</a>
|
<a href=/people/s/shashank-srivastava/>Shashank Srivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wnu-1--2><div class="card-body p-3 small">Transformer-based models have shown promising performance in numerous NLP tasks. However, recent work has shown the limitation of such models in showing compositional generalization, which requires models to generalize to novel compositions of known concepts. In this work, we explore two strategies for compositional generalization on the task of kinship prediction from stories, (1) data augmentation and (2) predicting and using intermediate structured representation (in form of kinship graphs). Our experiments show that data augmentation boosts generalization performance by around 20% on average relative to a baseline model from prior work not using these strategies. However, predicting and using intermediate kinship graphs leads to a deterioration in the generalization of kinship prediction by around 50% on average relative to models that only leverage data augmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wnu-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wnu-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wnu-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.3.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.wnu-1.3/>How to be Helpful on Online Support Forums?</a></strong><br><a href=/people/z/zhilin-wang/>Zhilin Wang</a>
|
<a href=/people/p/pablo-e-torres/>Pablo E. Torres</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wnu-1--3><div class="card-body p-3 small">Internet forums such as Reddit offer people a platform to ask for advice when they encounter various issues at work, school or in relationships. Telling helpful comments apart from unhelpful comments to these advice-seeking posts can help people and dialogue agents to become more helpful in offering advice. We propose a dataset that contains both helpful and unhelpful comments in response to such requests. We then relate helpfulness to the closely related construct of empathy. Finally, we analyze the language features that are associated with helpful and unhelpful comments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wnu-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wnu-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wnu-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.wnu-1.4/><span class=acl-fixed-case>GPT</span>-2-based Human-in-the-loop Theatre Play Script Generation</a></strong><br><a href=/people/r/rudolf-rosa/>Rudolf Rosa</a>
|
<a href=/people/p/patricia-schmidtova/>Patrícia Schmidtová</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/t/tomas-musil/>Tomáš Musil</a>
|
<a href=/people/d/david-marecek/>David Mareček</a>
|
<a href=/people/s/saad-obaid/>Saad Obaid</a>
|
<a href=/people/m/marie-novakova/>Marie Nováková</a>
|
<a href=/people/k/klara-vosecka/>Klára Vosecká</a>
|
<a href=/people/j/josef-dolezal/>Josef Doležal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wnu-1--4><div class="card-body p-3 small">We experiment with adapting generative language models for the generation of long coherent narratives in the form of theatre plays. Since fully automatic generation of whole plays is not currently feasible, we created an interactive tool that allows a human user to steer the generation somewhat while minimizing intervention. We pursue two approaches to long-text generation: a flat generation with summarization of context, and a hierarchical text-to-text two-stage approach, where a synopsis is generated first and then used to condition generation of the final script. Our preliminary results and discussions with theatre professionals show improvements over vanilla language model generation, but also identify important limitations of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wnu-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wnu-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wnu-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.wnu-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.wnu-1.5/><span class=acl-fixed-case>G</span>is<span class=acl-fixed-case>P</span>y: A Tool for Measuring Gist Inference Score in Text</a></strong><br><a href=/people/p/pedram-hosseini/>Pedram Hosseini</a>
|
<a href=/people/c/christopher-wolfe/>Christopher Wolfe</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/d/david-broniatowski/>David Broniatowski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wnu-1--5><div class="card-body p-3 small">Decision making theories such as Fuzzy-Trace Theory (FTT) suggest that individuals tend to rely on gist, or bottom-line meaning, in the text when making decisions. In this work, we delineate the process of developing GisPy, an opensource tool in Python for measuring the Gist Inference Score (GIS) in text. Evaluation of GisPy on documents in three benchmarks from the news and scientific text domains demonstrates that scores generated by our tool significantly distinguish low vs. high gist documents. Our tool is publicly available to use at: https: //github.com/phosseini/GisPy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wnu-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wnu-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wnu-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.wnu-1.6/>Heroes, Villains, and Victims, and <span class=acl-fixed-case>GPT</span>-3: Automated Extraction of Character Roles Without Training Data</a></strong><br><a href=/people/d/dominik-stammbach/>Dominik Stammbach</a>
|
<a href=/people/m/maria-antoniak/>Maria Antoniak</a>
|
<a href=/people/e/elliott-ash/>Elliott Ash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wnu-1--6><div class="card-body p-3 small">This paper shows how to use large-scale pretrained language models to extract character roles from narrative texts without domain-specific training data. Queried with a zero-shot question-answering prompt, GPT-3 can identify the hero, villain, and victim in diverse domains: newspaper articles, movie plot summaries, and political speeches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wnu-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wnu-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wnu-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.wnu-1.7/>Narrative Detection and Feature Analysis in Online Health Communities</a></strong><br><a href=/people/a/achyutarama-ganti/>Achyutarama Ganti</a>
|
<a href=/people/s/steven-wilson/>Steven Wilson</a>
|
<a href=/people/z/zexin-ma/>Zexin Ma</a>
|
<a href=/people/x/xinyan-zhao/>Xinyan Zhao</a>
|
<a href=/people/r/rong-ma/>Rong Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wnu-1--7><div class="card-body p-3 small">Narratives have been shown to be an effective way to communicate health risks and promote health behavior change, and given the growing amount of health information being shared on social media, it is crucial to study health-related narratives in social media. However, expert identification of a large number of narrative texts is a time consuming process, and larger scale studies on the use of narratives may be enabled through automatic text classification approaches. Prior work has demonstrated that automatic narrative detection is possible, but modern deep learning approaches have not been used for this task in the domain of online health communities. Therefore, in this paper, we explore the use of deep learning methods to automatically classify the presence of narratives in social media posts, finding that they outperform previously proposed approaches. We also find that in many cases, these models generalize well across posts from different health organizations. Finally, in order to better understand the increase in performance achieved by deep learning models, we use feature analysis techniques to explore the features that most contribute to narrative detection for posts in online health communities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wnu-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wnu-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wnu-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.wnu-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.wnu-1.8/>Looking from the Inside: How Children Render Character’s Perspectives in Freely Told Fantasy Stories</a></strong><br><a href=/people/m/max-van-duijn/>Max van Duijn</a>
|
<a href=/people/b/bram-van-dijk/>Bram van Dijk</a>
|
<a href=/people/m/marco-spruit/>Marco Spruit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wnu-1--8><div class="card-body p-3 small">Story characters not only perform actions, they typically also perceive, feel, think, and communicate. Here we are interested in how children render characters’ perspectives when freely telling a fantasy story. Drawing on a sample of 150 narratives elicited from Dutch children aged 4-12, we provide an inventory of 750 instances of character-perspective representation (CPR), distinguishing fourteen different types. Firstly, we observe that character perspectives are ubiquitous in freely told children’s stories and take more varied forms than traditional frameworks can accommodate. Secondly, we discuss variation in the use of different types of CPR across age groups, finding that character perspectives are being fleshed out in more advanced and diverse ways as children grow older. Thirdly, we explore whether such variation can be meaningfully linked to automatically extracted linguistic features, thereby probing the potential for using automated tools from NLP to extract and classify character perspectives in children’s stories.</div></div></div><hr><div id=2022woah-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.woah-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.woah-1/>Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.woah-1.0/>Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)</a></strong><br><a href=/people/k/kanika-narang/>Kanika Narang</a>
|
<a href=/people/a/aida-mostafazadeh-davani/>Aida Mostafazadeh Davani</a>
|
<a href=/people/l/lambert-mathias/>Lambert Mathias</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/z/zeerak-talat/>Zeerak Talat</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.1.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.woah-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.1/>Separating Hate Speech and Offensive Language Classes via Adversarial Debiasing</a></strong><br><a href=/people/s/shuzhou-yuan/>Shuzhou Yuan</a>
|
<a href=/people/a/antonis-maronikolakis/>Antonis Maronikolakis</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--1><div class="card-body p-3 small">Research to tackle hate speech plaguing online media has made strides in providing solutions, analyzing bias and curating data. A challenging problem is ambiguity between hate speech and offensive language, causing low performance both overall and specifically for the hate speech class. It can be argued that misclassifying actual hate speech content as merely offensive can lead to further harm against targeted groups. In our work, we mitigate this potentially harmful phenomenon by proposing an adversarial debiasing method to separate the two classes. We show that our method works for English, Arabic German and Hindi, plus in a multilingual setting, improving performance over baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.2.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.woah-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.2/>Towards Automatic Generation of Messages Countering Online Hate Speech and Microaggressions</a></strong><br><a href=/people/m/mana-ashida/>Mana Ashida</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--2><div class="card-body p-3 small">With the widespread use of social media, online hate is increasing, and microaggressions are receiving attention. We explore the potential for using pretrained language models to automatically generate messages that combat the associated offensive texts. Specifically, we focus on using prompting to steer model generation as it requires less data and computation than fine-tuning. We also propose a human evaluation perspective; offensiveness, stance, and informativeness. After obtaining 306 counterspeech and 42 microintervention messages generated by GPT-2, 3, Neo, we conducted a human evaluation using Amazon Mechanical Turk. The results indicate the potential of using prompting in the proposed generation task. All the generated texts along with the annotation are published to encourage future research on countering hate and microaggressions online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.woah-1.3/><span class=acl-fixed-case>G</span>rease<span class=acl-fixed-case>V</span>ision: Rewriting the Rules of the Interface</a></strong><br><a href=/people/s/siddhartha-datta/>Siddhartha Datta</a>
|
<a href=/people/k/konrad-kollnig/>Konrad Kollnig</a>
|
<a href=/people/n/nigel-shadbolt/>Nigel Shadbolt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--3><div class="card-body p-3 small">Digital harms can manifest across any interface. Key problems in addressing these harms include the high individuality of harms and the fast-changing nature of digital systems. We put forth GreaseVision, a collaborative human-in-the-loop learning framework that enables end-users to analyze their screenomes to annotate harms as well as render overlay interventions. We evaluate HITL intervention development with a set of completed tasks in a cognitive walkthrough, and test scalability with one-shot element removal and fine-tuning hate speech classification models. The contribution of the framework and tool allow individual end-users to study their usage history and create personalized interventions. Our contribution also enables researchers to study the distribution of multi-modal harms and interventions at scale.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.4.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.4/>Improving Generalization of Hate Speech Detection Systems to Novel Target Groups via Domain Adaptation</a></strong><br><a href=/people/f/florian-ludwig/>Florian Ludwig</a>
|
<a href=/people/k/klara-dolos/>Klara Dolos</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a>
|
<a href=/people/e/eleanor-hobley/>Eleanor Hobley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--4><div class="card-body p-3 small">Despite recent advances in machine learning based hate speech detection, classifiers still struggle with generalizing knowledge to out-of-domain data samples. In this paper, we investigate the generalization capabilities of deep learning models to different target groups of hate speech under clean experimental settings. Furthermore, we assess the efficacy of three different strategies of unsupervised domain adaptation to improve these capabilities. Given the diversity of hate and its rapid dynamics in the online world (e.g. the evolution of new target groups like virologists during the COVID-19 pandemic), robustly detecting hate aimed at newly identified target groups is a highly relevant research question. We show that naively trained models suffer from a target group specific bias, which can be reduced via domain adaptation. We were able to achieve a relative improvement of the F1-score between 5.8% and 10.7% for out-of-domain target groups of hate speech compared to baseline approaches by utilizing domain adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.5.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.woah-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.5/>“Zo Grof !”: A Comprehensive Corpus for Offensive and Abusive Language in <span class=acl-fixed-case>D</span>utch</a></strong><br><a href=/people/w/ward-ruitenbeek/>Ward Ruitenbeek</a>
|
<a href=/people/v/victor-zwart/>Victor Zwart</a>
|
<a href=/people/r/robin-van-der-noord/>Robin Van Der Noord</a>
|
<a href=/people/z/zhenja-gnezdilov/>Zhenja Gnezdilov</a>
|
<a href=/people/t/tommaso-caselli/>Tommaso Caselli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--5><div class="card-body p-3 small">This paper presents a comprehensive corpus for the study of socially unacceptable language in Dutch. The corpus extends and revise an existing resource with more data and introduces a new annotation dimension for offensive language, making it a unique resource in the Dutch language panorama. Each language phenomenon (abusive and offensive language) in the corpus has been annotated with a multi-layer annotation scheme modelling the explicitness and the target(s) of the message. We have conducted a new set of experiments with different classification algorithms on all annotation dimensions. Monolingual Pre-Trained Language Models prove as the best systems, obtaining a macro-average F1 of 0.828 for binary classification of offensive language, and 0.579 for the targets of offensive messages. Furthermore, the best system obtains a macro-average F1 of 0.667 for distinguishing between abusive and offensive messages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.6.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.6.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.6/>Counter-<span class=acl-fixed-case>TWIT</span>: An <span class=acl-fixed-case>I</span>talian Corpus for Online Counterspeech in Ecological Contexts</a></strong><br><a href=/people/p/pierpaolo-goffredo/>Pierpaolo Goffredo</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/b/bianca-cepollaro/>Bianca Cepollaro</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--6><div class="card-body p-3 small">This work describes the process of creating a corpus of Twitter conversations annotated for the presence of counterspeech in response to toxic speech related to axes of discrimination linked to sexism, racism and homophobia. The main novelty is an annotated dataset comprising relevant tweets in their context of occurrence. The corpus is made up of tweets and responses captured by different profiles replying to discriminatory content or objectionably couched news. An annotation scheme was created to make explicit the knowledge on the dimensions of toxic speech and counterspeech.An analysis of the collected and annotated data and of the IAA that emerged during the annotation process is included. Moreover, we report about preliminary experiments on automatic counterspeech detection, based on supervised automatic learning models trained on the new dataset. The results highlight the fundamental role played by the context in this detection task, confirming our intuitions about the importance to collect tweets in their context of occurrence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.7.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.7.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.woah-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.7/><span class=acl-fixed-case>S</span>tereo<span class=acl-fixed-case>KG</span>: Data-Driven Knowledge Graph Construction For Cultural Knowledge and Stereotypes</a></strong><br><a href=/people/a/awantee-deshpande/>Awantee Deshpande</a>
|
<a href=/people/d/dana-ruiter/>Dana Ruiter</a>
|
<a href=/people/m/marius-mosbach/>Marius Mosbach</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--7><div class="card-body p-3 small">Analyzing ethnic or religious bias is important for improving fairness, accountability, and transparency of natural language processing models. However, many techniques rely on human-compiled lists of bias terms, which are expensive to create and are limited in coverage. In this study, we present a fully data-driven pipeline for generating a knowledge graph (KG) of cultural knowledge and stereotypes. Our resulting KG covers 5 religious groups and 5 nationalities and can easily be extended to more entities. Our human evaluation shows that the majority (59.2%) of non-singleton entries are coherent and complete stereotypes. We further show that performing intermediate masked language model training on the verbalized KG leads to a higher level of cultural awareness in the model and has the potential to increase classification performance on knowledge-crucial samples on a related task, i.e., hate speech detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.8.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.8.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.8/>The subtle language of exclusion: Identifying the Toxic Speech of Trans-exclusionary Radical Feminists</a></strong><br><a href=/people/c/christina-lu/>Christina Lu</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--8><div class="card-body p-3 small">Toxic language can take many forms, from explicit hate speech to more subtle microaggressions. Within this space, models identifying transphobic language have largely focused on overt forms. However, a more pernicious and subtle source of transphobic comments comes in the form of statements made by Trans-exclusionary Radical Feminists (TERFs); these statements often appear seemingly-positive and promote women’s causes and issues, while simultaneously denying the inclusion of transgender women as women. Here, we introduce two models to mitigate this antisocial behavior. The first model identifies TERF users in social media, recognizing that these users are a main source of transphobic material that enters mainstream discussion and whom other users may not desire to engage with in good faith. The second model tackles the harder task of recognizing the masked rhetoric of TERF messages and introduces a new dataset to support this task. Finally, we discuss the ethics of deploying these models to mitigate the harm of this language, arguing for a balanced approach that allows for restorative interactions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.9.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.9.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.9/>Lost in Distillation: A Case Study in Toxicity Modeling</a></strong><br><a href=/people/a/alyssa-chvasta/>Alyssa Chvasta</a>
|
<a href=/people/a/alyssa-lees/>Alyssa Lees</a>
|
<a href=/people/j/jeffrey-sorensen/>Jeffrey Sorensen</a>
|
<a href=/people/l/lucy-vasserman/>Lucy Vasserman</a>
|
<a href=/people/n/nitesh-goyal/>Nitesh Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--9><div class="card-body p-3 small">In an era of increasingly large pre-trained language models, knowledge distillation is a powerful tool for transferring information from a large model to a smaller one. In particular, distillation is of tremendous benefit when it comes to real-world constraints such as serving latency or serving at scale. However, a loss of robustness in language understanding may be hidden in the process and not immediately revealed when looking at high-level evaluation metrics. In this work, we investigate the hidden costs: what is “lost in distillation”, especially in regards to identity-based model bias using the case study of toxicity modeling. With reproducible models using open source training sets, we investigate models distilled from a BERT teacher baseline. Using both open source and proprietary big data models, we investigate these hidden performance costs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.10.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.10.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.10/>Cleansing & expanding the <span class=acl-fixed-case>HURTLEX</span>(el) with a multidimensional categorization of offensive words</a></strong><br><a href=/people/v/vivian-stamou/>Vivian Stamou</a>
|
<a href=/people/i/iakovi-alexiou/>Iakovi Alexiou</a>
|
<a href=/people/a/antigone-klimi/>Antigone Klimi</a>
|
<a href=/people/e/eleftheria-molou/>Eleftheria Molou</a>
|
<a href=/people/a/alexandra-saivanidou/>Alexandra Saivanidou</a>
|
<a href=/people/s/stella-markantonatou/>Stella Markantonatou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--10><div class="card-body p-3 small">We present a cleansed version of the multilingual lexicon HURTLEX-(EL) comprising 737 offensive words of Modern Greek. We worked bottom-up in two annotation rounds and developed detailed guidelines by cross-classifying words on three dimensions: context, reference, and thematic domain. Our classification reveals a wider spectrum of thematic domains concerning the study of offensive language than previously thought Efthymiou et al. (2014) and reveals social and cultural aspects that are not included in the HURTLEX categories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.11.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.11.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.11/>Free speech or Free Hate Speech? Analyzing the Proliferation of Hate Speech in Parler</a></strong><br><a href=/people/a/abraham-israeli/>Abraham Israeli</a>
|
<a href=/people/o/oren-tsur/>Oren Tsur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--11><div class="card-body p-3 small">Social platforms such as Gab and Parler, branded as ‘free-speech’ networks, have seen a significant growth of their user base in recent years. This popularity is mainly attributed to the stricter moderation enforced by mainstream platforms such as Twitter, Facebook, and Reddit.In this work we provide the first large scale analysis of hate-speech on Parler. We experiment with an array of algorithms for hate-speech detection, demonstrating limitations of transfer learning in that domain, given the illusive and ever changing nature of the ways hate-speech is delivered. In order to improve classification accuracy we annotated 10K Parler posts, which we use to fine-tune a BERT classifier. Classification of individual posts is then leveraged for the classification of millions of users via label propagation over the social network. Classifying users by their propensity to disseminate hate, we find that hate mongers make 16.1% of Parler active users, and that they have distinct characteristics comparing to other user groups. We further complement our analysis by comparing the trends observed in Parler to those found in Gab. To the best of our knowledge, this is among the first works to analyze hate speech in Parler in a quantitative manner and on the user level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.12.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.12.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.12/>Resources for Multilingual Hate Speech Detection</a></strong><br><a href=/people/a/ayme-arango-monnar/>Ayme Arango Monnar</a>
|
<a href=/people/j/jorge-perez/>Jorge Perez</a>
|
<a href=/people/b/barbara-poblete/>Barbara Poblete</a>
|
<a href=/people/m/magdalena-saldana/>Magdalena Saldaña</a>
|
<a href=/people/v/valentina-proust/>Valentina Proust</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--12><div class="card-body p-3 small">Most of the published approaches and resources for hate speech detection are tailored for the English language. In consequence, cross-lingual and cross-cultural perspectives lack some essential resources. The lack of diversity of the datasets in Spanish is notable. Variations throughout Spanish-speaking countries make existing datasets not enough to encompass the task in the different Spanish variants. We annotated 9834 tweets from Chile to enrich the existing Spanish resources with different words and new targets of hate that have not been considered in previous studies. We conducted several cross-dataset evaluation experiments of the models published in the literature using our Chilean dataset and two others in English and Spanish. We propose a comparative framework for quickly conducting comparative experiments using different previously published models. In addition, we set up a Codalab competition for further comparison of new models in a standard scenario, that is, data partitions and evaluation metrics. All resources can be accessed trough a centralized repository for researchers to get a complete picture of the progress on the multilingual hate speech and offensive language detection task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.13.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.13.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.13/>Enriching Abusive Language Detection with Community Context</a></strong><br><a href=/people/h/haji-mohammad-saleem/>Haji Mohammad Saleem</a>
|
<a href=/people/j/jana-kurrek/>Jana Kurrek</a>
|
<a href=/people/d/derek-ruths/>Derek Ruths</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--13><div class="card-body p-3 small">Uses of pejorative expressions can be benign or actively empowering. When models for abuse detection misclassify these expressions as derogatory, they inadvertently censor productive conversations held by marginalized groups. One way to engage with non-dominant perspectives is to add context around conversations. Previous research has leveraged user- and thread-level features, but it often neglects the spaces within which productive conversations take place. Our paper highlights how community context can improve classification outcomes in abusive language detection. We make two main contributions to this end. First, we demonstrate that online communities cluster by the nature of their support towards victims of abuse. Second, we establish how community context improves accuracy and reduces the false positive rates of state-of-the-art abusive language classifiers. These findings suggest a promising direction for context-aware models in abusive language research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.14.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.14.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.woah-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.14/>DeTox: A Comprehensive Dataset for <span class=acl-fixed-case>G</span>erman Offensive Language and Conversation Analysis</a></strong><br><a href=/people/c/christoph-demus/>Christoph Demus</a>
|
<a href=/people/j/jonas-pitz/>Jonas Pitz</a>
|
<a href=/people/m/mina-schutz/>Mina Schütz</a>
|
<a href=/people/n/nadine-probol/>Nadine Probol</a>
|
<a href=/people/m/melanie-siegel/>Melanie Siegel</a>
|
<a href=/people/d/dirk-labudde/>Dirk Labudde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--14><div class="card-body p-3 small">In this work, we present a new publicly available offensive language dataset of 10.278 German social media comments collected in the first half of 2021 that were annotated by in total six annotators. With twelve different annotation categories, it is far more comprehensive than other datasets, and goes beyond just hate speech detection. The labels aim in particular also at toxicity, criminal relevance and discrimination types of comments. Furthermore, about half of the comments are from coherent parts of conversations, which opens the possibility to consider the comments’ contexts and do conversation analyses in order to research the contagion of offensive language in conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.15.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.15.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.woah-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.15/>Multilingual <span class=acl-fixed-case>H</span>ate<span class=acl-fixed-case>C</span>heck: Functional Tests for Multilingual Hate Speech Detection Models</a></strong><br><a href=/people/p/paul-rottger/>Paul Röttger</a>
|
<a href=/people/h/haitham-seelawi/>Haitham Seelawi</a>
|
<a href=/people/d/debora-nozza/>Debora Nozza</a>
|
<a href=/people/z/zeerak-talat/>Zeerak Talat</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--15><div class="card-body p-3 small">Hate speech detection models are typically evaluated on held-out test sets. However, this risks painting an incomplete and potentially misleading picture of model performance because of increasingly well-documented systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, recent research has thus introduced functional tests for hate speech detection models. However, these tests currently only exist for English-language content, which means that they cannot support the development of more effective models in other languages spoken by billions across the world. To help address this issue, we introduce Multilingual HateCheck (MHC), a suite of functional tests for multilingual hate speech detection models. MHC covers 34 functionalities across ten languages, which is more languages than any other hate speech dataset. To illustrate MHC’s utility, we train and test a high-performing multilingual hate speech detection model, and reveal critical model weaknesses for monolingual and cross-lingual applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.16.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.16.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.16/>Distributional properties of political dogwhistle representations in <span class=acl-fixed-case>S</span>wedish <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/n/niclas-hertzberg/>Niclas Hertzberg</a>
|
<a href=/people/r/robin-cooper/>Robin Cooper</a>
|
<a href=/people/e/elina-lindgren/>Elina Lindgren</a>
|
<a href=/people/b/bjorn-ronnerstrand/>Björn Rönnerstrand</a>
|
<a href=/people/g/gregor-rettenegger/>Gregor Rettenegger</a>
|
<a href=/people/e/ellen-breitholtz/>Ellen Breitholtz</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--16><div class="card-body p-3 small">“Dogwhistles” are expressions intended by the speaker have two messages: a socially-unacceptable “in-group” message understood by a subset of listeners, and a benign message intended for the out-group. We take the result of a word-replacement survey of the Swedish population intended to reveal how dogwhistles are understood, and we show that the difficulty of annotating dogwhistles is reflected in the separability in the space of a sentence-transformer Swedish BERT trained on general data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.17.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.17.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.17/>Hate Speech Criteria: A Modular Approach to Task-Specific Hate Speech Definitions</a></strong><br><a href=/people/u/urja-khurana/>Urja Khurana</a>
|
<a href=/people/i/ivar-vermeulen/>Ivar Vermeulen</a>
|
<a href=/people/e/eric-nalisnick/>Eric Nalisnick</a>
|
<a href=/people/m/marloes-van-noorloos/>Marloes Van Noorloos</a>
|
<a href=/people/a/antske-fokkens/>Antske Fokkens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--17><div class="card-body p-3 small">The subjectivity of automatic hate speech detection makes it a complex task, reflected in different and incomplete definitions in NLP. We present hate speech criteria, developed with insights from a law and social science expert, that help researchers create more explicit definitions and annotation guidelines on five aspects: (1) target groups and (2) dominance, (3) perpetrator characteristics, (4) explicit presence of negative interactions, and the (5) type of consequences/effects. Definitions can be structured so that they cover a more broad or more narrow phenomenon and conscious choices can be made on specifying criteria or leaving them open. We argue that the goal and exact task developers have in mind should determine how the scope of hate speech is defined. We provide an overview of the properties of datasets from hatespeechdata.com that may help select the most suitable dataset for a specific scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.18.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.18.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.18/>Accounting for Offensive Speech as a Practice of Resistance</a></strong><br><a href=/people/m/mark-diaz/>Mark Diaz</a>
|
<a href=/people/r/razvan-amironesei/>Razvan Amironesei</a>
|
<a href=/people/l/laura-weidinger/>Laura Weidinger</a>
|
<a href=/people/i/iason-gabriel/>Iason Gabriel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--18><div class="card-body p-3 small">Tasks such as toxicity detection, hate speech detection, and online harassment detection have been developed for identifying interactions involving offensive speech. In this work we articulate the need for a relational understanding of offensiveness to help distinguish denotative offensive speech from offensive speech serving as a mechanism through which marginalized communities resist oppressive social norms. Using examples from the queer community, we argue that evaluations of offensive speech must focus on the impacts of language use. We call this the cynic perspective– or a characteristic of language with roots in Cynic philosophy that pertains to employing offensive speech as a practice of resistance. We also explore the degree to which NLP systems may encounter limits to modeling relational context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.19.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.woah-1.19/>Towards a Multi-Entity Aspect-Based Sentiment Analysis for Characterizing Directed Social Regard in Online Messaging</a></strong><br><a href=/people/j/joan-zheng/>Joan Zheng</a>
|
<a href=/people/s/scott-friedman/>Scott Friedman</a>
|
<a href=/people/s/sonja-schmer-galunder/>Sonja Schmer-galunder</a>
|
<a href=/people/i/ian-magnusson/>Ian Magnusson</a>
|
<a href=/people/r/ruta-wheelock/>Ruta Wheelock</a>
|
<a href=/people/j/jeremy-gottlieb/>Jeremy Gottlieb</a>
|
<a href=/people/d/diana-gomez/>Diana Gomez</a>
|
<a href=/people/c/christopher-miller/>Christopher Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--19><div class="card-body p-3 small">Online messaging is dynamic, influential, and highly contextual, and a single post may contain contrasting sentiments towards multiple entities, such as dehumanizing one actor while empathizing with another in the same message. These complexities are important to capture for understanding the systematic abuse voiced within an online community, or for determining whether individuals are advocating for abuse, opposing abuse, or simply reporting abuse. In this work, we describe a formulation of directed social regard (DSR) as a problem of multi-entity aspect-based sentiment analysis (ME-ABSA), which models the degree of intensity of multiple sentiments that are associated with entities described by a text document. Our DSR schema is informed by Bandura’s psychosocial theory of moral disengagement and by recent work in ABSA. We present a dataset of over 2,900 posts and sentences, comprising over 24,000 entities annotated for DSR over nine psychosocial dimensions by three annotators. We present a novel transformer-based ME-ABSA model for DSR, achieving favorable preliminary results on this dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.20.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.woah-1.20/>Flexible text generation for counterfactual fairness probing</a></strong><br><a href=/people/z/zee-fryer/>Zee Fryer</a>
|
<a href=/people/v/vera-axelrod/>Vera Axelrod</a>
|
<a href=/people/b/ben-packer/>Ben Packer</a>
|
<a href=/people/a/alex-beutel/>Alex Beutel</a>
|
<a href=/people/j/jilin-chen/>Jilin Chen</a>
|
<a href=/people/k/kellie-webster/>Kellie Webster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--20><div class="card-body p-3 small">A common approach for testing fairness issues in text-based classifiers is through the use of counterfactuals: does the classifier output change if a sensitive attribute in the input is changed? Existing counterfactual generation methods typically rely on wordlists or templates, producing simple counterfactuals that fail to take into account grammar, context, or subtle sensitive attribute references, and could miss issues that the wordlist creators had not considered. In this paper, we introduce a task for generating counterfactuals that overcomes these shortcomings, and demonstrate how large language models (LLMs) can be leveraged to accomplish this task. We show that this LLM-based method can produce complex counterfactuals that existing methods cannot, comparing the performance of various counterfactual generation methods on the Civil Comments dataset and showing their value in evaluating a toxicity classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.21.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.21.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.21/>Users Hate Blondes: Detecting Sexism in User Comments on Online <span class=acl-fixed-case>R</span>omanian News</a></strong><br><a href=/people/a/andreea-moldovan/>Andreea Moldovan</a>
|
<a href=/people/k/karla-csuros/>Karla Csürös</a>
|
<a href=/people/a/ana-maria-bucur/>Ana-maria Bucur</a>
|
<a href=/people/l/loredana-bercuci/>Loredana Bercuci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--21><div class="card-body p-3 small">Romania ranks almost last in Europe when it comes to gender equality in political representation, with about 10% fewer women in politics than the E.U. average. We proceed from the assumption that this underrepresentation is also influenced by the sexism and verbal abuse female politicians face in the public sphere, especially in online media. We collect a novel dataset with sexist comments in Romanian language from newspaper articles about Romanian female politicians and propose baseline models using classical machine learning models and fine-tuned pretrained transformer models for the classification of sexist language in the online medium.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.22.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.woah-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.woah-1.22/>Targeted Identity Group Prediction in Hate Speech Corpora</a></strong><br><a href=/people/p/pratik-sachdeva/>Pratik Sachdeva</a>
|
<a href=/people/r/renata-barreto/>Renata Barreto</a>
|
<a href=/people/c/claudia-von-vacano/>Claudia Von Vacano</a>
|
<a href=/people/c/chris-kennedy/>Chris Kennedy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--22><div class="card-body p-3 small">The past decade has seen an abundance of work seeking to detect, characterize, and measure online hate speech. A related, but less studied problem, is the detection of identity groups targeted by that hate speech. Predictive accuracy on this task can supplement additional analyses beyond hate speech detection, motivating its study. Using the Measuring Hate Speech corpus, which provided annotations for targeted identity groups, we created neural network models to perform multi-label binary prediction of identity groups targeted by a comment. Specifically, we studied 8 broad identity groups and 12 identity sub-groups within race and gender identity. We found that these networks exhibited good predictive performance, achieving ROC AUCs of greater than 0.9 and PR AUCs of greater than 0.7 on several identity groups. We validated their performance on HateCheck and Gab Hate Corpora, finding that predictive performance generalized in most settings. We additionally examined the performance of the model on comments targeting multiple identity groups. Our results demonstrate the feasibility of simultaneously identifying targeted groups in social media comments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.23.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.23.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.woah-1.23" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.23/>Revisiting Queer Minorities in Lexicons</a></strong><br><a href=/people/k/krithika-ramesh/>Krithika Ramesh</a>
|
<a href=/people/s/sumeet-kumar/>Sumeet Kumar</a>
|
<a href=/people/a/ashiqur-khudabukhsh/>Ashiqur Khudabukhsh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--23><div class="card-body p-3 small">Lexicons play an important role in content moderation often being the first line of defense. However, little or no literature exists in analyzing the representation of queer-related words in them. In this paper, we consider twelve well-known lexicons containing inappropriate words and analyze how gender and sexual minorities are represented in these lexicons. Our analyses reveal that several of these lexicons barely make any distinction between pejorative and non-pejorative queer-related words. We express concern that such unfettered usage of non-pejorative queer-related words may impact queer presence in mainstream discourse. Our analyses further reveal that the lexicons have poor overlap in queer-related words. We finally present a quantifiable measure of consistency and show that several of these lexicons are not consistent in how they include (or omit) queer-related words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.woah-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.woah-1.24.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--woah-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.woah-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/2022.woah-1.24.mp4 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.woah-1.24" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2022.woah-1.24/><span class=acl-fixed-case>HATE</span>-<span class=acl-fixed-case>ITA</span>: Hate Speech Detection in <span class=acl-fixed-case>I</span>talian Social Media Text</a></strong><br><a href=/people/d/debora-nozza/>Debora Nozza</a>
|
<a href=/people/f/federico-bianchi/>Federico Bianchi</a>
|
<a href=/people/g/giuseppe-attanasio/>Giuseppe Attanasio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--woah-1--24><div class="card-body p-3 small">Online hate speech is a dangerous phenomenon that can (and should) be promptly counteracted properly. While Natural Language Processing supplies appropriate algorithms for trying to reach this objective, all research efforts are directed toward the English language. This strongly limits the classification power on non-English languages. In this paper, we test several learning frameworks for identifying hate speech in Italian text. We release HATE-ITA, a multi-language model trained on a large set of English data and available Italian datasets. HATE-ITA performs better than mono-lingual models and seems to adapt well also on language-specific slurs. We hope our findings will encourage the research in other mid-to-low resource communities and provide a valuable benchmarking tool for the Italian community.</div></div></div><hr><div id=2022wordplay-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wordplay-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1" href=/volumes/2022.wordplay-1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib&nbsp;(full)</a></span>
<a class=align-middle href=/volumes/2022.wordplay-1/>Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wordplay-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wordplay-1.0.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.wordplay-1.0/>Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022)</a></strong><br><a href=/people/m/marc-alexandre-cote/>Marc-Alexandre Côté</a>
|
<a href=/people/x/xingdi-yuan/>Xingdi Yuan</a>
|
<a href=/people/p/prithviraj-ammanabrolu/>Prithviraj Ammanabrolu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wordplay-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wordplay-1.1.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wordplay-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wordplay-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.wordplay-1.1/>A Systematic Survey of Text Worlds as Embodied Natural Language Environments</a></strong><br><a href=/people/p/peter-jansen/>Peter Jansen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wordplay-1--1><div class="card-body p-3 small">Text Worlds are virtual environments for embodied agents that, unlike 2D or 3D environments, are rendered exclusively using textual descriptions. These environments offer an alternative to higher-fidelity 3D environments due to their low barrier to entry, providing the ability to study semantics, compositional inference, and other high-level tasks with rich action spaces while controlling for perceptual input. This systematic survey outlines recent developments in tooling, environments, and agent modeling for Text Worlds, while examining recent trends in knowledge graphs, common sense reasoning, transfer learning of Text World performance to higher-fidelity environments, as well as near-term development targets that, once achieved, make Text Worlds an attractive general research paradigm for natural language processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wordplay-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wordplay-1.2.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wordplay-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wordplay-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.wordplay-1.2/>A Minimal Computational Improviser Based on Oral Thought</a></strong><br><a href=/people/n/nick-montfort/>Nick Montfort</a>
|
<a href=/people/s/sebastian-bartlett-fernandez/>Sebastian Bartlett Fernandez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wordplay-1--2><div class="card-body p-3 small">A prototype system for playing a minimal improvisational game with one or more human or computer players is discussed. The game, Chain Reaction, has players collectively build a chain of word pairs or solid compounds. With a basis in oral culture, it emphasizes memory and rapid improvisation. Chains are only locally coherent, so absurdity and humor increases during play. While it is trivial to develop a computer player using textual corpora and literature-culture concepts, our approach is unique in that we have grounded our work in the principles of oral culture according to Walter Ong, an early scholar of orature. We show how a simple computer model can be designed to embody many aspects of oral poetics as theorized by Ong, suggesting design directions for other work in oral improvisation and poetics. The opportunities for own our system’s further development include creating culturally specific automated players and situating play in different temporal, physical, and social contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wordplay-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wordplay-1.3.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wordplay-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wordplay-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2022.wordplay-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2022.wordplay-1.3/>Craft an Iron Sword: Dynamically Generating Interactive Game Characters by Prompting Large Language Models Tuned on Code</a></strong><br><a href=/people/r/ryan-volum/>Ryan Volum</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/m/michael-xu/>Michael Xu</a>
|
<a href=/people/g/gabriel-desgarennes/>Gabriel DesGarennes</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/o/olivia-deng/>Olivia Deng</a>
|
<a href=/people/a/akanksha-malhotra/>Akanksha Malhotra</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wordplay-1--3><div class="card-body p-3 small">Non-Player Characters (NPCs) significantly enhance the player experience in many games. Historically, players’ interactions with NPCs have tended to be highly scripted, to be limited to natural language responses to be selected by the player, and to not involve dynamic change in game state. In this work, we demonstrate that use of a few example conversational prompts can power a conversational agent to generate both natural language and novel code. This approach can permit development of NPCs with which players can have grounded conversations that are free-form and less repetitive. We demonstrate our approach using OpenAI Codex (GPT-3 finetuned on GitHub), with Minecraft game development as our test bed. We show that with a few example prompts, a Codex-based agent can generate novel code, hold multi-turn conversations and answer questions about structured data. We evaluate this application using experienced gamers in a Minecraft realm and provide analysis of failure cases and suggest possible directions for solutions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wordplay-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wordplay-1.4.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wordplay-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wordplay-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.wordplay-1.4/>A Sequence Modelling Approach to Question Answering in Text-Based Games</a></strong><br><a href=/people/g/gregory-furman/>Gregory Furman</a>
|
<a href=/people/e/edan-toledo/>Edan Toledo</a>
|
<a href=/people/j/jonathan-shock/>Jonathan Shock</a>
|
<a href=/people/j/jan-buys/>Jan Buys</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wordplay-1--4><div class="card-body p-3 small">Interactive Question Answering (IQA) requires an intelligent agent to interact with a dynamic environment in order to gather information necessary to answer a question. IQA tasks have been proposed as means of training systems to develop language or visual comprehension abilities. To this end, the Question Answering with Interactive Text (QAit) task was created to produce and benchmark interactive agents capable of seeking information and answering questions in unseen environments. While prior work has exclusively focused on IQA as a reinforcement learning problem, such methods suffer from low sample efficiency and poor accuracy in zero-shot evaluation. In this paper, we propose the use of the recently proposed Decision Transformer architecture to provide improvements upon prior baselines. By utilising a causally masked GPT-2 Transformer for command generation and a BERT model for question answer prediction, we show that the Decision Transformer achieves performance greater than or equal to current state-of-the-art RL baselines on the QAit task in a sample efficient manner. In addition, these results are achievable by training on sub-optimal random trajectories, therefore not requiring the use of online agents to gather data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2022.wordplay-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-secondary align-middle mr-1" href=/2022.wordplay-1.5.bib data-toggle=tooltip data-placement=top title="Export to BibTeX">bib
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2022--wordplay-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2022.wordplay-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2022.wordplay-1.5/>Automatic Exploration of Textual Environments with Language-Conditioned Autotelic Agents</a></strong><br><a href=/people/l/laetitia-teodorescu/>Laetitia Teodorescu</a>
|
<a href=/people/x/xingdi-yuan/>Xingdi Yuan</a>
|
<a href=/people/m/marc-alexandre-cote/>Marc-Alexandre Côté</a>
|
<a href=/people/p/pierre-yves-oudeyer/>Pierre-Yves Oudeyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2022--wordplay-1--5><div class="card-body p-3 small">The purpose of this extended abstract is to discuss the possible fruitful interactions between intrinsically-motivated language-conditioned agents and textual environments. We define autotelic agents as agents able to set their own goals. We identify desirable properties of textual nenvironments that makes them a good testbed for autotelic agents. We them list drivers of exploration for such agents that would allow them to achieve large repertoires of skills in these environments, enabling such agents to be repurposed for solving the benchmarks implemented in textual environments. We then discuss challenges and further perspectives brought about by this interaction.</div></div></div><hr></section></div><footer class="bg-gradient-light py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5"><div class=container><p class="text-muted small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2023 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="text-muted small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="text-muted small px-1"><i>Site last built on 12 October 2023 at 01:04 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/0ce844dda66e8495ed04357110fd57a0ce678727>commit 0ce844d</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>